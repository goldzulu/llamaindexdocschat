{"docstore/data": {"b792e8d0-274f-475a-9516-aafc9754602a": {"__data__": {"id_": "b792e8d0-274f-475a-9516-aafc9754602a", "embedding": null, "metadata": {"filename": "DOCS_README.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0833904f0cb3e584c30ea17552ca65d29c8ec2b", "node_type": "4", "metadata": {"filename": "DOCS_README.md", "author": "LlamaIndex"}, "hash": "9bfd6fcfb8e0a7bd043acc05c3d19cf851dfe01ef7e3b53c963af8abb4375cb1", "class_name": "RelatedNodeInfo"}}, "text": "# Documentation Guide\n\n## A guide for docs contributors\n\nThe `docs` directory contains the sphinx source text for LlamaIndex docs, visit\nhttps://docs.llamaindex.ai/en/stable/ to read the full documentation.\n\nThis guide is made for anyone who's interested in running LlamaIndex documentation locally,\nmaking changes to it and making contributions. LlamaIndex is made by the thriving community\nbehind it, and you're always welcome to make contributions to the project and the\ndocumentation.\n\n## Build Docs\n\nIf you haven't already, clone the LlamaIndex Github repo to a local directory:\n\n```bash\ngit clone https://github.com/run-llama/llama_index.git && cd llama_index\n```\n\nInstall all dependencies required for building docs (mainly `mkdocs` and its extension):\n\n- [Install poetry](https://python-poetry.org/docs/#installation) - this will help you manage package dependencies\n- `poetry shell` - this command creates a virtual environment, which keeps installed packages contained to this project\n- `poetry install --only docs` - this will install all dependencies needed for building docs\n\nBuild with mkdocs:\n\n```bash\ncd docs\nmkdocs serve --dirty\n```\n\n**NOTE:** The `--dirty` option will mean that only changed files will be re-built, decreasing the time it takes to iterate on a page.\n\nAnd open your browser at http://localhost:8000/ to view the generated docs.\n\nThis hosted version will re-build and update as changes are made to the docs.\n\n## Config\n\nAll config for mkdocs is in the `mkdocs.yml` file.\n\nRunning the command `python docs/prepare_for_build.py` from the root of the llama-index repo will update the mkdocs.yml API Reference and examples nav with the latest changes, as well as writing new api reference files.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ae4fbd2-958a-4f14-ab43-bab452bd5d99": {"__data__": {"id_": "6ae4fbd2-958a-4f14-ab43-bab452bd5d99", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7551b70-321a-4b67-9093-8f2cd66bcc46", "node_type": "1", "metadata": {}, "hash": "90470124c1046172cc489c28d7743e303f5ef16fbc80a0b18e14770e6c2dfee0", "class_name": "RelatedNodeInfo"}}, "text": "# ChangeLog\n\n## [2024-07-19]\n\n### `llama-index-core` [0.10.56]\n\n- Fixing the issue where the \\_apply_node_postprocessors function needs QueryBundle (#14839)\n- Add Context-Only Response Synthesizer (#14439)\n- Fix AgentRunner AgentRunStepStartEvent dispatch (#14828)\n- Improve output format system prompt in ReAct agent (#14814)\n- Remove double curly replacing from output parser utils (#14735)\n- Update simple_summarize.py (#14714)\n\n### `llama-index-tools-azure-code-interpreter` [0.2.0]\n\n- chore: read AZURE_POOL_MANAGEMENT_ENDPOINT from env vars (#14732)\n\n### `llama-index-llms-azure-inference` [0.1.0]\n\n- Azure AI Inference integration (#14672)\n\n### `llama-index-embeddings-azure-inference` [0.1.0]\n\n- Azure AI Inference integration (#14672)\n\n### `llama-index-llms-bedrock-converse` [0.1.5]\n\n- feat: \u2728 Implement async functionality in BedrockConverse (#14326)\n\n### `llama-index-embeddings-yandexgpt` [0.1.5]\n\n- Add new integration for YandexGPT Embedding Model (#14313)\n\n### `llama-index-tools-google` [0.1.6]\n\n- Update docstring for gmailtoolspec's search_messages tool (#14840)\n\n### `llama-index-postprocessor-nvidia-rerank` [0.1.5]\n\n- add support for nvidia/nv-rerankqa-mistral-4b-v3 (#14844)\n\n### `llama-index-embeddings-openai` [0.1.11]\n\n- Fix OpenAI Embedding async client bug (#14835)\n\n### `llama-index-embeddings-azure-openai` [0.1.11]\n\n- Fix Azure OpenAI LLM and Embedding async client bug (#14833)\n\n### `llama-index-llms-azure-openai` [0.1.9]\n\n- Fix Azure OpenAI LLM and Embedding async client bug (#14833)\n\n### `llama-index-multi-modal-llms-openai` [0.1.8]\n\n- Add support for gpt-4o-mini (#14820)\n\n### `llama-index-llms-openai` [0.1.26]\n\n- Add support for gpt-4o-mini (#14820)\n\n### `llama-index-llms-mistralai` [0.1.18]\n\n- Add support for mistralai nemo model (#14819)\n\n### `llama-index-graph-stores-neo4j` [0.2.8]\n\n- Fix bug when sanitize is used in neo4j property graph (#14812)\n- Add filter to get_triples in neo4j (#14811)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.12]\n\n- feat: add nested filters for azureaisearch (#14795)\n\n### `llama-index-vector-stores-qdrant` [0.2.13]\n\n- feat: Add NOT IN filter for Qdrant vector store (#14791)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.11]\n\n- feat: add azureaisearch supported conditions (#14787)\n- feat: azureaisearch support collection string (#14712)\n\n### `llama-index-tools-weather` [0.1.4]\n\n- Fix OpenWeatherMapToolSpec.forecast_tommorrow_at_location (#14745)\n\n### `llama-index-readers-microsoft-sharepoint` [0.2.6]\n\n- follow odata.nextLink (#14708)\n\n### `llama-index-vector-stores-qdrant` [0.2.12]\n\n- Adds Quantization option to QdrantVectorStore (#14740)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.10]\n\n- feat: improve azureai search deleting (#14693)\n\n### `llama-index-agent-openai` [0.2.9]\n\n- fix: tools are required for attachments in openai api (#14609)\n\n### `llama-index-readers-box` [0.1.0]\n\n- new integration\n\n### `llama-index-embeddings-fastembed` [0.1.6]\n\n- fix fastembed python version (#14710)\n\n## [2024-07-11]\n\n### `llama-index-core` [0.10.55]\n\n- Various docs updates\n\n### `llama-index-llms-cleanlab` [0.1.1]\n\n- Add user configurations for Cleanlab LLM integration (#14676)\n\n### `llama-index-readers-file` [0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7551b70-321a-4b67-9093-8f2cd66bcc46": {"__data__": {"id_": "c7551b70-321a-4b67-9093-8f2cd66bcc46", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ae4fbd2-958a-4f14-ab43-bab452bd5d99", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b88e454610fe546ac104671664a913a2e174e1ca630e4c3a4a1255b00d2a7f93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60928bce-84fc-4405-851f-03537938e3cf", "node_type": "1", "metadata": {}, "hash": "b4b361630931734d5d602ba3925ab0e115ad40ecfb0c761629947af6b9d2e092", "class_name": "RelatedNodeInfo"}}, "text": "1.1]\n\n- Add user configurations for Cleanlab LLM integration (#14676)\n\n### `llama-index-readers-file` [0.1.30]\n\n- race between concurrent pptx readers over a single temp filename (#14686)\n\n### `llama-index-tools-exa` [0.1.4]\n\n- changes to Exa search tool getting started and example notebook (#14690)\n\n## [2024-07-10]\n\n### `llama-index-core` [0.10.54]\n\n- fix: update operator logic for simple vector store filter (#14674)\n- Add AgentOps integration (#13935)\n\n### `llama-index-embeddings-fastembed` [0.1.5]\n\n- chore: update required python version in Qdrant fastembed package (#14677)\n\n### `llama-index-embeddings-huggingface-optimum-intel` [0.1.6]\n\n- Bump version llama-index-embeddings-huggingface-optimum-intel (#14670)\n\n### `llama-index-vector-stores-elasticsearch` [0.2.2]\n\n- Added support for custom index settings (#14655)\n\n### `llama-index-callbacks-agentops` [0.1.0]\n\n- Initial release\n\n### `llama-index-indices-managed-vertexai` [0.0.2]\n\n- Fix #14637 Llamaindex managed Vertex AI index needs to be updated. (#14641)\n\n### `llama-index-readers-file` [0.1.29]\n\n- fix unstructured import in simple file reader (#14642)\n\n## [2024-07-08]\n\n### `llama-index-core` [0.10.53]\n\n- fix handling react usage in `llm.predict_and_call` for llama-agents (#14556)\n- add the missing arg verbose when `ReActAgent` calling `super().__init__` (#14565)\n- fix `llama-index-core\\llama_index\\core\\node_parser\\text\\utils.py` error when use IngestionPipeline parallel (#14560)\n- deprecate `KnowledgeGraphIndex`, tweak docs (#14575)\n- Fix `ChatSummaryMemoryBuffer` fails to summary chat history with tool callings (#14563)\n- Added `DynamicLLMPathExtractor` for Entity Detection With a Schema inferred by LLMs on the fly (#14566)\n- add cloud document converter (#14608)\n- fix KnowledgeGraphIndex arg 'kg_triple_extract_template' typo error (#14619)\n- Fix: Update `UnstructuredElementNodeParser` due to change in unstructured (#14606)\n- Update ReAct Step to solve issue with incomplete generation (#14587)\n\n### `llama-index-callbacks-promptlayer` [0.1.3]\n\n- Conditions logging to promptlayer on successful request (#14632)\n\n### `llama-index-embeddings-databricks` [0.1.0]\n\n- Add integration embeddings databricks (#14590)\n\n### `llama-index-llms-ai21` [0.3.1]\n\n- Fix MessageRole import from the wrong package in AI21 Package (#14596)\n\n### `llama-index-llms-bedrock` [0.1.12]\n\n- handle empty response in Bedrock AnthropicProvider (#14479)\n- add claude 3.5 sonnet support to Bedrock InvokeAPI (#14594)\n\n### `llama-index-llms-bedrock-converse` [0.1.4]\n\n- Fix Bedrock Converse's tool use blocks, when there are multiple consecutive function calls (#14386)\n\n### `llama-index-llms-optimum-intel` [0.1.0]\n\n- add optimum intel with ipex backend to llama-index-integration (#14553)\n\n### `llama-index-llms-qianfan` [0.1.0]\n\n- add baidu-qianfan llm (#14414)\n\n### `llama-index-llms-text-generation-inference` [0.1.4]\n\n- fix: crash LLMMetadata in model name lookup (#14569)\n- Remove hf embeddings dep from text-embeddings-inference (#14592)\n\n### `llama-index-llms-yi` [0.1.1]\n\n- update yi llm context_window (#14578)\n\n### `llama-index-readers-file` [0.1.28]\n\n- add fs arg to PandasExcelReader.load_data (#14554)\n- UnstructuredReader enhancements (#14390)\n\n### `llama-index-readers-web` [0.1.22]\n\n- nit: firecrawl fixes for creating documents (#14579)\n\n### `llama-index-retrievers-bm25` [0.2.", "mimetype": "text/plain", "start_char_idx": 3108, "end_char_idx": 6463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60928bce-84fc-4405-851f-03537938e3cf": {"__data__": {"id_": "60928bce-84fc-4405-851f-03537938e3cf", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7551b70-321a-4b67-9093-8f2cd66bcc46", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "d02d19a42607f9408da6f3b8c72fe6c4b91e2aa72c46ff4a8f80106ebbbfae42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecfaa01e-1fbb-44f8-8144-c98dc436c2fe", "node_type": "1", "metadata": {}, "hash": "c8c0be94f6dc5620ddcd68b53cc622d88cb8742ade1917e64fe6384f74c447c7", "class_name": "RelatedNodeInfo"}}, "text": "22]\n\n- nit: firecrawl fixes for creating documents (#14579)\n\n### `llama-index-retrievers-bm25` [0.2.0]\n\n- Update BM25Retriever to use newer (and faster) bm25s library #(14581)\n\n### `llama-index-vector-stores-qdrant` [0.2.11]\n\n- refactor: Don't swallow exceptions from Qdrant collection_exists (#14564)\n- add support for qdrant bm42, setting sparse + dense configs (#14577)\n\n## [2024-07-03]\n\n### `llama-index-core` [0.10.52]\n\n- fix file reader path bug on windows (#14537)\n- follow up with kwargs propagation in colbert index due to change in parent class (#14522)\n- deprecate query pipeline agent in favor of FnAgentWorker (#14525O)\n\n### `llama-index-callbacks-arize-phoenix` [0.1.6]\n\n- support latest version of arize #14526\n\n### `llama-index-embeddings-litellm` [0.1.0]\n\n- Add support for LiteLLM Proxy Server for embeddings (#14523)\n\n### `llama-index-finetuning` [0.1.10]\n\n- Adding device choice from sentence_transformers (#14546)\n\n### `llama-index-graph-stores-neo4` [0.2.7]\n\n- Fixed ordering of returned nodes on vector queries (#14461)\n\n### `llama-index-llms-bedrock` [0.1.10]\n\n- handle empty response in Bedrock AnthropicProvider (#14479)\n\n### `llama-index-llms-bedrock-converse` [0.1.4]\n\n- Fix Bedrock Converse's join_two_dicts function when a new string kwarg is added (#14548)\n\n### `llama-index-llms-upstage` [0.1.4]\n\n- Add upstage tokenizer and token counting method (#14502)\n\n### `llama-index-readers-azstorage-blob` [0.1.7]\n\n- Fix bug with getting object name for blobs (#14547)\n\n### `llama-index-readers-file` [0.1.26]\n\n- Pandas excel reader load data fix for appending documents (#14501)\n\n### `llama-index-readers-iceberg` [0.1.0]\n\n- Add Iceberg Reader integration to LLamaIndex (#14477)\n\n### `llama-index-readers-notion` [0.1.8]\n\n- Added retries (#14488)\n- add `list_databases` method (#14488)\n\n### `llama-index-readers-slack` [0.1.5]\n\n- Enhance SlackReader to fetch Channel IDs from Channel Names/Patterns (#14429)\n\n### `llama-index-readers-web` [0.1.21]\n\n- Add API url to firecrawl reader (#14452)\n\n### `llama-index-retrievers-bm25` [0.1.5]\n\n- fix score in nodes returned by the BM25 retriever (#14495)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.9]\n\n- add async methods to azure ai search (#14496)\n\n### `llama-index-vector-stores-kdbai` [0.1.8]\n\n- Kdbai rest compatible (#14511)\n\n### `llama-index-vector-stores-mongodb` [0.1.6]\n\n- Adds Hybrid and Full-Text Search to MongoDBAtlasVectorSearch (#14490)\n\n## [2024-06-28]\n\n### `llama-index-core` [0.10.51]\n\n- fixed issue with function calling llms and empty tool calls (#14453)\n- Fix ChatMessage not considered as stringable in query pipeline (#14378)\n- Update schema llm path extractor to also take a list of valid triples (#14357)\n- Pass the kwargs on when `build_index_from_nodes` (#14341)\n\n### `llama-index-agent-dashscope` [0.1.0]\n\n- Add Alibaba Cloud dashscope agent (#14318)\n\n### `llama-index-graph-stores-neo4j` [0.2.6]\n\n- Add MetadataFilters to neo4j_property_graph (#14362)\n\n### `llama-index-llms-nvidia` [0.1.4]\n\n- add known context lengths for hosted models (#14436)\n\n### `llama-index-llms-perplexity` [0.1.4]\n\n- update available models (#14409)\n\n### `llama-index-llms-predibase` [0.1.6]\n\n- Better error handling for invalid API token (#14440)\n\n### `llama-index-llms-yi` [0.1.", "mimetype": "text/plain", "start_char_idx": 6363, "end_char_idx": 9625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecfaa01e-1fbb-44f8-8144-c98dc436c2fe": {"__data__": {"id_": "ecfaa01e-1fbb-44f8-8144-c98dc436c2fe", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60928bce-84fc-4405-851f-03537938e3cf", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "154ab53342770c23bd6629cd3a98594f24e920157c3c55c216aedec95fbf80e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44739aaf-b113-4ceb-bd8b-5cbe5ca3c979", "node_type": "1", "metadata": {}, "hash": "65f271529d086c00d90fde29d341e2ebf403525368a0ca4eefd3aa869c4ab259", "class_name": "RelatedNodeInfo"}}, "text": "1.6]\n\n- Better error handling for invalid API token (#14440)\n\n### `llama-index-llms-yi` [0.1.0]\n\n- Integrate Yi model (#14353)\n\n### `llama-index-readers-google` [0.2.9]\n\n- Creates Data Loader for Google Chat (#14397)\n\n### `llama-index-readers-s3` [0.1.10]\n\n- Invalidate s3fs cache in S3Reader (#14441)\n\n### `llama-index-readers-structured-data` [0.1.0]\n\n- Add StructuredDataReader support for xlsx, csv, json and jsonl (#14369)\n\n### `llama-index-tools-jina` [0.1.0]\n\n- Integrating a new tool called jina search (#14317)\n\n### `llama-index-vector-stores-astradb` [0.1.8]\n\n- Update Astra DB vector store to use modern astrapy library (#14407)\n\n### `llama-index-vector-stores-chromadb` [0.1.10]\n\n- Fix the index accessing of ids of chroma get (#14434)\n\n### `llama-index-vector-stores-deeplake` [0.1.4]\n\n- Implemented delete_nodes() and clear() in deeplake vector store (#14457)\n- Implemented get_nodes() in deeplake vector store (#14388)\n\n### `llama-index-vector-stores-elasticsearch` [0.2.1]\n\n- Add support for dynamic metadata fields in Elasticsearch index creation (#14431)\n\n### `llama-index-vector-stores-kdbai` [0.1.7]\n\n- Kdbai version compatible (#14402)\n\n## [2024-06-24]\n\n### `llama-index-core` [0.10.50]\n\n- added dead simple `FnAgentWorker` for custom agents (#14329)\n- Pass the kwargs on when build_index_from_nodes (#14341)\n- make async utils a bit more robust to nested async (#14356)\n\n### `llama-index-llms-upstage` [0.1.3]\n\n- every llm is a chat model (#14334)\n\n### `llama-index-packs-rag-evaluator` [0.1.5]\n\n- added possibility to run local embedding model in RAG evaluation packages (#14352)\n\n## [2024-06-23]\n\n### `llama-index-core` [0.10.49]\n\n- Improvements to `llama-cloud` and client dependencies (#14254)\n\n### `llama-index-indices-managed-llama-cloud` [0.2.1]\n\n- Improve the interface and client interactions in `LlamaCloudIndex` (#14254)\n\n### `llama-index-llms-bedrock-converse` [0.1.3]\n\n- add claude sonnet 3.5 to bedrock converse (#14306)\n\n### `llama-index-llms-upstage` [0.1.2]\n\n- set default context size (#14293)\n- add api_key alias on upstage llm and embeddings (#14233)\n\n### `llama-index-storage-kvstore-azure` [0.1.2]\n\n- Optimized inserts (#14321)\n\n### `llama-index-utils-azure` [0.1.1]\n\n- azure_table_storage params bug (#14182)\n\n### `llama-index-vector-stores-neo4jvector` [0.1.6]\n\n- Add neo4j client method (#14314)\n\n## [2024-06-21]\n\n### `llama-index-core` [0.10.48]\n\n- Improve efficiency of average precision (#14260)\n- add crewai + llamaindex cookbook (#14266)\n- Add mimetype field to TextNode (#14279)\n- Improve IBM watsonx.ai docs (#14271)\n- Updated frontpage of docs, added agents guide, and more (#14089)\n\n### `llama-index-llms-anthropic` [0.1.14]\n\n- Add support for claude 3.5 (#14277)\n\n### `llama-index-llms-bedrock-converse` [0.1.4]\n\n- Implement Bedrock Converse API for function calling (#14055)\n\n## [2024-06-19]\n\n### `llama-index-core` [0.10.47]\n\n- added average precision as a retrieval metric (#14189)\n- added `.show_jupyter_graph()` method visualizing default simple graph_store in jupyter notebooks (#14104)\n- corrected the behaviour of nltk file lookup (#14040)\n- Added helper args to generate_qa_pairs (#14054)\n- Add new chunking semantic chunking method: double-pass merging (#13629)\n- enable stepwise execution of query pipelines (#14117)\n- Replace tenacity upper limit by only rejecting 8.4.", "mimetype": "text/plain", "start_char_idx": 9532, "end_char_idx": 12870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44739aaf-b113-4ceb-bd8b-5cbe5ca3c979": {"__data__": {"id_": "44739aaf-b113-4ceb-bd8b-5cbe5ca3c979", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecfaa01e-1fbb-44f8-8144-c98dc436c2fe", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "caaa6e95837aeec882593094734f25fdb7eec04fa15965f7bfd763329574b49b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a9e57c-1b6b-452e-99ec-fe1c9420e635", "node_type": "1", "metadata": {}, "hash": "08dfd9f4214d02cbbbe7e7ce02a121b62d572208205c7f4270524436ed73d934", "class_name": "RelatedNodeInfo"}}, "text": "4.0 (#14218)\n- propagate error_on_no_tool_call kwarg in `llm.predict_and_call()` (#14253)\n- in query pipeline, avoid casting nodes as strings and use `get_content()` instead (#14242)\n- Fix NLSQLTableQueryEngine response metadata (#14169)\n- do not overwrite relations in default simple property graph (#14244)\n\n### `llama-index-embeddings-ipex-llm` [0.1.5]\n\n- Enable selecting Intel GPU for ipex embedding integrations (#14214)\n\n### `llama-index-embeddings-mixedbreadai` [0.1.0]\n\n- add mixedbread ai integration (#14161)\n\n### `llama-index-graph-stores-neo4j` [0.2.5]\n\n- Add default node property to neo4j upsert relations (#14095)\n\n### `llama-index-indices-managed-postgresml` [0.3.0]\n\n- Added re-ranking into the PostgresML Managed Index (#14134)\n\n### `llama-index-llms-ai21` [0.3.0]\n\n- use async AI21 client for async methods (#14193)\n\n### `llama-index-llms-bedrock-converse` [0.1.2]\n\n- Added (fake) async calls to avoid errors (#14241)\n\n### `llama-index-llms-deepinfra` [0.1.3]\n\n- Add function calling to deep infra llm (#14127)\n\n### `llama-index-llms-ipex-llm` [0.1.8]\n\n- Enable selecting Intel GPU for ipex embedding integrations (#14214)\n\n### `llama-index-llms-oci-genai` [0.1.1]\n\n- add command r support oci genai (#14080)\n\n### `llama-index-llms-premai` [0.1.7]\n\n- Prem AI Templates Llama Index support (#14105)\n\n### `llama-index-llms-you` [0.1.0]\n\n- Integrate You.com conversational APIs (#14207)\n\n### `llama-index-readers-mongodb` [0.1.8]\n\n- Add metadata field \"collection_name\" to SimpleMongoReader (#14245)\n\n### `llama-index-readers-pdf-marker` [0.1.0]\n\n- add marker-pdf reader (#14099)\n\n### `llama-index-readers-upstage` [0.1.0]\n\n- Added upstage as a reader (#13415)\n\n### `llama-index-postprocessor-mixedbreadai-rerank` [0.1.0]\n\n- add mixedbread ai integration (#14161)\n\n### `llama-index-vector-stores-lancedb` [0.1.6]\n\n- LanceDB: code cleanup, minor updates (#14077)\n\n### `llama-index-vector-stores-opensearch` [0.1.12]\n\n- add option to customize default OpenSearch Client and Engine (#14249)\n\n## [2024-06-17]\n\n### `llama-index-core`[0.10.46]\n\n- Fix Pin tenacity and numpy in core (#14203)\n- Add precision and recall metrics (#14170)\n- Enable Function calling and agent runner for Vertex AI (#14088)\n- Fix for batch_gather (#14162)\n\n### `llama-index-utils-huggingface` [0.1.1]\n\n- Remove sentence-transformers dependency from HuggingFace utils package (#14204)\n\n### `llama-index-finetuning` [0.1.8]\n\n- Add MistralAI Finetuning API support (#14101)\n\n### `llama-index-llms-mistralai` [0.1.16]\n\n- Update MistralAI (#14199)\n\n### `llama-index-llms-bedrock-converse` [0.1.0]\n\n- fix: \ud83d\udc1b Fix Bedrock Converse' pyproject.toml for the PyPI release (#14197)\n\n### `llama-index-utils-azure` [0.1.1]\n\n- Use typical include llama_index/ (#14196)\n- Feature/azure_table_storage (#14182)\n\n### `llama-index-embeddings-nvidia` [0.1.4]\n\n- add support for nvidia/nv-embed-v1 (https://huggingface.co/nvidia/NV-Embed-v1) (#14194)\n\n### `llama-index-retrievers-you` [0.1.3]\n\n- add news retriever (#13934)\n\n### `llama-index-storage-kvstore-azure` [0.1.1]\n\n- Fixes a bug where there is a missing await. (#14177)\n\n### `llama-index-embeddings-nomic` [0.4.", "mimetype": "text/plain", "start_char_idx": 12868, "end_char_idx": 16003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2a9e57c-1b6b-452e-99ec-fe1c9420e635": {"__data__": {"id_": "e2a9e57c-1b6b-452e-99ec-fe1c9420e635", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44739aaf-b113-4ceb-bd8b-5cbe5ca3c979", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "d95ca99665dc400ff007a09c32fa88d42ecff43d46e6f19348eec18818626569", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b9deccf-7e51-4399-94c4-bdc360486765", "node_type": "1", "metadata": {}, "hash": "58f6743e5060113d571e94f83d9134896f06c5e5b60d0c23d342ad23be011d3a", "class_name": "RelatedNodeInfo"}}, "text": "(#14177)\n\n### `llama-index-embeddings-nomic` [0.4.0post1]\n\n- Restore Nomic Embed einops dependency (#14176)\n\n### `llama-index-retrievers-bm25` [0.1.4]\n\n- Changing BM25Retriever \\_retrieve to use numpy methods (#14015)\n\n### `llama-index-llms-gemini` [0.1.11]\n\n- Add missing @llm_chat_callback() to Gemini.stream_chat (#14166)\n\n### `llama-index-llms-vertex` [0.2.0]\n\n- Enable Function calling and agent runner for Vertex AI (#14088)\n\n### `llama-index-vector-stores-opensearch` [0.1.11]\n\n- feat: support VectorStoreQueryMode.TEXT_SEARCH on OpenSearch VectorStore (#14153)\n\n## [2024-06-14]\n\n### `llama-index-core` [0.10.45]\n\n- Fix parsing sql query.py (#14109)\n- Implement NDCG metric (#14100)\n- Fixed System Prompts for Structured Generation (#14026)\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n- Add PandasExcelReader class for parsing excel files (#13991)\n- feat: add spans to ingestion pipeline (#14062)\n\n### `llama-index-vector-stores-qdrant` [0.2.10]\n\n- Fix Qdrant nodes (#14149)\n\n### `llama-index-readers-mongodb` [0.1.7]\n\n- Fixes TypeError: sequence item : expected str instance, int found\n\n### `llama-index-indices-managed-vertexai` [0.0.1]\n\n- feat: Add Managed Index for LlamaIndex on Vertex AI for RAG (#13626)\n\n### `llama-index-llms-oci-genai` [0.1.1]\n\n- Feature/add command r support oci genai (#14080)\n\n### `llama-index-vector-stores-milvus` [0.1.20]\n\n- MilvusVectorStore: always include text_key in output_fields (#14076)\n\n### `llama-index-packs-mixture-of-agents` [0.1.0]\n\n- Add Mixture Of Agents paper implementation (#14112)\n\n### `llama-index-llms-text-generation-inference` [0.1.0]\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-llms-huggingface-api` [0.1.0]\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-embeddings-huggingface-api` [0.1.0]\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-utils-huggingface` [0.1.0]\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-llms-watsonx` [0.1.8]\n\n- Feat: IBM watsonx.ai llm and embeddings integration (#13600)\n\n### `llama-index-llms-ibm` [0.1.0]\n\n- Feat: IBM watsonx.ai llm and embeddings integration (#13600)\n\n### `llama-index-embeddings-ibm` [0.1.0]\n\n- Feat: IBM watsonx.ai llm and embeddings integration (#13600)\n\n### `llama-index-vector-stores-milvus` [0.1.19]\n\n- Fix to milvus filter enum parsing (#14111)\n\n### `llama-index-llms-anthropic` [0.1.13]\n\n- fix anthropic llm calls (#14108)\n\n### `llama-index-storage-index-store-postgres` [0.1.4]\n\n- Wrong mongo name was used instead of Postgres (#14107)\n\n### `llama-index-embeddings-bedrock` [0.2.1]\n\n- Remove unnecessary excluded from fields in Bedrock embedding (#14085)\n\n### `llama-index-finetuning` [0.1.7]\n\n- Feature/added trust remote code (#14102)\n\n### `llama-index-readers-file` [0.1.25]\n\n- nit: fix for pandas excel reader (#14086)\n\n### `llama-index-llms-anthropic` [0.1.12]\n\n- Update anthropic dependency to 0.26.2 minimum version (#14091)\n\n### `llama-index-llms-llama-cpp` [0.1.", "mimetype": "text/plain", "start_char_idx": 15953, "end_char_idx": 19170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b9deccf-7e51-4399-94c4-bdc360486765": {"__data__": {"id_": "8b9deccf-7e51-4399-94c4-bdc360486765", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a9e57c-1b6b-452e-99ec-fe1c9420e635", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "a976b89d97c450eafa0a802772aff182d298e28f5c96cde9da9d5216c6dc0e16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "608f9d5c-4558-4073-ba14-f392b4bac5c6", "node_type": "1", "metadata": {}, "hash": "007aa3637457b2141fd6a56d87608cbb88f328f66ae08b562a674defeaea3fdc", "class_name": "RelatedNodeInfo"}}, "text": "26.2 minimum version (#14091)\n\n### `llama-index-llms-llama-cpp` [0.1.4]\n\n- Add support for Llama 3 Instruct prompt format (#14072)\n\n### `llama-index-llms-bedrock-converse` [0.1.8]\n\n- Implement Bedrock Converse API for function calling (#14055)\n\n### `llama-index-vector-stores-postgres` [0.1.11]\n\n- fix/postgres-metadata-in-filter-single-elem (#14035)\n\n### `llama-index-readers-file` [0.1.24]\n\n- Add PandasExcelReader class for parsing excel files (#13991)\n\n### `llama-index-embeddings-ipex-llm` [0.1.4]\n\n- Update dependency of llama-index-embeddings-ipex-llm\n\n### `llama-index-embeddings-gemini` [0.1.8]\n\n- Add api key as field in Gemini Embedding (#14061)\n\n### `llama-index-vector-stores-milvus` [0.1.18]\n\n- Expand milvus vector store filter options (#13961)\n\n## [2024-06-10]\n\n### `llama-index-core` [0.10.44]\n\n- Add WEBP and GIF to supported image types for SimpleDirectoryReader (#14038)\n- refactor: add spans to abstractmethods via mixin (#14003)\n- Adding streaming support for SQLAutoVectorQueryEngine (#13947)\n- add option to specify embed_model to NLSQLTableQueryEngine (#14006)\n- add spans for multimodal LLMs (#13966)\n- change to compact in auto prev next (#13940)\n- feat: add exception events for streaming errors (#13917)\n- feat: add spans for tools (#13916)\n\n### `llama-index-embeddings-azure-openai` [0.1.10]\n\n- Fix error when using azure_ad without setting the API key (#13970)\n\n### `llama-index-embeddings-jinaai` [0.2.0]\n\n- add Jina Embeddings MultiModal (#13861)\n\n### `llama-index-embeddings-nomic` [0.3.0]\n\n- Add Nomic multi modal embeddings (#13920)\n\n### `llama-index-graph-stores-neo4j` [0.2.3]\n\n- ensure cypher returns list before iterating (#13938)\n\n### `llama-index-llms-ai21` [0.2.0]\n\n- Add AI21 Labs Jamba-Instruct Support (#14030)\n\n### `llama-index-llms-deepinfra` [0.1.2]\n\n- fix(deepinfrallm): default max_tokens (#13998)\n\n### `llama-index-llms-vllm` [0.1.8]\n\n- correct `__del__()` Vllm (#14053)\n\n### `llama-index-packs-zenguard` [0.1.0]\n\n- Add ZenGuard llamapack (#13959)\n\n### `llama-index-readers-google` [0.2.7]\n\n- fix how class attributes are set in google drive reader (#14022)\n- Add Google Maps Text Search Reader (#13884)\n\n### `llama-index-readers-jira` [0.1.4]\n\n- Jira personal access token with hosted instances (#13890)\n\n### `llama-index-readers-mongodb` [0.1.6]\n\n- set document ids when loading (#14000)\n\n### `llama-index-retrievers-duckdb-retriever` [0.1.0]\n\n- Add DuckDBRetriever (#13929)\n\n### `llama-index-vector-stores-chroma` [0.1.9]\n\n- Add inclusion filter to chromadb (#14010)\n\n### `llama-index-vector-stores-lancedb` [0.1.5]\n\n- Fix LanceDBVectorStore `add()` logic (#13993)\n\n### `llama-index-vector-stores-milvus` [0.1.17]\n\n- Support all filter operators for Milvus vector store (#13745)\n\n### `llama-index-vector-stores-postgres` [0.1.10]\n\n- Broaden SQLAlchemy support in llama-index-vector-stores-postgres to 1.4+ (#13936)\n\n### `llama-index-vector-stores-qdrant` [0.2.9]\n\n- Qdrant: Create payload index for `doc_id` (#14001)\n\n## [2024-06-02]\n\n### `llama-index-core` [0.10.", "mimetype": "text/plain", "start_char_idx": 19101, "end_char_idx": 22120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "608f9d5c-4558-4073-ba14-f392b4bac5c6": {"__data__": {"id_": "608f9d5c-4558-4073-ba14-f392b4bac5c6", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b9deccf-7e51-4399-94c4-bdc360486765", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "03acf8fc7cbaf5d8dce718d1dba2a143d2d5b5ca67c409018136afd03b37f7d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "589e32ed-ca8a-4e99-989e-1cf14851e02e", "node_type": "1", "metadata": {}, "hash": "bfad71ec863a9174ec38830d4f696d9a4439f73b151c81ace6886fd7736653d3", "class_name": "RelatedNodeInfo"}}, "text": "10.43]\n\n- use default UUIDs when possible for property graph index vector stores (#13886)\n- avoid empty or duplicate inserts in property graph index (#13891)\n- Fix cur depth for `get_rel_map` in simple property graph store (#13888)\n- (bandaid) disable instrumentation from logging generators (#13901)\n- Add backwards compatibility to Dispatcher.get_dispatch_event() method (#13895)\n- Fix: Incorrect naming of acreate_plan in StructuredPlannerAgent (#13879)\n\n### `llama-index-graph-stores-neo4j` [0.2.2]\n\n- Handle cases where type is missing (neo4j property graph) (#13875)\n- Rename `Neo4jPGStore` to `Neo4jPropertyGraphStore` (with backward compat) (#13891)\n\n### `llama-index-llms-openai` [0.1.22]\n\n- Improve the retry mechanism of OpenAI (#13878)\n\n### `llama-index-readers-web` [0.1.18]\n\n- AsyncWebPageReader: made it actually async; it was exhibiting blocking behavior (#13897)\n\n### `llama-index-vector-stores-opensearch` [0.1.10]\n\n- Fix/OpenSearch filter logic (#13804)\n\n## [2024-05-31]\n\n### `llama-index-core` [0.10.42]\n\n- Allow proper setting of the vector store in property graph index (#13816)\n- fix imports in langchain bridge (#13871)\n\n### `llama-index-graph-stores-nebula` [0.2.0]\n\n- NebulaGraph support for PropertyGraphStore (#13816)\n\n### `llama-index-llms-langchain` [0.1.5]\n\n- fix fireworks imports in langchain llm (#13871)\n\n### `llama-index-llms-openllm` [0.1.5]\n\n- feat(openllm): 0.5 sdk integrations update (#13848)\n\n### `llama-index-llms-premai` [0.1.5]\n\n- Update SDK compatibility (#13836)\n\n### `llama-index-readers-google` [0.2.6]\n\n- Fixed a bug with tokens causing an infinite loop in GoogleDriveReader (#13863)\n\n## [2024-05-30]\n\n### `llama-index-core` [0.10.41]\n\n- pass embeddings from index to property graph retriever (#13843)\n- protect instrumentation event/span handlers from each other (#13823)\n- add missing events for completion streaming (#13824)\n- missing callback_manager.on_event_end when there is exception (#13825)\n\n### `llama-index-llms-gemini` [0.1.10]\n\n- use `model` kwarg for model name for gemini (#13791)\n\n### `llama-index-llms-mistralai` [0.1.15]\n\n- Add mistral code model (#13807)\n- update mistral codestral with fill in middle endpoint (#13810)\n\n### `llama-index-llms-openllm` [0.1.5]\n\n- 0.5 integrations update (#13848)\n\n### `llama-index-llms-vertex` [0.1.8]\n\n- Safety setting for Pydantic Error for Vertex Integration (#13817)\n\n### `llama-index-readers-smart-pdf-loader` [0.1.5]\n\n- handle path objects in smart pdf reader (#13847)\n\n## [2024-05-28]\n\n### `llama-index-core` [0.10.40]\n\n- Added `PropertyGraphIndex` and other supporting abstractions. See the [full guide](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/) for more details (#13747)\n- Updated `AutoPrevNextNodePostprocessor` to allow passing in response mode and LLM (#13771)\n- fix type handling with return direct (#13776)\n- Correct the method name to `_aget_retrieved_ids_and_texts` in retrievval evaluator (#13765)\n- fix: QueryTransformComponent incorrect call `self._query_transform` (#13756)\n- implement more filters for `SimpleVectorStoreIndex` (#13365)\n\n### `llama-index-embeddings-bedrock` [0.2.0]\n\n- Added support for Bedrock Titan Embeddings v2 (#13580)\n\n### `llama-index-embeddings-oci-genai` [0.1.0]\n\n- add Oracle Cloud Infrastructure (OCI) Generative AI (#13631)\n\n### `llama-index-embeddings-huggingface` [0.2.1]\n\n- Expose \"safe_serialization\" parameter from AutoModel (#11939)\n\n### `llama-index-graph-stores-neo4j` [0.2.", "mimetype": "text/plain", "start_char_idx": 22117, "end_char_idx": 25587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "589e32ed-ca8a-4e99-989e-1cf14851e02e": {"__data__": {"id_": "589e32ed-ca8a-4e99-989e-1cf14851e02e", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "608f9d5c-4558-4073-ba14-f392b4bac5c6", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "cd6da39f111bac38f253cd07c47cd5c2d57a5766c5bb555c7bddea4ee0ddf11d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a94e78-c521-4ba0-aa52-cb4545e39784", "node_type": "1", "metadata": {}, "hash": "9f73fba357627cddb230bd373637e08616f1d765a79472400ee4625cefac4a64", "class_name": "RelatedNodeInfo"}}, "text": "2.0]\n\n- Added `Neo4jPGStore` for property graph support (#13747)\n\n### `llama-index-indices-managed-dashscope` [0.1.1]\n\n- Added dashscope managed index (#13378)\n\n### `llama-index-llms-oci-genai` [0.1.0]\n\n- add Oracle Cloud Infrastructure (OCI) Generative AI (#13631)\n\n### `llama-index-readers-feishu-wiki` [0.1.1]\n\n- fix undefined variable (#13768)\n\n### `llama-index-packs-secgpt` [0.1.0]\n\n- SecGPT - LlamaIndex Integration #13127\n\n### `llama-index-vector-stores-hologres` [0.1.0]\n\n- Add Hologres vector db (#13619)\n\n### `llama-index-vector-stores-milvus` [0.1.16]\n\n- Remove FlagEmbedding as Milvus's dependency (#13767)\n  Unify the collection construction regardless of the value of enable_sparse (#13773)\n\n### `llama-index-vector-stores-opensearch` [0.1.9]\n\n- refactor to put helper methods inside class definition (#13749)\n\n## [2024-05-24]\n\n### `llama-index-core` [0.10.39]\n\n- Add VectorMemory and SimpleComposableMemory (#13352)\n- Improve MarkdownReader to ignore headers in code blocks (#13694)\n- proper async element node parsers (#13698)\n- return only the message content in function calling worker (#13677)\n- nit: fix multimodal query engine to use metadata (#13712)\n- Add notebook with workaround for lengthy tool descriptions and QueryPlanTool (#13701)\n\n### `llama-index-embeddings-ipex-llm` [0.1.2]\n\n- Improve device selection (#13644)\n\n### `llama-index-indices-managed-postgresml` [0.1.3]\n\n- Add the PostgresML Managed Index (#13623)\n\n### `llama-index-indices-managed-vectara` [0.1.4]\n\n- Added chat engine, streaming, factual consistency score, and more (#13639)\n\n### `llama-index-llms-deepinfra` [0.0.1]\n\n- Add Integration for DeepInfra LLM Models (#13652)\n\n### `llama-index-llm-ipex-llm` [0.1.3]\n\n- add GPU support for llama-index-llm-ipex-llm (#13691)\n\n### `llama-index-llms-lmstudio` [0.1.0]\n\n- lmstudio integration (#13557)\n\n### `llama-index-llms-ollama` [0.1.5]\n\n- Use aiter_lines function to iterate over lines in ollama integration (#13699)\n\n### `llama-index-llms-vertex` [0.1.6]\n\n- Added safety_settings parameter for gemini (#13568)\n\n### `llama-index-postprocessor-voyageai-rerank` [0.1.3]\n\n- VoyageAI reranking bug fix (#13622)\n\n### `llama-index-retrievers-mongodb-atlas-bm25-retriever` [0.1.4]\n\n- Add missing return (#13720)\n\n### `llama-index-readers-web` [0.1.17]\n\n- Add Scrapfly Web Loader (#13654)\n\n### `llama-index-vector-stores-postgres` [0.1.9]\n\n- fix bug with delete and special chars (#13651)\n\n### `llama-index-vector-stores-supabase` [0.1.5]\n\n- Try-catch in case the .\\_client attribute is not present (#13681)\n\n## [2024-05-21]\n\n### `llama-index-core` [0.10.38]\n\n- Enabling streaming in BaseSQLTableQueryEngine (#13599)\n- Fix nonetype errors in relational node parsers (#13615)\n- feat(instrumentation): new spans for ALL llms (#13565)\n- Properly Limit the number of generated questions (#13596)\n- Pass 'exclude_llm_metadata_keys' and 'exclude_embed_metadata_keys' in element Node Parsers (#13567)\n- Add batch mode to QueryPipeline (#13203)\n- Improve SentenceEmbeddingOptimizer to respect Settings.embed_model (#13514)\n- ReAct output parser robustness changes (#13459)\n- fix for pydantic tool calling with a single argument (#13522)\n- Avoid unexpected error when stream chat doesn't yield (#13422)\n\n### `llama-index-embeddings-nomic` [0.2.0]\n\n- Implement local Nomic Embed with the inference_mode parameter (#13607)\n\n### `llama-index-embeddings-nvidia` [0.", "mimetype": "text/plain", "start_char_idx": 25585, "end_char_idx": 28972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04a94e78-c521-4ba0-aa52-cb4545e39784": {"__data__": {"id_": "04a94e78-c521-4ba0-aa52-cb4545e39784", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "589e32ed-ca8a-4e99-989e-1cf14851e02e", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b1037f46d693357f7b44ca0ede0eeb23f9b9cc67ebf58e3840beb0f82e14483f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce22dbf3-ab7e-4e89-8c40-25ab6446f0f4", "node_type": "1", "metadata": {}, "hash": "0b8b67ee51e52bf531b392dc66dd396f54970f685c64e47fc5b1117c3340e3af", "class_name": "RelatedNodeInfo"}}, "text": "0]\n\n- Implement local Nomic Embed with the inference_mode parameter (#13607)\n\n### `llama-index-embeddings-nvidia` [0.1.3]\n\n- Deprecate `mode()` in favor of `__init__(base_url=.)` (#13572)\n- add snowflake/arctic-embed-l support (#13555)\n\n### `llama-index-embeddings-openai` [0.1.10]\n\n- update how retries get triggered for openai (#13608)\n\n### `llama-index-embeddings-upstage` [0.1.0]\n\n- Integrations: upstage LLM and Embeddings (#13193)\n\n### `llama-index-llms-gemini` [0.1.8]\n\n- feat: add gemini new models to multimodal LLM and regular (#13539)\n\n### `llama-index-llms-groq` [0.1.4]\n\n- fix: enable tool use (#13566)\n\n### `llama-index-llms-lmstudio` [0.1.0]\n\n- Add support for lmstudio integration (#13557)\n\n### `llama-index-llms-nvidia` [0.1.3]\n\n- Deprecate `mode()` in favor of `__init__(base_url=.)` (#13572)\n\n### `llama-index-llms-openai` [0.1.20]\n\n- update how retries get triggered for openai (#13608)\n\n### `llama-index-llms-unify` [0.1.0]\n\n- Add Unify LLM Support (#12921)\n\n### `llama-index-llms-upstage` [0.1.0]\n\n- Integrations: upstage LLM and Embeddings (#13193)\n\n### `llama-index-llms-vertex` [0.1.6]\n\n- Adding Support for MedLM Models (#11911)\n\n### `llama_index.postprocessor.dashscope_rerank` [0.1.0]\n\n- Add dashscope rerank for postprocessor (#13353)\n\n### `llama-index-postprocessor-nvidia-rerank` [0.1.2]\n\n- Deprecate `mode()` in favor of `__init__(base_url=.)` (#13572)\n\n### `llama-index-readers-mongodb` [0.1.5]\n\n- SimpleMongoReader should allow optional fields in metadata (#13575)\n\n### `llama-index-readers-papers` [0.1.5]\n\n- fix: (ArxivReader) set exclude_hidden to False when reading data from hidden directory (#13578)\n\n### `llama-index-readers-sec-filings` [0.1.5]\n\n- fix: sec_filings header when making requests to sec.gov #13548\n\n### `llama-index-readers-web` [0.1.16]\n\n- Added firecrawl search mode (#13560)\n- Updated Browserbase web reader (#13535)\n\n### `llama-index-tools-cassandra` [0.1.0]\n\n- added Cassandra database tool spec for agents (#13423)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.7]\n\n- Allow querying AzureAISearch without non-null metadata field (#13531)\n\n### `llama-index-vector-stores-elasticsearch` [0.2.0]\n\n- Integrate VectorStore from Elasticsearch client (#13291)\n\n### `llama-index-vector-stores-milvus` [0.1.14]\n\n- Fix the filter expression construction of Milvus vector store (#13591)\n\n### `llama-index-vector-stores-supabase` [0.1.4]\n\n- Disconnect when deleted (#13611)\n\n### `llama-index-vector-stores-wordlift` [0.1.0]\n\n- Added the WordLift Vector Store (#13028)\n\n## [2024-05-14]\n\n### `llama-index-core` [0.10.37]\n\n- Add image_documents at call time for `MultiModalLLMCompletionProgram` (#13467)\n- fix RuntimeError by switching to asyncio from threading (#13486)\n- Add support for prompt kwarg (#13405)\n- VectorStore -> BasePydanticVectorStore (#13439)\n- fix: user_message does not exist bug (#13432)\n- import missing response type (#13382)\n- add `CallbackManager` to `MultiModalLLM` (#13400)\n\n### `llama-index-llms-bedrock` [0.1.8]\n\n- Remove \"Truncate\" parameter from Bedrock Cohere invoke model request (#13442)\n\n### `llama-index-readers-web` [0.1.14]\n\n- Trafilatura kwargs and progress bar for trafilatura web reader (#13454)\n\n### `llama-index-vector-stores-postgres` [0.1.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce22dbf3-ab7e-4e89-8c40-25ab6446f0f4": {"__data__": {"id_": "ce22dbf3-ab7e-4e89-8c40-25ab6446f0f4", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a94e78-c521-4ba0-aa52-cb4545e39784", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "371d58ab10754d3531d34b876c33e9b1ce7d17b00851c319c4ec74b41033d1a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a6735b3-217b-4935-97de-65b95e6ba33f", "node_type": "1", "metadata": {}, "hash": "d69955e3b72f791d8c6e8bdfda30c822591e9e964cc6a13aa49b1bfed1110b94", "class_name": "RelatedNodeInfo"}}, "text": "1.8]\n\n- Fix #9522 - SQLAlchemy warning when using hybrid search (#13476)\n\n### `llama-index-vector-stores-lantern` [0.1.4]\n\n- Fix #9522 - SQLAlchemy warning when using hybrid search (#13476)\n\n### `llama-index-callbacks-uptrain` [0.2.0]\n\n- update UpTrain Callback Handler to support new Upgratin eval schema (#13479)\n\n### `llama-index-vector-stores-zep` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-vearch` [0.1.1]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-upstash` [0.1.4]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-typesense` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-timescalerevector` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-tencentvectordb` [0.1.4]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-tair` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-singlestoredb` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-rocksetdb` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-neptune` [0.1.1]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-neo4jvector` [0.1.5]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-myscale` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-metal` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-jaguar` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-epsilla` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-dynamodb` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-dashvector` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-chatgpt-plugin` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-baiduvectordb` [0.1.1]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-bagel` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-awsdocdb` [0.1.5]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-awadb` [0.1.3]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-alibabacloud-opensearch` [0.1.1]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-readers-wordlift` [0.1.4]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-readers-guru` [0.1.4]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-readers-pebblo` [0.1.1]\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-postprocessor-voyageai-rerank` [0.1.2]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-sbert-rerank` [0.1.", "mimetype": "text/plain", "start_char_idx": 32094, "end_char_idx": 35180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a6735b3-217b-4935-97de-65b95e6ba33f": {"__data__": {"id_": "8a6735b3-217b-4935-97de-65b95e6ba33f", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce22dbf3-ab7e-4e89-8c40-25ab6446f0f4", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "0296b1185938dcda2d68c574611ee3c125f5fb5d4c349c1d8612ed48c6498e4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "521bd1ef-cb12-4147-8df9-9e402cc342cd", "node_type": "1", "metadata": {}, "hash": "66aef7ad882eb7e3c0a60fb1e4f1d2a129b056437ade07d75cc2b653a34535f0", "class_name": "RelatedNodeInfo"}}, "text": "1.2]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-sbert-rerank` [0.1.4]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-rankllm-rerank` [0.1.3]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-rankgpt-rerank` [0.1.4]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-openvino-rerank` [0.1.3]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-nvidia-rerank` [0.1.1]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-jinaai-rerank` [0.1.3]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-flag-embedding-rerank` [0.1.3]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-colbert-rerank` [0.1.2]\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-cohere-rerank` [0.1.6]\n\n- bump rerank versions (#13465)\n\n### `llama-index-multi-modal-llms-openai` [0.1.6]\n\n- gpt-4o support (#13463)\n\n### `llama-index-llms-openai` [0.1.19]\n\n- gpt-4o support (#13463)\n\n### `llama-index-packs-rag-fusion-query-pipeline` [0.1.4]\n\n- fix the RAG fusion pipeline (#13413)\n\n### `llama-index-agent-openai` [0.2.5]\n\n- fix: update OpenAIAssistantAgent to use attachments (#13341)\n\n### `llama-index-embeddings-deepinfra` [0.1.0]\n\n- new embeddings integration (#13323)\n\n### `llama-index-llms-mlx` [0.1.0]\n\n- new llm integration (#13231)\n\n### `llama-index-vector-stores-milvus` [0.1.12]\n\n- fix: Corrected connection parameters in connections.connect() (#13448)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.6]\n\n- fix AzureAiSearchVectorStore metadata f-string (#13435)\n\n### `llama-index-vector-stores-mongodb` [0.1.5]\n\n- adds Unit and Integration tests for MongoDBAtlasVectorSearch (#12854)\n\n### `llama-index-llms-huggingface` [0.2.0]\n\n- update llama-index-llms-huggingface dependency (#13420)\n\n### `llama-index-vector-store-relyt` [0.1.0]\n\n- new vector store integration\n\n### `llama-index-storage-kvstore-redis` [0.1.5]\n\n- Implement async methods in RedisKVStore (#12943)\n\n### `llama-index-packs-cohere-citation-chat` [0.1.5]\n\n- pin llama-index-llms-cohere dependency (#13417)\n\n### `llama-index-llms-cohere` [0.2.0]\n\n- pin cohere dependency (#13417)\n\n### `llama-index-tools-azure-code-interpreter` [0.1.1]\n\n- fix indexing issue and runtime error message (#13414)\n\n### `llama-index-postprocessor-cohere-rerank` [0.1.5]\n\n- fix Cohere Rerank bug (#13410)\n\n### `llama-index-indices-managed-llama-cloud` [0.1.7]\n\n- fix retriever integration (#13409)\n\n### `llama-index-tools-azure-code-interpreter` [0.1.0]\n\n- new tool\n\n### `llama-index-readers-google` [0.2.5]\n\n- fix missing authorized_user_info check on GoogleDriveReader (#13394)\n\n### `llama-index-storage-kvstore-firestore` [0.2.1]\n\n- await Firestore's AsyncDocumentReference (#13386)\n\n### `llama-index-llms-nvidia` [0.1.2]\n\n- add dynamic model listing support (#13398)\n\n## [2024-05-09]\n\n### `llama-index-core` [0.10.36]\n\n- add start_char_idx and end_char_idx with MarkdownElementParser (#13377)\n- use handlers from global default (#13368)\n\n### `llama-index-readers-pebblo` [0.1.0]\n\n- Initial release (#13128)\n\n### `llama-index-llms-cohere` [0.1.", "mimetype": "text/plain", "start_char_idx": 35091, "end_char_idx": 38199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "521bd1ef-cb12-4147-8df9-9e402cc342cd": {"__data__": {"id_": "521bd1ef-cb12-4147-8df9-9e402cc342cd", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a6735b3-217b-4935-97de-65b95e6ba33f", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "00f5d4d40aa1d5d8224929b7727d4006e3245888fd38b4bb886ec2e0eb49c7ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6736c08-bd27-4d4c-b26f-f703ce2ed11b", "node_type": "1", "metadata": {}, "hash": "5208408557cb97e12b9a10c2a7cff27d6dbab6268fc59a55aaf495a6298f6944", "class_name": "RelatedNodeInfo"}}, "text": "1.0]\n\n- Initial release (#13128)\n\n### `llama-index-llms-cohere` [0.1.7]\n\n- Call Cohere RAG inference with documents argument (#13196)\n\n### `llama-index-vector-scores-kdbai` [0.1.6]\n\n- update add method decode utf-8 (#13194)\n\n### `llama-index-vector-stores-alibabacloud-opensearch` [0.1.0]\n\n- Initial release (#13286)\n\n### `llama-index-tools-multion` [0.2.0]\n\n- update tool to use updated api/sdk (#13373)\n\n### `llama-index-vector-sores-weaviate` [1.0.0]\n\n- Update to weaviate client v4 (#13229)\n\n### `llama-index-readers-file` [0.1.22]\n\n- fix bug where PDFReader ignores extra_info (#13369)\n\n### `llama-index-llms-azure-openai` [0.1.8]\n\n- Add sync httpx client support (#13370)\n\n### `llama-index-llms-openai` [0.1.18]\n\n- Add sync httpx client support (#13370)\n- Add missing openai model token context (#13337)\n\n### `llama-index-readers-github` [0.1.9]\n\n- Add fail_on_http_error (#13366)\n\n### `llama-index-vector-stores-pinecone` [0.1.7]\n\n- Add attribution tag for pinecone (#13329)\n\n### `llama-index-llms-nvidia` [0.1.1]\n\n- set default max_tokens to 1024 (#13371)\n\n### `llama-index-readers-papers` [0.1.5]\n\n- Fix hiddent temp directory issue for arxiv reader (#13351)\n\n### `llama-index-embeddings-nvidia` [0.1.1]\n\n- fix truncate passing aget_query_embedding and get_text_embedding (#13367)\n\n### `llama-index-llms-anyscare` [0.1.4]\n\n- Add llama-3 models (#13336)\n\n## [2024-05-07]\n\n### `llama-index-agent-introspective` [0.1.0]\n\n- Add CRITIC and reflection agent integrations (#13108)\n\n### `llama-index-core` [0.10.35]\n\n- fix `from_defaults()` erasing summary memory buffer history (#13325)\n- use existing async event loop instead of `asyncio.run()` in core (#13309)\n- fix async streaming from query engine in condense question chat engine (#13306)\n- Handle ValueError in extract_table_summaries in element node parsers (#13318)\n- Handle llm properly for QASummaryQueryEngineBuilder and RouterQueryEngine (#13281)\n- expand instrumentation payloads (#13302)\n- Fix Bug in sql join statement missing schema (#13277)\n\n### `llama-index-embeddings-jinaai` [0.1.5]\n\n- add encoding_type parameters in JinaEmbedding class (#13172)\n- fix encoding type access in JinaEmbeddings (#13315)\n\n### `llama-index-embeddings-nvidia` [0.1.0]\n\n- add nvidia nim embeddings support (#13177)\n\n### `llama-index-llms-mistralai` [0.1.12]\n\n- Fix async issue when streaming with Mistral AI (#13292)\n\n### `llama-index-llms-nvidia` [0.1.0]\n\n- add nvidia nim llm support (#13176)\n\n### `llama-index-postprocessor-nvidia-rerank` [0.1.0]\n\n- add nvidia nim rerank support (#13178)\n\n### `llama-index-readers-file` [0.1.21]\n\n- Update MarkdownReader to parse text before first header (#13327)\n\n### `llama-index-readers-web` [0.1.13]\n\n- feat: Spider Web Loader (#13200)\n\n### `llama-index-vector-stores-vespa` [0.1.0]\n\n- Add VectorStore integration for Vespa (#13213)\n\n### `llama-index-vector-stores-vertexaivectorsearch` [0.1.0]\n\n- Add support for Vertex AI Vector Search as Vector Store (#13186)\n\n## [2024-05-02]\n\n### `llama-index-core` [0.10.34]\n\n- remove error ignoring during chat engine streaming (#13160)\n- add structured planning agent (#13149)\n- update base class for planner agent (#13228)\n- Fix: Error when parse file using SimpleFileNodeParser and file's extension doesn't in FILE_NODE_PARSERS (#13156)\n- add matching `source_node.", "mimetype": "text/plain", "start_char_idx": 38130, "end_char_idx": 41429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6736c08-bd27-4d4c-b26f-f703ce2ed11b": {"__data__": {"id_": "e6736c08-bd27-4d4c-b26f-f703ce2ed11b", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "521bd1ef-cb12-4147-8df9-9e402cc342cd", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "93015251e25371be8606125f6bf5c8510064a5bc07cf10d2d955af1354e95656", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe40c17c-7acc-4ed5-9847-47507a6203ee", "node_type": "1", "metadata": {}, "hash": "83a2fd672dbc67ddd7f23a0c8d70282163243cd81e78f3752c01fa0c912c8332", "class_name": "RelatedNodeInfo"}}, "text": "node_id` verification to node parsers (#13109)\n- Retrieval Metrics: Updating HitRate and MRR for Evaluation@K documents retrieved. Also adding RR as a separate metric (#12997)\n- Add chat summary memory buffer (#13155)\n\n### `llama-index-indices-managed-zilliz` [0.1.3]\n\n- ZillizCloudPipelineIndex accepts flexible params to create pipelines (#10134, #10112)\n\n### `llama-index-llms-huggingface` [0.1.7]\n\n- Add tool usage support with text-generation-inference integration from Hugging Face (#12471)\n\n### `llama-index-llms-maritalk` [0.2.0]\n\n- Add streaming for maritalk (#13207)\n\n### `llama-index-llms-mistral-rs` [0.1.0]\n\n- Integrate mistral.rs LLM (#13105)\n\n### `llama-index-llms-mymagic` [0.1.7]\n\n- mymagicai api update (#13148)\n\n### `llama-index-llms-nvidia-triton` [0.1.5]\n\n- Streaming Support for Nvidia's Triton Integration (#13135)\n\n### `llama-index-llms-ollama` [0.1.3]\n\n- added async support to ollama llms (#13150)\n\n### `llama-index-readers-microsoft-sharepoint` [0.2.2]\n\n- Exclude access control metadata keys from LLMs and embeddings - SharePoint Reader (#13184)\n\n### `llama-index-readers-web` [0.1.11]\n\n- feat: Browserbase Web Reader (#12877)\n\n### `llama-index-readers-youtube-metadata` [0.1.0]\n\n- Added YouTube Metadata Reader (#12975)\n\n### `llama-index-storage-kvstore-redis` [0.1.4]\n\n- fix redis kvstore key that was in bytes (#13201)\n\n### `llama-index-vector-stores-azureaisearch` [0.1.5]\n\n- Respect filter condition for Azure AI Search (#13215)\n\n### `llama-index-vector-stores-chroma` [0.1.7]\n\n- small bump for new chroma client version (#13158)\n\n### `llama-index-vector-stores-firestore` [0.1.0]\n\n- Adding Firestore Vector Store (#12048)\n\n### `llama-index-vector-stores-kdbai` [0.1.5]\n\n- small fix to returned IDs after `add()` (#12515)\n\n### `llama-index-vector-stores-milvus` [0.1.11]\n\n- Add hybrid retrieval mode to MilvusVectorStore (#13122)\n\n### `llama-index-vector-stores-postgres` [0.1.7]\n\n- parameterize queries in pgvector store (#13199)\n\n## [2024-04-27]\n\n### `llama-index-core` [0.10.33]\n\n- add agent_worker.as_agent() (#13061)\n\n### `llama-index-embeddings-bedrock` [0.1.5]\n\n- Use Bedrock cohere character limit (#13126)\n\n### `llama-index-tools-google` [0.1.5]\n\n- Change default value for attendees to empty list (#13134)\n\n### `llama-index-graph-stores-falkordb` [0.1.4]\n\n- Skip index creation error when index already exists (#13085)\n\n### `llama-index-tools-google` [0.1.4]\n\n- Fix datetime for google calendar create_event api (#13132)\n\n### `llama-index-llms-anthropic` [0.1.11]\n\n- Merge multiple prompts into one (#13131)\n\n### `llama-index-indices-managed-llama-cloud` [0.1.6]\n\n- Use MetadataFilters in LlamaCloud Retriever (#13117)\n\n### `llama-index-graph-stores-kuzu` [0.1.3]\n\n- Fix kuzu integration .execute() calls (#13100)\n\n### `llama-index-vector-stores-lantern` [0.1.3]\n\n- Maintenance update to keep up to date with lantern builds (#13116)\n\n## [2024-04-25]\n\n### `llama-index-core` [0.10.32]\n\n- Corrected wrong output type for `OutputKeys.", "mimetype": "text/plain", "start_char_idx": 41429, "end_char_idx": 44403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe40c17c-7acc-4ed5-9847-47507a6203ee": {"__data__": {"id_": "fe40c17c-7acc-4ed5-9847-47507a6203ee", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6736c08-bd27-4d4c-b26f-f703ce2ed11b", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b8f4db610f8c1c25e501a1f439cd0f54fe953b936ff6bee7a1be5a3ff6f648c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81398365-cc64-4ef7-8738-ab73077c371f", "node_type": "1", "metadata": {}, "hash": "6c2facfb9d37e965833ad166d923cf9c6bf83896ae80705948ae3784e0075fac", "class_name": "RelatedNodeInfo"}}, "text": "10.32]\n\n- Corrected wrong output type for `OutputKeys.from_keys()` (#13086)\n- add run_jobs to aws base embedding (#13096)\n- allow user to customize the keyword extractor prompt template (#13083)\n- (CondenseQuestionChatEngine) Do not condense the question if there's no conversation history (#13069)\n- QueryPlanTool: Execute tool calls in subsequent (dependent) nodes in the query plan (#13047)\n- Fix for fusion retriever sometime return Nonetype query(s) before similarity search (#13112)\n\n### `llama-index-embeddings-ipex-llm` [0.1.1]\n\n- Support llama-index-embeddings-ipex-llm for Intel GPUs (#13097)\n\n### `llama-index-packs-raft-dataset` [0.1.4]\n\n- Fix bug in raft dataset generator - multiple system prompts (#12751)\n\n### `llama-index-readers-microsoft-sharepoint` [0.2.1]\n\n- Add access control related metadata to SharePoint reader (#13067)\n\n### `llama-index-vector-stores-pinecone` [0.1.6]\n\n- Nested metadata filter support (#13113)\n\n### `llama-index-vector-stores-qdrant` [0.2.8]\n\n- Nested metadata filter support (#13113)\n\n## [2024-04-23]\n\n### `llama-index-core` [0.10.31]\n\n- fix async streaming response from query engine (#12953)\n- enforce uuid in element node parsers (#12951)\n- add function calling LLM program (#12980)\n- make the PydanticSingleSelector work with async api (#12964)\n- fix query pipeline's arun_with_intermediates (#13002)\n\n### `llama-index-agent-coa` [0.1.0]\n\n- Add COA Agent integration (#13043)\n\n### `llama-index-agent-lats` [0.1.0]\n\n- Official LATs agent integration (#13031)\n\n### `llama-index-agent-llm-compiler` [0.1.0]\n\n- Add LLMCompiler Agent Integration (#13044)\n\n### `llama-index-llms-anthropic` [0.1.10]\n\n- Add the ability to pass custom headers to Anthropic LLM requests (#12819)\n\n### `llama-index-llms-bedrock` [0.1.7]\n\n- Adding claude 3 opus to BedRock integration (#13033)\n\n### `llama-index-llms-fireworks` [0.1.5]\n\n- Add new Llama 3 and Mixtral 8x22b model into Llama Index for Fireworks (#12970)\n\n### `llama-index-llms-openai` [0.1.16]\n\n- Fix AsyncOpenAI \"RuntimeError: Event loop is closed bug\" when instances of AsyncOpenAI are rapidly created & destroyed (#12946)\n- Don't retry on all OpenAI APIStatusError exceptions - just InternalServerError (#12947)\n\n### `llama-index-llms-watsonx` [0.1.7]\n\n- Updated IBM watsonx foundation models (#12973)\n\n### `llama-index-packs-code-hierarchy` [0.1.6]\n\n- Return the parent node if the query node is not present (#12983)\n- fixed bug when function is defined twice (#12941)\n\n### `llama-index-program-openai` [0.1.6]\n\n- dding support for streaming partial instances of Pydantic output class in OpenAIPydanticProgram (#13021)\n\n### `llama-index-readers-openapi` [0.1.0]\n\n- add reader for openapi files (#12998)\n\n### `llama-index-readers-slack` [0.1.4]\n\n- Avoid infinite loop when not handled exception is raised (#12963)\n\n### `llama-index-readers-web` [0.1.10]\n\n- Improve whole site reader to remove duplicate links (#12977)\n\n### `llama-index-retrievers-bedrock` [0.1.1]\n\n- Fix Bedrock KB retriever to use query bundle (#12910)\n\n### `llama-index-vector-stores-awsdocdb` [0.1.0]\n\n- Integrating AWS DocumentDB as a vector storage method (#12217)\n\n### `llama-index-vector-stores-databricks` [0.1.2]\n\n- Fix databricks vector search metadata (#12999)\n\n### `llama-index-vector-stores-neo4j` [0.1.4]\n\n- Neo4j metadata filtering support (#12923)\n\n### `llama-index-vector-stores-pinecone` [0.1.", "mimetype": "text/plain", "start_char_idx": 44349, "end_char_idx": 47717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81398365-cc64-4ef7-8738-ab73077c371f": {"__data__": {"id_": "81398365-cc64-4ef7-8738-ab73077c371f", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe40c17c-7acc-4ed5-9847-47507a6203ee", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "7856f5bfd8f61e2c6299f4111a57b4c026008fc7f262f18445359ef374105854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7ebd903-a483-4bb4-af32-c97f1b92d8c9", "node_type": "1", "metadata": {}, "hash": "c50144b7f9d2ed56ef8d64b148aef98ba32fbcaaea9ac208dde348fa0a5e8c18", "class_name": "RelatedNodeInfo"}}, "text": "1.4]\n\n- Neo4j metadata filtering support (#12923)\n\n### `llama-index-vector-stores-pinecone` [0.1.5]\n\n- Fix error querying PineconeVectorStore using sparse query mode (#12967)\n\n### `llama-index-vector-stores-qdrant` [0.2.5]\n\n- Many fixes for async and checking if collection exists (#12916)\n\n### `llama-index-vector-stores-weaviate` [0.1.5]\n\n- Adds the index deletion functionality to the WeviateVectoreStore (#12993)\n\n## [2024-04-17]\n\n### `llama-index-core` [0.10.30]\n\n- Add intermediate outputs to QueryPipeline (#12683)\n- Fix show progress causing results to be out of order (#12897)\n- add OR filter condition support to simple vector store (#12823)\n- improved custom agent init (#12824)\n- fix pipeline load without docstore (#12808)\n- Use async `_aprocess_actions` in `_arun_step_stream` (#12846)\n- provide the exception to the StreamChatErrorEvent (#12879)\n- fix bug in load and search tool spec (#12902)\n\n### `llama-index-embeddings-azure-opena` [0.1.7]\n\n- Expose azure_ad_token_provider argument to support token expiration (#12818)\n\n### `llama-index-embeddings-cohere` [0.1.8]\n\n- Add httpx_async_client option (#12896)\n\n### `llama-index-embeddings-ipex-llm` [0.1.0]\n\n- add ipex-llm embedding integration (#12740)\n\n### `llama-index-embeddings-octoai` [0.1.0]\n\n- add octoai embeddings (#12857)\n\n### `llama-index-llms-azure-openai` [0.1.6]\n\n- Expose azure_ad_token_provider argument to support token expiration (#12818)\n\n### `llama-index-llms-ipex-llm` [0.1.2]\n\n- add support for loading \"low-bit format\" model to IpexLLM integration (#12785)\n\n### `llama-index-llms-mistralai` [0.1.11]\n\n- support `open-mixtral-8x22b` (#12894)\n\n### `llama-index-packs-agents-lats` [0.1.0]\n\n- added LATS agent pack (#12735)\n\n### `llama-index-readers-smart-pdf-loader` [0.1.4]\n\n- Use passed in metadata for documents (#12844)\n\n### `llama-index-readers-web` [0.1.9]\n\n- added Firecrawl Web Loader (#12825)\n\n### `llama-index-vector-stores-milvus` [0.1.10]\n\n- use batch insertions into Milvus vector store (#12837)\n\n### `llama-index-vector-stores-vearch` [0.1.0]\n\n- add vearch to vector stores (#10972)\n\n## [2024-04-13]\n\n### `llama-index-core` [0.10.29]\n\n- **BREAKING** Moved `PandasQueryEngine` and `PandasInstruction` parser to `llama-index-experimental` (#12419)\n  - new install: `pip install -U llama-index-experimental`\n  - new import: `from llama_index.experimental.query_engine import PandasQueryEngine`\n- Fixed some core dependencies to make python3.12 work nicely (#12762)\n- update async utils `run_jobs()` to include tqdm description (#12812)\n- Refactor kvdocstore delete methods (#12681)\n\n### `llama-index-llms-bedrock` [0.1.6]\n\n- Support for Mistral Large from Bedrock (#12804)\n\n### `llama-index-llms-openvino` [0.1.0]\n\n- Added OpenVino LLMs (#12639)\n\n### `llama-index-llms-predibase` [0.1.4]\n\n- Update LlamaIndex-Predibase Integration to latest API (#12736)\n- Enable choice of either Predibase-hosted or HuggingFace-hosted fine-tuned adapters in LlamaIndex-Predibase integration (#12789)\n\n### `llama-index-output-parsers-guardrails` [0.1.3]\n\n- Modernize GuardrailsOutputParser (#12676)\n\n### `llama-index-packs-agents-coa` [0.1.0]\n\n- Chain-of-Abstraction Agent Pack (#12757)\n\n### `llama-index-packs-code-hierarchy` [0.1.3]\n\n- Fixed issue with chunking multi-byte characters (#12715)\n\n### `llama-index-packs-raft-dataset` [0.", "mimetype": "text/plain", "start_char_idx": 47620, "end_char_idx": 50937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7ebd903-a483-4bb4-af32-c97f1b92d8c9": {"__data__": {"id_": "a7ebd903-a483-4bb4-af32-c97f1b92d8c9", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81398365-cc64-4ef7-8738-ab73077c371f", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "302f8cf9ef05f85dc6b825713d58101bb42ba6ce71654a726131a92681330dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea03da5d-436e-4faf-bdf1-2347f2c90a72", "node_type": "1", "metadata": {}, "hash": "4d24df5191888157edd82e78cbeb9cb1e904ffd3ec29661ba503e38bd9020daa", "class_name": "RelatedNodeInfo"}}, "text": "1.3]\n\n- Fixed issue with chunking multi-byte characters (#12715)\n\n### `llama-index-packs-raft-dataset` [0.1.4]\n\n- Fix bug in raft dataset generator - multiple system prompts (#12751)\n\n### `llama-index-postprocessor-openvino-rerank` [0.1.2]\n\n- Add openvino rerank support (#12688)\n\n### `llama-index-readers-file` [0.1.18]\n\n- convert to Path in docx reader if input path str (#12807)\n- make pip check work for optional pdf packages (#12758)\n\n### `llama-index-readers-s3` [0.1.7]\n\n- wrong doc id when using default s3 endpoint in S3Reader (#12803)\n\n### `llama-index-retrievers-bedrock` [0.1.0]\n\n- Add Amazon Bedrock knowledge base integration as retriever (#12737)\n\n### `llama-index-retrievers-mongodb-atlas-bm25-retriever` [0.1.3]\n\n- Add mongodb atlas bm25 retriever (#12519)\n\n### `llama-index-storage-chat-store-redis` [0.1.3]\n\n- fix message serialization in redis chat store (#12802)\n\n### `llama-index-vector-stores-astra-db` [0.1.6]\n\n- Relax dependency version to accept astrapy `1.*` (#12792)\n\n### `llama-index-vector-stores-couchbase` [0.1.0]\n\n- Add support for Couchbase as a Vector Store (#12680)\n\n### `llama-index-vector-stores-elasticsearch` [0.1.7]\n\n- Fix elasticsearch hybrid rrf window_size (#12695)\n\n### `llama-index-vector-stores-milvus` [0.1.8]\n\n- Added support to retrieve metadata fields from milvus (#12626)\n\n### `llama-index-vector-stores-redis` [0.2.0]\n\n- Modernize redis vector store, use redisvl (#12386)\n\n### `llama-index-vector-stores-qdrant` [0.2.0]\n\n- refactor: Switch default Qdrant sparse encoder (#12512)\n\n## [2024-04-09]\n\n### `llama-index-core` [0.10.28]\n\n- Support indented code block fences in markdown node parser (#12393)\n- Pass in output parser to guideline evaluator (#12646)\n- Added example of query pipeline + memory (#12654)\n- Add missing node postprocessor in CondensePlusContextChatEngine async mode (#12663)\n- Added `return_direct` option to tools /tool metadata (#12587)\n- Add retry for batch eval runner (#12647)\n- Thread-safe instrumentation (#12638)\n- Coroutine-safe instrumentation Spans #12589\n- Add in-memory loading for non-default filesystems in PDFReader (#12659)\n- Remove redundant tokenizer call in sentence splitter (#12655)\n- Add SynthesizeComponent import to shortcut imports (#12655)\n- Improved truncation in SimpleSummarize (#12655)\n- adding err handling in eval_utils default_parser for correctness (#12624)\n- Add async_postprocess_nodes at RankGPT Postprocessor Nodes (#12620)\n- Fix MarkdownNodeParser ref_doc_id (#12615)\n\n### `llama-index-embeddings-openvino` [0.1.5]\n\n- Added initial support for openvino embeddings (#12643)\n\n### `llama-index-llms-anthropic` [0.1.9]\n\n- add anthropic tool calling (#12591)\n\n### `llama-index-llms-ipex-llm` [0.1.1]\n\n- add ipex-llm integration (#12322)\n- add more data types support to ipex-llm llm integration (#12635)\n\n### `llama-index-llms-openllm` [0.1.4]\n\n- Proper PrivateAttr usage in OpenLLM (#12655)\n\n### `llama-index-multi-modal-llms-anthropic` [0.1.4]\n\n- Bumped anthropic dep version (#12655)\n\n### `llama-index-multi-modal-llms-gemini` [0.1.5]\n\n- bump generativeai dep (#12645)\n\n### `llama-index-packs-dense-x-retrieval` [0.1.4]\n\n- Add streaming support for DenseXRetrievalPack (#12607)\n\n### `llama-index-readers-mongodb` [0.1.4]\n\n- Improve efficiency of MongoDB reader (#12664)\n\n### `llama-index-readers-wikipedia` [0.1.4]\n\n- Added multilingual support for the Wikipedia reader (#12616)\n\n### `llama-index-storage-index-store-elasticsearch` [0.1.", "mimetype": "text/plain", "start_char_idx": 50831, "end_char_idx": 54279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea03da5d-436e-4faf-bdf1-2347f2c90a72": {"__data__": {"id_": "ea03da5d-436e-4faf-bdf1-2347f2c90a72", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7ebd903-a483-4bb4-af32-c97f1b92d8c9", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "18e7a2a1ff12145c77e89c3c8a88308ff648a43e297a7b3a33432d2312612018", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7715c403-8d93-49fd-8c16-c389ec67101c", "node_type": "1", "metadata": {}, "hash": "7baa5566851ade9482873c9236ca33c0c2e8a33ed0ca8f8bebac217a3639eb46", "class_name": "RelatedNodeInfo"}}, "text": "4]\n\n- Added multilingual support for the Wikipedia reader (#12616)\n\n### `llama-index-storage-index-store-elasticsearch` [0.1.3]\n\n- remove invalid chars from default collection name (#12672)\n\n### `llama-index-vector-stores-milvus` [0.1.8]\n\n- Added support to retrieve metadata fields from milvus (#12626)\n- Bug fix - Similarity metric is always IP for MilvusVectorStore (#12611)\n\n## [2024-04-04]\n\n### `llama-index-agent-openai` [0.2.2]\n\n- Update imports for message thread typing (#12437)\n\n### `llama-index-core` [0.10.27]\n\n- Fix for pydantic query engine outputs being blank (#12469)\n- Add span_id attribute to Events (instrumentation) (#12417)\n- Fix RedisDocstore node retrieval from docs property (#12324)\n- Add node-postprocessors to retriever_tool (#12415)\n- FLAREInstructQueryEngine : delegating retriever api if the query engine supports it (#12503)\n- Make chat message to dict safer (#12526)\n- fix check in batch eval runner for multi-kwargs (#12563)\n- Fixes agent_react_multimodal_step.py bug with partial args (#12566)\n\n### `llama-index-embeddings-clip` [0.1.5]\n\n- Added support to load clip model from local file path (#12577)\n\n### `llama-index-embeddings-cloudflar-workersai` [0.1.0]\n\n- text embedding integration: Cloudflare Workers AI (#12446)\n\n### `llama-index-embeddings-voyageai` [0.1.4]\n\n- Fix pydantic issue in class definition (#12469)\n\n### `llama-index-finetuning` [0.1.5]\n\n- Small typo fix in QA generation prompt (#12470)\n\n### `llama-index-graph-stores-falkordb` [0.1.3]\n\n- Replace redis driver with FalkorDB driver (#12434)\n\n### `llama-index-llms-anthropic` [0.1.8]\n\n- Add ability to pass custom HTTP headers to Anthropic client (#12558)\n\n### `llama-index-llms-cohere` [0.1.6]\n\n- Add support for Cohere Command R+ model (#12581)\n\n### `llama-index-llms-databricks` [0.1.0]\n\n- Integrations with DataBricks LLM API (#12432)\n\n### `llama-index-llms-watsonx` [0.1.6]\n\n- Updated Watsonx foundation models (#12493)\n- Updated base model name on watsonx integration #12491\n\n### `lama-index-postprocessor-rankllm-rerank` [0.1.2]\n\n- Add RankGPT support inside RankLLM (#12475)\n\n### `llama-index-readers-microsoft-sharepoint` [0.1.7]\n\n- Use recursive strategy by default for SharePoint (#12557)\n\n### `llama-index-readers-web` [0.1.8]\n\n- Readability web page reader fix playwright async api bug (#12520)\n\n### `llama-index-vector-stores-kdbai` [0.1.5]\n\n- small `to_list` fix (#12515)\n\n### `llama-index-vector-stores-neptune` [0.1.0]\n\n- Add support for Neptune Analytics as a Vector Store (#12423)\n\n### `llama-index-vector-stores-postgres` [0.1.5]\n\n- fix(postgres): numeric metadata filters (#12583)\n\n## [2024-03-31]\n\n### `llama-index-core` [0.10.26]\n\n- pass proper query bundle in QueryFusionRetriever (#12387)\n- Update llama_parse_json_element.py to fix error on lists (#12402)\n- Add node postprocessors to retriever tool (#12415)\n- Fix bug where user specified llm is not respected in fallback logic in element node parsers(#12403)\n- log proper LLM response key for async callback manager events (#12421)\n- Deduplicate the two built-in react system prompts; Also make it read from a Markdown file (#12307)\n- fix bug in BatchEvalRunner for multi-evaluator eval_kwargs_lists (#12418)\n- add the callback manager event for vector store index insert_nodes (#12443)\n- fixes an issue with serializing chat messages into chat stores when they contain pydantic API objects (#12394)\n- fixes an issue with slow memory.", "mimetype": "text/plain", "start_char_idx": 54154, "end_char_idx": 57571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7715c403-8d93-49fd-8c16-c389ec67101c": {"__data__": {"id_": "7715c403-8d93-49fd-8c16-c389ec67101c", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea03da5d-436e-4faf-bdf1-2347f2c90a72", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "92984faa787b0f750b67eead8ce448186ebe5ef57ff669da412294e792ce841e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a624113c-d800-4688-a23f-449bf1b4eb08", "node_type": "1", "metadata": {}, "hash": "67c1c3afb637be793e179a8ace34a8dfa52731b068b46ae27bce846aaa8ddb3c", "class_name": "RelatedNodeInfo"}}, "text": "get() operation (caused by multiple calls to get_all()) (#12394)\n- fixes an issue where an agent+tool message pair is cut from the memory (#12394)\n- Added `FnNodeMapping` for object index (#12391)\n- Make object mapping optional / hidden for object index (#12391)\n- Make object index easier to create from existing vector db (#12391)\n- When LLM failed to follow the react response template, tell it so #12300\n\n### `llama-index-embeddings-cohere` [0.1.5]\n\n- Bump cohere version to 5.1.1 (#12279)\n\n### `llama-index-embeddings-itrex` [0.1.0]\n\n- add Intel Extension for Transformers embedding model (#12410)\n\n### `llama-index-graph-stores-neo4j` [0.1.4]\n\n- make neo4j query insensitive (#12337)\n\n### `llama-index-llms-cohere` [0.1.5]\n\n- Bump cohere version to 5.1.1 (#12279)\n\n### `llama-index-llms-ipex-llm` [0.1.0]\n\n- add ipex-llm integration (#12322)\n\n### `llama-index-llms-litellm` [0.1.4]\n\n- Fix litellm ChatMessage role validation error (#12449)\n\n### `llama-index-llms-openai` [0.1.14]\n\n- Use `FunctionCallingLLM` base class in OpenAI (#12227)\n\n### `llama-index-packs-self-rag` [0.1.4]\n\n- Fix llama-index-core dep (#12374)\n\n### `llama-index-postprocessor-cohere-rerank` [0.1.4]\n\n- Bump cohere version to 5.1.1 (#12279)\n\n### `llama-index-postprocessor-rankllm-rerank` [0.1.1]\n\n- Added RankLLM rerank (#12296)\n- RankLLM fixes (#12399)\n\n### `llama-index-readers-papers` [0.1.4]\n\n- Fixed bug with path names (#12366)\n\n### `llama-index-vector-stores-analyticdb` [0.1.1]\n\n- Add AnalyticDB VectorStore (#12230)\n\n### `llama-index-vector-stores-kdbai` [0.1.4]\n\n- Fixed typo in imports/readme (#12370)\n\n### `llama-index-vector-stores-qdrant` [0.1.5]\n\n- add `in` filter operator for qdrant (#12376)\n\n## [2024-03-27]\n\n### `llama-index-core` [0.10.25]\n\n- Add score to NodeWithScore in KnowledgeGraphQueryEngine (#12326)\n- Batch eval runner fixes (#12302)\n\n### `llama-index-embeddings-cohere` [0.1.5]\n\n- Added support for binary / quantized embeddings (#12321)\n\n### `llama-index-llms-mistralai` [0.1.10]\n\n- add support for custom endpoints to MistralAI (#12328)\n\n### `llama-index-storage-kvstore-redis` [0.1.3]\n\n- Fix RedisDocstore node retrieval from docs property (#12324)\n\n## [2024-03-26]\n\n### `llama-index-core` [0.10.24]\n\n- pretty prints in `LlamaDebugHandler` (#12216)\n- stricter interpreter constraints on pandas query engine (#12278)\n- PandasQueryEngine can now execute 'pd.\\*' functions (#12240)\n- delete proper metadata in docstore delete function (#12276)\n- improved openai agent parsing function hook (#12062)\n- add raise_on_error flag for SimpleDirectoryReader (#12263)\n- remove un-caught openai import in core (#12262)\n- Fix download_llama_dataset and download_llama_pack (#12273)\n- Implement EvalQueryEngineTool (#11679)\n- Expand instrumenation Span coverage for AgentRunner (#12249)\n- Adding concept of function calling agent/llm (mistral supported for now) (#12222, )\n\n### `llama-index-embeddings-huggingface` [0.2.0]\n\n- Use `sentence-transformers` as a backend (#12277)\n\n### `llama-index-postprocessor-voyageai-rerank` [0.1.0]\n\n- Added voyageai as a reranker (#12111)\n\n### `llama-index-readers-gcs` [0.1.0]\n\n- Added google cloud storage reader (#12259)\n\n### `llama-index-readers-google` [0.2.", "mimetype": "text/plain", "start_char_idx": 57571, "end_char_idx": 60767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a624113c-d800-4688-a23f-449bf1b4eb08": {"__data__": {"id_": "a624113c-d800-4688-a23f-449bf1b4eb08", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7715c403-8d93-49fd-8c16-c389ec67101c", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "f5bc488c4e3c91a5145fda17f460b957f11bcf883ba7186aecc68ad14b469a9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8539a10-0e5e-41d4-8730-0c5f31de2a2a", "node_type": "1", "metadata": {}, "hash": "ac2b2bff40ba9a7fd1db5cd7bbc09272631f7d1a65c187f6b6e3c7c08a31efb3", "class_name": "RelatedNodeInfo"}}, "text": "1.0]\n\n- Added google cloud storage reader (#12259)\n\n### `llama-index-readers-google` [0.2.1]\n\n- Support for different drives (#12146)\n- Remove unnecessary PyDrive dependency from Google Drive Reader (#12257)\n\n### `llama-index-readers-readme` [0.1.0]\n\n- added readme.com reader (#12246)\n\n### `llama-index-packs-raft` [0.1.3]\n\n- added pack for RAFT (#12275)\n\n## [2024-03-23]\n\n### `llama-index-core` [0.10.23]\n\n- Added `(a)predict_and_call()` function to base LLM class + openai + mistralai (#12188)\n- fixed bug with `wait()` in async agent streaming (#12187)\n\n### `llama-index-embeddings-alephalpha` [0.1.0]\n\n- Added alephalpha embeddings (#12149)\n\n### `llama-index-llms-alephalpha` [0.1.0]\n\n- Added alephalpha LLM (#12149)\n\n### `llama-index-llms-openai` [0.1.7]\n\n- fixed bug with `wait()` in async agent streaming (#12187)\n\n### `llama-index-readers-docugami` [0.1.4]\n\n- fixed import errors in docugami reader (#12154)\n\n### `llama-index-readers-file` [0.1.12]\n\n- fix PDFReader for remote fs (#12186)\n\n## [2024-03-21]\n\n### `llama-index-core` [0.10.22]\n\n- Updated docs backend from sphinx to mkdocs, added ALL api reference, some light re-org, better search (#11301)\n- Added async loading to `BaseReader` class (although its fake async for now) (#12156)\n- Fix path implementation for non-local FS in `SimpleDirectoryReader` (#12141)\n- add args/kwargs to spans, payloads for retrieval events, in instrumentation (#12147)\n- [react agent] Upon exception, say so, so that Agent can correct itself (#12137)\n\n### `llama-index-embeddings-together` [0.1.3]\n\n- Added rate limit handling (#12127)\n\n### `llama-index-graph-stores-neptune` [0.1.3]\n\n- Add Amazon Neptune Support as Graph Store (#12097)\n\n### `llama-index-llms-vllm` [0.1.7]\n\n- fix VllmServer to work without CUDA-required vllm core (#12003)\n\n### `llama-index-readers-s3` [0.1.4]\n\n- Use S3FS in S3Reader (#12061)\n\n### `llama-index-storage-docstore-postgres` [0.1.3]\n\n- Added proper kvstore dep (#12157)\n\n### `llama-index-storage-index-store-postgres` [0.1.3]\n\n- Added proper kvstore dep (#12157)\n\n### `llama-index-vector-stores-elasticsearch` [0.1.6]\n\n- fix unclosed session in es add function #12135\n\n### `llama-index-vector-stores-kdbai` [0.1.3]\n\n- Add support for `KDBAIVectorStore` (#11967)\n\n## [2024-03-20]\n\n### `llama-index-core` [0.10.21]\n\n- Lazy init for async elements StreamingAgentChatResponse (#12116)\n- Fix streaming generators get bug by SynthesisEndEvent (#12092)\n- CLIP embedding more models (#12063)\n\n### `llama-index-packs-raptor` [0.1.3]\n\n- Add `num_workers` to summary module (#)\n\n### `llama-index-readers-telegram` [0.1.5]\n\n- Fix datetime fields (#12112)\n- Add ability to select time period of posts/messages (#12078)\n\n### `llama-index-embeddings-openai` [0.1.7]\n\n- Add api version/base api as optional for open ai embedding (#12091)\n\n### `llama-index-networks` [0.2.1]\n\n- Add node postprocessing to network retriever (#12027)\n- Add privacy-safe networks demo (#12027)\n\n### `llama-index-callbacks-langfuse` [0.1.3]\n\n- Chore: bumps min version of langfuse dep (#12077)\n\n### `llama-index-embeddings-google` [0.1.4]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-embeddings-gemini` [0.1.5]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-llms-gemini` [0.1.", "mimetype": "text/plain", "start_char_idx": 60677, "end_char_idx": 63937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8539a10-0e5e-41d4-8730-0c5f31de2a2a": {"__data__": {"id_": "c8539a10-0e5e-41d4-8730-0c5f31de2a2a", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a624113c-d800-4688-a23f-449bf1b4eb08", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "ce788247dfd81f7a7a880590354741932e7f76a0fd4c29734476536a79ff828b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09b3b9d6-140d-44b1-878e-bc27a3f7960e", "node_type": "1", "metadata": {}, "hash": "5fad79e9afb171896ebcf4ba54d6a12aa2a59977f29af53fa608e3870bd88827", "class_name": "RelatedNodeInfo"}}, "text": "5]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-llms-gemini` [0.1.6]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-llms-palm` [0.1.4]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-multi-modal-llms-google` [0.1.4]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-vector-stores-google` [0.1.5]\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-storage-kvstore-elasticsearch` [0.1.0]\n\n- New integration (#12068)\n\n### `llama-index-readers-google` [0.1.7]\n\n- Fix - Google Drive Issue of not loading same name files (#12022)\n\n### `llama-index-vector-stores-upstash` [0.1.3]\n\n- Adding Metadata Filtering support for UpstashVectorStore (#12054)\n\n### `llama-index-packs-raptor` [0.1.2]\n\n- Fix: prevent RaptorPack infinite recursion (#12008)\n\n### `llama-index-embeddings-huggingface-optimum` [0.1.4]\n\n- Fix(OptimumEmbedding): removing token_type_ids causing ONNX validation issues\n\n### `llama-index-llms-anthropic` [0.1.7]\n\n- Fix: Anthropic LLM merge consecutive messages with same role (#12013)\n\n### `llama-index-packs-diff-private-simple-dataset` [0.1.0]\n\n- DiffPrivacy ICL Pack - OpenAI Completion LLMs (#11881)\n\n### `llama-index-cli` [0.1.11]\n\n- Remove llama_hub_url keyword from download_llama_dataset of command (#12038)\n\n## [2024-03-14]\n\n### `llama-index-core` [0.10.20]\n\n- New `instrumentation` module for observability (#11831)\n- Allow passing in LLM for `CitationQueryEngine` (#11914)\n- Updated keyval docstore to allow changing suffix in addition to namespace (#11873)\n- Add (some) async streaming support to query_engine #11949\n\n### `llama-index-embeddings-dashscope` [0.1.3]\n\n- Fixed embedding type for query texts (#11901)\n\n### `llama-index-embeddings-premai` [0.1.3]\n\n- Support for premai embeddings (#11954)\n\n### `llama-index-networks` [0.2.0]\n\n- Added support for network retrievers (#11800)\n\n### `llama-index-llms-anthropic` [0.1.6]\n\n- Added support for haiku (#11916)\n\n### `llama-index-llms-mistralai` [0.1.6]\n\n- Fixed import error for ChatMessage (#11902)\n\n### `llama-index-llms-openai` [0.1.11]\n\n- added gpt-35-turbo-0125 for AZURE_TURBO_MODELS (#11956)\n- fixed error with nontype in logprobs (#11967)\n\n### `llama-index-llms-premai` [0.1.4]\n\n- Support for premai llm (#11954)\n\n### `llama-index-llms-solar` [0.1.3]\n\n- Support for solar as an LLM class (#11710)\n\n### `llama-index-llms-vertex` [0.1.5]\n\n- Add support for medlm in vertex (#11911)\n\n### `llama-index-readers-goolge` [0.1.6]\n\n- added README files and query string for google drive reader (#11724)\n\n### `llama-index-readers-file` [0.1.11]\n\n- Updated ImageReader to add `plain_text` option to trigger pytesseract (#11913)\n\n### `llama-index-readers-pathway` [0.1.3]\n\n- use pure requests to reduce deps, simplify code (#11924)\n\n### `llama-index-retrievers-pathway` [0.1.3]\n\n- use pure requests to reduce deps, simplify code (#11924)\n\n### `llama-index-storage-docstore-mongodb` [0.1.3]\n\n- Allow changing suffix for mongodb docstore (#11873)\n\n### `llama-index-vector-stores-databricks` [0.1.1]\n\n- Support for databricks vector search as a vector store (#10754)\n\n### `llama-index-vector-stores-opensearch` [0.1.", "mimetype": "text/plain", "start_char_idx": 63849, "end_char_idx": 67023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09b3b9d6-140d-44b1-878e-bc27a3f7960e": {"__data__": {"id_": "09b3b9d6-140d-44b1-878e-bc27a3f7960e", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8539a10-0e5e-41d4-8730-0c5f31de2a2a", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "bb871bae7337ee02da77f13d3bcda653e138ae1168195f0b7202f3f4fd65974a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "944baf46-a119-49dd-b99e-856985851a41", "node_type": "1", "metadata": {}, "hash": "ea28af0ef50bb146d871f8eb3558f7d32950dff53f5ec46cc28377b83234e3c2", "class_name": "RelatedNodeInfo"}}, "text": "1.8]\n\n- (re)implement proper delete (#11959)\n\n### `llama-index-vector-stores-postgres` [0.1.4]\n\n- Fixes for IN filters and OR text search (#11872, #11927)\n\n## [2024-03-12]\n\n### `llama-index-cli` [0.1.9]\n\n- Removed chroma as a bundled dep to reduce `llama-index` deps\n\n### `llama-index-core` [0.10.19]\n\n- Introduce retries for rate limits in `OpenAI` llm class (#11867)\n- Added table comments to SQL table schemas in `SQLDatabase` (#11774)\n- Added `LogProb` type to `ChatResponse` object (#11795)\n- Introduced `LabelledSimpleDataset` (#11805)\n- Fixed insert `IndexNode` objects with unserializable objects (#11836)\n- Fixed stream chat type error when writing response to history in `CondenseQuestionChatEngine` (#11856)\n- Improve post-processing for json query engine (#11862)\n\n### `llama-index-embeddings-cohere` [0.1.4]\n\n- Fixed async kwarg error (#11822)\n\n### `llama-index-embeddings-dashscope` [0.1.2]\n\n- Fixed pydantic import (#11765)\n\n### `llama-index-graph-stores-neo4j` [0.1.3]\n\n- Properly close connection after verifying connectivity (#11821)\n\n### `llama-index-llms-cohere` [0.1.3]\n\n- Add support for new `command-r` model (#11852)\n\n### `llama-index-llms-huggingface` [0.1.4]\n\n- Fixed streaming decoding with special tokens (#11807)\n\n### `llama-index-llms-mistralai` [0.1.5]\n\n- Added support for latest and open models (#11792)\n\n### `llama-index-tools-finance` [0.1.1]\n\n- Fixed small bug when passing in the API get for stock news (#11772)\n\n### `llama-index-vector-stores-chroma` [0.1.6]\n\n- Slimmed down chroma deps (#11775)\n\n### `llama-index-vector-stores-lancedb` [0.1.3]\n\n- Fixes for deleting (#11825)\n\n### `llama-index-vector-stores-postgres` [0.1.3]\n\n- Support for nested metadata filters (#11778)\n\n## [2024-03-07]\n\n### `llama-index-callbacks-deepeval` [0.1.3]\n\n- Update import path for callback handler (#11754)\n\n### `llama-index-core` [0.10.18]\n\n- Ensure `LoadAndSearchToolSpec` loads document objects (#11733)\n- Fixed bug for no nodes in `QueryFusionRetriever` (#11759)\n- Allow using different runtime kwargs for different evaluators in `BatchEvalRunner` (#11727)\n- Fixed issues with fsspec + `SimpleDirectoryReader` (#11665)\n- Remove `asyncio.run()` requirement from guideline evaluator (#11719)\n\n### `llama-index-embeddings-voyageai` [0.1.3]\n\n- Update voyage embeddings to use proper clients (#11721)\n\n### `llama-index-indices-managed-vectara` [0.1.3]\n\n- Fixed issues with vectara query engine in non-summary mode (#11668)\n\n### `llama-index-llms-mymagic` [0.1.5]\n\n- Add `return_output` option for json output with query and response (#11761)\n\n### `llama-index-packs-code-hierarchy` [0.1.0]\n\n- Added support for a `CodeHiearchyAgentPack` that allows for agentic traversal of a codebase (#10671)\n\n### `llama-index-packs-cohere-citation-chat` [0.1.3]\n\n- Added a new llama-pack for citations + chat with cohere (#11697)\n\n### `llama-index-vector-stores-milvus` [0.1.6]\n\n- Prevent forced `flush()` on document add (#11734)\n\n### `llama-index-vector-stores-opensearch` [0.1.7]\n\n- Small typo in metadata column name (#11751)\n\n### `llama-index-vector-stores-tidbvector` [0.1.0]\n\n- Initial support for TiDB vector store (#11635)\n\n### `llama-index-vector-stores-weaviate` [0.1.4]\n\n- Small fix for `int` fields in metadata filters (#11742)\n\n## [2024-03-06]\n\nNew format! Going to try out reporting changes per package.\n\n### `llama-index-cli` [0.1.", "mimetype": "text/plain", "start_char_idx": 67021, "end_char_idx": 70373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "944baf46-a119-49dd-b99e-856985851a41": {"__data__": {"id_": "944baf46-a119-49dd-b99e-856985851a41", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09b3b9d6-140d-44b1-878e-bc27a3f7960e", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "cac6c5629e7fc93b2322ca7d3860d5ce8e1c68ca5a84fe50f6d0426415d34dbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06034f25-15ea-497d-95d6-cc94392e5dfc", "node_type": "1", "metadata": {}, "hash": "42e0aabe5c60aeb1a28ba0d5825cbfd0729eca7733b113535bbaaae5776986d9", "class_name": "RelatedNodeInfo"}}, "text": "Going to try out reporting changes per package.\n\n### `llama-index-cli` [0.1.8]\n\n- Update mappings for `upgrade` command (#11699)\n\n### `llama-index-core` [0.10.17]\n\n- add `relative_score` and `dist_based_score` to `QueryFusionRetriever` (#11667)\n- check for `none` in async agent queue (#11669)\n- allow refine template for `BaseSQLTableQueryEngine` (#11378)\n- update mappings for llama-packs (#11699)\n- fixed index error for extracting rel texts in KG index (#11695)\n- return proper response types from synthesizer when no nodes (#11701)\n- Inherit metadata to summaries in DocumentSummaryIndex (#11671)\n- Inherit callback manager in sql query engines (#11662)\n- Fixed bug with agent streaming not being written to chat history (#11675)\n- Fixed a small bug with `none` deltas when streaming a function call with an agent (#11713)\n\n### `llama-index-multi-modal-llms-anthropic` [0.1.2]\n\n- Added support for new multi-modal models `haiku` and `sonnet` (#11656)\n\n### `llama-index-packs-finchat` [0.1.0]\n\n- Added a new llama-pack for hierarchical agents + finance chat (#11387)\n\n### `llama-index-readers-file` [0.1.8]\n\n- Added support for checking if NLTK files are already downloaded (#11676)\n\n### `llama-index-readers-json` [0.1.4]\n\n- Use the metadata passed in when creating documents (#11626)\n\n### `llama-index-vector-stores-astra-db` [0.1.4]\n\n- Update wording in warning message (#11702)\n\n### `llama-index-vector-stores-opensearch` [0.1.7]\n\n- Avoid calling `nest_asyncio.apply()` in code to avoid confusing errors for users (#11707)\n\n### `llama-index-vector-stores-qdrant` [0.1.4]\n\n- Catch RPC errors (#11657)\n\n## [0.10.16] - 2024-03-05\n\n### New Features\n\n- Anthropic support for new models (#11623, #11612)\n- Easier creation of chat prompts (#11583)\n- Added a raptor retriever llama-pack (#11527)\n- Improve batch cohere embeddings through bedrock (#11572)\n- Added support for vertex AI embeddings (#11561)\n\n### Bug Fixes / Nits\n\n- Ensure order in async embeddings generation (#11562)\n- Fixed empty metadata for csv reader (#11563)\n- Serializable fix for composable retrievers (#11617)\n- Fixed milvus metadata filter support (#11566)\n- FIxed pydantic import in clickhouse vector store (#11631)\n- Fixed system prompts for gemini/vertext-gemini (#11511)\n\n## [0.10.15] - 2024-03-01\n\n### New Features\n\n- Added FeishuWikiReader (#11491)\n- Added videodb retriever integration (#11463)\n- Added async to opensearch vector store (#11513)\n- New LangFuse one-click callback handler (#11324)\n\n### Bug Fixes / Nits\n\n- Fixed deadlock issue with async chat streaming (#11548)\n- Improved hidden file check in SimpleDirectoryReader (#11496)\n- Fixed null values in document metadata when using SimpleDirectoryReader (#11501)\n- Fix for sqlite utils in jsonalyze query engine (#11519)\n- Added base url and timeout to ollama multimodal LLM (#11526)\n- Updated duplicate handling in query fusion retriever (#11542)\n- Fixed bug in kg indexx struct updating (#11475)\n\n## [0.10.", "mimetype": "text/plain", "start_char_idx": 70297, "end_char_idx": 73247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06034f25-15ea-497d-95d6-cc94392e5dfc": {"__data__": {"id_": "06034f25-15ea-497d-95d6-cc94392e5dfc", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "944baf46-a119-49dd-b99e-856985851a41", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "c01199b76890510c3ae44c6bd2f46aba8cbfb4543ab1e41897ca11fb0d035edd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "818fc19d-0d24-43e8-bab3-b0d64ba44c71", "node_type": "1", "metadata": {}, "hash": "fd550f85dcb98664568cecac3edde96ae461392bcab08e9377c361d05950b74c", "class_name": "RelatedNodeInfo"}}, "text": "10.14] - 2024-02-28\n\n### New Features\n\n- Released llama-index-networks (#11413)\n- Jina reranker (#11291)\n- Added DuckDuckGo agent search tool (#11386)\n- helper functions for chatml (#10272)\n- added brave search tool for agents (#11468)\n- Added Friendli LLM integration (#11384)\n- metadata only queries for chromadb (#11328)\n\n### Bug Fixes / Nits\n\n- Fixed inheriting llm callback in synthesizers (#11404)\n- Catch delete error in milvus (#11315)\n- Fixed pinecone kwargs issue (#11422)\n- Supabase metadata filtering fix (#11428)\n- api base fix in gemini embeddings (#11393)\n- fix elasticsearch vector store await (#11438)\n- vllm server cuda fix (#11442)\n- fix for passing LLM to context chat engine (#11444)\n- set input types for cohere embeddings (#11288)\n- default value for azure ad token (#10377)\n- added back prompt mixin for react agent (#10610)\n- fixed system roles for gemini (#11481)\n- fixed mean agg pooling returning numpy float values (#11458)\n- improved json path parsing for JSONQueryEngine (#9097)\n\n## [0.10.13] - 2024-02-26\n\n### New Features\n\n- Added a llama-pack for KodaRetriever, for on-the-fly alpha tuning (#11311)\n- Added support for `mistral-large` (#11398)\n- Last token pooling mode for huggingface embeddings models like SFR-Embedding-Mistral (#11373)\n- Added fsspec support to SimpleDirectoryReader (#11303)\n\n### Bug Fixes / Nits\n\n- Fixed an issue with context window + prompt helper (#11379)\n- Moved OpenSearch vector store to BasePydanticVectorStore (#11400)\n- Fixed function calling in fireworks LLM (#11363)\n- Made cohere embedding types more automatic (#11288)\n- Improve function calling in react agent (#11280)\n- Fixed MockLLM imports (#11376)\n\n## [0.10.12] - 2024-02-22\n\n### New Features\n\n- Added `llama-index-postprocessor-colbert-rerank` package (#11057)\n- `MyMagicAI` LLM (#11263)\n- `MariaTalk` LLM (#10925)\n- Add retries to github reader (#10980)\n- Added FireworksAI embedding and LLM modules (#10959)\n\n### Bug Fixes / Nits\n\n- Fixed string formatting in weaviate (#11294)\n- Fixed off-by-one error in semantic splitter (#11295)\n- Fixed `download_llama_pack` for multiple files (#11272)\n- Removed `BUILD` files from packages (#11267)\n- Loosened python version reqs for all packages (#11267)\n- Fixed args issue with chromadb (#11104)\n\n## [0.10.11] - 2024-02-21\n\n### Bug Fixes / Nits\n\n- Fixed multi-modal LLM for async acomplete (#11064)\n- Fixed issue with llamaindex-cli imports (#11068)\n\n## [0.10.10] - 2024-02-20\n\nI'm still a bit wonky with our publishing process -- apologies. This is just a version\nbump to ensure the changes that were supposed to happen in 0.10.9 actually\ndid get published. (AF)\n\n## [0.10.9] - 2024-02-20\n\n- add llama-index-cli dependency\n\n## [0.10.7] - 2024-02-19\n\n### New Features\n\n- Added Self-Discover llamapack (#10951)\n\n### Bug Fixes / Nits\n\n- Fixed linting in CICD (#10945)\n- Fixed using remote graph stores (#10971)\n- Added missing LLM kwarg in NoText response synthesizer (#10971)\n- Fixed openai import in rankgpt (#10971)\n- Fixed resolving model name to string in openai embeddings (#10971)\n- Off by one error in sentence window node parser (#10971)\n\n## [0.10.6] - 2024-02-17\n\nFirst, apologies for missing the changelog the last few versions. Trying to figure out the best process with 400+ packages.\n\nAt some point, each package will have a dedicated changelog.\n\nBut for now, onto the \"master\" changelog.", "mimetype": "text/plain", "start_char_idx": 73244, "end_char_idx": 76613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "818fc19d-0d24-43e8-bab3-b0d64ba44c71": {"__data__": {"id_": "818fc19d-0d24-43e8-bab3-b0d64ba44c71", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06034f25-15ea-497d-95d6-cc94392e5dfc", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "93e8aad734a62e9b0ed729b44d367ccd03c94bcc294c73f1a00e4c21ddd8cf6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19e2764b-cc4e-4442-b19f-78d2ce6179ef", "node_type": "1", "metadata": {}, "hash": "6a55ac059eaa0bfab74bf0e49bd4812026bd30dfb854e5c790e45d85fa3cb493", "class_name": "RelatedNodeInfo"}}, "text": "At some point, each package will have a dedicated changelog.\n\nBut for now, onto the \"master\" changelog.\n\n### New Features\n\n- Added `NomicHFEmbedding` (#10762)\n- Added `MinioReader` (#10744)\n\n### Bug Fixes / Nits\n\n- Various fixes for clickhouse vector store (#10799)\n- Fix index name in neo4j vector store (#10749)\n- Fixes to sagemaker embeddings (#10778)\n- Fixed performance issues when splitting nodes (#10766)\n- Fix non-float values in reranker + b25 (#10930)\n- OpenAI-agent should be a dep of openai program (#10930)\n- Add missing shortcut imports for query pipeline components (#10930)\n- Fix NLTK and tiktoken not being bundled properly with core (#10930)\n- Add back `llama_index.core.__version__` (#10930)\n\n## [0.10.3] - 2024-02-13\n\n### Bug Fixes / Nits\n\n- Fixed passing in LLM to `as_chat_engine` (#10605)\n- Fixed system prompt formatting for anthropic (#10603)\n- Fixed elasticsearch vector store error on `__version__` (#10656)\n- Fixed import on openai pydantic program (#10657)\n- Added client back to opensearch vector store exports (#10660)\n- Fixed bug in SimpleDirectoryReader not using file loaders properly (#10655)\n- Added lazy LLM initialization to RankGPT (#10648)\n- Fixed bedrock embedding `from_credentials` passing ing the model name (#10640)\n- Added back recent changes to TelegramReader (#10625)\n\n## [0.10.0, 0.10.1] - 2024-02-12\n\n### Breaking Changes\n\n- Several changes are introduced. See the [full blog post](https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8) for complete details.\n\n## [0.9.48] - 2024-02-12\n\n### Bug Fixes / Nits\n\n- Add back deprecated API for BedrockEmbdding (#10581)\n\n## [0.9.47] - 2024-02-11\n\nLast patch before v0.10!\n\n### New Features\n\n- add conditional links to query pipeline (#10520)\n- refactor conditional links + add to cookbook (#10544)\n- agent + query pipeline cleanups (#10563)\n\n### Bug Fixes / Nits\n\n- Add sleep to fix lag in chat stream (#10339)\n- OllamaMultiModal kwargs (#10541)\n- Update Ingestion Pipeline to handle empty documents (#10543)\n- Fixing minor spelling error (#10516)\n- fix elasticsearch async check (#10549)\n- Docs/update slack demo colab (#10534)\n- Adding the possibility to use the IN operator for PGVectorStore (#10547)\n- fix agent reset (#10562)\n- Fix MD duplicated Node id from multiple docs (#10564)\n\n## [0.9.46] - 2024-02-08\n\n### New Features\n\n- Update pooling strategy for embedding models (#10536)\n- Add Multimodal Video RAG example (#10530)\n- Add SECURITY.md (#10531)\n- Move agent module guide up one-level (#10519)\n\n### Bug Fixes / Nits\n\n- Deeplake fixes (#10529)\n- Add Cohere section for llamaindex (#10523)\n- Fix md element (#10510)\n\n## [0.9.45.post1] - 2024-02-07\n\n### New Features\n\n- Upgraded deeplake vector database to use BasePydanticVectorStore (#10504)\n\n### Bug Fixes / Nits\n\n- Fix MD parser for inconsistency tables (#10488)\n- Fix ImportError for pypdf in MetadataExtractionSEC.ipynb (#10491)\n\n## [0.9.45] - 2024-02-07\n\n### New Features\n\n- Refactor: add AgentRunner.from_llm method (#10452)\n- Support custom prompt formatting for non-chat LLMS (#10466)\n- Bump cryptography from 41.0.7 to 42.0.0 (#10467)\n- Add persist and load method for Colbert Index (#10477)\n- Allow custom agent to take in user inputs (#10450)\n\n### Bug Fixes / Nits\n\n- remove exporter from arize-phoenix global callback handler (#10465)\n- Fixing Dashscope qwen llm bug (#10471)\n- Fix: calling AWS Bedrock models (#10443)\n- Update Azure AI Search (fka Azure Cognitive Search) vector store integration to latest client SDK 11.4.0 stable + updating jupyter notebook sample (#10416)\n- fix some imports (#10485)\n\n## [0.9.", "mimetype": "text/plain", "start_char_idx": 76510, "end_char_idx": 80094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19e2764b-cc4e-4442-b19f-78d2ce6179ef": {"__data__": {"id_": "19e2764b-cc4e-4442-b19f-78d2ce6179ef", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "818fc19d-0d24-43e8-bab3-b0d64ba44c71", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "96bf99d5b6afba13700bea91440d3b8bd990cfd013d7b63c4a20b90037d2f6dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "875c60f6-1092-4990-bc34-90063190e68b", "node_type": "1", "metadata": {}, "hash": "0d22f23b56a34a6bb6fbe70888402f268f664137503769eff7725a0e62dffb32", "class_name": "RelatedNodeInfo"}}, "text": "4.0 stable + updating jupyter notebook sample (#10416)\n- fix some imports (#10485)\n\n## [0.9.44] - 2024-02-05\n\n### New Features\n\n- ollama vision cookbook (#10438)\n- Support Gemini \"transport\" configuration (#10457)\n- Add Upstash Vector (#10451)\n\n## [0.9.43] - 2024-02-03\n\n### New Features\n\n- Add multi-modal ollama (#10434)\n\n### Bug Fixes / Nits\n\n- update base class for astradb (#10435)\n\n## [0.9.42.post1] - 2024-02-02\n\n### New Features\n\n- Add Async support for Base nodes parser (#10418)\n\n## [0.9.42] - 2024-02-02\n\n### New Features\n\n- Add support for `gpt-3.5-turbo-0125` (#10412)\n- Added `create-llama` support to rag cli (#10405)\n\n### Bug Fixes / Nits\n\n- Fixed minor bugs in lance-db vector store (#10404)\n- Fixed streaming bug in ollama (#10407)\n\n## [0.9.41] - 2024-02-01\n\n### New Features\n\n- Nomic Embedding (#10388)\n- Dashvector support sparse vector (#10386)\n- Table QA with MarkDownParser and Benchmarking (#10382)\n- Simple web page reader (#10395)\n\n### Bug Fixes / Nits\n\n- fix full node content in KeywordExtractor (#10398)\n\n## [0.9.40] - 2024-01-30\n\n### New Features\n\n- Improve and fix bugs for MarkdownElementNodeParser (#10340)\n- Fixed and improve Perplexity support for new models (#10319)\n- Ensure system_prompt is passed to Perplexity LLM (#10326)\n- Extended BaseRetrievalEvaluator to include an optional PostProcessor (#10321)\n\n## [0.9.39] - 2024-01-26\n\n### New Features\n\n- Support for new GPT Turbo Models (#10291)\n- Support Multiple docs for Sentence Transformer Fine tuning(#10297)\n\n### Bug Fixes / Nits\n\n- Marvin imports fixed (#9864)\n\n## [0.9.38] - 2024-01-25\n\n### New Features\n\n- Support for new OpenAI v3 embedding models (#10279)\n\n### Bug Fixes / Nits\n\n- Extra checks on sparse embeddings for qdrant (#10275)\n\n## [0.9.37] - 2024-01-24\n\n### New Features\n\n- Added a RAG CLI utility (#10193)\n- Added a textai vector store (#10240)\n- Added a Postgresql based docstore and index store (#10233)\n- specify tool spec in tool specs (#10263)\n\n### Bug Fixes / Nits\n\n- Fixed serialization error in ollama chat (#10230)\n- Added missing fields to `SentenceTransformerRerank` (#10225)\n- Fixed title extraction (#10209, #10226)\n- nit: make chainable output parser more exposed in library/docs (#10262)\n- :bug: summary index not carrying over excluded metadata keys (#10259)\n\n## [0.9.36] - 2024-01-23\n\n### New Features\n\n- Added support for `SageMakerEmbedding` (#10207)\n\n### Bug Fixes / Nits\n\n- Fix duplicated `file_id` on openai assistant (#10223)\n- Fix circular dependencies for programs (#10222)\n- Run `TitleExtractor` on groups of nodes from the same parent document (#10209)\n- Improve vectara auto-retrieval (#10195)\n\n## [0.9.35] - 2024-01-22\n\n### New Features\n\n- `beautifulsoup4` dependency to new optional extra `html` (#10156)\n- make `BaseNode.", "mimetype": "text/plain", "start_char_idx": 80002, "end_char_idx": 82761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "875c60f6-1092-4990-bc34-90063190e68b": {"__data__": {"id_": "875c60f6-1092-4990-bc34-90063190e68b", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19e2764b-cc4e-4442-b19f-78d2ce6179ef", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "c4665f54d9e9e8990b52b2c6a695431d1585a5ef416269049be656ad8bcb77c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e95cb2f3-e0d4-4cd2-9ae9-168e21d70207", "node_type": "1", "metadata": {}, "hash": "042d20cbe5c7e41c93fd0582703732544262556756d42e58f32d41c5b7ce7705", "class_name": "RelatedNodeInfo"}}, "text": "hash` an `@property` (#10163)\n- Neutrino (#10150)\n- feat: JSONalyze Query Engine (#10067)\n- [wip] add custom hybrid retriever notebook (#10164)\n- add from_collection method to ChromaVectorStore class (#10167)\n- CLI experiment v0: ask (#10168)\n- make react agent prompts more editable (#10154)\n- Add agent query pipeline (#10180)\n\n### Bug Fixes / Nits\n\n- Update supabase vecs metadata filter function to support multiple fields (#10133)\n- Bugfix/code improvement for LanceDB integration (#10144)\n- `beautifulsoup4` optional dependency (#10156)\n- Fix qdrant aquery hybrid search (#10159)\n- make hash a @property (#10163)\n- fix: bug on poetry install of llama-index[postgres] (#10171)\n- [doc] update jaguar vector store documentation (#10179)\n- Remove use of not-launched finish_message (#10188)\n- Updates to Lantern vector stores docs (#10192)\n- fix typo in multi_document_agents.ipynb (#10196)\n\n## [0.9.34] - 2024-01-19\n\n### New Features\n\n- Added SageMakerEndpointLLM (#10140)\n- Added support for Qdrant filters (#10136)\n\n### Bug Fixes / Nits\n\n- Update bedrock utils for Claude 2:1 (#10139)\n- BugFix: deadlocks using multiprocessing (#10125)\n\n## [0.9.33] - 2024-01-17\n\n### New Features\n\n- Added RankGPT as a postprocessor (#10054)\n- Ensure backwards compatibility with new Pinecone client version bifucation (#9995)\n- Recursive retriever all the things (#10019)\n\n### Bug Fixes / Nits\n\n- BugFix: When using markdown element parser on a table containing comma (#9926)\n- extend auto-retrieval notebook (#10065)\n- Updated the Attribute name in llm_generators (#10070)\n- jaguar vector store add text_tag to add_kwargs in add() (#10057)\n\n## [0.9.32] - 2024-01-16\n\n### New Features\n\n- added query-time row retrieval + fix nits with query pipeline over structured data (#10061)\n- ReActive Agents w/ Context + updated stale link (#10058)\n\n## [0.9.31] - 2024-01-15\n\n### New Features\n\n- Added selectors and routers to query pipeline (#9979)\n- Added sparse-only search to qdrant vector store (#10041)\n- Added Tonic evaluators (#10000)\n- Adding async support to firestore docstore (#9983)\n- Implement mongodb docstore `put_all` method (#10014)\n\n### Bug Fixes / Nits\n\n- Properly truncate sql results based on `max_string_length` (#10015)\n- Fixed `node.resolve_image()` for base64 strings (#10026)\n- Fixed cohere system prompt role (#10020)\n- Remove redundant token counting operation in SentenceSplitter (#10053)\n\n## [0.9.30] - 2024-01-11\n\n### New Features\n\n- Implements a Node Parser using embeddings for Semantic Splitting (#9988)\n- Add Anyscale Embedding model support (#9470)\n\n### Bug Fixes / Nits\n\n- nit: fix pandas get prompt (#10001)\n- Fix: Token counting bug (#9912)\n- Bump jinja2 from 3.1.2 to 3.1.3 (#9997)\n- Fix corner case for qdrant hybrid search (#9993)\n- Bugfix: sphinx generation errors (#9944)\n- Fix: `language` used before assignment in `CodeSplitter` (#9987)\n- fix inconsistent name \"text_parser\" in section \"Use a Text Splitter\u2026 (#9980)\n- :bug: fixing batch size (#9982)\n- add auto-async execution to query pipelines (#9967)\n- :bug: fixing init (#9977)\n- Parallel Loading with SimpleDirectoryReader (#9965)\n- do not force delete an index in milvus (#9974)\n\n## [0.9.29] - 2024-01-10\n\n### New Features\n\n- Added support for together.ai models (#9962)\n- Added support for batch redis/firestore kvstores, async firestore kvstore (#9827)\n- Parallelize `IngestionPipeline.run()` (#9920)\n- Added new query pipeline components: function, argpack, kwargpack (#9952)\n\n### Bug Fixes / Nits\n\n- Updated optional langchain imports to avoid warnings (#9964)\n- Raise an error if empty nodes are embedded (#9953)\n\n## [0.9.", "mimetype": "text/plain", "start_char_idx": 82761, "end_char_idx": 86370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e95cb2f3-e0d4-4cd2-9ae9-168e21d70207": {"__data__": {"id_": "e95cb2f3-e0d4-4cd2-9ae9-168e21d70207", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "875c60f6-1092-4990-bc34-90063190e68b", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "9e6a5a5571259f8646e63681558fd016932b55aba39990cfdc778b54bfccf4c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d741cc39-c5d4-4b24-b9d1-ebd89740b8a3", "node_type": "1", "metadata": {}, "hash": "a4ec13eaa1848673b671307fafdb3c6ef3532046d3c3deec2e0fd2a6352ef84f", "class_name": "RelatedNodeInfo"}}, "text": "9.28] - 2024-01-09\n\n### New Features\n\n- Added support for Nvidia TenorRT LLM (#9842)\n- Allow `tool_choice` to be set during agent construction (#9924)\n- Added streaming support for `QueryPipeline` (#9919)\n\n### Bug Fixes / Nits\n\n- Set consistent doc-ids for llama-index readers (#9923, #9916)\n- Remove unneeded model inputs for HuggingFaceEmbedding (#9922)\n- Propagate `tool_choice` flag to downstream APIs (#9901)\n- Add `chat_store_key` to chat memory `from_defaults()` (#9928)\n\n## [0.9.27] - 2024-01-08\n\n### New Features\n\n- add query pipeline (#9908)\n- Feature: Azure Multi Modal (fixes: #9471) (#9843)\n- add postgres docker (#9906)\n- Vectara auto_retriever (#9865)\n- Redis Chat Store support (#9880)\n- move more classes to core (#9871)\n\n### Bug Fixes / Nits / Smaller Features\n\n- Propagate `tool_choice` flag to downstream APIs (#9901)\n- filter out negative indexes from faiss query (#9907)\n- added NE filter for qdrant payloads (#9897)\n- Fix incorrect id assignment in MyScale query result (#9900)\n- Qdrant Text Match Filter (#9895)\n- Fusion top k for hybrid search (#9894)\n- Fix (#9867) sync_to_async to avoid blocking during asynchronous calls (#9869)\n- A single node passed into compute_scores returns as a float (#9866)\n- Remove extra linting steps (#9878)\n- add vectara links (#9886)\n\n## [0.9.26] - 2024-01-05\n\n### New Features\n\n- Added a `BaseChatStore` and `SimpleChatStore` abstraction for dedicated chat memory storage (#9863)\n- Enable custom `tree_sitter` parser to be passed into `CodeSplitter` (#9845)\n- Created a `BaseAutoRetriever` base class, to allow other retrievers to extend to auto modes (#9846)\n- Added support for Nvidia Triton LLM (#9488)\n- Added `DeepEval` one-click observability (#9801)\n\n### Bug Fixes / Nits\n\n- Updated the guidance integration to work with the latest version (#9830)\n- Made text storage optional for doctores/ingestion pipeline (#9847)\n- Added missing `sphinx-automodapi` dependency for docs (#9852)\n- Return actual node ids in weaviate query results (#9854)\n- Added prompt formatting to LangChainLLM (#9844)\n\n## [0.9.25] - 2024-01-03\n\n### New Features\n\n- Added concurrancy limits for dataset generation (#9779)\n- New `deepeval` one-click observability handler (#9801)\n- Added jaguar vector store (#9754)\n- Add beta multimodal ReAct agent (#9807)\n\n### Bug Fixes / Nits\n\n- Changed default batch size for OpenAI embeddings to 100 (#9805)\n- Use batch size properly for qdrant upserts (#9814)\n- `_verify_source_safety` uses AST, not regexes, for proper safety checks (#9789)\n- use provided LLM in element node parsers (#9776)\n- updated legacy vectordb loading function to be more robust (#9773)\n- Use provided http client in AzureOpenAI (#9772)\n\n## [0.9.24] - 2023-12-30\n\n### New Features\n\n- Add reranker for BEIR evaluation (#9743)\n- Add Pathway integration. (#9719)\n- custom agents implementation + notebook (#9746)\n\n### Bug Fixes / Nits\n\n- fix beam search for vllm: add missing parameter (#9741)\n- Fix alpha for hrbrid search (#9742)\n- fix token counter (#9744)\n- BM25 tokenizer lowercase (#9745)\n\n## [0.9.23] - 2023-12-28\n\n### Bug Fixes / Nits\n\n- docs: fixes qdrant_hybrid.ipynb typos (#9729)\n- make llm completion program more general (#9731)\n- Refactor MM Vector store and Index for empty collection (#9717)\n- Adding IF statement to check for Schema using \"Select\" (#9712)\n- allow skipping module loading in `download_module` and `download_llama_pack` (#9734)\n\n## [0.9.22] - 2023-12-26\n\n### New Features\n\n- Added `.", "mimetype": "text/plain", "start_char_idx": 86368, "end_char_idx": 89832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d741cc39-c5d4-4b24-b9d1-ebd89740b8a3": {"__data__": {"id_": "d741cc39-c5d4-4b24-b9d1-ebd89740b8a3", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e95cb2f3-e0d4-4cd2-9ae9-168e21d70207", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "8341cb8f67ef30898fb7fe490eaeb98937c6d4e50882fc01555c182f752e42c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee5e3059-c337-46db-bb7d-1c48a027ff2a", "node_type": "1", "metadata": {}, "hash": "30c05ca49a1079ff5fae3be2c3f59cd3d013cb9a443cb188bbc530f7506d566f", "class_name": "RelatedNodeInfo"}}, "text": "9.22] - 2023-12-26\n\n### New Features\n\n- Added `.iter_data()` method to `SimpleDirectoryReader` (#9658)\n- Added async support to `Ollama` LLM (#9689)\n- Expanding pinecone filter support for `in` and `not in` (#9683)\n\n### Bug Fixes / Nits\n\n- Improve BM25Retriever performance (#9675)\n- Improved qdrant hybrid search error handling (#9707)\n- Fixed `None` handling in `ChromaVectorStore` (#9697)\n- Fixed postgres schema creation if not existing (#9712)\n\n## [0.9.21] - 2023-12-23\n\n### New Features\n\n- Added zilliz cloud as a managed index (#9605)\n\n### Bug Fixes / Nits\n\n- Bedrock client and LLM fixes (#9671, #9646)\n\n## [0.9.20] - 2023-12-21\n\n### New Features\n\n- Added `insert_batch_size` to limit number of embeddings held in memory when creating an index, defaults to 2048 (#9630)\n- Improve auto-retrieval (#9647)\n- Configurable Node ID Generating Function (#9574)\n- Introduced action input parser (#9575)\n- qdrant sparse vector support (#9644)\n- Introduced upserts and delete in ingestion pipeline (#9643)\n- Add Zilliz Cloud Pipeline as a Managed Index (#9605)\n- Add support for Google Gemini models via VertexAI (#9624)\n- support allowing additional metadata filters on autoretriever (#9662)\n\n### Bug Fixes / Nits\n\n- Fix pip install commands in LM Format Enforcer notebooks (#9648)\n- Fixing some more links and documentations (#9633)\n- some bedrock nits and fixes (#9646)\n\n## [0.9.19] - 2023-12-20\n\n### New Features\n\n- new llama datasets `LabelledEvaluatorDataset` & `LabelledPairwiseEvaluatorDataset` (#9531)\n\n## [0.9.18] - 2023-12-20\n\n### New Features\n\n- multi-doc auto-retrieval guide (#9631)\n\n### Bug Fixes / Nits\n\n- fix(vllm): make Vllm's 'complete' method behave the same as other LLM class (#9634)\n- FIx Doc links and other documentation issue (#9632)\n\n## [0.9.17] - 2023-12-19\n\n### New Features\n\n- [example] adding user feedback (#9601)\n- FEATURE: Cohere ReRank Relevancy Metric for Retrieval Eval (#9495)\n\n### Bug Fixes / Nits\n\n- Fix Gemini Chat Mode (#9599)\n- Fixed `types-protobuf` from being a primary dependency (#9595)\n- Adding an optional auth token to the TextEmbeddingInference class (#9606)\n- fix: out of index get latest tool call (#9608)\n- fix(azure_openai.py): add missing return to subclass override (#9598)\n- fix mix up b/w 'formatted' and 'format' params for ollama api call (#9594)\n\n## [0.9.16] - 2023-12-18\n\n### New Features\n\n- agent refactor: step-wise execution (#9584)\n- Add OpenRouter, with Mixtral demo (#9464)\n- Add hybrid search to neo4j vector store (#9530)\n- Add support for auth service accounts for Google Semantic Retriever (#9545)\n\n### Bug Fixes / Nits\n\n- Fixed missing `default=None` for `LLM.system_prompt` (#9504)\n- Fix #9580 : Incorporate metadata properly (#9582)\n- Integrations: Gradient[Embeddings,LLM] - sdk-upgrade (#9528)\n- Add mixtral 8x7b model to anyscale available models (#9573)\n- Gemini Model Checks (#9563)\n- Update OpenAI fine-tuning with latest changes (#9564)\n- fix/Reintroduce `WHERE` filter to the Sparse Query for PgVectorStore (#9529)\n- Update Ollama API to ollama v0.1.16 (#9558)\n- ollama: strip invalid `formatted` option (#9555)\n- add a device in optimum push #9541 (#9554)\n- Title vs content difference for Gemini Embedding (#9547)\n- fix pydantic fields to float (#9542)\n\n## [0.9.", "mimetype": "text/plain", "start_char_idx": 89784, "end_char_idx": 93030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee5e3059-c337-46db-bb7d-1c48a027ff2a": {"__data__": {"id_": "ee5e3059-c337-46db-bb7d-1c48a027ff2a", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d741cc39-c5d4-4b24-b9d1-ebd89740b8a3", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "0625226bd723b00885e720fb1df8414f2dc8a9e60212b3c0b63c49302dace375", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "885c847a-6ad7-4d8b-8dc3-5f82718ecd23", "node_type": "1", "metadata": {}, "hash": "cdf98490895df146a586a0d56a1a48bf699a8295ad463d8509c9d9da309b56ef", "class_name": "RelatedNodeInfo"}}, "text": "9.15] - 2023-12-13\n\n### New Features\n\n- Added full support for Google Gemini text+vision models (#9452)\n- Added new Google Semantic Retriever (#9440)\n- added `from_existing()` method + async support to OpenAI assistants (#9367)\n\n### Bug Fixes / Nits\n\n- Fixed huggingface LLM system prompt and messages to prompt (#9463)\n- Fixed ollama additional kwargs usage (#9455)\n\n## [0.9.14] - 2023-12-11\n\n### New Features\n\n- Add MistralAI LLM (#9444)\n- Add MistralAI Embeddings (#9441)\n- Add `Ollama` Embedding class (#9341)\n- Add `FlagEmbeddingReranker` for reranking (#9285)\n- feat: PgVectorStore support advanced metadata filtering (#9377)\n- Added `sql_only` parameter to SQL query engines to avoid executing SQL (#9422)\n\n### Bug Fixes / Nits\n\n- Feat/PgVector Support custom hnsw.ef_search and ivfflat.probes (#9420)\n- fix F1 score definition, update copyright year (#9424)\n- Change more than one image input for Replicate Multi-modal models from error to warning (#9360)\n- Removed GPT-Licensed `aiostream` dependency (#9403)\n- Fix result of BedrockEmbedding with Cohere model (#9396)\n- Only capture valid tool names in react agent (#9412)\n- Fixed `top_k` being multiplied by 10 in azure cosmos (#9438)\n- Fixed hybrid search for OpenSearch (#9430)\n\n### Breaking Changes\n\n- Updated the base `LLM` interface to match `LLMPredictor` (#9388)\n- Deprecated `LLMPredictor` (#9388)\n\n## [0.9.13] - 2023-12-06\n\n### New Features\n\n- Added batch prediction support for `LabelledRagDataset` (#9332)\n\n### Bug Fixes / Nits\n\n- Fixed save and load for faiss vector store (#9330)\n\n## [0.9.12] - 2023-12-05\n\n### New Features\n\n- Added an option `reuse_client` to openai/azure to help with async timeouts. Set to `False` to see improvements (#9301)\n- Added support for `vLLM` llm (#9257)\n- Add support for python 3.12 (#9304)\n- Support for `claude-2.1` model name (#9275)\n\n### Bug Fixes / Nits\n\n- Fix embedding format for bedrock cohere embeddings (#9265)\n- Use `delete_kwargs` for filtering in weaviate vector store (#9300)\n- Fixed automatic qdrant client construction (#9267)\n\n## [0.9.11] - 2023-12-03\n\n### New Features\n\n- Make `reference_contexts` optional in `LabelledRagDataset` (#9266)\n- Re-organize `download` module (#9253)\n- Added document management to ingestion pipeline (#9135)\n- Add docs for `LabelledRagDataset` (#9228)\n- Add submission template notebook and other doc updates for `LabelledRagDataset` (#9273)\n\n### Bug Fixes / Nits\n\n- Convert numpy to list for `InstructorEmbedding` (#9255)\n\n## [0.9.10] - 2023-11-30\n\n### New Features\n\n- Advanced Metadata filter for vector stores (#9216)\n- Amazon Bedrock Embeddings New models (#9222)\n- Added PromptLayer callback integration (#9190)\n- Reuse file ids for `OpenAIAssistant` (#9125)\n\n### Breaking Changes / Deprecations\n\n- Deprecate ExactMatchFilter (#9216)\n\n## [0.9.9] - 2023-11-29\n\n### New Features\n\n- Add new abstractions for `LlamaDataset`'s (#9165)\n- Add metadata filtering and MMR mode support for `AstraDBVectorStore` (#9193)\n- Allowing newest `scikit-learn` versions (#9213)\n\n### Breaking Changes / Deprecations\n\n- Added `LocalAI` demo and began deprecation cycle (#9151)\n- Deprecate `QueryResponseDataset` and `DatasetGenerator` of `evaluation` module (#9165)\n\n### Bug Fixes / Nits\n\n- Fix bug in `download_utils.py` with pointing to wrong repo (#9215)\n- Use `azure_deployment` kwarg in `AzureOpenAILLM` (#9174)\n- Fix similarity score return for `AstraDBVectorStore` Integration (#9193)\n\n## [0.9.", "mimetype": "text/plain", "start_char_idx": 93028, "end_char_idx": 96465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "885c847a-6ad7-4d8b-8dc3-5f82718ecd23": {"__data__": {"id_": "885c847a-6ad7-4d8b-8dc3-5f82718ecd23", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee5e3059-c337-46db-bb7d-1c48a027ff2a", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "e77427e08531c4e00771722dc26a7ba6e37f519d65c12be7cadae10b38d669c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a48e8673-3a35-478b-8a2b-a06f593b7947", "node_type": "1", "metadata": {}, "hash": "81573e148d9bc1e48affe50d1276c5fe04afcc26bcd01202ee18997aed8ba323", "class_name": "RelatedNodeInfo"}}, "text": "9.8] - 2023-11-26\n\n### New Features\n\n- Add `persist` and `persist_from_dir` methods to `ObjectIndex` that are able to support it (#9064)\n- Added async metadata extraction + pipeline support (#9121)\n- Added back support for start/end char idx in nodes (#9143)\n\n### Bug Fixes / Nits\n\n- Fix for some kwargs not being set properly in global service context (#9137)\n- Small fix for `memory.get()` when system/prefix messages are large (#9149)\n- Minor fixes for global service context (#9137)\n\n## [0.9.7] - 2023-11-24\n\n### New Features\n\n- Add support for `PGVectoRsStore` (#9087)\n- Enforcing `requests>=2.31` for security, while unpinning `urllib3` (#9108)\n\n### Bug Fixes / Nits\n\n- Increased default memory token limit for context chat engine (#9123)\n- Added system prompt to `CondensePlusContextChatEngine` that gets prepended to the `context_prompt` (#9123)\n- Fixed bug in `CondensePlusContextChatEngine` not using chat history properly (#9129)\n\n## [0.9.6] - 2023-11-22\n\n### New Features\n\n- Added `default_headers` argument to openai LLMs (#9090)\n- Added support for `download_llama_pack()` and LlamaPack integrations\n- Added support for `llamaindex-cli` command line tool\n\n### Bug Fixed / Nits\n\n- store normalize as bool for huggingface embedding (#9089)\n\n## [0.9.5] - 2023-11-21\n\n### Bug Fixes / Nits\n\n- Fixed bug with AzureOpenAI logic for inferring if stream chunk is a tool call (#9018)\n\n### New Features\n\n- `FastEmbed` embeddings provider (#9043)\n- More precise testing of `OpenAILike` (#9026)\n- Added callback manager to each retriever (#8871)\n- Ability to bypass `max_tokens` inference with `OpenAILike` (#9032)\n\n### Bug Fixes / Nits\n\n- Fixed bug in formatting chat prompt templates when estimating chunk sizes (#9025)\n- Sandboxed Pandas execution, remediate CVE-2023-39662 (#8890)\n- Restored `mypy` for Python 3.8 (#9031)\n- Loosened `dataclasses-json` version range,\n  and removes unnecessary `jinja2` extra from `pandas` (#9042)\n\n## [0.9.4] - 2023-11-19\n\n### New Features\n\n- Added `CondensePlusContextChatEngine` (#8949)\n\n### Smaller Features / Bug Fixes / Nits\n\n- Fixed bug with `OpenAIAgent` inserting errors into chat history (#9000)\n- Fixed various bugs with LiteLLM and the new OpenAI client (#9003)\n- Added context window attribute to perplexity llm (#9012)\n- Add `node_parser` attribute back to service context (#9013)\n- Refactor MM retriever classes (#8998)\n- Fix TextNode instantiation on SupabaseVectorIndexDemo (#8994)\n\n## [0.9.3] - 2023-11-17\n\n### New Features\n\n- Add perplexity LLM integration (#8734)\n\n### Bug Fixes / Nits\n\n- Fix token counting for new openai client (#8981)\n- Fix small pydantic bug in postgres vector db (#8962)\n- Fixed `chunk_overlap` and `doc_id` bugs in `HierarchicalNodeParser` (#8983)\n\n## [0.9.2] - 2023-11-16\n\n### New Features\n\n- Added new notebook guide for Multi-Modal Rag Evaluation (#8945)\n- Added `MultiModalRelevancyEvaluator`, and `MultiModalFaithfulnessEvaluator` (#8945)\n\n## [0.9.1] - 2023-11-15\n\n### New Features\n\n- Added Cohere Reranker fine-tuning (#8859)\n- Support for custom httpx client in `AzureOpenAI` LLM (#8920)\n\n### Bug Fixes / Nits\n\n- Fixed issue with `set_global_service_context` not propagating settings (#8940)\n- Fixed issue with building index with Google Palm embeddings (#8936)\n- Fixed small issue with parsing ImageDocuments/Nodes that have no text (#8938)\n- Fixed issue with large data inserts in Astra DB (#8937)\n- Optimize `QueryEngineTool` for agents (#8933)\n\n## [0.9.", "mimetype": "text/plain", "start_char_idx": 96463, "end_char_idx": 99907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a48e8673-3a35-478b-8a2b-a06f593b7947": {"__data__": {"id_": "a48e8673-3a35-478b-8a2b-a06f593b7947", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "885c847a-6ad7-4d8b-8dc3-5f82718ecd23", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "e9352297308f4344255d88c2f7493d8219fb349983a266e4def46883edb9bcf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f3b36ad-c843-478a-bb90-ffc96aec3687", "node_type": "1", "metadata": {}, "hash": "fbab9784996c4c6c28ab5024ec9d9d5b04ed454b50686dc60d421265c21a1db7", "class_name": "RelatedNodeInfo"}}, "text": "9.0] - 2023-11-15\n\n### New Features / Breaking Changes / Deprecations\n\n- New `IngestionPipeline` concept for ingesting and transforming data\n- Data ingestion and transforms are now automatically cached\n- Updated interface for node parsing/text splitting/metadata extraction modules\n- Changes to the default tokenizer, as well as customizing the tokenizer\n- Packaging/Installation changes with PyPi (reduced bloat, new install options)\n- More predictable and consistent import paths\n- Plus, in beta: MultiModal RAG Modules for handling text and images!\n- Find more details at: `https://medium.com/@llama_index/719f03282945`\n\n## [0.8.69.post1] - 2023-11-13\n\n### Bug Fixes / Nits\n\n- Increase max weaivate delete size to max of 10,000 (#8887)\n- Final pickling remnant fix (#8902)\n\n## [0.8.69] - 2023-11-13\n\n### Bug Fixes / Nits\n\n- Fixed bug in loading pickled objects (#8880)\n- Fix `custom_path` vs `custom_dir` in `download_loader` (#8865)\n\n## [0.8.68] - 2023-11-11\n\n### New Features\n\n- openai assistant agent + advanced retrieval cookbook (#8863)\n- add retrieval API benchmark (#8850)\n- Add JinaEmbedding class (#8704)\n\n### Bug Fixes / Nits\n\n- Improved default timeouts/retries for OpenAI (#8819)\n- Add back key validation for OpenAI (#8819)\n- Disable automatic LLM/Embedding model downloads, give informative error (#8819)\n- fix openai assistant tool creation + retrieval notebook (#8862)\n- Quick fix Replicate MultiModal example (#8861)\n- fix: paths treated as hidden (#8860)\n- fix Replicate multi-modal LLM + notebook (#8854)\n- Feature/citation metadata (#8722)\n- Fix ImageNode type from NodeWithScore for SimpleMultiModalQueryEngine (#8844)\n\n## [0.8.67] - 2023-11-10\n\n### New Features\n\n- Advanced Multi Modal Retrieval Example and docs (#8822, #8823)\n\n### Bug Fixes / Nits\n\n- Fix retriever node postprocessors for `CitationQueryEngine` (#8818)\n- Fix `cannot pickle 'builtins.CoreBPE' object` in most scenarios (#8835)\n\n## [0.8.66] - 2023-11-09\n\n### New Features\n\n- Support parallel function calling with new OpenAI client in `OpenAIPydanticProgram` (#8793)\n\n### Bug Fixes / Nits\n\n- Fix bug in pydantic programs with new OpenAI client (#8793)\n- Fixed bug with un-listable fsspec objects (#8795)\n\n## [0.8.65] - 2023-11-08\n\n### New Features\n\n- `OpenAIAgent` parallel function calling (#8738)\n\n### New Features\n\n- Properly supporting Hugging Face recommended model (#8784)\n\n### Bug Fixes / Nits\n\n- Fixed missing import for `embeddings.__all__` (#8779)\n\n### Breaking Changes / Deprecations\n\n- Use `tool_choice` over `function_call` and `tool` over `functions` in `OpenAI(LLM)` (#8738)\n- Deprecate `to_openai_function` in favor of `to_openai_tool` (#8738)\n\n## [0.8.64] - 2023-11-06\n\n### New Features\n\n- `OpenAIAgent` parallel function calling (#8738)\n- Add AI assistant agent (#8735)\n- OpenAI GPT4v Abstraction (#8719)\n- Add support for `Lantern` VectorStore (#8714)\n\n### Bug Fixes / Nits\n\n- Fix returning zero nodes in elastic search vector store (#8746)\n- Add try/except for `SimpleDirectoryReader` loop to avoid crashing on a single document (#8744)\n- Fix for `deployment_name` in async embeddings (#8748)\n\n## [0.8.63] - 2023-11-05\n\n### New Features\n\n- added native sync and async client support for the lasted `openai` client package (#8712)\n- added support for `AzureOpenAIEmbedding` (#8712)\n\n### Bug Fixes / Nits\n\n- Fixed errors about \"no host supplied\" with `download_loader` (#8723)\n\n### Breaking Changes\n\n- `OpenAIEmbedding` no longer supports azure, moved into the `AzureOpenAIEmbedding` class (#8712)\n\n## [0.8.62.", "mimetype": "text/plain", "start_char_idx": 99905, "end_char_idx": 103425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f3b36ad-c843-478a-bb90-ffc96aec3687": {"__data__": {"id_": "5f3b36ad-c843-478a-bb90-ffc96aec3687", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a48e8673-3a35-478b-8a2b-a06f593b7947", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "7a7245438e032fa09552a3eb741ea0be72259c434d68afe32c04a49f05bf91e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbf0e854-ef91-45ec-965d-23688e4af1d3", "node_type": "1", "metadata": {}, "hash": "c5a0e96da042d818bc1827b6a68b76de20beb483b4129ef6f4431bfd00023214", "class_name": "RelatedNodeInfo"}}, "text": "moved into the `AzureOpenAIEmbedding` class (#8712)\n\n## [0.8.62.post1] - 2023-11-05\n\n### Breaking Changes\n\n- add new devday models (#8713)\n- moved `max_docs` parameter from constructor to `lazy_load_data()` for `SimpleMongoReader` (#8686)\n\n## [0.8.61] - 2023-11-05\n\n### New Features\n\n- [experimental] Hyperparameter tuner (#8687)\n\n### Bug Fixes / Nits\n\n- Fix typo error in CohereAIModelName class: cohere light models was missing v3 (#8684)\n- Update deeplake.py (#8683)\n\n## [0.8.60] - 2023-11-04\n\n### New Features\n\n- prompt optimization guide (#8659)\n- VoyageEmbedding (#8634)\n- Multilingual support for `YoutubeTranscriptReader` (#8673)\n- emotion prompt guide (#8674)\n\n### Bug Fixes / Nits\n\n- Adds mistral 7b instruct v0.1 to available anyscale models (#8652)\n- Make pgvector's setup (extension, schema, and table creation) optional (#8656)\n- Allow init of stores_text variable for Pinecone vector store (#8633)\n- fix: azure ad support (#8667)\n- Fix nltk bug in multi-threaded environments (#8668)\n- Fix google colab link in cohereai notebook (#8677)\n- passing max_tokens to the `Cohere` llm (#8672)\n\n## [0.8.59] - 2023-11-02\n\n- Deepmemory support (#8625)\n- Add CohereAI embeddings (#8650)\n- Add Azure AD (Microsoft Entra ID) support (#8667)\n\n## [0.8.58] - 2023-11-02\n\n### New Features\n\n- Add `lm-format-enforcer` integration for structured output (#8601)\n- Google Vertex Support (#8626)\n\n## [0.8.57] - 2023-10-31\n\n### New Features\n\n- Add `VoyageAIEmbedding` integration (#8634)\n- Add fine-tuning evaluator notebooks (#8596)\n- Add `SingleStoreDB` integration (#7991)\n- Add support for ChromaDB PersistentClient (#8582)\n- Add DataStax Astra DB support (#8609)\n\n### Bug Fixes / Nits\n\n- Update dataType in Weaviate (#8608)\n- In Knowledge Graph Index with hybrid retriever_mode,\n  - return the nodes found by keyword search when 'No Relationship found'\n- Fix exceed context length error in chat engines (#8530)\n- Retrieve actual content of all the triplets from KG (#8579)\n- Return the nodes found by Keywords when no relationship is found by embeddings in hybrid retriever_mode in `KnowledgeGraphIndex` (#8575)\n- Optimize content of retriever tool and minor bug fix (#8588)\n\n## [0.8.56] - 2023-10-30\n\n### New Features\n\n- Add Amazon `BedrockEmbedding` (#8550)\n- Moves `HuggingFaceEmbedding` to center on `Pooling` enum for pooling (#8467)\n- Add IBM WatsonX LLM support (#8587)\n\n### Bug Fixes / Nits\n\n- [Bug] Patch Clarifai classes (#8529)\n- fix retries for bedrock llm (#8528)\n- Fix : VectorStore\u2019s QueryResult always returns saved Node as TextNode (#8521)\n- Added default file_metadata to get basic metadata that many postprocessors use, for SimpleDirectoryReader (#8486)\n- Handle metadata with None values in chromadb (#8584)\n\n## [0.8.55] - 2023-10-29\n\n### New Features\n\n- allow prompts to take in functions with `function_mappings` (#8548)\n- add advanced prompt + \"prompt engineering for RAG\" notebook (#8555)\n- Leverage Replicate API for serving LLaVa modal (#8539)\n\n### Bug Fixes / Nits\n\n- Update pull request template with google colab support inclusion (#8525)\n\n## [0.8.54] - 2023-10-28\n\n### New Features\n\n- notebook showing how to fine-tune llama2 on structured outputs (#8540)\n  - added GradientAIFineTuningHandler\n  - added pydantic_program_mode to ServiceContext\n- Initialize MultiModal Retrieval using LlamaIndex (#8507)\n\n### Bug Fixes / Nits\n\n- Add missing import to `ChatEngine` usage pattern `.", "mimetype": "text/plain", "start_char_idx": 103361, "end_char_idx": 106767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbf0e854-ef91-45ec-965d-23688e4af1d3": {"__data__": {"id_": "bbf0e854-ef91-45ec-965d-23688e4af1d3", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f3b36ad-c843-478a-bb90-ffc96aec3687", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "e6a97324eb29bf920e9cd8e1d1ff6503c765a8a82c90ee6a8463903bf5d881b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "554672da-ce4b-4d97-86b3-062d879d378a", "node_type": "1", "metadata": {}, "hash": "761d997eba823aa1172a7cc09b5e67b79f308f0858c5b6be429a36f2abf06b9b", "class_name": "RelatedNodeInfo"}}, "text": "md` doc (#8518)\n- :bug: fixed async add (#8531)\n- fix: add the needed CondenseQuestionChatEngine import in the usage_pa\u2026 (#8518)\n- Add import LongLLMLinguaPostprocessor for LongLLMLingua.ipynb (#8519)\n\n## [0.8.53] - 2023-10-27\n\n### New Features\n\n- Docs refactor (#8500)\n  An overhaul of the docs organization. Major changes\n  - Added a big new \"understanding\" section\n  - Added a big new \"optimizing\" section\n  - Overhauled Getting Started content\n  - Categorized and moved module guides to a single section\n\n## [0.8.52] - 2023-10-26\n\n### New Features\n\n- Add longllmlingua (#8485)\n- Add google colab support for notebooks (#7560)\n\n### Bug Fixes / Nits\n\n- Adapt Cassandra VectorStore constructor DB connection through cassio.init (#8255)\n- Allow configuration of service context and storage context in managed index (#8487)\n\n## [0.8.51.post1] - 2023-10-25\n\n### New Features\n\n- Add Llava MultiModal QA examples for Tesla 10k RAG (#8271)\n- fix bug streaming on react chat agent not working as expected (#8459)\n\n### Bug Fixes / Nits\n\n- patch: add selected result to response metadata for router query engines, fix bug (#8483)\n- add Jina AI embeddings notebook + huggingface embedding fix (#8478)\n- add `is_chat_model` to replicate (#8469)\n- Brought back `toml-sort` to `pre-commit` (#8267)\n- Added `LocationConstraint` for local `test_s3_kvstore` (#8263)\n\n## [0.8.50] - 2023-10-24\n\n### New Features\n\n- Expose prompts in different modules (query engines, synthesizers, and more) (#8275)\n\n## [0.8.49] - 2023-10-23\n\n### New Features\n\n- New LLM integrations\n  - Support for Hugging Face Inference API's `conversational`, `text_generation`,\n    and `feature_extraction` endpoints via `huggingface_hub[inference]` (#8098)\n  - Add Amazon Bedrock LLMs (#8223)\n  - Add AI21 Labs LLMs (#8233)\n  - Add OpenAILike LLM class for OpenAI-compatible api servers (#7973)\n- New / updated vector store integrations\n  - Add DashVector (#7772)\n  - Add Tencent VectorDB (#8173)\n  - Add option for custom Postgres schema on PGVectorStore instead of only allowing public schema (#8080)\n- Add Gradient fine tuning engine (#8208)\n- docs(FAQ): frequently asked questions (#8249)\n\n### Bug Fixes / Nits\n\n- Fix inconsistencies with `ReActAgent.stream_chat` (#8147)\n- Deprecate some functions for GuardrailsOutputParser (#8016)\n- Simplify dependencies (#8236)\n- Bug fixes for LiteLLM (#7885)\n- Update for Predibase LLM (#8211)\n\n## [0.8.48] - 2023-10-20\n\n### New Features\n\n- Add `DELETE` for MyScale vector store (#8159)\n- Add SQL Retriever (#8197)\n- add semantic kernel document format (#8226)\n- Improve MyScale Hybrid Search and Add `DELETE` for MyScale vector store (#8159)\n\n### Bug Fixes / Nits\n\n- Fixed additional kwargs in ReActAgent.from_tools() (#8206)\n- Fixed missing spaces in prompt templates (#8190)\n- Remove auto-download of llama2-13B on exception (#8225)\n\n## [0.8.47] - 2023-10-19\n\n### New Features\n\n- add response synthesis to text-to-SQL (#8196)\n- Added support for `LLMRailsEmbedding` (#8169)\n- Inferring MPS device with PyTorch (#8195)\n- Consolidated query/text prepending (#8189)\n\n## [0.8.", "mimetype": "text/plain", "start_char_idx": 106767, "end_char_idx": 109840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "554672da-ce4b-4d97-86b3-062d879d378a": {"__data__": {"id_": "554672da-ce4b-4d97-86b3-062d879d378a", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbf0e854-ef91-45ec-965d-23688e4af1d3", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "1db3709252057e7b50f420ccc2ccf290cc59f95c81302d930b1912719285391a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b255d14f-2ee5-4650-b02c-cbb6dd5c726b", "node_type": "1", "metadata": {}, "hash": "3a80a64a0331a14107f803f18bce837716284e542a818b5b6a27fab7e225b15e", "class_name": "RelatedNodeInfo"}}, "text": "8.46] - 2023-10-18\n\n### New Features\n\n- Add fine-tuning router support + embedding selector (#8174)\n- add more document converters (#8156)\n\n### Bug Fixes / Nits\n\n- Add normalization to huggingface embeddings (#8145)\n- Improve MyScale Hybrid Search (#8159)\n- Fixed duplicate `FORMAT_STR` being inside prompt (#8171)\n- Added: support for output_kwargs={'max_colwidth': xx} for PandasQueryEngine (#8110)\n- Minor fix in the description for an argument in cohere llm (#8163)\n- Fix Firestore client info (#8166)\n\n## [0.8.45] - 2023-10-13\n\n### New Features\n\n- Added support for fine-tuning cross encoders (#7705)\n- Added `QueryFusionRetriever` for merging multiple retrievers + query augmentation (#8100)\n- Added `nb-clean` to `pre-commit` to minimize PR diffs (#8108)\n- Support for `TextEmbeddingInference` embeddings (#8122)\n\n### Bug Fixes / Nits\n\n- Improved the `BM25Retriever` interface to accept `BaseNode` objects (#8096)\n- Fixed bug with `BM25Retriever` tokenizer not working as expected (#8096)\n- Brought mypy to pass in Python 3.8 (#8107)\n- `ReActAgent` adding missing `super().__init__` call (#8125)\n\n## [0.8.44] - 2023-10-12\n\n### New Features\n\n- add pgvector sql query engine (#8087)\n- Added HoneyHive one-click observability (#7944)\n- Add support for both SQLAlchemy V1 and V2 (#8060)\n\n## [0.8.43.post1] - 2023-10-11\n\n### New Features\n\n- Moves `codespell` to `pre-commit` (#8040)\n- Added `prettier` for autoformatting extensions besides `.py` (#8072)\n\n### Bug Fixes / Nits\n\n- Fixed forgotten f-str in `HuggingFaceLLM` (#8075)\n- Relaxed numpy/panadas reqs\n\n## [0.8.43] - 2023-10-10\n\n### New Features\n\n- Added support for `GradientEmbedding` embed models (#8050)\n\n### Bug Fixes / Nits\n\n- added `messages_to_prompt` kwarg to `HuggingFaceLLM` (#8054)\n- improved selection and sql parsing for open-source models (#8054)\n- fixed bug when agents hallucinate too many kwargs for a tool (#8054)\n- improved prompts and debugging for selection+question generation (#8056)\n\n## [0.8.42] - 2023-10-10\n\n### New Features\n\n- `LocalAI` more intuitive module-level var names (#8028)\n- Enable `codespell` for markdown docs (#7972)\n- add unstructured table element node parser (#8036)\n- Add: Async upserting for Qdrant vector store (#7968)\n- Add cohere llm (#8023)\n\n### Bug Fixes / Nits\n\n- Parse multi-line outputs in react agent answers (#8029)\n- Add properly named kwargs to keyword `as_retriever` calls (#8011)\n- Updating Reference to RAGAS LlamaIndex Integration (#8035)\n- Vectara bugfix (#8032)\n- Fix: ChromaVectorStore can attempt to add in excess of chromadb batch\u2026 (#8019)\n- Fix get_content method in Mbox reader (#8012)\n- Apply kwarg filters in WeaviateVectorStore (#8017)\n- Avoid ZeroDivisionError (#8027)\n- `LocalAI` intuitive module-level var names (#8028)\n- zep/fix: imports & typing (#8030)\n- refactor: use `str.join` (#8020)\n- use proper metadata str for node parsing (#7987)\n\n## [0.8.41] - 2023-10-07\n\n### New Features\n\n- You.com retriever (#8024)\n- Pull fields from mongodb into metadata with `metadata_names` argument (#8001)\n- Simplified `LocalAI.__init__` preserving the same behaviors (#7982)\n\n### Bug Fixes / Nits\n\n- Use longest metadata string for metadata aware text splitting (#7987)\n- Handle lists of strings in mongodb reader (#8002)\n- Removes `OpenAI.class_type` as it was dead code (#7983)\n- Fixing `HuggingFaceLLM.device_map` type hint (#7989)\n\n## [0.8.", "mimetype": "text/plain", "start_char_idx": 109838, "end_char_idx": 113205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b255d14f-2ee5-4650-b02c-cbb6dd5c726b": {"__data__": {"id_": "b255d14f-2ee5-4650-b02c-cbb6dd5c726b", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "554672da-ce4b-4d97-86b3-062d879d378a", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "679bc6a7aa9a2243d976dd53f7c3cee3aefb3908214170bbdad7f77f928b0ee6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d85e59f-b606-4363-8536-4641286809af", "node_type": "1", "metadata": {}, "hash": "f35693b988b35a581984f0882a4f8b0462f1d44699c87cdb524bfd16d1d865a2", "class_name": "RelatedNodeInfo"}}, "text": "device_map` type hint (#7989)\n\n## [0.8.40] - 2023-10-05\n\n### New Features\n\n- Added support for `Clarifai` LLM (#7967)\n- Add support for function fine-tuning (#7971)\n\n### Breaking Changes\n\n- Update document summary index (#7815)\n  - change default retrieval mode to embedding\n  - embed summaries into vector store by default at indexing time (instead of calculating embedding on the fly)\n  - support configuring top k in llm retriever\n\n## [0.8.39] - 2023-10-03\n\n### New Features\n\n- Added support for pydantic object outputs with query engines (#7893)\n- `ClarifaiEmbedding` class added for embedding support (#7940)\n- Markdown node parser, flat file reader and simple file node parser (#7863)\n- Added support for mongdb atlas `$vectorSearch` (#7866)\n\n### Bug Fixes / Nits\n\n- Adds support for using message metadata in discord reader (#7906)\n- Fix `LocalAI` chat capability without `max_tokens` (#7942)\n- Added `codespell` for automated checking (#7941)\n- `ruff` modernization and autofixes (#7889)\n- Implement own SQLDatabase class (#7929)\n- Update LlamaCPP context_params property (#7945)\n- fix duplicate embedding (#7949)\n- Adds `codespell` tool for enforcing good spelling (#7941)\n- Supporting `mypy` local usage with `venv` (#7952)\n- Vectara - minor update (#7954)\n- Avoiding `pydantic` reinstalls in CI (#7956)\n- move tree_sitter_languages into data_requirements.txt (#7955)\n- Add `cache_okay` param to `PGVectorStore` to help suppress TSVector warnings (#7950)\n\n## [0.8.38] - 2023-10-02\n\n### New Features\n\n- Updated `KeywordNodePostprocessor` to use spacy to support more languages (#7894)\n- `LocalAI` supporting global or per-query `/chat/completions` vs `/completions` (#7921)\n- Added notebook on using REBEL + Wikipedia filtering for knowledge graphs (#7919)\n- Added support for `ElasticsearchEmbedding` (#7914)\n\n## [0.8.37] - 2023-09-30\n\n### New Features\n\n- Supporting `LocalAI` LLMs (#7913)\n- Validations protecting against misconfigured chunk sizes (#7917)\n\n### Bug Fixes / Nits\n\n- Simplify NL SQL response to SQL parsing, with expanded NL SQL prompt (#7868)\n- Improve vector store retrieval speed for vectordb integrations (#7876)\n- Added replacing {{ and }}, and fixed JSON parsing recursion (#7888)\n- Nice-ified JSON decoding error (#7891)\n- Nice-ified SQL error from LLM not providing SQL (#7900)\n- Nice-ified `ImportError` for `HuggingFaceLLM` (#7904)\n- eval fixes: fix dataset response generation, add score to evaluators (#7915)\n\n## [0.8.36] - 2023-09-27\n\n### New Features\n\n- add \"build RAG from scratch notebook\" - OSS/local (#7864)\n\n### Bug Fixes / Nits\n\n- Fix elasticsearch hybrid scoring (#7852)\n- Replace `get_color_mapping` and `print_text` Langchain dependency with internal implementation (#7845)\n- Fix async streaming with azure (#7856)\n- Avoid `NotImplementedError()` in sub question generator (#7855)\n- Patch predibase initialization (#7859)\n- Bumped min langchain version and changed prompt imports from langchain (#7862)\n\n## [0.8.35] - 2023-09-27\n\n### Bug Fixes / Nits\n\n- Fix dropping textnodes in recursive retriever (#7840)\n- share callback_manager between agent and its llm when callback_manager is None (#7844)\n- fix pandas query engine (#7847)\n\n## [0.8.", "mimetype": "text/plain", "start_char_idx": 113166, "end_char_idx": 116354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d85e59f-b606-4363-8536-4641286809af": {"__data__": {"id_": "0d85e59f-b606-4363-8536-4641286809af", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b255d14f-2ee5-4650-b02c-cbb6dd5c726b", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "a4c5b9a34748d77463805f8a7f2201f7c03363faa9d13251e44166aa407ba7d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16d0b0ff-ef0e-4b97-8de1-4f9d3cd5b94f", "node_type": "1", "metadata": {}, "hash": "ada3ac67763e2c4421f4bb7c45aa54da8879fcc14f53450401305dbfbd0db6e2", "class_name": "RelatedNodeInfo"}}, "text": "8.34] - 2023-09-26\n\n### New Features\n\n- Added `Konko` LLM support (#7775)\n- Add before/after context sentence (#7821)\n- EverlyAI integration with LlamaIndex through OpenAI library (#7820)\n- add Arize Phoenix tracer to global handlers (#7835)\n\n### Bug Fixes / Nits\n\n- Normalize scores returned from ElasticSearch vector store (#7792)\n- Fixed `refresh_ref_docs()` bug with order of operations (#7664)\n- Delay postgresql connection for `PGVectorStore` until actually needed (#7793)\n- Fix KeyError in delete method of `SimpleVectorStore` related to metadata filters (#7829)\n- Fix KeyError in delete method of `SimpleVectorStore` related to metadata filters (#7831)\n- Addressing PyYAML import error (#7784)\n- ElasticsearchStore: Update User-Agent + Add example docker compose (#7832)\n- `StorageContext.persist` supporting `Path` (#7783)\n- Update ollama.py (#7839)\n- fix bug for self.\\_session_pool (#7834)\n\n## [0.8.33] - 2023-09-25\n\n### New Features\n\n- add pairwise evaluator + benchmark auto-merging retriever (#7810)\n\n### Bug Fixes / Nits\n\n- Minor cleanup in embedding class (#7813)\n- Misc updates to `OpenAIEmbedding` (#7811)\n\n## [0.8.32] - 2023-09-24\n\n### New Features\n\n- Added native support for `HuggingFaceEmbedding`, `InstructorEmbedding`, and `OptimumEmbedding` (#7795)\n- Added metadata filtering and hybrid search to MyScale vector store (#7780)\n- Allowing custom text field name for Milvus (#7790)\n- Add support for `vector_store_query_mode` to `VectorIndexAutoRetriever` (#7797)\n\n### Bug Fixes / Nits\n\n- Update `LanceDBVectorStore` to handle score and distance (#7754)\n- Pass LLM to `memory_cls` in `CondenseQuestionChatEngine` (#7785)\n\n## [0.8.31] - 2023-09-22\n\n### New Features\n\n- add pydantic metadata extractor (#7778)\n- Allow users to set the embedding dimensions in azure cognitive vector store (#7734)\n- Add semantic similarity evaluator (#7770)\n\n### Bug Fixes / Nits\n\n- \ud83d\udcdddocs: Update Chatbot Tutorial and Notebook (#7767)\n- Fixed response synthesizers with empty nodes (#7773)\n- Fix `NotImplementedError` in auto vector retriever (#7764)\n- Multiple kwargs values in \"KnowledgeGraphQueryEngine\" bug-fix (#7763)\n- Allow setting azure cognitive search dimensionality (#7734)\n- Pass service context to index for dataset generator (#7748)\n- Fix output parsers for selector templates (#7774)\n- Update Chatbot_SEC.ipynb (#7711)\n- linter/typechecker-friendly improvements to cassandra test (#7771)\n- Expose debug option of `PgVectorStore` (#7776)\n- llms/openai: fix Azure OpenAI by considering `prompt_filter_results` field (#7755)\n\n## [0.8.30] - 2023-09-21\n\n### New Features\n\n- Add support for `gpt-3.5-turbo-instruct` (#7729)\n- Add support for `TimescaleVectorStore` (#7727)\n- Added `LongContextReorder` for lost-in-the-middle issues (#7719)\n- Add retrieval evals (#7738)\n\n### Bug Fixes / Nits\n\n- Added node post-processors to async context chat engine (#7731)\n- Added unique index name for postgres tsv column (#7741)\n\n## [0.8.29.post1] - 2023-09-18\n\n### Bug Fixes / Nits\n\n- Fix langchain import error for embeddings (#7714)\n\n## [0.8.29] - 2023-09-18\n\n### New Features\n\n- Added metadata filtering to the base simple vector store (#7564)\n- add low-level router guide (#7708)\n- Add CustomQueryEngine class (#7703)\n\n### Bug Fixes / Nits\n\n- Fix context window metadata in lite-llm (#7696)\n\n## [0.8.", "mimetype": "text/plain", "start_char_idx": 116352, "end_char_idx": 119656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16d0b0ff-ef0e-4b97-8de1-4f9d3cd5b94f": {"__data__": {"id_": "16d0b0ff-ef0e-4b97-8de1-4f9d3cd5b94f", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d85e59f-b606-4363-8536-4641286809af", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "0deac1ab73853924697af78860305ca13e51d46624cd9e6c081475ca2fadcd88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f9bb34-958a-40f5-8873-009d3c2aec91", "node_type": "1", "metadata": {}, "hash": "64c70f915ad1fff968a6eb8dfb775ff2712b12bba4ebd34ad58decf895dabb41", "class_name": "RelatedNodeInfo"}}, "text": "8.28] - 2023-09-16\n\n### New Features\n\n- Add CorrectnessEvaluator (#7661)\n- Added support for `Ollama` LLMs (#7635)\n- Added `HWPReader` (#7672)\n- Simplified portkey LLM interface (#7669)\n- Added async operation support to `ElasticsearchStore` vector store (#7613)\n- Added support for `LiteLLM` (#7600)\n- Added batch evaluation runner (#7692)\n\n### Bug Fixes / Nits\n\n- Avoid `NotImplementedError` for async langchain embeddings (#7668)\n- Imrpoved reliability of LLM selectors (#7678)\n- Fixed `query_wrapper_prompt` and `system_prompt` for output parsers and completion models (#7678)\n- Fixed node attribute inheritance in citation query engine (#7675)\n\n### Breaking Changes\n\n- Refactor and update `BaseEvaluator` interface to be more consistent (#7661)\n  - Use `evaluate` function for generic input\n  - Use `evaluate_response` function with `Response` objects from llama index query engine\n- Update existing evaluators with more explicit naming\n  - `ResponseEvaluator` -> `FaithfulnessEvaluator`\n  - `QueryResponseEvaluator` -> `RelevancyEvaluator`\n  - old names are kept as class aliases for backwards compatibility\n\n## [0.8.27] - 2023-09-14\n\n### New Features\n\n- add low-level tutorial section (#7673)\n\n### Bug Fixes / Nits\n\n- default delta should be a dict (#7665)\n- better query wrapper logic on LLMPredictor (#7667)\n\n## [0.8.26] - 2023-09-12\n\n### New Features\n\n- add non-linear embedding adapter (#7658)\n- Add \"finetune + RAG\" evaluation to knowledge fine-tuning notebook (#7643)\n\n### Bug Fixes / Nits\n\n- Fixed chunk-overlap for sentence splitter (#7590)\n\n## [0.8.25] - 2023-09-12\n\n### New Features\n\n- Added `AGENT_STEP` callback event type (#7652)\n\n### Bug Fixes / Nits\n\n- Allowed `simple` mode to work with `as_chat_engine()` (#7637)\n- Fixed index error in azure streaming (#7646)\n- Removed `pdb` from llama-cpp (#7651)\n\n## [0.8.24] - 2023-09-11\n\n## New Features\n\n- guide: fine-tuning to memorize knowledge (#7626)\n- added ability to customize prompt template for eval modules (#7626)\n\n### Bug Fixes\n\n- Properly detect `llama-cpp-python` version for loading the default GGML or GGUF `llama2-chat-13b` model (#7616)\n- Pass in `summary_template` properly with `RetrieverQueryEngine.from_args()` (#7621)\n- Fix span types in wandb callback (#7631)\n\n## [0.8.23] - 2023-09-09\n\n### Bug Fixes\n\n- Make sure context and system prompt is included in prompt for first chat for llama2 (#7597)\n- Avoid negative chunk size error in refine process (#7607)\n- Fix relationships for small documents in hierarchical node parser (#7611)\n- Update Anyscale Endpoints integration with full streaming and async support (#7602)\n- Better support of passing credentials as LLM constructor args in `OpenAI`, `AzureOpenAI`, and `Anyscale` (#7602)\n\n### Breaking Changes\n\n- Update milvus vector store to support filters and dynamic schemas (#7286)\n  - See the [updated notebook](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo.html) for usage\n- Added NLTK to core dependencies to support the default sentence splitter (#7606)\n\n## [0.8.22] - 2023-09-07\n\n### New Features\n\n- Added support for ElasticSearch Vector Store (#7543)\n\n### Bug Fixes / Nits\n\n- Fixed small `_index` bug in `ElasticSearchReader` (#7570)\n- Fixed bug with prompt helper settings in global service contexts (#7576)\n- Remove newlines from openai embeddings again (#7588)\n- Fixed small bug with setting `query_wrapper_prompt` in the service context (#7585)\n\n### Breaking/Deprecated API Changes\n\n- Clean up vector store interface to use `BaseNode` instead of `NodeWithEmbedding`\n  - For majority of users, this is a no-op change\n  - For users directly operating with the `VectorStore` abstraction and manually constructing `NodeWithEmbedding` objects, this is a minor breaking change. Use `TextNode` with `embedding` set directly, instead of `NodeWithEmbedding`.", "mimetype": "text/plain", "start_char_idx": 119654, "end_char_idx": 123481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16f9bb34-958a-40f5-8873-009d3c2aec91": {"__data__": {"id_": "16f9bb34-958a-40f5-8873-009d3c2aec91", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16d0b0ff-ef0e-4b97-8de1-4f9d3cd5b94f", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b00534e4b3096ee0c792c96331c04deed43e32f2b2590db7764d048ab11bf67d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d74ca02f-8e69-4060-897a-d223cbb2f8ad", "node_type": "1", "metadata": {}, "hash": "29a90249a1dc6fe5f623fdd63d650de7ebac59858abce92b360791caf2ded7d2", "class_name": "RelatedNodeInfo"}}, "text": "this is a minor breaking change. Use `TextNode` with `embedding` set directly, instead of `NodeWithEmbedding`.\n\n## [0.8.21] - 2023-09-06\n\n### New Features\n\n- add embedding adapter fine-tuning engine + guide (#7565)\n- Added support for Azure Cognitive Search vector store (#7469)\n- Support delete in supabase (#6951)\n- Added support for Espilla vector store (#7539)\n- Added support for AnyScale LLM (#7497)\n\n### Bug Fixes / Nits\n\n- Default to user-configurable top-k in `VectorIndexAutoRetriever` (#7556)\n- Catch validation errors for structured responses (#7523)\n- Fix streaming refine template (#7561)\n\n## [0.8.20] - 2023-09-04\n\n### New Features\n\n- Added Portkey LLM integration (#7508)\n- Support postgres/pgvector hybrid search (#7501)\n- upgrade recursive retriever node reference notebook (#7537)\n\n## [0.8.19] - 2023-09-03\n\n### New Features\n\n- replace list index with summary index (#7478)\n- rename list index to summary index part 2 (#7531)\n\n## [0.8.18] - 2023-09-03\n\n### New Features\n\n- add agent finetuning guide (#7526)\n\n## [0.8.17] - 2023-09-02\n\n### New Features\n\n- Make (some) loaders serializable (#7498)\n- add node references to recursive retrieval (#7522)\n\n### Bug Fixes / Nits\n\n- Raise informative error when metadata is too large during splitting (#7513)\n- Allow langchain splitter in simple node parser (#7517)\n\n## [0.8.16] - 2023-09-01\n\n### Bug Fixes / Nits\n\n- fix link to Marvin notebook in docs (#7504)\n- Ensure metadata is not `None` in `SimpleWebPageReader` (#7499)\n- Fixed KGIndex visualization (#7493)\n- Improved empty response in KG Index (#7493)\n\n## [0.8.15] - 2023-08-31\n\n### New Features\n\n- Added support for `MarvinEntityExtractor` metadata extractor (#7438)\n- Added a url_metadata callback to SimpleWebPageReader (#7445)\n- Expanded callback logging events (#7472)\n\n### Bug Fixes / Nits\n\n- Only convert newlines to spaces for text 001 embedding models in OpenAI (#7484)\n- Fix `KnowledgeGraphRagRetriever` for non-nebula indexes (#7488)\n- Support defined embedding dimension in `PGVectorStore` (#7491)\n- Greatly improved similarity calculation speed for the base vector store (#7494)\n\n## [0.8.14] - 2023-08-30\n\n### New Features\n\n- feat: non-kg heterogeneous graph support in Graph RAG (#7459)\n- rag guide (#7480)\n\n### Bug Fixes / Nits\n\n- Improve openai fine-tuned model parsing (#7474)\n- doing some code de-duplication (#7468)\n- support both str and templates for query_wrapper_prompt in HF LLMs (#7473)\n\n## [0.8.13] - 2023-08-29\n\n### New Features\n\n- Add embedding finetuning (#7452)\n- Added support for RunGPT LLM (#7401)\n- Integration guide and notebook with DeepEval (#7425)\n- Added `VectorIndex` and `VectaraRetriever` as a managed index (#7440)\n- Added support for `to_tool_list` to detect and use async functions (#7282)\n\n## [0.8.12] - 2023-08-28\n\n### New Features\n\n- add openai finetuning class (#7442)\n- Service Context to/from dict (#7395)\n- add finetuning guide (#7429)\n\n### Smaller Features / Nits / Bug Fixes\n\n- Add example how to run FalkorDB docker (#7441)\n- Update root.md to use get_response_synthesizer expected type. (#7437)\n- Bugfix MonsterAPI Pydantic version v2/v1 support.", "mimetype": "text/plain", "start_char_idx": 123371, "end_char_idx": 126491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d74ca02f-8e69-4060-897a-d223cbb2f8ad": {"__data__": {"id_": "d74ca02f-8e69-4060-897a-d223cbb2f8ad", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16f9bb34-958a-40f5-8873-009d3c2aec91", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "3c8bebea72c1f7ff2cae2471948210c1c34609bf08925c4db4a8829a4acfce65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "283ed427-fcf5-4640-be7f-79c87068aad4", "node_type": "1", "metadata": {}, "hash": "49233cc012a1a29b0fd89a6b0fdcd239f2ad56602c630e57cc67dac141395408", "class_name": "RelatedNodeInfo"}}, "text": "(#7437)\n- Bugfix MonsterAPI Pydantic version v2/v1 support. Doc Update (#7432)\n\n## [0.8.11.post3] - 2023-08-27\n\n### New Features\n\n- AutoMergingRetriever (#7420)\n\n## [0.8.10.post1] - 2023-08-25\n\n### New Features\n\n- Added support for `MonsterLLM` using MonsterAPI (#7343)\n- Support comments fields in NebulaGraphStore and int type VID (#7402)\n- Added configurable endpoint for DynamoDB (#6777)\n- Add structured answer filtering for Refine response synthesizer (#7317)\n\n### Bug Fixes / Nits\n\n- Use `utf-8` for json file reader (#7390)\n- Fix entity extractor initialization (#7407)\n\n## [0.8.9] - 2023-08-24\n\n### New Features\n\n- Added support for FalkorDB/RedisGraph graph store (#7346)\n- Added directed sub-graph RAG (#7378)\n- Added support for `BM25Retriever` (#7342)\n\n### Bug Fixes / Nits\n\n- Added `max_tokens` to `Xinference` LLM (#7372)\n- Support cache dir creation in multithreaded apps (#7365)\n- Ensure temperature is a float for openai (#7382)\n- Remove duplicate subjects in knowledge graph retriever (#7378)\n- Added support for both pydantic v1 and v2 to allow other apps to move forward (#7394)\n\n### Breaking/Deprecated API Changes\n\n- Refactor prompt template (#7319)\n  - Use `BasePromptTemplate` for generic typing\n  - Use `PromptTemplate`, `ChatPromptTemplate`, `SelectorPromptTemplate` as core implementations\n  - Use `LangchainPromptTemplate` for compatibility with Langchain prompt templates\n  - Fully replace specific prompt classes (e.g. `SummaryPrompt`) with generic `BasePromptTemplate` for typing in codebase.\n  - Keep `Prompt` as an alias for `PromptTemplate` for backwards compatibility.\n  - BREAKING CHANGE: remove support for `Prompt.from_langchain_prompt`, please use `template=LangchainPromptTemplate(lc_template)` instead.\n\n## [0.8.8] - 2023-08-23\n\n### New Features\n\n- `OpenAIFineTuningHandler` for collecting LLM inputs/outputs for OpenAI fine tuning (#7367)\n\n### Bug Fixes / Nits\n\n- Add support for `claude-instant-1.2` (#7369)\n\n## [0.8.7] - 2023-08-22\n\n### New Features\n\n- Support fine-tuned OpenAI models (#7364)\n- Added support for Cassandra vector store (#6784)\n- Support pydantic fields in tool functions (#7348)\n\n### Bug Fixes / Nits\n\n- Fix infinite looping with forced function call in `OpenAIAgent` (#7363)\n\n## [0.8.6] - 2023-08-22\n\n### New Features\n\n- auto vs. recursive retriever notebook (#7353)\n- Reader and Vector Store for BagelDB with example notebooks (#7311)\n\n### Bug Fixes / Nits\n\n- Use service context for intermediate index in retry source query engine (#7341)\n- temp fix for prompt helper + chat models (#7350)\n- Properly skip unit-tests when packages not installed (#7351)\n\n## [0.8.5.post2] - 2023-08-20\n\n### New Features\n\n- Added FireStore docstore/index store support (#7305)\n- add recursive agent notebook (#7330)\n\n### Bug Fixes / Nits\n\n- Fix Azure pydantic error (#7329)\n- fix callback trace ids (make them a context var) (#7331)\n\n## [0.8.5.post1] - 2023-08-18\n\n### New Features\n\n- Awadb Vector Store (#7291)\n\n### Bug Fixes / Nits\n\n- Fix bug in OpenAI llm temperature type\n\n## [0.8.5] - 2023-08-18\n\n### New Features\n\n- Expose a system prompt/query wrapper prompt in the service context for open-source LLMs (#6647)\n- Changed default MyScale index format to `MSTG` (#7288)\n- Added tracing to chat engines/agents (#7304)\n- move LLM and embeddings to pydantic (#7289)\n\n### Bug Fixes / Nits\n\n- Fix sentence splitter bug (#7303)\n- Fix sentence splitter infinite loop (#7295)\n\n## [0.8.", "mimetype": "text/plain", "start_char_idx": 126432, "end_char_idx": 129861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "283ed427-fcf5-4640-be7f-79c87068aad4": {"__data__": {"id_": "283ed427-fcf5-4640-be7f-79c87068aad4", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d74ca02f-8e69-4060-897a-d223cbb2f8ad", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "abd0bb3fb6351e61561ef3b0e0101ba9aa77c16531452617a39c6206d5c13da1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfacf8d7-300a-4ecd-80c6-54a34f4b960b", "node_type": "1", "metadata": {}, "hash": "8763493164cf870713e050e39ea32cc9693ae0481d2915983d3bf44ef037eeb3", "class_name": "RelatedNodeInfo"}}, "text": "8.4] - 2023-08-17\n\n### Bug Fixes / Nits\n\n- Improve SQL Query parsing (#7283)\n- Fix loading embed_model from global service context (#7284)\n- Limit langchain version until we migrate to pydantic v2 (#7297)\n\n## [0.8.3] - 2023-08-16\n\n### New Features\n\n- Added Knowledge Graph RAG Retriever (#7204)\n\n### Bug Fixes / Nits\n\n- accept `api_key` kwarg in OpenAI LLM class constructor (#7263)\n- Fix to create separate queue instances for separate instances of `StreamingAgentChatResponse` (#7264)\n\n## [0.8.2.post1] - 2023-08-14\n\n### New Features\n\n- Added support for Rockset as a vector store (#7111)\n\n### Bug Fixes\n\n- Fixed bug in service context definition that could disable LLM (#7261)\n\n## [0.8.2] - 2023-08-14\n\n### New Features\n\n- Enable the LLM or embedding model to be disabled by setting to `None` in the service context (#7255)\n- Resolve nearly any huggingface embedding model using the `embed_model=\"local:<model_name>\"` syntax (#7255)\n- Async tool-calling support (#7239)\n\n### Bug Fixes / Nits\n\n- Updated supabase kwargs for add and query (#7103)\n- Small tweak to default prompts to allow for more general purpose queries (#7254)\n- Make callback manager optional for `CustomLLM` + docs update (#7257)\n\n## [0.8.1] - 2023-08-13\n\n### New Features\n\n- feat: add node_postprocessors to ContextChatEngine (#7232)\n- add ensemble query engine tutorial (#7247)\n\n### Smaller Features\n\n- Allow EMPTY keys for Fastchat/local OpenAI API endpoints (#7224)\n\n## [0.8.0] - 2023-08-11\n\n### New Features\n\n- Added \"LLAMA_INDEX_CACHE_DIR\" to control cached files (#7233)\n- Default to pydantic selectors when possible (#7154, #7223)\n- Remove the need for langchain wrappers on `embed_model` in the service context (#7157)\n- Metadata extractors take an `LLM` object now, in addition to `LLMPredictor` (#7202)\n- Added local mode + fallback to llama.cpp + llama2 (#7200)\n- Added local fallback for embeddings to `BAAI/bge-small-en` (#7200)\n- Added `SentenceWindowNodeParser` + `MetadataReplacementPostProcessor` (#7211)\n\n### Breaking Changes\n\n- Change default LLM to gpt-3.5-turbo from text-davinci-003 (#7223)\n- Change prompts for compact/refine/tree_summarize to work better with gpt-3.5-turbo (#7150, #7179, #7223)\n- Increase default LLM temperature to 0.1 (#7180)\n\n## [0.7.24.post1] - 2023-08-11\n\n### Other Changes\n\n- Reverted #7223 changes to defaults (#7235)\n\n## [0.7.24] - 2023-08-10\n\n### New Features\n\n- Default to pydantic selectors when possible (#7154, #7223)\n- Remove the need for langchain wrappers on `embed_model` in the service context (#7157)\n- Metadata extractors take an `LLM` object now, in addition to `LLMPredictor` (#7202)\n- Added local mode + fallback to llama.cpp + llama2 (#7200)\n- Added local fallback for embeddings to `BAAI/bge-small-en` (#7200)\n- Added `SentenceWindowNodeParser` + `MetadataReplacementPostProcessor` (#7211)\n\n### Breaking Changes\n\n- Change default LLM to gpt-3.5-turbo from text-davinci-003 (#7223)\n- Change prompts for compact/refine/tree_summarize to work better with gpt-3.5-turbo (#7150, #7179, #7223)\n- Increase default LLM temperature to 0.1 (#7180)\n\n### Other Changes\n\n- docs: Improvements to Mendable Search (#7220)\n- Refactor openai agent (#7077)\n\n### Bug Fixes / Nits\n\n- Use `1 - cosine_distance` for pgvector/postgres vector db (#7217)\n- fix metadata formatting and extraction (#7216)\n- fix(readers): Fix non-ASCII JSON Reader bug (#7086)\n- Chore: change PgVectorStore variable name from `sim` to `distance` for clarity (#7226)\n\n## [0.7.", "mimetype": "text/plain", "start_char_idx": 129859, "end_char_idx": 133329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfacf8d7-300a-4ecd-80c6-54a34f4b960b": {"__data__": {"id_": "bfacf8d7-300a-4ecd-80c6-54a34f4b960b", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "283ed427-fcf5-4640-be7f-79c87068aad4", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "4d8d273d2d132114d911e174afe47107e664583a34a6def25f75d553cf76e9ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e78faeaf-698b-4b8f-ad76-2167e81edbd6", "node_type": "1", "metadata": {}, "hash": "6121d2f3eb344bdc5566d67c4d78f1e8543c042a1b3ce930f67c1a311e46392e", "class_name": "RelatedNodeInfo"}}, "text": "7.23] - 2023-08-10\n\n### Bug Fixes / Nits\n\n- Fixed metadata formatting with custom tempalates and inheritance (#7216)\n\n## [0.7.23] - 2023-08-10\n\n### New Features\n\n- Add \"one click observability\" page to docs (#7183)\n- Added Xorbits inference for local deployments (#7151)\n- Added Zep vector store integration (#7203)\n- feat/zep vectorstore (#7203)\n\n### Bug Fixes / Nits\n\n- Update the default `EntityExtractor` model (#7209)\n- Make `ChatMemoryBuffer` pickleable (#7205)\n- Refactored `BaseOpenAIAgent` (#7077)\n\n## [0.7.22] - 2023-08-08\n\n### New Features\n\n- add ensemble retriever notebook (#7190)\n- DOCS: added local llama2 notebook (#7146)\n\n### Bug Fixes / Nits\n\n- Fix for `AttributeError: 'OpenAIAgent' object has no attribute 'callback_manager'` by calling super constructor within `BaseOpenAIAgent`\n- Remove backticks from nebula queries (#7192)\n\n## [0.7.21] - 2023-08-07\n\n### New Features\n\n- Added an `EntityExtractor` for metadata extraction (#7163)\n\n## [0.7.20] - 2023-08-06\n\n### New Features\n\n- add router module docs (#7171)\n- add retriever router (#7166)\n\n### New Features\n\n- Added a `RouterRetriever` for routing queries to specific retrievers (#7166)\n\n### Bug Fixes / Nits\n\n- Fix for issue where having multiple concurrent streamed responses from `OpenAIAgent` would result in interleaving of tokens across each response stream. (#7164)\n- fix llms callbacks issue (args[0] error) (#7165)\n\n## [0.7.19] - 2023-08-04\n\n### New Features\n\n- Added metadata filtering to weaviate (#7130)\n- Added token counting (and all callbacks) to agents and streaming (#7122)\n\n## [0.7.18] - 2023-08-03\n\n### New Features\n\n- Added `to/from_string` and `to/from_dict` methods to memory objects (#7128)\n- Include columns comments from db tables in table info for SQL queries (#7124)\n- Add Neo4j support (#7122)\n\n### Bug Fixes / Nits\n\n- Added `Azure AD` validation support to the `AzureOpenAI` class (#7127)\n- add `flush=True` when printing agent/chat engine response stream (#7129)\n- Added `Azure AD` support to the `AzureOpenAI` class (#7127)\n- Update LLM question generator prompt to mention JSON markdown (#7105)\n- Fixed `astream_chat` in chat engines (#7139)\n\n## [0.7.17] - 2023-08-02\n\n### New Features\n\n- Update `ReActAgent` to support memory modules (minor breaking change since the constructor takes `memory` instead of `chat_history`, but the main `from_tools` method remains backward compatible.)", "mimetype": "text/plain", "start_char_idx": 133327, "end_char_idx": 135716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e78faeaf-698b-4b8f-ad76-2167e81edbd6": {"__data__": {"id_": "e78faeaf-698b-4b8f-ad76-2167e81edbd6", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfacf8d7-300a-4ecd-80c6-54a34f4b960b", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "fc42ba96c74a1cd8edd0f2aec2d95e3b3139e0478c787daf8944cb47c59e9a34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ef35aa0-fe90-4724-b110-31aead47cd65", "node_type": "1", "metadata": {}, "hash": "4d352313215147605ae1b80ecac7072567af90c90d3e6bafb89b5558cbcde436", "class_name": "RelatedNodeInfo"}}, "text": "(#7116)\n- Update `ReActAgent` to support streaming (#7119)\n- Added Neo4j graph store and query engine integrations (#7122)\n- add object streaming (#7117)\n\n## [0.7.16] - 2023-07-30\n\n### New Features\n\n- Chat source nodes (#7078)\n\n## [0.7.15] - 2023-07-29\n\n### Bug Fixes / Nits\n\n- anthropic api key customization (#7082)\n- Fix broken link to API reference in Contributor Docs (#7080)\n- Update vector store docs (#7076)\n- Update comment (#7073)\n\n## [0.7.14] - 2023-07-28\n\n### New Features\n\n- Added HotpotQADistractor benchmark evaluator (#7034)\n- Add metadata filter and delete support for LanceDB (#7048)\n- Use MetadataFilters in opensearch (#7005)\n- Added support for `KuzuGraphStore` (#6970)\n- Added `kg_triplet_extract_fn` to customize how KGs are built (#7068)\n\n### Bug Fixes / Nits\n\n- Fix string formatting in context chat engine (#7050)\n- Fixed tracing for async events (#7052)\n- Less strict triplet extraction for KGs (#7059)\n- Add configurable limit to KG data retrieved (#7059)\n- Nebula connection improvements (#7059)\n- Bug fix in building source nodes for agent response (#7067)\n\n## [0.7.13] - 2023-07-26\n\n### New Features\n\n- Support function calling api for AzureOpenAI (#7041)\n\n### Bug Fixes / Nits\n\n- tune prompt to get rid of KeyError in SubQ engine (#7039)\n- Fix validation of Azure OpenAI keys (#7042)\n\n## [0.7.12] - 2023-07-25\n\n### New Features\n\n- Added `kwargs` to `ComposableGraph` for the underlying query engines (#6990)\n- Validate openai key on init (#6940)\n- Added async embeddings and async RetrieverQueryEngine (#6587)\n- Added async `aquery` and `async_add` to PGVectorStore (#7031)\n- Added `.source_nodes` attribute to chat engine and agent responses (#7029)\n- Added `OpenInferenceCallback` for storing generation data in OpenInference format (#6998)\n\n### Bug Fixes / Nits\n\n- Fix achat memory initialization for data agents (#7000)\n- Add `print_response_stream()` to agengt/chat engine response class (#7018)\n\n### Bug Fixes / Nits\n\n- Fix achat memory initialization for data agents (#7000)\n- Add `print_response_stream()` to agengt/chat engine response class (#7018)\n\n## [v0.7.11.post1] - 2023-07-20\n\n### New Features\n\n- Default to pydantic question generation when possible for sub-question query engine (#6979)\n\n### Bug Fixes / Nits\n\n- Fix returned order of messages in large chat memory (#6979)\n\n## [v0.7.11] - 2023-07-19\n\n### New Features\n\n- Added a `SentenceTransformerRerank` node post-processor for fast local re-ranking (#6934)\n- Add numpy support for evaluating queries in pandas query engine (#6935)\n- Add metadata filtering support for Postgres Vector Storage integration (#6968)\n- Proper llama2 support for agents and query engines (#6969)\n\n### Bug Fixes / Nits\n\n- Added `model_name` to LLMMetadata (#6911)\n- Fallback to retriever service context in query engines (#6911)\n- Fixed `as_chat_engine()` ValueError with extra kwargs (#6971\n\n## [v0.7.10.post1] - 2023-07-18\n\n### New Features\n\n- Add support for Replicate LLM (vicuna & llama 2!)\n\n### Bug Fixes / Nits\n\n- fix streaming for condense chat engine (#6958)\n\n## [v0.7.10] - 2023-07-17\n\n### New Features\n\n- Add support for chroma v0.4.0 (#6937)\n- Log embedding vectors to callback manager (#6962)\n\n### Bug Fixes / Nits\n\n- add more robust embedding timeouts (#6779)\n- improved connection session management on postgres vector store (#6843)\n\n## [v0.7.9] - 2023-07-15\n\n### New Features\n\n- specify `embed_model=\"local\"` to use default local embbeddings in the service context (#6806)\n- Add async `acall` endpoint to `BasePydanticProgram` (defaults to sync version).", "mimetype": "text/plain", "start_char_idx": 135717, "end_char_idx": 139266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ef35aa0-fe90-4724-b110-31aead47cd65": {"__data__": {"id_": "9ef35aa0-fe90-4724-b110-31aead47cd65", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e78faeaf-698b-4b8f-ad76-2167e81edbd6", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "e51f94d12a70fe92e5b16922a37c616bc02904f40e4338d7c34341e8c80d2864", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5f3f4e7-c37e-4f71-bf29-c093547e12dc", "node_type": "1", "metadata": {}, "hash": "a75d421235e3b2b99ae2ff5a64ece7f3d03eda82ec4c783cd989a0223d14f0f9", "class_name": "RelatedNodeInfo"}}, "text": "Implement for `OpenAIPydanticProgram`\n\n### Bug Fixes / Nits\n\n- fix null metadata for searching existing vector dbs (#6912)\n- add module guide docs for `SimpleDirectoryReader` (#6916)\n- make sure `CondenseQuestionChatEngine` streaming chat endpoints work even if not explicitly setting `streaming=True` in the underlying query engine.\n\n## [v0.7.8] - 2023-07-13\n\n### New Features\n\n- Added embedding speed benchmark (#6876)\n- Added BEIR retrieval benchmark (#6825)\n\n### Bug Fixes / Nits\n\n- remove toctrees from deprecated_terms (#6895)\n- Relax typing dependencies (#6879)\n- docs: modification to evaluation notebook (#6840)\n- raise error if the model does not support functions (#6896)\n- fix(bench embeddings): bug not taking into account string length (#6899)x\n\n## [v0.7.7] - 2023-07-13\n\n### New Features\n\n- Improved milvus consistency support and output fields support (#6452)\n- Added support for knowledge graph querying w/ cypyer+nebula (#6642)\n- Added `Document.example()` to create documents for fast prototyping (#6739)\n- Replace react chat engine to use native reactive agent (#6870)\n\n### Bug Fixes / Nits\n\n- chore: added a help message to makefile (#6861)\n\n### Bug Fixes / Nits\n\n- Fixed support for using SQLTableSchema context_str attribute (#6891)\n\n## [v0.7.6] - 2023-07-12\n\n### New Features\n\n- Added sources to agent/chat engine responses (#6854)\n- Added basic chat buffer memory to agents / chat engines (#6857)\n- Adding load and search tool (#6871)\n- Add simple agent benchmark (#6869)\n- add agent docs (#6866)\n- add react agent (#6865)\n\n### Breaking/Deprecated API Changes\n\n- Replace react chat engine with native react agent (#6870)\n- Set default chat mode to \"best\": use openai agent when possible, otherwise use react agent (#6870)\n\n### Bug Fixes / Nits\n\n- Fixed support for legacy vector store metadata (#6867)\n- fix chroma notebook in docs (#6872)\n- update LC embeddings docs (#6868)\n\n## [v0.7.5] - 2023-07-11\n\n### New Features\n\n- Add `Anthropic` LLM implementation (#6855)\n\n### Bug Fixes / Nits\n\n- Fix indexing error in `SentenceEmbeddingOptimizer` (#6850)\n- fix doc for custom embedding model (#6851)\n- fix(silent error): Add validation to `SimpleDirectoryReader` (#6819)\n- Fix link in docs (#6833)\n- Fixes Azure gpt-35-turbo model not recognized (#6828)\n- Update Chatbot_SEC.ipynb (#6808)\n- Rename leftover original name to LlamaIndex (#6792)\n- patch nested traces of the same type (#6791)\n\n## [v0.7.4] - 2023-07-08\n\n### New Features\n\n- `MetadataExtractor` - Documnent Metadata Augmentation via LLM-based feature extractors (#6764)\n\n### Bug Fixes / Nits\n\n- fixed passing in query bundle to node postprocessors (#6780)\n- fixed error in callback manager with nested traces (#6791)\n\n## [v0.7.3] - 2023-07-07\n\n### New Features\n\n- Sub question query engine returns source nodes of sub questions in the callback manager (#6745)\n- trulens integration (#6741)\n- Add sources to subquestion engine (#6745)\n\n### Bug Fixes / Nits\n\n- Added/Fixed streaming support to simple and condense chat engines (#6717)\n- fixed `response_mode=\"no_text\"` response synthesizer (#6755)\n- fixed error setting `num_output` and `context_window` in service context (#6766)\n- Fix missing as_query_engine() in tutorial (#6747)\n- Fixed variable sql_query_engine in the notebook (#6778)\n- fix required function fields (#6761)\n- Remove usage of stop token in Prompt, SQL gen (#6782)\n\n## [v0.7.2] - 2023-07-06\n\n### New Features\n\n- Support Azure OpenAI (#6718)\n- Support prefix messages (e.g.", "mimetype": "text/plain", "start_char_idx": 139267, "end_char_idx": 142740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5f3f4e7-c37e-4f71-bf29-c093547e12dc": {"__data__": {"id_": "d5f3f4e7-c37e-4f71-bf29-c093547e12dc", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ef35aa0-fe90-4724-b110-31aead47cd65", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "87c0c0f24565504301f780b6a08c22ddea3022312092c020f44587ff3f46729c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9694e8bd-4a25-4afa-bc22-3f2dd81b599a", "node_type": "1", "metadata": {}, "hash": "1e14f85c0227083ad847f77ab4f7ef3ec771976cae7b4c4de49470fb577e48a5", "class_name": "RelatedNodeInfo"}}, "text": "system prompt) in chat engine and OpenAI agent (#6723)\n- Added `CBEventType.SUB_QUESTIONS` event type for tracking sub question queries/responses (#6716)\n\n### Bug Fixes / Nits\n\n- Fix HF LLM output error (#6737)\n- Add system message support for langchain message templates (#6743)\n- Fixed applying node-postprocessors (#6749)\n- Add missing `CustomLLM` import under `llama_index.llms` (#6752)\n- fix(typo): `get_transformer_tokenizer_fn` (#6729)\n- feat(formatting): `black[jupyter]` (#6732)\n- fix(test): `test_optimizer_chinese` (#6730)\n\n## [v0.7.1] - 2023-07-05\n\n### New Features\n\n- Streaming support for OpenAI agents (#6694)\n- add recursive retriever + notebook example (#6682)\n\n## [v0.7.0] - 2023-07-04\n\n### New Features\n\n- Index creation progress bars (#6583)\n\n### Bug Fixes/ Nits\n\n- Improved chat refine template (#6645)\n\n### Breaking/Deprecated API Changes\n\n- Change `BaseOpenAIAgent` to use `llama_index.llms.OpenAI`. Adjust `chat_history` to use `List[ChatMessage]]` as type.\n- Remove (previously deprecated) `llama_index.langchain_helpers.chain_wrapper` module.\n- Remove (previously deprecated) `llama_index.token_counter.token_counter` module. See [migration guide](/how_to/callbacks/token_counting_migration.html) for more details on new callback based token counting.\n- Remove `ChatGPTLLMPredictor` and `HuggingFaceLLMPredictor`. See [migration guide](/how_to/customization/llms_migration_guide.html) for more details on replacements.\n- Remove support for setting `cache` via `LLMPredictor` constructor.\n- Update `BaseChatEngine` interface:\n  - adjust `chat_history` to use `List[ChatMessage]]` as type\n  - expose `chat_history` state as a property\n  - support overriding `chat_history` in `chat` and `achat` endpoints\n- Remove deprecated arguments for `PromptHelper`: `max_input_size`, `embedding_limit`, `max_chunk_overlap`\n- Update all notebooks to use native openai integration (#6696)\n\n## [v0.6.38] - 2023-07-02\n\n### New Features\n\n- add optional tqdm progress during index creation (#6583)\n- Added async support for \"compact\" and \"refine\" response modes (#6590)\n- [feature]add transformer tokenize functionalities for optimizer (chinese) (#6659)\n- Add simple benchmark for vector store (#6670)\n- Introduce `llama_index.llms` module, with new `LLM` interface, and `OpenAI`, `HuggingFaceLLM`, `LangChainLLM` implementations. (#6615)\n- Evaporate pydantic program (#6666)\n\n### Bug Fixes / Nits\n\n- Improve metadata/node storage and retrieval for RedisVectorStore (#6678)\n- Fixed node vs.", "mimetype": "text/plain", "start_char_idx": 142741, "end_char_idx": 145238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9694e8bd-4a25-4afa-bc22-3f2dd81b599a": {"__data__": {"id_": "9694e8bd-4a25-4afa-bc22-3f2dd81b599a", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5f3f4e7-c37e-4f71-bf29-c093547e12dc", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "8626a2b4171e7edbb5fb0e5e59616b1af7ad515c55201a5a1fc35d01d28fd47c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64141801-9822-4204-93e5-5e3120f2724d", "node_type": "1", "metadata": {}, "hash": "c6bd23e90eec0ba250ee3681a397c2189b43db10e70885c005aa9286d935b40c", "class_name": "RelatedNodeInfo"}}, "text": "document filtering in vector stores (#6677)\n- add context retrieval agent notebook link to docs (#6660)\n- Allow null values for the 'image' property in the ImageNode class and se\u2026 (#6661)\n- Fix broken links in docs (#6669)\n- update milvus to store node content (#6667)\n\n## [v0.6.37] - 2023-06-30\n\n### New Features\n\n- add context augmented openai agent (#6655)\n\n## [v0.6.36] - 2023-06-29\n\n### New Features\n\n- Redis support for index stores and docstores (#6575)\n- DuckDB + SQL query engine notebook (#6628)\n- add notebook showcasing deplot data loader (#6638)\n\n### Bug Fixes / Nits\n\n- More robust JSON parsing from LLM for `SelectionOutputParser` (#6610)\n- bring our loaders back in line with llama-hub (#6630)\n- Remove usage of SQLStructStoreIndex in notebooks (#6585)\n- MD reader: remove html tags and leave linebreaks alone (#6618)\n- bump min langchain version to latest version (#6632)\n- Fix metadata column name in postgres vector store (#6622)\n- Postgres metadata fixes (#6626, #6634)\n- fixed links to dataloaders in contribution.md (#6636)\n- fix: typo in docs in creating custom_llm huggingface example (#6639)\n- Updated SelectionOutputParser to handle JSON objects and arrays (#6610)\n- Fixed docstring argument typo (#6652)\n\n## [v0.6.35] - 2023-06-28\n\n- refactor structured output + pydantic programs (#6604)\n\n### Bug Fixes / Nits\n\n- Fix serialization for OpenSearch vector stores (#6612)\n- patch docs relationships (#6606)\n- Bug fix for ignoring directories while parsing git repo (#4196)\n- updated Chroma notebook (#6572)\n- Backport old node name (#6614)\n- Add the ability to change chroma implementation (#6601)\n\n## [v0.6.34] - 2023-06-26\n\n### Patch Update (v0.6.34.post1)\n\n- Patch imports for Document obj for backwards compatibility (#6597)\n\n### New Features\n\n- New `TextNode`/`Document` object classes based on pydantic (#6586)\n- `TextNode`/`Document` objects support metadata customization (metadata templates, exclude metadata from LLM or embeddings) (#6586)\n- Nodes no longer require flat metadata dictionaries, unless the vector store you use requires it (#6586)\n\n### Bug Fixes / Nits\n\n- use `NLTK_DATA` env var to control NLTK download location (#6579)\n- [discord] save author as metadata in group_conversations.py (#6592)\n- bs4 -> beautifulsoup4 in requirements (#6582)\n- negate euclidean distance (#6564)\n- add df output parser notebook link to docs (#6581)\n\n### Breaking/Deprecated API Changes\n\n- `Node` has been renamed to `TextNode` and is imported from `llama_index.schema` (#6586)\n- `TextNode` and `Document` must be instantiated with kwargs: `Document(text=text)` (#6586)\n- `TextNode` (fka `Node`) has a `id_` or `node_id` property, rather than `doc_id` (#6586)\n- `TextNode` and `Document` have a metadata property, which replaces the extra_info property (#6586)\n- `TextNode` no longer has a `node_info` property (start/end indexes are accessed directly with `start/end_char_idx` attributes) (#6586)\n\n## [v0.6.33] - 2023-06-25\n\n### New Features\n\n- Add typesense vector store (#6561)\n- add df output parser (#6576)\n\n### Bug Fixes / Nits\n\n- Track langchain dependency via bridge module.", "mimetype": "text/plain", "start_char_idx": 145239, "end_char_idx": 148349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64141801-9822-4204-93e5-5e3120f2724d": {"__data__": {"id_": "64141801-9822-4204-93e5-5e3120f2724d", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9694e8bd-4a25-4afa-bc22-3f2dd81b599a", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "c6675dfda917ceaaef29036152179bef519550c73ab79567e038bf81592f71b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baedf5c7-709a-45fc-a244-4bafbc97a1e5", "node_type": "1", "metadata": {}, "hash": "c362398bc7a8f21064d3a700c5557e43c43e3acb493a45d5a7ecf8b616eec0de", "class_name": "RelatedNodeInfo"}}, "text": "(#6573)\n\n## [v0.6.32] - 2023-06-23\n\n### New Features\n\n- add object index (#6548)\n- add SQL Schema Node Mapping + SQLTableRetrieverQueryEngine + obj index fixes (#6569)\n- sql refactor (NLSQLTableQueryEngine) (#6529)\n\n### Bug Fixes / Nits\n\n- Update vector_stores.md (#6562)\n- Minor `BaseResponseBuilder` interface cleanup (#6557)\n- Refactor TreeSummarize (#6550)\n\n## [v0.6.31] - 2023-06-22\n\n### Bug Fixes / Nits\n\n- properly convert weaviate distance to score (#6545)\n- refactor tree summarize and fix bug to not truncate context (#6550)\n- fix custom KG retrieval notebook nits (#6551)\n\n## [v0.6.30] - 2023-06-21\n\n### New Features\n\n- multi-selector support in router query engine (#6518)\n- pydantic selector support in router query engine using OpenAI function calling API (#6518)\n- streaming response support in `CondenseQuestionChatEngine` and `SimpleChatEngine` (#6524)\n- metadata filtering support in `QdrantVectorStore` (#6476)\n- add `PGVectorStore` to support postgres with pgvector (#6190)\n\n### Bug Fixes / Nits\n\n- better error handling in the mbox reader (#6248)\n- Fix blank similarity score when using weaviate (#6512)\n- fix for sorted nodes in `PrevNextNodePostprocessor` (#6048)\n\n### Breaking/Deprecated API Changes\n\n- Refactor PandasQueryEngine to take in df directly, deprecate PandasIndex (#6527)\n\n## [v0.6.29] - 2023-06-20\n\n### New Features\n\n- query planning tool with OpenAI Function API (#6520)\n- docs: example of kg+vector index (#6497)\n- Set context window sizes for Cohere and AI21(J2 model) (#6485)\n\n### Bug Fixes / Nits\n\n- add default input size for Cohere and AI21 (#6485)\n- docs: replace comma with colon in dict object (#6439)\n- extra space in prompt and error message update (#6443)\n- [Issue 6417] Fix prompt_templates docs page (#6499)\n- Rip out monkey patch and update model to context window mapping (#6490)\n\n## [v0.6.28] - 2023-06-19\n\n### New Features\n\n- New OpenAI Agent + Query Engine Cookbook (#6496)\n- allow recursive data extraction (pydantic program) (#6503)\n\n### Bug Fixes / Nits\n\n- update mongo interface (#6501)\n- fixes that we forgot to include for openai pydantic program (#6503) (#6504)\n- Fix github pics in Airbyte notebook (#6493)\n\n## [v0.6.27] - 2023-06-16\n\n### New Features\n\n- Add node doc_id filtering to weaviate (#6467)\n- New `TokenCountingCallback` to customize and track embedding, prompt, and completion token usage (#6440)\n- OpenAI Retrieval Function Agent (#6491)\n\n### Breaking/Deprecated API Changes\n\n- Deprecated current token tracking (llm predictor and embed model will no longer track tokens in the future, please use the `TokenCountingCallback` (#6440)\n- Add maximal marginal relevance to the Simple Vector Store, which can be enabled as a query mode (#6446)\n\n### Bug Fixes / Nits\n\n- `as_chat_engine` properly inherits the current service context (#6470)\n- Use namespace when deleting from pinecone (#6475)\n- Fix paths when using fsspec on windows (#3778)\n- Fix for using custom file readers in `SimpleDirectoryReader` (#6477)\n- Edit MMR Notebook (#6486)\n- FLARE fixes (#6484)\n\n## [v0.6.26] - 2023-06-14\n\n### New Features\n\n- Add OpenAIAgent and tutorial notebook for \"build your own agent\" (#6461)\n- Add OpenAIPydanticProgram (#6462)\n\n### Bug Fixes / Nits\n\n- Fix citation engine import (#6456)\n\n## [v0.6.25] - 2023-06-13\n\n### New Features\n\n- Added FLARE query engine (#6419).\n\n## [v0.6.24] - 2023-06-12\n\n### New Features\n\n- Added better support for vector store with existing data (e.g. allow configurable text key) for Pinecone and Weaviate.", "mimetype": "text/plain", "start_char_idx": 148350, "end_char_idx": 151849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baedf5c7-709a-45fc-a244-4bafbc97a1e5": {"__data__": {"id_": "baedf5c7-709a-45fc-a244-4bafbc97a1e5", "embedding": null, "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421", "node_type": "4", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "b090065a7916f9ce966d28034a6adc63346ac6634569b633856d6d006ee0b4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64141801-9822-4204-93e5-5e3120f2724d", "node_type": "1", "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}, "hash": "0d57911399a9ce8a0f36a1f0ec046569444b20421dd59069a058e3e626d2dc91", "class_name": "RelatedNodeInfo"}}, "text": "allow configurable text key) for Pinecone and Weaviate. (#6393)\n- Support batched upsert for Pineone (#6393)\n- Added initial [guidance](https://github.com/microsoft/guidance/) integration. Added `GuidancePydanticProgram` for generic structured output generation and `GuidanceQuestionGenerator` for generating sub-questions in `SubQuestionQueryEngine` (#6246).\n\n## [v0.6.23] - 2023-06-11\n\n### Bug Fixes / Nits\n\n- Remove hardcoded chunk size for citation query engine (#6408)\n- Mongo demo improvements (#6406)\n- Fix notebook (#6418)\n- Cleanup RetryQuery notebook (#6381)\n\n## [v0.6.22] - 2023-06-10\n\n### New Features\n\n- Added `SQLJoinQueryEngine` (generalization of `SQLAutoVectorQueryEngine`) (#6265)\n- Added support for graph stores under the hood, and initial support for Nebula KG. More docs coming soon! (#2581)\n- Added guideline evaluator to allow llm to provide feedback based on user guidelines (#4664)\n- Added support for MongoDB Vector stores to enable Atlas knnbeta search (#6379)\n- Added new CitationQueryEngine for inline citations of sources in response text (#6239)\n\n### Bug Fixes\n\n- Fixed bug with `delete_ref_doc` not removing all metadata from the docstore (#6192)\n- FIxed bug with loading existing QDrantVectorStore (#6230)\n\n### Miscellaneous\n\n- Added changelog officially to github repo (#6191)\n\n## [v0.6.21] - 2023-06-06\n\n### New Features\n\n- SimpleDirectoryReader has new `filename_as_id` flag to automatically set the doc_id (useful for `refresh_ref_docs()`)\n- DocArray vector store integration\n- Tair vector store integration\n- Weights and Biases callback handler for tracing and versioning indexes\n- Can initialize indexes directly from a vector store: `index = VectorStoreIndex.from_vector_store(vector_store=vector_store)`\n\n### Bug Fixes\n\n- Fixed multimodal notebook\n- Updated/fixed the SQL tutorial in the docs\n\n### Miscellaneous\n\n- Minor docs updates\n- Added github pull-requset templates\n- Added github issue-forms\n\n## [v0.6.20] - 2023-06-04\n\n### New Features\n\n- Added new JSONQueryEngine that uses JSON schema to deliver more accurate JSON query answers\n- Metadata support for redis vector-store\n- Added Supabase vector store integration\n\n### Bug Fixes\n\n- Fixed typo in text-to-sql prompt\n\n### Breaking/Deprecated API Changes\n\n- Removed GPT prefix from indexes (old imports/names are still supported though)\n\n### Miscellaneous\n\n- Major docs updates, brought important modules to the top level\n\n## [v0.6.19] - 2023-06-02\n\n### New Features\n\n- Added agent tool abstraction for llama-hub data loaders\n\n### Miscellaneous\n\n- Minor doc updates\n\n## [v0.6.18] - 2023-06-02\n\n### Miscellaneous\n\n- Added `Discover LlamaIndex` video series to the tutorials docs section\n- Minor docs updates", "mimetype": "text/plain", "start_char_idx": 151794, "end_char_idx": 154498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1c8e200-b34e-450d-ac57-0f6e502ea432": {"__data__": {"id_": "a1c8e200-b34e-450d-ac57-0f6e502ea432", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "854cc8dd-8283-458c-b0f5-ea525a501db6", "node_type": "1", "metadata": {}, "hash": "f5455d8de658e037d89ebd8e71168153f9333f9f984aaae2a4b3cfb576e6497d", "class_name": "RelatedNodeInfo"}}, "text": "# Contributing to LlamaIndex\n\nInterested in contributing to LlamaIndex? Here's how to get started!\n\n## QuickStart\n\nFor python users who just want to dive in and start contributing, here's a quick guide on the env setup (if any of this doesn't make sense, read on to the [full guide](#development-guidelines)):\n\n1. Fork the repo and clone your fork\n2. `cd llama_index`\n3. Setup a new venv with `poetry shell`\n4. Install dev (and/or docs) dependencies with `poetry install --only dev,docs`\n5. Install the packages you intend to edit (i.e. `pip install -e  llama-index-core` or `pip install -e llama-index-integrations/llms/llama-index-llms-openai`)\n\n## Contribution Guideline\n\nThe best part of LlamaIndex is our community of users and contributors.\n\n### What should I work on?\n\n1. \ud83c\udd95 Extend core modules by contributing an integration\n2. \ud83d\udce6 Contribute a Tool, Reader, Pack, or Dataset (formerly from llama-hub)\n3. \ud83e\udde0 Add new capabilities to core\n4. \ud83d\udc1b Fix bugs\n5. \ud83c\udf89 Add usage examples\n6. \ud83e\uddea Add experimental features\n7. \ud83d\udcc4 Improve code quality & documentation\n\nAlso, join our Discord for ideas and discussions: <https://discord.gg/dGcwcsnxhU>.\n\n### 1. \ud83c\udd95 Extend Core Modules\n\nThe most impactful way to contribute to LlamaIndex is by extending our core modules:\n![LlamaIndex modules](https://github.com/run-llama/llama_index/raw/main/docs/docs/_static/contribution/contrib.png)\n\nWe welcome contributions in _all_ modules shown above.\nSo far, we have implemented a core set of functionalities for each, all of\nwhich are encapsulated in the LlamaIndex core package. As a contributor,\nyou can help each module unlock its full potential. Provided below are\nbrief description of these modules. You can also refer to their respective\nfolders within this Github repository for some example integrations.\n\nContributing an integration involves submitting the source code for a new Python\npackage. For now, these integrations will live in the LlamaIndex Github repository\nand the team will be responsible for publishing the package to PyPi. (Having\nthese packages live outside of this repository and maintained by our community\nmembers is in consideration.)\n\n#### Creating A New Integration Package\n\nBoth `llama-index` and `llama-index-core` come equipped\nwith a command-line tool that can be used to initialize a new integration package.\n\n```shell\ncd ./llama-index-integrations/llms\nllamaindex-cli new-package --kind \"llms\" --name \"gemini\"\n```\n\nExecuting the above commands will create a new folder called `llama-index-llms-gemini`\nwithin the `llama-index-integrations/llms` directory.\n\nPlease ensure to add a detailed README for your new package as it will appear in\nboth [llamahub.ai](https://llamahub.ai) as well as the PyPi.org website.\nIn addition to preparing your source code and supplying a detailed README, we\nalso ask that you fill in some\nmetadata for your package to appear in [llamahub.ai](https://llamahub.ai) with the\ncorrect information. You do so by adding the required metadata under the `[tool.llamahub]`\nsection with your new package's `pyproject.toml`.\n\nBelow is the example of the metadata required for all of our integration packages. Please\nreplace the default author \"llama-index\" with your own Github user name.\n\n```toml\n[tool.llamahub]\ncontains_example = false\nimport_path = \"llama_index.llms.anthropic\"\n\n[tool.llamahub.class_authors]\nAnthropic = \"llama-index\"\n```\n\n([source](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/llms/llama-index-llms-anthropic/pyproject.toml))\n\n#### Module Details\n\nBelow, we will describe what each module does, give a high-level idea of the interface, show existing implementations, and give some ideas for contribution.\n\n---\n\n#### Data Loaders\n\nA data loader ingests data of any format from anywhere into `Document` objects, which can then be parsed and indexed.\n\n**Interface**:\n\n- `load_data` takes arbitrary arguments as input (e.g. path to data), and outputs a sequence of `Document` objects.\n- `lazy_load_data` takes arbitrary arguments as input (e.g.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "854cc8dd-8283-458c-b0f5-ea525a501db6": {"__data__": {"id_": "854cc8dd-8283-458c-b0f5-ea525a501db6", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1c8e200-b34e-450d-ac57-0f6e502ea432", "node_type": "1", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "cd3f09d0d2431da003f8109e3b2055c5b30396a85fd228179ce20ad3733a0036", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de201b1-8334-4afa-8b37-aa214a838d04", "node_type": "1", "metadata": {}, "hash": "315d1c8550d052e7f8e58e152169eb133fa7b59404ad44d5f75ac0620be3345a", "class_name": "RelatedNodeInfo"}}, "text": "path to data), and outputs a sequence of `Document` objects.\n- `lazy_load_data` takes arbitrary arguments as input (e.g. path to data), and outputs an iterable object of `Document` objects. This is a lazy version of `load_data`, which is useful for large datasets.\n\n> **Note**: If only `lazy_load_data` is implemented, `load_data` will be delegated to it.\n\n**Examples**:\n\n- [Database Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-database)\n- [Jira Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-jira)\n- [MongoDB Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-mongodb)\n\nContributing a data loader is easy and super impactful for the community.\n\n**Ideas**\n\n- Want to load something but there's no LlamaHub data loader for it yet? Make a PR!\n\n---\n\n#### Node Parser\n\nA node parser parses `Document` objects into `Node` objects (atomic units of data that LlamaIndex operates over, e.g., chunk of text, image, or table).\nIt is responsible for splitting text (via text splitters) and explicitly modeling the relationship between units of data (e.g. A is the source of B, C is a chunk after D).\n\n**Interface**: `get_nodes_from_documents` takes a sequence of `Document` objects as input, and outputs a sequence of `Node` objects.\n\n**Examples**:\n\n- [Hierarchical Node Parser](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/relational/hierarchical.py)\n\nSee [the API reference](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/) for full details.\n\n**Ideas**:\n\n- Add new `Node` relationships to model hierarchical documents (e.g. play-act-scene, chapter-section-heading).\n\n---\n\n#### Text Splitters\n\nText splitter splits a long text `str` into smaller text `str` chunks with desired size and splitting \"strategy\" since LLMs have a limited context window size, and the quality of text chunk used as context impacts the quality of query results.\n\n**Interface**: `split_text` takes a `str` as input, and outputs a sequence of `str`\n\n**Examples**:\n\n- [Token Text Splitter](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/token.py)\n- [Sentence Splitter](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/sentence.py)\n- [Code Splitter](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/code.py)\n\n---\n\n#### Document/Index/KV Stores\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize Document Stores (where ingested documents (i.e., `Node` objects) are stored), and Index Stores (where index metadata are stored)\n\nWe have an underlying key-value abstraction backing the document/index stores.\nCurrently we support in-memory and MongoDB storage for these stores. Open to contributions!\n\nSee [the API reference](https://docs.llamaindex.ai/en/stable/api_reference/storage/kvstore/) for details.\n\n---\n\n#### Managed Index\n\nA managed index is used to represent an index that's managed via an API, exposing API calls to index documents and query documents.\n\nFor example, we support the [VectaraIndex](https://github.com/run-llama/llama_index/tree/ca09272af000307762d301c99da46ddc70d3bfd2/llama_index/indices/managed/vectara).\nOpen to contributions!\n\nSee [Managed Index docs](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices/) for details.\n\n---\n\n#### Vector Stores\n\nOur vector store classes store embeddings and support lookup via similarity search.\nThese serve as the main data store and retrieval engine for our vector index.\n\n**Interface**:\n\n- `add` takes in a sequence of `NodeWithEmbeddings` and inserts the embeddings (and possibly the node contents & metadata) into the vector store.\n- `delete` removes entries given document IDs.\n- `query` retrieves top-k most similar entries given a query embedding.", "mimetype": "text/plain", "start_char_idx": 3903, "end_char_idx": 7978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6de201b1-8334-4afa-8b37-aa214a838d04": {"__data__": {"id_": "6de201b1-8334-4afa-8b37-aa214a838d04", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "854cc8dd-8283-458c-b0f5-ea525a501db6", "node_type": "1", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc92ef77ef04c4684ab9678d2ae74f39dfb320cf0c94bbb525f31966c98d495a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7be4176-06cc-45b8-bebe-4c619f551900", "node_type": "1", "metadata": {}, "hash": "b975c64523b2a18eda33448e581ac66f6c93bb9df84611a1ac61eb954ef474cf", "class_name": "RelatedNodeInfo"}}, "text": "- `delete` removes entries given document IDs.\n- `query` retrieves top-k most similar entries given a query embedding.\n- `get_nodes` get nodes by ID or filters\n- `delete_nodes` delete nodes by ID or filters\n- `clear` clears an entire db of data\n\n**Examples**:\n\n- [Chroma](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-chroma)\n- [Qdrant](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-qdrant)\n- [Pinecone](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-pinecone)\n- [Faiss](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-faiss)\n\n**Ideas**:\n\n- See a vector database out there that we don't support yet? A vector store missing methods like `get_nodes` and `delete_nodes`? Make a PR!\n\nSee [reference](https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/) for full details.\n\n---\n\n#### Retrievers\n\nOur retriever classes are lightweight classes that implement a `retrieve` method.\nThey may take in an index class as input - by default, each of our indices\n(list, vector, keyword) has an associated retriever. The output is a set of\n`NodeWithScore` objects (a `Node` object with an extra `score` field).\n\nYou may also choose to implement your own retriever classes on top of your own\ndata if you wish.\n\n**Interface**:\n\n- `retrieve` takes in a `str` or `QueryBundle` as input, and outputs a list of `NodeWithScore` objects\n\n**Examples**:\n\n- [Vector Index Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py)\n- [Property Graph Index Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/property_graph/retriever.py)\n- [Router Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/router_retriever.py)\n\n**Ideas**:\n\n- Besides the \"default\" retrievers built on top of each index, what about fancier retrievers? E.g. retrievers that take in other retrievers as input? Or other types of data?\n\n---\n\n#### Query Engines\n\nOur query engine classes are lightweight classes that implement a `query` method; the query returns a response type.\nFor instance, they may take in a retriever class as input; our `RetrieverQueryEngine`\ntakes in a `retriever` as input as well as a `BaseSynthesizer` class for response synthesis, and\nthe `query` method performs retrieval and synthesis before returning the final result.\nThey may take in other query engine classes as input too.\n\n**Interface**:\n\n- `query` takes in a `str` or `QueryBundle` as input, and outputs a `Response` object.\n\n**Examples**:\n\n- [Retriever Query Engine](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/query_engine/retriever_query_engine.py)\n- [Citation Query Engine](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/query_engine/citation_query_engine.py)\n\n---\n\n#### Query Transforms\n\nA query transform augments a raw query string with associated transformations to improve index querying.\nThis can interpreted as a pre-processing stage, before the core index query logic is executed.\n\n**Interface**: `run` takes in a `str` or `Querybundle` as input, and outputs a transformed `QueryBundle`.", "mimetype": "text/plain", "start_char_idx": 7860, "end_char_idx": 11354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7be4176-06cc-45b8-bebe-4c619f551900": {"__data__": {"id_": "c7be4176-06cc-45b8-bebe-4c619f551900", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6de201b1-8334-4afa-8b37-aa214a838d04", "node_type": "1", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "f2f4311ec9e4851960681ecbcc991469e70165b5cc5f6e796c78e7506e7b984d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "029e0587-a00e-4065-a6d8-a06346b333a7", "node_type": "1", "metadata": {}, "hash": "f443cad6fb6e87a3e1da9b6f3eda562f676fbc3ac884a78800211e191368fc55", "class_name": "RelatedNodeInfo"}}, "text": "**Interface**: `run` takes in a `str` or `Querybundle` as input, and outputs a transformed `QueryBundle`.\n\n**Examples**:\n\n- [Hypothetical Document Embeddings](https://github.com/run-llama/llama_index/blob/e490158e1562c903d99a7fb4a3cb4407b192d63a/llama-index-core/llama_index/core/indices/query/query_transform/base.py#L109)\n- [Query Decompose](https://github.com/run-llama/llama_index/blob/e490158e1562c903d99a7fb4a3cb4407b192d63a/llama-index-core/llama_index/core/indices/query/query_transform/base.py#L165)\n\nSee [guide](https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/?h=query+transform) for more information.\n\n---\n\n#### Node Postprocessors\n\nA node postprocessor refines a list of retrieved nodes given configuration and context.\n\n**Interface**: `postprocess_nodes` takes a list of `Nodes` and extra metadata (e.g. similarity and query), and outputs a refined list of `Nodes`.\n\n**Examples**:\n\n- [Keyword Postprocessor](https://github.com/run-llama/llama_index/blob/e490158e1562c903d99a7fb4a3cb4407b192d63a/llama-index-core/llama_index/core/postprocessor/node.py#L20): filters nodes based on keyword match\n- [Colbert Rerank Postprocessor](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-colbert-rerank/llama_index/postprocessor/colbert_rerank): reranks retrieved nodes.\n- [Presidio Postprocessor](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-presidio): provides some data privacy on retrieved nodes by omitting personal information.\n\n---\n\n#### Output Parsers\n\nAn output parser enables us to extract structured output from the plain text output generated by the LLM.\n\n**Interface**:\n\n- `format`: formats a query `str` with structured output formatting instructions, and outputs the formatted `str`\n- `parse`: takes a `str` (from LLM response) as input, and gives a parsed structured output (optionally also validated, error-corrected).\n\n**Examples**:\n\n- [Guardrails Output Parser](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/output_parsers/llama-index-output-parsers-guardrails)\n- [Langchain Output Parser](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/output_parsers/llama-index-output-parsers-langchain)\n\nSee [guide](https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/) for more information.\n\n---\n\n### 2. \ud83d\udce6 Contribute a Pack, Reader, Tool, or Dataset (formerly from llama-hub)\n\nContributing a new Reader or Tool involves submitting a new package within\nthe [llama-index-integrations/readers](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers) and [llama-index-integrations/tools](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools),\nfolders respectively.\n\nThe LlamaIndex command-line tool can be used to initialize new Packs and Integrations. (NOTE: `llama-index-cli` comes installed with `llama-index`.)\n\n```shell\ncd ./llama-index-packs\nllamaindex-cli new-package --kind \"packs\" --name \"my new pack\"\n\ncd ./llama-index-integrations/readers\nllamaindex-cli new-package --kind \"readers\" --name \"new reader\"\n```\n\nExecuting the first set of shell commands will create a new folder called `llama-index-packs-my-new-pack`\nwithin the `llama-index-packs` directory. While the second set will create a new\npackage directory called `llama-index-readers-new-reader` within the `llama-index-integrations/readers` directory.\n\nPlease ensure to add a detailed README for your new package as it will appear in\nboth [llamahub.ai](https://llamahub.ai) as well as the PyPi.org website.", "mimetype": "text/plain", "start_char_idx": 11249, "end_char_idx": 14955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "029e0587-a00e-4065-a6d8-a06346b333a7": {"__data__": {"id_": "029e0587-a00e-4065-a6d8-a06346b333a7", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7be4176-06cc-45b8-bebe-4c619f551900", "node_type": "1", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "454f8e8cc3cf4ef1f9d3cf64cd163850a510c5303d4d444d59b4786f2a216c70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52f42635-7b7b-4392-abf3-cc931a7fcd76", "node_type": "1", "metadata": {}, "hash": "af803b97b65f112bb660b9907b0f4dd795ad741c10b9fb5b23bb2b1b7c4681f8", "class_name": "RelatedNodeInfo"}}, "text": "In addition to preparing your source code and supplying a detailed README, we\nalso ask that you fill in some\nmetadata for your package to appear in [llamahub.ai](https://llamahub.ai) with the\ncorrect information. You do so by adding the required metadata under the `[tool.llamahub]`\nsection with your new package's `pyproject.toml`.\n\nBelow is the example of the metadata required for packs, readers and tools:\n\n```toml\n[tool.llamahub]\ncontains_example = true\nimport_path = \"llama_index.packs.agent_search_retriever\"\n\n[tool.llamahub.class_authors]\nAgentSearchRetrieverPack = \"logan-markewich\"\n```\n\n([source](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-agent-search-retriever/pyproject.toml))\n\n### 3. \ud83e\udde0 Add new capabilities to core\n\nWe would greatly appreciate any and all contributions to our core abstractions\nthat represent enhancements from the current set of capabilities.\nGeneral improvements that make these core abstractions more robust and thus\neasier to build on are also welcome!\n\n### 4. \ud83d\udc1b Fix Bugs\n\nMost bugs are reported and tracked in the [Github Issues Page](https://github.com/run-llama/llama_index/issues).\nWe try our best in triaging and tagging these issues:\n\n- Issues tagged as `bug` are confirmed bugs.\n- New contributors may want to start with issues tagged with `good first issue`.\n\nPlease feel free to open an issue and/or assign an issue to yourself.\n\n### 5. \ud83c\udf89 Add Usage Examples\n\nIf you have applied LlamaIndex to a unique use-case (e.g. interesting dataset, customized index structure, complex query), we would love your contribution in the form of:\n\n1. a guide: e.g. [Guide to LlamIndex + Structured Data](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/structured_data/)\n2. an example notebook: e.g. [Email Info Extraction](https://docs.llamaindex.ai/en/stable/examples/usecases/email_data_extraction/)\n\n### 6. \ud83e\uddea Add Experimental Features\n\nIf you have a crazy idea, make a PR for it!\nWhether if it's the latest research, or what you thought of in the shower, we'd love to see creative ways to improve LlamaIndex.\n\n### 7. \ud83d\udcc4 Improve Code Quality & Documentation\n\nWe would love your help in making the project cleaner, more robust, and more understandable. If you find something confusing, it most likely is for other people as well. Help us be better!\n\n## Development Guidelines\n\n### Repo Structure\n\nThe `llama_index` repo is structured as a mono-repo of many packages. For example, `llama-index-core/`, `llama-index-integrations/llms/llama-index-llms-openai`, and `llama-index-integrations/embeddings/llama-index-embeddings-openai` are all separate python packages. This organization should hopefully direct you to where you want to make a change or add a new modules.\n\n### Setting up environment\n\nLlamaIndex is a Python package. We've tested primarily with Python versions >= 3.8. Here's a quick\nand dirty guide to setting up your environment for local development.\n\n1. Fork [LlamaIndex Github repo][ghr]\\* and clone it locally. (New to GitHub / git? Here's [how][frk].)\n2. In a terminal, `cd` into the directory of your local clone of your forked repo.\n3. Install [pre-commit hooks][pch]\\* by running `pre-commit install`. These hooks are small house-keeping scripts executed every time you make a git commit, which automates away a lot of chores.\n4. Prepare a [virtual environment][vev].\n   1. [Install Poetry][pet]\\*. This will help you manage package dependencies.\n   2. Execute `poetry shell`. This command will create a [virtual environment][vev] specific for this package, which keeps installed packages contained to this project. (New to Poetry, the dependency & packaging manager for Python? Read about its basic usage [here][bus].)\n   3. Execute `poetry install --only dev,docs`\\*. This will install all dependencies needed for local development. To see what will be installed, read the `pyproject.toml` under that directory.\n5. `cd` into the specific package you want to work on.", "mimetype": "text/plain", "start_char_idx": 14956, "end_char_idx": 18943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52f42635-7b7b-4392-abf3-cc931a7fcd76": {"__data__": {"id_": "52f42635-7b7b-4392-abf3-cc931a7fcd76", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "029e0587-a00e-4065-a6d8-a06346b333a7", "node_type": "1", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "30ca567e80abf82d69e52474e17109377a64bd633ae3ccfde567e2984db695aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b7e4be6-e3e3-4d68-93df-f307de14a25c", "node_type": "1", "metadata": {}, "hash": "915f5d9f42aaa7ae8999ded4a3531ad0359c34f2b2f106f062ecebdc6756e9cf", "class_name": "RelatedNodeInfo"}}, "text": "5. `cd` into the specific package you want to work on. For example, if I want to work on the core package, I execute `cd llama-index-core/`. (New to terminal / command line? Here's a [getting started guide][gsg].)\n6. Install that specific integration with `pip install -e .` (or alternatively, `pip install -e <path to package>`). This will install the package in editable mode, which means any changes you make to that package will show up when you run your code again. **NOTE:** If working in a notebook, you will need to restart it for changes to packages to show up.\n\n[frk]: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo\n[ghr]: https://github.com/run-llama/llama_index/\n[pch]: https://pre-commit.com/\n[gsg]: https://www.freecodecamp.org/news/command-line-for-beginners/\n[pet]: https://python-poetry.org/docs/#installation\n[vev]: https://python-poetry.org/docs/managing-environments/\n[bus]: https://python-poetry.org/docs/basic-usage/\n\nSteps marked with an asterisk (`*`) are one-time tasks. You don't have to repeat them when you attempt to contribute on something else next time.\n\nNow you should be set!\n\n### Validating your Change\n\nLet's make sure to `format/lint` our change. For bigger changes,\nlet's also make sure to `test` it and perhaps create an `example notebook`.\n\n#### Formatting/Linting\n\nWe run an assortment of linters: `black`, `ruff`, `mypy`.\n\nIf you have installed pre-commit hooks in this repo, they should have taken care of the formatting and linting automatically.\n\nIf -- for whatever reason -- you would like to do it manually, you can format and lint your changes with the following commands in the root directory:\n\n```bash\nmake format; make lint\n```\n\nUnder the hood, we still install pre-commit hooks for you, so that you don't have to do this manually next time.\n\n#### Testing\n\nIf you modified or added code logic, **create test(s)**, because they help preventing other maintainers from accidentally breaking the nice things you added / re-introducing the bugs you fixed.\n\n- In almost all cases, add **unit tests**.\n- If your change involves adding a new integration, also add **integration tests**. When doing so, please [mock away][mck] the remote system that you're integrating LlamaIndex with, so that when the remote system changes, LlamaIndex developers won't see test failures.\n\nReciprocally, you should **run existing tests** (from every package that you touched) before making a git commit, so that you can be sure you didn't break someone else's good work.\n\n(By the way, when a test is run with the goal of detecting whether something broke in a new version of the codebase, it's referred to as a \"[regression test][reg]\". You'll also hear people say \"the test _regressed_\" as a more diplomatic way of saying \"the test _failed_\".)\n\nOur tests are stored in the `tests` folders under each package directory. We use the testing framework [pytest][pyt], so you can **just run `pytest` in each package you touched** to run all its tests.\n\nRegardless of whether you have run them locally, a [CI system][cis] will run all affected tests on your PR when you submit one anyway. There, tests are orchestrated with [Pants][pts], the build system of our choice. There is a slight chance that tests broke on CI didn't break on your local machine or the other way around. When that happens, please take our CI as the source of truth. This is because our release pipeline (which builds the packages users are going to download from PyPI) are run in the CI, not on your machine (even if you volunteer), so it's the CI that is the golden standard.\n\n[reg]: https://www.browserstack.com/guide/regression-testing\n[mck]: https://pytest-mock.readthedocs.io/en/latest/\n[pyt]: https://docs.pytest.org/\n[mkf]: https://makefiletutorial.com/\n[cis]: https://www.atlassian.com/continuous-delivery/continuous-integration\n[pts]: https://www.pantsbuild.org/\n\n### Creating an Example Notebook\n\nFor changes that involve entirely new features, it may be worth adding an example Jupyter notebook to showcase\nthis feature.\n\nExample notebooks can be found in [this folder](https://github.com/run-llama/llama_index/tree/main/docs/docs/examples).", "mimetype": "text/plain", "start_char_idx": 18889, "end_char_idx": 23093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b7e4be6-e3e3-4d68-93df-f307de14a25c": {"__data__": {"id_": "3b7e4be6-e3e3-4d68-93df-f307de14a25c", "embedding": null, "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "988ecacad3cb4bb087af10226cb90ba28644358b", "node_type": "4", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "dc177bce95546ee5b22b48c80784bade319bd2a2622f702f1b53804f7585ec01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52f42635-7b7b-4392-abf3-cc931a7fcd76", "node_type": "1", "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}, "hash": "ac7921eb53e953404e1e3db6d6556a413f4216164ed19afd2cfbae9a4bae4d5d", "class_name": "RelatedNodeInfo"}}, "text": "Example notebooks can be found in [this folder](https://github.com/run-llama/llama_index/tree/main/docs/docs/examples).\n\n### Creating a pull request\n\nSee [these instructions](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork)\nto open a pull request against the main LlamaIndex repo.", "mimetype": "text/plain", "start_char_idx": 22974, "end_char_idx": 23366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2742e447-2626-41af-b45d-3710c119dccc": {"__data__": {"id_": "2742e447-2626-41af-b45d-3710c119dccc", "embedding": null, "metadata": {"filename": "DOCS_README.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0833904f0cb3e584c30ea17552ca65d29c8ec2b", "node_type": "4", "metadata": {"filename": "DOCS_README.md", "author": "LlamaIndex"}, "hash": "9bfd6fcfb8e0a7bd043acc05c3d19cf851dfe01ef7e3b53c963af8abb4375cb1", "class_name": "RelatedNodeInfo"}}, "text": "# Documentation Guide\n\n## A guide for docs contributors\n\nThe `docs` directory contains the sphinx source text for LlamaIndex docs, visit\nhttps://docs.llamaindex.ai/en/stable/ to read the full documentation.\n\nThis guide is made for anyone who's interested in running LlamaIndex documentation locally,\nmaking changes to it and making contributions. LlamaIndex is made by the thriving community\nbehind it, and you're always welcome to make contributions to the project and the\ndocumentation.\n\n## Build Docs\n\nIf you haven't already, clone the LlamaIndex Github repo to a local directory:\n\n```bash\ngit clone https://github.com/run-llama/llama_index.git && cd llama_index\n```\n\nInstall all dependencies required for building docs (mainly `mkdocs` and its extension):\n\n- [Install poetry](https://python-poetry.org/docs/#installation) - this will help you manage package dependencies\n- `poetry shell` - this command creates a virtual environment, which keeps installed packages contained to this project\n- `poetry install --only docs` - this will install all dependencies needed for building docs\n\nBuild with mkdocs:\n\n```bash\ncd docs\nmkdocs serve --dirty\n```\n\n**NOTE:** The `--dirty` option will mean that only changed files will be re-built, decreasing the time it takes to iterate on a page.\n\nAnd open your browser at http://localhost:8000/ to view the generated docs.\n\nThis hosted version will re-build and update as changes are made to the docs.\n\n## Config\n\nAll config for mkdocs is in the `mkdocs.yml` file.\n\nRunning the command `python docs/prepare_for_build.py` from the root of the llama-index repo will update the mkdocs.yml API Reference and examples nav with the latest changes, as well as writing new api reference files.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e90053a0-d6d8-4d53-8647-d854c523a368": {"__data__": {"id_": "e90053a0-d6d8-4d53-8647-d854c523a368", "embedding": null, "metadata": {"filename": "coa.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69ca05844fee838474d2f699701f5b17e635f74d", "node_type": "4", "metadata": {"filename": "coa.md", "author": "LlamaIndex"}, "hash": "fb1288aaf9c8fe246f2b06665f692e5af6a46bc3db540386656f00d5d28931bb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.coa\n    options:\n      members:\n        - CoAAgentWorker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88e74cb6-b027-4a57-92dd-e59bc6155cc5": {"__data__": {"id_": "88e74cb6-b027-4a57-92dd-e59bc6155cc5", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "603373f629406ccc3e1b52c40fa0eb613eb35457", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "1feba33d39337061f0f0fe9c5f98ff8bdb21703173105c9e7f3000dfea4f9774", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.dashscope\n    options:\n      members:\n        - DashScopeAgent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74c21f5b-c2e1-4e28-8925-5b518d20052f": {"__data__": {"id_": "74c21f5b-c2e1-4e28-8925-5b518d20052f", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17ae72b0a22f88e84377b8a0ffb1ae56cd6ba12b", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "ad4aade27e986d0684ef133c26a527a11ef751f745cb91c4a02d84d1fa3f1411", "class_name": "RelatedNodeInfo"}}, "text": "# Core Agent Classes\n\n## Base Types\n\n::: llama_index.core.agent.types\n    options:\n        heading_level: 3\n        members:\n            - BaseAgent\n            - BaseAgentWorker\n            - AgentChatResponse\n            - Task\n            - TaskStep\n            - TaskStepOutput\n\n## Runners\n\n::: llama_index.core.agent\n    options:\n      heading_level: 3\n      members:\n        - AgentRunner\n        - ParallelAgentRunner\n\n## Workers\n\n::: llama_index.core.agent\n    options:\n      heading_level: 3\n      members:\n        - CustomSimpleAgentWorker\n        - MultimodalReActAgentWorker\n        - QueryPipelineAgentWorker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68977cbe-09b8-4be1-bffd-66150b67d4bb": {"__data__": {"id_": "68977cbe-09b8-4be1-bffd-66150b67d4bb", "embedding": null, "metadata": {"filename": "introspective.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f616528dd75e506a8a56ccfb0cfb05ae13c7f85b", "node_type": "4", "metadata": {"filename": "introspective.md", "author": "LlamaIndex"}, "hash": "933580d417d3606652b7c0aa3de8e491c1b0ca8ed66cc10ab2d70539d8807bc4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.introspective\n    options:\n      members:\n        - IntrospectiveAgentWorker\n        - SelfReflectionAgentWorker\n        - ToolInteractiveReflectionAgentWorker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba7952cd-0dda-4fa3-a9e6-a32deba4eb25": {"__data__": {"id_": "ba7952cd-0dda-4fa3-a9e6-a32deba4eb25", "embedding": null, "metadata": {"filename": "lats.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d4a7ee83286a7f6372eb2170aca4ea1a5ed07d5", "node_type": "4", "metadata": {"filename": "lats.md", "author": "LlamaIndex"}, "hash": "9e3d24ca6d647b2f45e17d1beae4c7c40544242498ed75d1b0e5903bd925026a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.lats\n    options:\n      members:\n        - LATSAgentWorker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b6cbf9f-0880-4d3a-8248-3e42a3bd0159": {"__data__": {"id_": "7b6cbf9f-0880-4d3a-8248-3e42a3bd0159", "embedding": null, "metadata": {"filename": "llm_compiler.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f38e027b0cc23677cb881afa76a110b6475461a", "node_type": "4", "metadata": {"filename": "llm_compiler.md", "author": "LlamaIndex"}, "hash": "5e443cfa9718cc60a5428ae491b5a1db8a930a9570848c323207b0b6f29bef21", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.llm_compiler\n    options:\n      members:\n        - LLMCompilerAgentWorker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1b0d932-082a-44e2-a23e-6174a76a253d": {"__data__": {"id_": "e1b0d932-082a-44e2-a23e-6174a76a253d", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "287aae35a75dc0952398524a3f68b45b27ce867b", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "3dee6742545ce9002288fe1401e21ad5bfe0c3f99655ea8333116038ac0359f2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.openai\n    options:\n      members:\n        - OpenAIAgent\n        - OpenAIAssistantAgent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f4a17ad-c422-49d9-92fe-7a4612024492": {"__data__": {"id_": "4f4a17ad-c422-49d9-92fe-7a4612024492", "embedding": null, "metadata": {"filename": "openai_legacy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a5ae2117aec4e482af006c69ef5ccdade10fdb2", "node_type": "4", "metadata": {"filename": "openai_legacy.md", "author": "LlamaIndex"}, "hash": "d682f22587302781f5c41f599c6d63ca7d69758568e5f5a3f03f59738a82b280", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.agent.openai_legacy\n    options:\n      members:\n        - ContextRetrieverOpenAIAgent\n        - FnRetrieverOpenAIAgent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a907c07-3a30-4e32-9f3a-cde247efa738": {"__data__": {"id_": "9a907c07-3a30-4e32-9f3a-cde247efa738", "embedding": null, "metadata": {"filename": "react.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4af66f1c81f129a0c5969183122e82bc38e1894c", "node_type": "4", "metadata": {"filename": "react.md", "author": "LlamaIndex"}, "hash": "1ae6498b7f280c4d2c8191e22f95328de8496282ad91119813cf28e534c70aa6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.agent.react\n    options:\n      members:\n        - ReActAgent\n        - ReActAgentWorker\n        - ReActChatFormatter", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7989c642-bdef-4462-80f5-f92dc3c69a35": {"__data__": {"id_": "7989c642-bdef-4462-80f5-f92dc3c69a35", "embedding": null, "metadata": {"filename": "agentops.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0cc9017a2af39284ef0b905c15f7491cb10dfd1", "node_type": "4", "metadata": {"filename": "agentops.md", "author": "LlamaIndex"}, "hash": "95cc5a30424f28d409500c5be21039c6ec7b5d6beeffe4b385917483aa40bf9b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.agentops\n    options:\n      members:\n        - AgentOpsEventHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffe31ca3-208b-479c-af24-37269a933e4f": {"__data__": {"id_": "ffe31ca3-208b-479c-af24-37269a933e4f", "embedding": null, "metadata": {"filename": "aim.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39dd143435c8abf079e0a2092f0b2ab6be3f80f2", "node_type": "4", "metadata": {"filename": "aim.md", "author": "LlamaIndex"}, "hash": "f701ee7d6a2b9f721ecc00b13eb1e68f20f60a06d1bb4d6b3084b56ba9ea0f0e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.aim\n    options:\n      members:\n        - AimCallback", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d19041e6-1840-40c1-815a-b572885325ee": {"__data__": {"id_": "d19041e6-1840-40c1-815a-b572885325ee", "embedding": null, "metadata": {"filename": "argilla.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2646025f16cf79a5afbbfda01563ce0beee9f097", "node_type": "4", "metadata": {"filename": "argilla.md", "author": "LlamaIndex"}, "hash": "7c5224d10ebfd12dc6dee314b6762421b35618da5912bc9367aa5a39e714cb79", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.argilla\n    options:\n      members:\n        - argilla_callback_handler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4781a09-f477-4e08-9821-5672418c7806": {"__data__": {"id_": "e4781a09-f477-4e08-9821-5672418c7806", "embedding": null, "metadata": {"filename": "arize_phoenix.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "757f1e0deb93afae14ffaa7599289796dcad6840", "node_type": "4", "metadata": {"filename": "arize_phoenix.md", "author": "LlamaIndex"}, "hash": "9c0a1d5319653eff53a6a14d887a81515716c08e5da33168b6181483cae976a1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.arize_phoenix\n    options:\n      members:\n        - arize_phoenix_callback_handler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "462dbdab-caad-43e7-a108-a68ec651e23f": {"__data__": {"id_": "462dbdab-caad-43e7-a108-a68ec651e23f", "embedding": null, "metadata": {"filename": "deepeval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aab0d51281e9294d03fbdadc24e31f57e19e1c47", "node_type": "4", "metadata": {"filename": "deepeval.md", "author": "LlamaIndex"}, "hash": "070bf14b7e006ae849e1faa0bb116b13b20620bdf7ee85cac531bd543604d4c3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.deepeval\n    options:\n      members:\n        - deepeval_callback_handler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e60145ca-a605-4cb8-a459-d0c468e08869": {"__data__": {"id_": "e60145ca-a605-4cb8-a459-d0c468e08869", "embedding": null, "metadata": {"filename": "honeyhive.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90cbae31b63e7e07f4bc552f8e6e536d789d260b", "node_type": "4", "metadata": {"filename": "honeyhive.md", "author": "LlamaIndex"}, "hash": "29b4ac78e61b93d4ed7f072ba1b74c0d8f89493d4806ba32d0ce85364314ec1f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.honeyhive\n    options:\n      members:\n        - honeyhive_callback_handler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebd04fac-a813-40f5-be8b-98bc83615fc5": {"__data__": {"id_": "ebd04fac-a813-40f5-be8b-98bc83615fc5", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42380e7880a85169f13f65af52b8cec8eb056ac8", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "3445b8aef6cb09d42477ba96c30d7afb3894a2c92f15a0b9401f7fac17efc863", "class_name": "RelatedNodeInfo"}}, "text": "# Core Callback Classes\n\n::: llama_index.core.callbacks.base\n    options:\n      members:\n        - CallbackManager\n\n::: llama_index.core.callbacks.base_handler\n    options:\n      members:\n        - BaseCallbackHandler\n\n::: llama_index.core.callbacks.schema\n    options:\n      members:\n        - CBEvent\n        - CBEventType\n        - EventPayload", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f309e88-3c76-4169-a922-fe8a9df50868": {"__data__": {"id_": "9f309e88-3c76-4169-a922-fe8a9df50868", "embedding": null, "metadata": {"filename": "langfuse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b53cf4d8ba360a2757bf01bb6c6e32c2c3ef67a", "node_type": "4", "metadata": {"filename": "langfuse.md", "author": "LlamaIndex"}, "hash": "6c61ec38f21b9f7f851bd8ed492fdcf0c798a0e36f0b81a94ef4c9b4b4d24215", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.langfuse\n    options:\n      members:\n        - langfuse_callback_handler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96ce022a-68b4-4646-bd46-47f7d0800554": {"__data__": {"id_": "96ce022a-68b4-4646-bd46-47f7d0800554", "embedding": null, "metadata": {"filename": "llama_debug.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2bf6d446f263898423a27c71e85d004d8fdee467", "node_type": "4", "metadata": {"filename": "llama_debug.md", "author": "LlamaIndex"}, "hash": "5352cba113e12ebbb346e23f69e73536cc9904f7a7c933cdc122ba88f40da1e4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.callbacks.llama_debug\n    options:\n      members:\n        - LlamaDebugHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52e68092-2c76-40ae-ab58-8f3e0f8dd877": {"__data__": {"id_": "52e68092-2c76-40ae-ab58-8f3e0f8dd877", "embedding": null, "metadata": {"filename": "openinference.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0190c850bf0c26acd86872d3abc6e39a542b5482", "node_type": "4", "metadata": {"filename": "openinference.md", "author": "LlamaIndex"}, "hash": "b4bc9e2c73f54c56c9d3bce48a51c418a4053687a12e1dfab10ba329b9be4c99", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.openinference\n    options:\n      members:\n        - OpenInferenceCallbackHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4bf58cc-7a88-45e8-99a3-1cec931c9ad9": {"__data__": {"id_": "c4bf58cc-7a88-45e8-99a3-1cec931c9ad9", "embedding": null, "metadata": {"filename": "promptlayer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd9a6279365ace6475d0d451ff571c029152707e", "node_type": "4", "metadata": {"filename": "promptlayer.md", "author": "LlamaIndex"}, "hash": "b9b4c13c261c0fdf2af8504daa8db8791d5bf3e40a2f5a5c7ae4b1aefc9f6f44", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.promptlayer\n    options:\n      members:\n        - PromptLayerHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "142d3fe2-8e36-4c6d-8ac8-0684e3b6f87b": {"__data__": {"id_": "142d3fe2-8e36-4c6d-8ac8-0684e3b6f87b", "embedding": null, "metadata": {"filename": "token_counter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7ddd153e8feeed45afb4f15a6bf9b80541dfc92", "node_type": "4", "metadata": {"filename": "token_counter.md", "author": "LlamaIndex"}, "hash": "956e9892031746db433540358dd3edef127123e8e32b64786f70a903351b1857", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.callbacks.token_counting\n    options:\n      members:\n        - TokenCountingHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ed26d83-2824-4ab5-aa27-60210c0c009d": {"__data__": {"id_": "4ed26d83-2824-4ab5-aa27-60210c0c009d", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d1069b68f636d9791598c1642fe6b7f865e7e2d", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "0f8a6bb3d423ed8861f641deba8b94570e0666e349f1d9a59f5e8a6f9fe6d01e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.uptrain\n    options:\n      members:\n        - UpTrainCallbackHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3690a6e7-63fe-439b-96d5-6bf410ec4bb7": {"__data__": {"id_": "3690a6e7-63fe-439b-96d5-6bf410ec4bb7", "embedding": null, "metadata": {"filename": "wandb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c4e3ececa5d957c3fffcb76f11a47af50304323", "node_type": "4", "metadata": {"filename": "wandb.md", "author": "LlamaIndex"}, "hash": "ac00ba2f4d71f0e038ba7f491e587216f392d8fd085d3139f337a4b6cb7b61d0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.callbacks.wandb\n    options:\n      members:\n        - WandbCallbackHandler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb3645d3-92cf-4ced-80a9-cecc368d87f1": {"__data__": {"id_": "bb3645d3-92cf-4ced-80a9-cecc368d87f1", "embedding": null, "metadata": {"filename": "condense_plus_context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "542938dd7521d5b512d56345c5492c95a6aa7061", "node_type": "4", "metadata": {"filename": "condense_plus_context.md", "author": "LlamaIndex"}, "hash": "3501d30ad7675d8b18f7247483b21f98a044afd64c295c9c86b261b99ad85dc2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.chat_engine\n    options:\n      members:\n        - CondensePlusContextChatEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bb87352-fc99-4d40-ba92-e2f3d4e051c7": {"__data__": {"id_": "9bb87352-fc99-4d40-ba92-e2f3d4e051c7", "embedding": null, "metadata": {"filename": "condense_question.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac22564adcb0a4a58e4a020d60c94599c6e60a0d", "node_type": "4", "metadata": {"filename": "condense_question.md", "author": "LlamaIndex"}, "hash": "fd0b1cb1e72df82a1b627ca04f645a29ad32025c37a39488d3f0203322d01532", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.chat_engine\n    options:\n      members:\n        - CondenseQuestionChatEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d368abd4-8c23-419c-910c-b8337450a2e2": {"__data__": {"id_": "d368abd4-8c23-419c-910c-b8337450a2e2", "embedding": null, "metadata": {"filename": "context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7048fa82244f1053af906675eee700b6c9880a86", "node_type": "4", "metadata": {"filename": "context.md", "author": "LlamaIndex"}, "hash": "66c6156634e954cbbef4742aa020da9541a79e3c4f76aa9fa5cd8e89fe71e057", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.chat_engine\n    options:\n      members:\n        - ContextChatEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70f4b5ca-ce57-46eb-8b67-897a5a9c8d03": {"__data__": {"id_": "70f4b5ca-ce57-46eb-8b67-897a5a9c8d03", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ff746c7e9b22a37a0814146b898783132e9eff8", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "478f9b56ae2106d9dcba8b82304104a7e4134be63f66ff5ac282228eef42fc6c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.chat_engine.types", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 38, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "403ea2a1-315a-4594-96f4-f817146847c1": {"__data__": {"id_": "403ea2a1-315a-4594-96f4-f817146847c1", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2a39240b5d10ee437f77cc2d2c468c975c0df9d", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "c964c11e1d1eb8d8a46e39a6af5a8f6c44b3b6baabf19b579fde38bb40dd0afd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.chat_engine\n    options:\n      members:\n        - SimpleChatEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be4cdbce-7f62-4350-bfd3-5c448d85363c": {"__data__": {"id_": "be4cdbce-7f62-4350-bfd3-5c448d85363c", "embedding": null, "metadata": {"filename": "adapter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fb7e6316c7c692ef1b578c5b851e9e7cea01695", "node_type": "4", "metadata": {"filename": "adapter.md", "author": "LlamaIndex"}, "hash": "fd04d20e4fbdb7a29af742e0c54b87a2c66aacefc18b066834086c1798b71ced", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.adapter\n    options:\n      members:\n        - AdapterEmbeddingModel\n        - LinearAdapterEmbeddingModel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a565ec5-7eee-4465-b328-54b788f5731b": {"__data__": {"id_": "6a565ec5-7eee-4465-b328-54b788f5731b", "embedding": null, "metadata": {"filename": "alephalpha.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d03a1780455f535a3e892e33a0817b27ff33ddd0", "node_type": "4", "metadata": {"filename": "alephalpha.md", "author": "LlamaIndex"}, "hash": "c45a74475731037968ff4ac251da9e70e5271a0790a301f9491c3ceb4a20ea98", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.alephalpha\n    options:\n      members:\n        - AlephAlphaEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cd1c7b8-ed60-4223-842f-491fae4ee2ca": {"__data__": {"id_": "8cd1c7b8-ed60-4223-842f-491fae4ee2ca", "embedding": null, "metadata": {"filename": "anyscale.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20dfb2c99844a7132271e6b5f98472602faa4e9c", "node_type": "4", "metadata": {"filename": "anyscale.md", "author": "LlamaIndex"}, "hash": "f94e1d8b0b1c53d6a1e3f40f25909a79d7d02f5eb84b591f2d6806260f777078", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.anyscale\n    options:\n      members:\n        - AnyscaleEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b40ca217-23fb-4cf5-a721-33a5c456c8a8": {"__data__": {"id_": "b40ca217-23fb-4cf5-a721-33a5c456c8a8", "embedding": null, "metadata": {"filename": "azure_inference.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58fafe285f7f61df92909f18b6ea223f91247dde", "node_type": "4", "metadata": {"filename": "azure_inference.md", "author": "LlamaIndex"}, "hash": "c3d803fe093ed48766a18e120ab5402da92e95aa8a5c94391273825bd1435c48", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.azure_inference\n    options:\n      members:\n        - AzureAIEmbeddingsModel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80510c5e-9623-4ff0-8dfa-6aed4bdf1e68": {"__data__": {"id_": "80510c5e-9623-4ff0-8dfa-6aed4bdf1e68", "embedding": null, "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59b4215f02794470bc32c9e13b0e4c7c7b6fec64", "node_type": "4", "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}, "hash": "6961657dc9bd2f56d01d4c563921447c296da43b0b0b61b1532be262616e0d08", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.azure_openai\n    options:\n      members:\n        - AzureOpenAIEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc68fceb-51e2-4126-ac2e-03c34f694f58": {"__data__": {"id_": "bc68fceb-51e2-4126-ac2e-03c34f694f58", "embedding": null, "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd0e6cf50a32507bbfe5faf6d2567605f029fb07", "node_type": "4", "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}, "hash": "a3ee97ffc4b1c255502de80121b14f4014af028d855c49d7eb7bec9d97073c7f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.bedrock\n    options:\n      members:\n        - BedrockEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1142694f-6228-44b8-b05e-5df5b00cd7b5": {"__data__": {"id_": "1142694f-6228-44b8-b05e-5df5b00cd7b5", "embedding": null, "metadata": {"filename": "clarifai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "deb879454f442ff04a98190d5767a7249d28c344", "node_type": "4", "metadata": {"filename": "clarifai.md", "author": "LlamaIndex"}, "hash": "109200bd0fa9104718126117152d31d188f251903681c13f35e9820aa2670242", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.clarifai\n    options:\n      members:\n        - ClarifaiEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05ed793d-cdea-4353-b1af-ca79d1727e16": {"__data__": {"id_": "05ed793d-cdea-4353-b1af-ca79d1727e16", "embedding": null, "metadata": {"filename": "clip.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c4169243c44062c6071b43428bb6a2f1f759625", "node_type": "4", "metadata": {"filename": "clip.md", "author": "LlamaIndex"}, "hash": "7b91f3789bd897bd82d3c72a325ec76d20d373902a78a280c85d17b3a5b4b469", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.clip\n    options:\n      members:\n        - ClipEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ab53cca-fd7e-4833-a8d6-00f92101f1fd": {"__data__": {"id_": "3ab53cca-fd7e-4833-a8d6-00f92101f1fd", "embedding": null, "metadata": {"filename": "cloudflare_workersai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e75627093e093dc1157afa7f5e2d8b4d93a4bf3f", "node_type": "4", "metadata": {"filename": "cloudflare_workersai.md", "author": "LlamaIndex"}, "hash": "b898563cbf7b9e78d53f7971c6c86f31fb8c00d5affc7938b684aaebcf7c8ed5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.cloudflare_workersai\n    options:\n      members:\n        - CloudflareEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26b69072-2e3a-4b91-877d-35b3234ba613": {"__data__": {"id_": "26b69072-2e3a-4b91-877d-35b3234ba613", "embedding": null, "metadata": {"filename": "cohere.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f95d2cd62e28e7916481e1c72688a302391b4830", "node_type": "4", "metadata": {"filename": "cohere.md", "author": "LlamaIndex"}, "hash": "2a92480daea09b03ee46fe3982ccd55c586f75f3ea52f31a02bd2e28612e3a0d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.cohere\n    options:\n      members:\n        - CohereEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e6798c6-f370-4aaa-8a61-b0e9073964ba": {"__data__": {"id_": "4e6798c6-f370-4aaa-8a61-b0e9073964ba", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "650939e22e20b56a4e25f03d7ee9ae9f70ec7843", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "862149d61e7b6e2da93c1ad364f20b8dd44bfd75d78bfc57bc575a67596717f0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.dashscope\n    options:\n      members:\n        - DashScopeEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11e71d21-c88a-4dae-a096-0fc050374794": {"__data__": {"id_": "11e71d21-c88a-4dae-a096-0fc050374794", "embedding": null, "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e5f0679d046dce9c09e45a7265b9ae776c527ce", "node_type": "4", "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}, "hash": "1224434e76b8f65e7717a1486594bec58fe3f13601390e94c3fa868798526087", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.databricks\n    options:\n      members:\n        - DatabricksEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52868fd2-eb3c-407d-83eb-fe99a8d6f238": {"__data__": {"id_": "52868fd2-eb3c-407d-83eb-fe99a8d6f238", "embedding": null, "metadata": {"filename": "deepinfra.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09edad536982ffb11596ff8668ece20d93dffc99", "node_type": "4", "metadata": {"filename": "deepinfra.md", "author": "LlamaIndex"}, "hash": "39d4e6856fb71718564786d360b520b14aea46d70a66f41e2f032f8a98bf173d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.deepinfra\n    options:\n      members:\n        - DeepInfraEmbeddingModel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f1e1d32-7a9f-46fd-a1dc-75ec1aede38c": {"__data__": {"id_": "2f1e1d32-7a9f-46fd-a1dc-75ec1aede38c", "embedding": null, "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43e9011ce30940c5c1e3a635d83bbc8cc87e4402", "node_type": "4", "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "hash": "cf52b10cbb7224a7900220d35644a415ecf82c7825ba6f89f3d4621bd740156b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.elasticsearch\n    options:\n      members:\n        - ElasticsearchEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04dcfcf5-90c0-418d-99f1-e78420157555": {"__data__": {"id_": "04dcfcf5-90c0-418d-99f1-e78420157555", "embedding": null, "metadata": {"filename": "fastembed.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46465555d31a9669feb53cc28d0ff1c748ebe69e", "node_type": "4", "metadata": {"filename": "fastembed.md", "author": "LlamaIndex"}, "hash": "b1460127e063f836e062d235efd45366e830286cbc11c524cdd27830d087cc65", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.fastembed\n    options:\n      members:\n        - FastEmbedEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e371af7-7895-45a9-aa70-6c6d80e65938": {"__data__": {"id_": "0e371af7-7895-45a9-aa70-6c6d80e65938", "embedding": null, "metadata": {"filename": "fireworks.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "879fbccee52df8f271f71a4c5ea48dda22638d40", "node_type": "4", "metadata": {"filename": "fireworks.md", "author": "LlamaIndex"}, "hash": "84c22193de5edbb8835fed20da2d76532ccb5588f0866eb2a84af7f3c243466d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.fireworks\n    options:\n      members:\n        - FireworksEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f7e55f0-7a5a-493e-8c43-85103f33b9ba": {"__data__": {"id_": "3f7e55f0-7a5a-493e-8c43-85103f33b9ba", "embedding": null, "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0ecd56ecfebe51234fbdc56933728e191581f585", "node_type": "4", "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}, "hash": "a1b1ac5167aac35b17a913fbdff32330436373197b786b0e5c6fd77ac0bc3364", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.gemini\n    options:\n      members:\n        - GeminiEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b181ce6-b21e-49cf-8554-dca5e85f3537": {"__data__": {"id_": "5b181ce6-b21e-49cf-8554-dca5e85f3537", "embedding": null, "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a67cec3bc1de08e112b0dda0746cd2e15be76228", "node_type": "4", "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "hash": "e00a97ed411c2f38997b32f92b894c43e86671b222633a58e390c6b171da19b2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.google\n    options:\n      members:\n        - GeminiEmbedding\n        - GooglePaLMEmbedding\n        - GoogleUnivSentEncoderEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed717d8a-582e-4e2d-b352-02e664ffca04": {"__data__": {"id_": "ed717d8a-582e-4e2d-b352-02e664ffca04", "embedding": null, "metadata": {"filename": "gradient.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ed192170e17debba7cd23e5681b8faee03dfebf", "node_type": "4", "metadata": {"filename": "gradient.md", "author": "LlamaIndex"}, "hash": "6ec6f955f129e710c0bab1f0c38923555463cd99f7e8295da2514d0986002368", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.gradient\n    options:\n      members:\n        - GradientEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee8a6591-03d6-4562-a760-c4502a7f809a": {"__data__": {"id_": "ee8a6591-03d6-4562-a760-c4502a7f809a", "embedding": null, "metadata": {"filename": "huggingface.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "597bd6798f3a59cec74173593b0d668b4cfecd32", "node_type": "4", "metadata": {"filename": "huggingface.md", "author": "LlamaIndex"}, "hash": "c47e6690056e1f6b40220f731cf5b10ca4a040572d5dc3ddd8f9d31921921f07", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.huggingface\n    options:\n      members:\n        - HuggingFaceEmbedding\n        - HuggingFaceInferenceAPIEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ce34e09-dcb7-4f78-8ed3-265cf316f9cd": {"__data__": {"id_": "4ce34e09-dcb7-4f78-8ed3-265cf316f9cd", "embedding": null, "metadata": {"filename": "huggingface_api.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26888df65a1c3ce297ef5ad574454781cec100c1", "node_type": "4", "metadata": {"filename": "huggingface_api.md", "author": "LlamaIndex"}, "hash": "4d67cf150cf58e5ae008df379450beac1290992668b037e941beafa9bc3de75f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.huggingface_api\n    options:\n      members:\n        - HuggingFaceInferenceAPIEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53522978-ecc8-4da9-aaa2-3f15fdbafb2b": {"__data__": {"id_": "53522978-ecc8-4da9-aaa2-3f15fdbafb2b", "embedding": null, "metadata": {"filename": "huggingface_itrex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "98c987f9abda163920bbcf93bef712338341e766", "node_type": "4", "metadata": {"filename": "huggingface_itrex.md", "author": "LlamaIndex"}, "hash": "0b2400b567421ff67831a268c5c3c450c8482a11cf6bc2b339f6aa86ddaeef4c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.huggingface_itrex\n    options:\n      members:\n        - QuantizedBgeEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4926ef6-fa79-41da-a575-6c7a22b4991f": {"__data__": {"id_": "d4926ef6-fa79-41da-a575-6c7a22b4991f", "embedding": null, "metadata": {"filename": "huggingface_openvino.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d82ce271042dc3d97f67d77463cdac19558c6908", "node_type": "4", "metadata": {"filename": "huggingface_openvino.md", "author": "LlamaIndex"}, "hash": "4f1f9f606ee65bc0e27fd15b9a7b11776a3a6e8bb245d11763055232556d8495", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.huggingface_openvino\n    options:\n      members:\n        - OpenVINOEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5140af2a-8063-4fae-988d-17bd0b853008": {"__data__": {"id_": "5140af2a-8063-4fae-988d-17bd0b853008", "embedding": null, "metadata": {"filename": "huggingface_optimum.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d978ccdcc20b0d6cb39ed74fc2747e75c60f4346", "node_type": "4", "metadata": {"filename": "huggingface_optimum.md", "author": "LlamaIndex"}, "hash": "b6f2b7a255275c8af95e1fbf54e6cc4dd731777470a11a89152728d55439bf1e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.huggingface_optimum\n    options:\n      members:\n        - OptimumEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "078f43e1-bef1-4bf7-bcbb-3075c37f28ea": {"__data__": {"id_": "078f43e1-bef1-4bf7-bcbb-3075c37f28ea", "embedding": null, "metadata": {"filename": "huggingface_optimum_intel.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78c17615708ef4c3aa78b327ff01f25a1daa1974", "node_type": "4", "metadata": {"filename": "huggingface_optimum_intel.md", "author": "LlamaIndex"}, "hash": "6539e30852f6f716a51df64b3e0e36f60bb58e8b17f8dccb4df835e149f2abd4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.huggingface_optimum_intel\n    options:\n      members:\n        - IntelEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4db5f860-562c-4bd7-9df5-9073880a2958": {"__data__": {"id_": "4db5f860-562c-4bd7-9df5-9073880a2958", "embedding": null, "metadata": {"filename": "ibm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d6e265a30ed48986b89510d6ad1faa09d8bb6eb", "node_type": "4", "metadata": {"filename": "ibm.md", "author": "LlamaIndex"}, "hash": "aa3cb1df8870a541b344502f7c6eb6317102944ad82179c659a67a61bac1bb7f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.ibm\n    options:\n      members:\n        - WatsonxEmbeddings", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7338529f-8fea-4061-a39e-080f447be73d": {"__data__": {"id_": "7338529f-8fea-4061-a39e-080f447be73d", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a53fe6942f8027d3b5dfc76eaad3e5254fcfc4e", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c8a3ef0df88ce9ef25b985ccac84b6544bbe6a331a1c3f708e9dcfd0301e9fd5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.embeddings\n    options:\n      members:\n        - BaseEmbedding\n        - resolve_embed_model", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbc0d42e-f6f4-4057-96f4-eb0e3f6fbb96": {"__data__": {"id_": "dbc0d42e-f6f4-4057-96f4-eb0e3f6fbb96", "embedding": null, "metadata": {"filename": "instructor.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12e8722618cc239c739fb8c561e29084743e6996", "node_type": "4", "metadata": {"filename": "instructor.md", "author": "LlamaIndex"}, "hash": "6e8332cfcb8d132559ef46056f44ee689325a1b5e46faddb302f42e7b0f3d5c4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.instructor\n    options:\n      members:\n        - InstructorEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86e24547-9b19-43c4-9c43-691579dead5f": {"__data__": {"id_": "86e24547-9b19-43c4-9c43-691579dead5f", "embedding": null, "metadata": {"filename": "ipex_llm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ee9de231fbf091b59efc4b2b809806de3b72494", "node_type": "4", "metadata": {"filename": "ipex_llm.md", "author": "LlamaIndex"}, "hash": "2c99d87d0e70a1002abc63c4f68f71300ae2574dfd7435591f75362573036408", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.ipex_llm\n    options:\n      members:\n        - IpexLLMEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9276ae1f-7304-406c-bac7-a966b97db906": {"__data__": {"id_": "9276ae1f-7304-406c-bac7-a966b97db906", "embedding": null, "metadata": {"filename": "jinaai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c52e55b8f66941a7fa94eb7aac76922c8ecde3e", "node_type": "4", "metadata": {"filename": "jinaai.md", "author": "LlamaIndex"}, "hash": "3d52c56487da215e77fd93bacc07dd09c9eb35bb552db7d7c8d0a3899fb7d54c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.jinaai\n    options:\n      members:\n        - JinaEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e766e0ce-23ea-4b75-b3f9-06a742d4854a": {"__data__": {"id_": "e766e0ce-23ea-4b75-b3f9-06a742d4854a", "embedding": null, "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35f749210c09fdb470716d46b794b5f45eb42284", "node_type": "4", "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "hash": "f1ffe4f9daabe327959897e0560a542addd67c1c43170e746ebba55fc9ce746f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.langchain\n    options:\n      members:\n        - LangchainEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a155cb10-9e7c-4905-aa9b-bc762fb2c9c6": {"__data__": {"id_": "a155cb10-9e7c-4905-aa9b-bc762fb2c9c6", "embedding": null, "metadata": {"filename": "litellm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc6bcc9d27ec11896321ffe29054948f16833d5b", "node_type": "4", "metadata": {"filename": "litellm.md", "author": "LlamaIndex"}, "hash": "68a989aa937c8f37771f61e2fdff963f9c3050dd622aa8ade6183c36e8b88e08", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.litellm\n    options:\n      members:\n        - LiteLLMEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "013737e0-09b2-4744-9b7d-e8f6cc7f5080": {"__data__": {"id_": "013737e0-09b2-4744-9b7d-e8f6cc7f5080", "embedding": null, "metadata": {"filename": "llamafile.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea5e8752d599a6c3a16cc00317585cba0f09c1d", "node_type": "4", "metadata": {"filename": "llamafile.md", "author": "LlamaIndex"}, "hash": "e244319f47de40ca5cb870f90f8671ef3e8a073e3b37ee163b6fe4e144af9f94", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.llamafile\n    options:\n      members:\n        - LlamafileEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c2c87eb-b2ef-488a-a9f4-1ded1b9ed427": {"__data__": {"id_": "9c2c87eb-b2ef-488a-a9f4-1ded1b9ed427", "embedding": null, "metadata": {"filename": "llm_rails.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7fc32264593d3d829edf075fa8cb61ce0bfe1f02", "node_type": "4", "metadata": {"filename": "llm_rails.md", "author": "LlamaIndex"}, "hash": "4e828ef2f0f6669d37777ef36ce75b24d2efc2e6d6a03d986c95bcbf3c6a8f21", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.llm_rails\n    options:\n      members:\n        - LLMRailsEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd118f23-569f-46a2-a0d0-e77dd2484896": {"__data__": {"id_": "bd118f23-569f-46a2-a0d0-e77dd2484896", "embedding": null, "metadata": {"filename": "mistralai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a24a208766627a9c8dda41d10a4c9b371893dbb", "node_type": "4", "metadata": {"filename": "mistralai.md", "author": "LlamaIndex"}, "hash": "57677bc59fd6724f9e331908e080ee9604c7c9c84261f83dca9a24ac43e5c9df", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.mistralai\n    options:\n      members:\n        - MistralAIEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1315267b-b38c-4a92-be10-1d82f11792f8": {"__data__": {"id_": "1315267b-b38c-4a92-be10-1d82f11792f8", "embedding": null, "metadata": {"filename": "mixedbreadai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5f62b2305f5b3a1266816a748c340735ac6d6eb", "node_type": "4", "metadata": {"filename": "mixedbreadai.md", "author": "LlamaIndex"}, "hash": "d01994ba016641c82f452d1e946010739cf5f0e3fc76a8749b1c8740e7b63200", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.mixedbreadai\n    options:\n      members:\n        - MixedbreadAIEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c3961ea-568b-4668-84e7-9b058925a2cb": {"__data__": {"id_": "6c3961ea-568b-4668-84e7-9b058925a2cb", "embedding": null, "metadata": {"filename": "nomic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4168de5426fced8f70c1599b129094c399edda1e", "node_type": "4", "metadata": {"filename": "nomic.md", "author": "LlamaIndex"}, "hash": "b8c6485d9bae06566e07e4499aeecc93410f8c4713865571e79d5ef859e68bd5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.nomic\n    options:\n      members:\n        - NomicEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf0342d1-6e42-4b71-9e71-6f12b7dc8f69": {"__data__": {"id_": "cf0342d1-6e42-4b71-9e71-6f12b7dc8f69", "embedding": null, "metadata": {"filename": "nvidia.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6cbd46cb8b1ea2a0e8bf46a763d92dd33093cf68", "node_type": "4", "metadata": {"filename": "nvidia.md", "author": "LlamaIndex"}, "hash": "8691c903271bce52f9df1b761b9cbd6708b89b6c37e3b49bfb27f5d610c95976", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.nvidia\n    options:\n      members:\n        - NVIDIA", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96c90f72-dbb6-42b6-aefd-7d46d0eeb695": {"__data__": {"id_": "96c90f72-dbb6-42b6-aefd-7d46d0eeb695", "embedding": null, "metadata": {"filename": "oci_genai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ea78a1d30232e796833bf40391bec307db04e99", "node_type": "4", "metadata": {"filename": "oci_genai.md", "author": "LlamaIndex"}, "hash": "687de75a8b686755693b58dd23db6d377a953b6f06679c5025d41b627112abae", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.oci_genai\n    options:\n      members:\n        - OCIGenAIEmbeddings", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcd9e1ee-baa0-45fb-b33d-8b91631ea281": {"__data__": {"id_": "fcd9e1ee-baa0-45fb-b33d-8b91631ea281", "embedding": null, "metadata": {"filename": "octoai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60071a4c2cf6d03df19c2d78dd2ea311dc0078bd", "node_type": "4", "metadata": {"filename": "octoai.md", "author": "LlamaIndex"}, "hash": "dc0320e8a726d1d8228b66fa8ef1534b19ce93ef3b84199e78e1c3fd863bf7a9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.octoai\n    options:\n      members:\n        - OctoAIEmbeddings", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c60fc95-7338-4a3a-9a16-46e133a5f6cf": {"__data__": {"id_": "2c60fc95-7338-4a3a-9a16-46e133a5f6cf", "embedding": null, "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28bd274caed599637ddcb651116ece0586cc6e63", "node_type": "4", "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}, "hash": "185c44efb5a49520a3099ed82f20a252dcd82316c477a15ed900469c610db4c4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.ollama\n    options:\n      members:\n        - OllamaEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26dd8d38-db48-4ab8-901c-05bc168d1ecc": {"__data__": {"id_": "26dd8d38-db48-4ab8-901c-05bc168d1ecc", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6692b3046ff299377dc55595e44ab37a6ded22a6", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "5b79fa558ec040855c8727504fbc11eaa726a36d1551c84a60303863b2334120", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.openai\n    options:\n      members:\n        - OpenAIEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7873598-4f78-4d25-b96d-99f7ae237790": {"__data__": {"id_": "a7873598-4f78-4d25-b96d-99f7ae237790", "embedding": null, "metadata": {"filename": "premai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a2b769d8d75382122edfe502a0ba3ccb4ce4ca4", "node_type": "4", "metadata": {"filename": "premai.md", "author": "LlamaIndex"}, "hash": "70d7ef9fcb5e7aa7bcf57aa5755ee2e74a7ff0180cd2a8d31b94af2bc5dfba6d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.premai\n    options:\n      members:\n        - PremAIEmbeddings", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae8b9f92-6558-4e9a-967b-37580ca01af2": {"__data__": {"id_": "ae8b9f92-6558-4e9a-967b-37580ca01af2", "embedding": null, "metadata": {"filename": "sagemaker_endpoint.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7606e62ac5a13833186e84288ce5e225ab24888", "node_type": "4", "metadata": {"filename": "sagemaker_endpoint.md", "author": "LlamaIndex"}, "hash": "fbdf8ae1c50b311c3ba9c9a0930985b18674f4f4637810748a6a866d20772221", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.sagemaker_endpoint\n    options:\n      members:\n        - SageMakerEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "884450ad-1f0a-4cba-857e-6dc56fb1291b": {"__data__": {"id_": "884450ad-1f0a-4cba-857e-6dc56fb1291b", "embedding": null, "metadata": {"filename": "text_embeddings_inference.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cc68deec351468387158cbefcb3f0e3b317d47b", "node_type": "4", "metadata": {"filename": "text_embeddings_inference.md", "author": "LlamaIndex"}, "hash": "824a0cda7788967a460622d1c98f97eba6e7723b28ddde00d6ec4ca82ac9a0fa", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.text_embeddings_inference\n    options:\n      members:\n        - TextEmbeddingsInference", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "875f77c0-6b71-487b-b79e-454387e0c7e4": {"__data__": {"id_": "875f77c0-6b71-487b-b79e-454387e0c7e4", "embedding": null, "metadata": {"filename": "together.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8e23654b4ebacb82d6e70bc56335dc6e1902a85", "node_type": "4", "metadata": {"filename": "together.md", "author": "LlamaIndex"}, "hash": "a1a0c12183fdb5768355c0c59552e7feda0f2838edd1ca025a9520d9e8f87df6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.together\n    options:\n      members:\n        - TogetherEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72bfbc57-e17b-4c79-9e77-da3dfaae4f6b": {"__data__": {"id_": "72bfbc57-e17b-4c79-9e77-da3dfaae4f6b", "embedding": null, "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab961b59ebc7d588aed8db130b6cdf93a5259d7b", "node_type": "4", "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}, "hash": "873c144abdddfc2b5a752e1abe4637a62e164c4bcc69add047a4bc3538c0eb69", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.upstage\n    options:\n      members:\n        - UpstageEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94ac891e-2db8-4302-bc04-7eb75f894077": {"__data__": {"id_": "94ac891e-2db8-4302-bc04-7eb75f894077", "embedding": null, "metadata": {"filename": "vertex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6e27b2309465f8cfc56b77048050f6b39a7cc0a", "node_type": "4", "metadata": {"filename": "vertex.md", "author": "LlamaIndex"}, "hash": "882486c2fc6262d552934e1889c731c9501edce3574edfdeeb76d1133ab81808", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.vertex\n    options:\n      members:\n        - VertexMultiModalEmbedding\n        - VertexTextEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17bd13df-427f-487b-9b0a-0204a4699a7d": {"__data__": {"id_": "17bd13df-427f-487b-9b0a-0204a4699a7d", "embedding": null, "metadata": {"filename": "voyageai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b388d315826c8c8c2c9ab1750ed30bc6742bb3bc", "node_type": "4", "metadata": {"filename": "voyageai.md", "author": "LlamaIndex"}, "hash": "f4e4c6cfc47420e54400c12dbac8ecfa8866ebafedb936e37932f7f0a2e6167e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.voyageai\n    options:\n      members:\n        - VoyageEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "769b2334-f764-49ce-9206-40bb7237613b": {"__data__": {"id_": "769b2334-f764-49ce-9206-40bb7237613b", "embedding": null, "metadata": {"filename": "yandexgpt.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0d3735a7baf29d05eae2f2990246d9223027e58", "node_type": "4", "metadata": {"filename": "yandexgpt.md", "author": "LlamaIndex"}, "hash": "be6d2185ae628f0efd3c31a761a65eedb100b6b5efe7d2c694f2a1117fcf12bd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.embeddings.yandexgpt\n    options:\n      members:\n        - YandexGPTEmbedding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46e4c544-2eba-4bf4-a930-b232b7eb04c7": {"__data__": {"id_": "46e4c544-2eba-4bf4-a930-b232b7eb04c7", "embedding": null, "metadata": {"filename": "answer_relevancy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15b91eec0647e7ed5d1f8c5e7907933f8cd1b6c8", "node_type": "4", "metadata": {"filename": "answer_relevancy.md", "author": "LlamaIndex"}, "hash": "ea4f4667a67d82f72d5ba2a3577f2c291523c7525a7bceb1cd03c665c35022ba", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - AnswerRelevancyEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7445092-5016-47f4-b47f-ffba1c08d7ea": {"__data__": {"id_": "c7445092-5016-47f4-b47f-ffba1c08d7ea", "embedding": null, "metadata": {"filename": "context_relevancy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56a5d36345a1321b8e3cef5d39e7e6938f58a270", "node_type": "4", "metadata": {"filename": "context_relevancy.md", "author": "LlamaIndex"}, "hash": "904eb1815ba8001895815af1e22a3224ed0509a754ad4953d1d65cf2f5966fe0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - ContextRelevancyEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6c401e9-206e-477b-a598-ea41ac6040b3": {"__data__": {"id_": "f6c401e9-206e-477b-a598-ea41ac6040b3", "embedding": null, "metadata": {"filename": "correctness.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb29bb1bbd40d934161e8b0c52c8fa725ee88eb9", "node_type": "4", "metadata": {"filename": "correctness.md", "author": "LlamaIndex"}, "hash": "51d57d46dfe947725ba1e283f0f94a6149fb63a640d67ca9c23222f6937b8745", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - CorrectnessEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d64a7acc-fbae-4430-92f8-f67610e918f7": {"__data__": {"id_": "d64a7acc-fbae-4430-92f8-f67610e918f7", "embedding": null, "metadata": {"filename": "dataset_generation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbda5302f5b1e4202a5996618f34751b257540b7", "node_type": "4", "metadata": {"filename": "dataset_generation.md", "author": "LlamaIndex"}, "hash": "7177791c8e57d17a3293a21d73487f28fddd17ed9a6fa4df72db7cf151e701ab", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - DatasetGenerator\n        - QueryResponseDataset", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edf0c907-91f7-41c2-9f44-3634f750bdd0": {"__data__": {"id_": "edf0c907-91f7-41c2-9f44-3634f750bdd0", "embedding": null, "metadata": {"filename": "faithfullness.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8af2aa0e87dc4995c9c22439e5dfde16b7d8824", "node_type": "4", "metadata": {"filename": "faithfullness.md", "author": "LlamaIndex"}, "hash": "5e79d0e476635b51581df741334fa5307ff9c0b15514cdced2cdb2a670903ee3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - FaithfulnessEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ebdb769-d84c-4808-a30e-8475fddd5a17": {"__data__": {"id_": "5ebdb769-d84c-4808-a30e-8475fddd5a17", "embedding": null, "metadata": {"filename": "guideline.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d48c81cfc6c86e9ca9b3f03537ebdb56c3175d4", "node_type": "4", "metadata": {"filename": "guideline.md", "author": "LlamaIndex"}, "hash": "e9dde6dff160cc4eb5cf07ce81c0f951c7071592c16f12eb4f3c71c253025fc1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - GuidelineEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "865b05c5-740a-4f1b-8424-bed6689918c8": {"__data__": {"id_": "865b05c5-740a-4f1b-8424-bed6689918c8", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "054370016c232d223a94b5ab0a735a835e0d07c5", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "549ca70b20b701c11e55c34d0310f9598f6b1d898d9f2ab38b11ee4c9b044407", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - BaseEvaluator\n        - EvaluationResult\n        - BatchEvalRunner", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb24c6ea-d8ef-4526-a3d6-e5b445010858": {"__data__": {"id_": "fb24c6ea-d8ef-4526-a3d6-e5b445010858", "embedding": null, "metadata": {"filename": "metrics.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f46d3a172fe4021651bd442ee85d6e196864369b", "node_type": "4", "metadata": {"filename": "metrics.md", "author": "LlamaIndex"}, "hash": "c337c0bf66724325497f98d8135bc932b3ca45bb8ddb2973242eb91fe11d45f0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - MRR\n        - HitRate\n        - RetrievalMetricResult\n        - resolve_metrics", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79f2908b-41ed-47e4-96c2-bbdbdf0697be": {"__data__": {"id_": "79f2908b-41ed-47e4-96c2-bbdbdf0697be", "embedding": null, "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e021feb3d795fed17742261332e3a0c6dc4b7ea0", "node_type": "4", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "c454b3251043c680d4ff3898b417ca94708251b89bee7efc63b3d02ad2844dd9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - MultiModalRetrieverEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7558c0d6-f51c-48fb-a8cd-017257ac1a7a": {"__data__": {"id_": "7558c0d6-f51c-48fb-a8cd-017257ac1a7a", "embedding": null, "metadata": {"filename": "pairwise_comparison.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e5fd6aaa813de68511947117ed3433e555bed76", "node_type": "4", "metadata": {"filename": "pairwise_comparison.md", "author": "LlamaIndex"}, "hash": "d0bd7f625951520e3b20e46309a4849de1ad2d11b31c3b4704f61d9468d54583", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - PairwiseComparisonEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4208bd86-6ea0-4d9b-83be-6f21e8a104eb": {"__data__": {"id_": "4208bd86-6ea0-4d9b-83be-6f21e8a104eb", "embedding": null, "metadata": {"filename": "query_response.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a14267d569b4fd0e1b47ab6d1deffab320431983", "node_type": "4", "metadata": {"filename": "query_response.md", "author": "LlamaIndex"}, "hash": "05dac679fa5906f1106262b1046d90c3d17b82b5941522ec5b20973f457c4e38", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - QueryResponseEvaluator\n        - RelevancyEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96804470-ffae-40d3-bed0-ded6bc7792fe": {"__data__": {"id_": "96804470-ffae-40d3-bed0-ded6bc7792fe", "embedding": null, "metadata": {"filename": "response.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e4378809ed87be6eb84130a8c8cebf2600aaa46", "node_type": "4", "metadata": {"filename": "response.md", "author": "LlamaIndex"}, "hash": "84409ddcee9a3954f1aa6d7e13cbc1763c70ae45ff9ebcb0806f91cdff4ee0e6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - ResponseEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01add33b-cbf1-436d-a912-61cbf7d37044": {"__data__": {"id_": "01add33b-cbf1-436d-a912-61cbf7d37044", "embedding": null, "metadata": {"filename": "retrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a0d70287e1506ee9bd9846b76e5a7885d86cf99", "node_type": "4", "metadata": {"filename": "retrieval.md", "author": "LlamaIndex"}, "hash": "15e9a0154f3566bb932a5709f9ffadf6f6892eb3aa40495fb8c38b4282debd38", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - BaseRetrievalEvaluator\n        - RetrieverEvaluator\n        - RetrievalEvalResult", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3147d8f6-f7eb-4ba1-a928-f61e9a0312f2": {"__data__": {"id_": "3147d8f6-f7eb-4ba1-a928-f61e9a0312f2", "embedding": null, "metadata": {"filename": "semantic_similarity.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ec1682e064b854d4c0530818d4a4cd8fa691bf0", "node_type": "4", "metadata": {"filename": "semantic_similarity.md", "author": "LlamaIndex"}, "hash": "750de7bd5213f84298e728c31f391559cce5b4cd98ea86e938454207191e4637", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.evaluation\n    options:\n      members:\n        - SemanticSimilarityEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3e8e12d-8bc1-46e4-8030-d250bcd51974": {"__data__": {"id_": "f3e8e12d-8bc1-46e4-8030-d250bcd51974", "embedding": null, "metadata": {"filename": "tonic_validate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d284f695b264749117a7b11dcdab395f3e40431", "node_type": "4", "metadata": {"filename": "tonic_validate.md", "author": "LlamaIndex"}, "hash": "844a4340c130c9c781734f6be46fa29869dccf1f85ec48da868f4574c098a397", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.evaluation.tonic_validate\n    options:\n      members:\n        - AnswerConsistencyBinaryEvaluator\n        - AnswerConsistencyEvaluator\n        - AnswerSimilarityEvaluator\n        - AugmentationAccuracyEvaluator\n        - AugmentationPrecisionEvaluator\n        - RetrievalPrecisionEvaluator\n        - TonicValidateEvaluator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edbaf4f6-7fd4-4ee9-a9b5-42c7a878d05a": {"__data__": {"id_": "edbaf4f6-7fd4-4ee9-a9b5-42c7a878d05a", "embedding": null, "metadata": {"filename": "entity.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08383b48b4feebb8a1f994773514ba990dfb50e2", "node_type": "4", "metadata": {"filename": "entity.md", "author": "LlamaIndex"}, "hash": "ff3fb97688fa9f8d72dc6c396e8ad494d86069e7d32451d98a22bc6931be77ae", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.extractors.entity\n    options:\n      members:\n        - EntityExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25fc1f5f-1f1e-478f-9f7e-05f2be85a8a7": {"__data__": {"id_": "25fc1f5f-1f1e-478f-9f7e-05f2be85a8a7", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af7cce494818be3843e5fb4f2df22938900a98de", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f464103ce637c4e0e3de2555c9cf07451b77c908a5267879ce1fd3be7ce15361", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.extractors.interface\n    options:\n      members:\n        - BaseExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4a8e040-ec6b-47bb-985f-71ca95f8ce5b": {"__data__": {"id_": "e4a8e040-ec6b-47bb-985f-71ca95f8ce5b", "embedding": null, "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43b2d8ff0c00fd536467985bee8cd3a73b666556", "node_type": "4", "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "hash": "9e277492245b264bd54a71ba0ad91072ed58bb515d668a82f8bde2aa2c11561c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.extractors\n    options:\n      members:\n        - KeywordExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff840cae-decb-4741-8143-58405d021743": {"__data__": {"id_": "ff840cae-decb-4741-8143-58405d021743", "embedding": null, "metadata": {"filename": "marvin.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43d74ded76ea7c3c9f5cb4b2915d487990f99525", "node_type": "4", "metadata": {"filename": "marvin.md", "author": "LlamaIndex"}, "hash": "c6953772c13138eb304f21fccdf17e0929575935267225506d00541d85bc4153", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.extractors.marvin\n    options:\n      members:\n        - MarvinMetadataExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "387d9672-baef-460d-8a64-b1b156a28348": {"__data__": {"id_": "387d9672-baef-460d-8a64-b1b156a28348", "embedding": null, "metadata": {"filename": "pydantic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34dc85f0d49b0ad409469f5ef5ef54edd6e860fd", "node_type": "4", "metadata": {"filename": "pydantic.md", "author": "LlamaIndex"}, "hash": "4f0ca7b5726474aed4b0eb8a0e50c5c412e05142eac4b373ec1ea46cc5bbb2cb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.extractors\n    options:\n      members:\n        - PydanticProgramExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13ec55b1-45c5-4678-a500-fe89c727f9c2": {"__data__": {"id_": "13ec55b1-45c5-4678-a500-fe89c727f9c2", "embedding": null, "metadata": {"filename": "question.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "162e80ac7102b451126e00f218131e488dec80f3", "node_type": "4", "metadata": {"filename": "question.md", "author": "LlamaIndex"}, "hash": "a674947d2674629e7daa2e24d28395da5bc1bf1649b37d80dd886709278109d4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.extractors\n    options:\n      members:\n        - QuestionsAnsweredExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47fd8cf9-f5df-424e-ad19-f5064a3a0063": {"__data__": {"id_": "47fd8cf9-f5df-424e-ad19-f5064a3a0063", "embedding": null, "metadata": {"filename": "summary.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54ad5d2f7128fcc54caa013b0de0888311341722", "node_type": "4", "metadata": {"filename": "summary.md", "author": "LlamaIndex"}, "hash": "68afa96496e9a623e2c6a3aba01d95f9cd02f95454c34d8e3ddd25575dfffd72", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.extractors\n    options:\n      members:\n        - SummaryExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5357751f-4d11-4884-934e-873bfe452639": {"__data__": {"id_": "5357751f-4d11-4884-934e-873bfe452639", "embedding": null, "metadata": {"filename": "title.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a39fcd5f089d403851b69caacc1e5fc7ed47c264", "node_type": "4", "metadata": {"filename": "title.md", "author": "LlamaIndex"}, "hash": "4479d8109150146c3b546de640ca62004599d1a02fc5d24cd639683048afde50", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.extractors\n    options:\n      members:\n        - TitleExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f16c3eed-248f-4691-99a5-9c0cfbe74768": {"__data__": {"id_": "f16c3eed-248f-4691-99a5-9c0cfbe74768", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed48bdd383dc09e2944a569eb0d4646c91fa59bc", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "cf37db81f08669bdf657ea54b4e036657ab13dac0ab477a01b59f54cea56574c", "class_name": "RelatedNodeInfo"}}, "text": "# API Reference\n\nLlamaIndex provides thorough documentation of modules and integrations used in the framework.\n\nUse the navigation or search to find the classes you are interested in!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bf50750-46ca-4ba4-a0b4-82adeed6d434": {"__data__": {"id_": "5bf50750-46ca-4ba4-a0b4-82adeed6d434", "embedding": null, "metadata": {"filename": "colbert.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae0cb598d1a7331a43eb36ec679ee34e3422146", "node_type": "4", "metadata": {"filename": "colbert.md", "author": "LlamaIndex"}, "hash": "083c523500b3bcf76640d370f9876c9b39d4bbf7500eee52125b787ea69d5531", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.colbert\n    options:\n      members:\n        - ColbertIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c335cae6-cc33-4f79-ba59-523a1b854b3a": {"__data__": {"id_": "c335cae6-cc33-4f79-ba59-523a1b854b3a", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4f3ffb9b7497c715ded13a91fd97216cf5324fb", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "2ce101df3a30609b793fe40814c02b5c8ce76ce6eda8da8c1c6403d9b4af4d39", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.dashscope\n    options:\n      members:\n        - DashScopeCloudIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc42c7f4-01d9-4d37-ba86-6a57ba73e074": {"__data__": {"id_": "fc42c7f4-01d9-4d37-ba86-6a57ba73e074", "embedding": null, "metadata": {"filename": "document_summary.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9af803ed3c3f35dc34b33870976455911eafddc1", "node_type": "4", "metadata": {"filename": "document_summary.md", "author": "LlamaIndex"}, "hash": "a10d2ace3771bc4ff712fab93255372aa91b65e43dfa79937cba6ea4b577eabc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      members:\n        - DocumentSummaryIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f18cf8a-38ab-45e9-9aad-ec86f0324697": {"__data__": {"id_": "7f18cf8a-38ab-45e9-9aad-ec86f0324697", "embedding": null, "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c32021044eea7b39eb2d9e3fba03d0308e187f9d", "node_type": "4", "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "hash": "b6cf2d68ba1c9d8c7801d05f44dbce40311e237f47ff24ae528866abae8f60f2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.google\n    options:\n      members:\n        - GoogleIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0135998b-137c-479f-925a-d95d8f0a838f": {"__data__": {"id_": "0135998b-137c-479f-925a-d95d8f0a838f", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d23d3f649889ec210eb96989d83365b43b0551e", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "8ca20beb18c2a2a2847f702829c0fca17827133733b54a957f51e70080c0b5e1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices.base\n    options:\n      members:\n        - BaseIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89e7350c-0630-4383-9b66-95c37ee45daf": {"__data__": {"id_": "89e7350c-0630-4383-9b66-95c37ee45daf", "embedding": null, "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "683e8c50ebda5a9d8ad6b082e64fceebc3261f5e", "node_type": "4", "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "hash": "fccfee9f1ff4fe2d52d34cc27dc8e407d7b52434dbf0762dbfc76d1977645541", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      members:\n        - KeywordTableIndex\n        - SimpleKeywordTableIndex\n        - RAKEKeywordTableIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b600fef-f262-4ca8-b435-674241e52075": {"__data__": {"id_": "6b600fef-f262-4ca8-b435-674241e52075", "embedding": null, "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83a19316c19a6246ac7be1ded127fd1603907be4", "node_type": "4", "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}, "hash": "772d2674a6c77e949a3e760ee3d3c3194db120663bffcb8bdc4758925c9fbaf1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      members:\n        - KnowledgeGraphIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f4a2d54-95d0-4ccf-8a24-d439595ccccb": {"__data__": {"id_": "1f4a2d54-95d0-4ccf-8a24-d439595ccccb", "embedding": null, "metadata": {"filename": "llama_cloud.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2511eebb101dffbf1c87fdb0b73a582882d9a63c", "node_type": "4", "metadata": {"filename": "llama_cloud.md", "author": "LlamaIndex"}, "hash": "4ed7b4fa9dee54a36afdec59c707c2a80123f90318d72bd65b5bd7ded791fad3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.llama_cloud\n    options:\n      members:\n        - LlamaCloudIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc314758-d006-4dcd-b9d6-9f408c12e846": {"__data__": {"id_": "cc314758-d006-4dcd-b9d6-9f408c12e846", "embedding": null, "metadata": {"filename": "postgresml.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d45b19879802dd16a33b9dd9ef68e3a121109c8", "node_type": "4", "metadata": {"filename": "postgresml.md", "author": "LlamaIndex"}, "hash": "4b2340dc0d1cda54159d57ad62a1ab3119a1c69d32e2f4409c35f4d08dbd82b8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.postgresml\n    options:\n      members:\n        - PostgresMLRetriever\n        - PostgresmlIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a81383d5-102c-48b6-9785-17640f0206da": {"__data__": {"id_": "a81383d5-102c-48b6-9785-17640f0206da", "embedding": null, "metadata": {"filename": "property_graph.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1f0eff012ad405263922913b4070cd48b98fdb5", "node_type": "4", "metadata": {"filename": "property_graph.md", "author": "LlamaIndex"}, "hash": "7e8ce95dbf72d637ee1c399dd153d4fb771c85113647b9302d4cacae532ae5a8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      members:\n        - PropertyGraphIndex\n        - PGRetriever\n        - BasePGRetriever\n        - CustomPGRetriever\n        - CypherTemplateRetriever\n        - LLMSynonymRetriever\n        - TextToCypherRetriever\n        - VectorContextRetriever\n        - ImplicitPathExtractor\n        - SchemaLLMPathExtractor\n        - SimpleLLMPathExtractor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fa07f9a-5e8b-4ae0-8b09-a59c4d8840ad": {"__data__": {"id_": "9fa07f9a-5e8b-4ae0-8b09-a59c4d8840ad", "embedding": null, "metadata": {"filename": "summary.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af381e6b3d6d2ee802f342437983d48ff8018002", "node_type": "4", "metadata": {"filename": "summary.md", "author": "LlamaIndex"}, "hash": "547264136b803e7283a126bbdbb9cd94d01e487e578664beb0f1a6b1eac88cd4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      members:\n        - SummaryIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f7a3799-1ff1-4baf-a1d8-f8a5c3bf86b7": {"__data__": {"id_": "5f7a3799-1ff1-4baf-a1d8-f8a5c3bf86b7", "embedding": null, "metadata": {"filename": "tree.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb6a6e230d166a9550f76454e95bd23bbc6d717e", "node_type": "4", "metadata": {"filename": "tree.md", "author": "LlamaIndex"}, "hash": "d23214b4893039ee0d10a88868132abf01056ff9bd957703b0d901d0ad4a0dc6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      show_root_heading: False\n      members:\n        - TreeIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c722246a-a835-4500-bf18-c5b6d9b05edf": {"__data__": {"id_": "c722246a-a835-4500-bf18-c5b6d9b05edf", "embedding": null, "metadata": {"filename": "vectara.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b59c4dbecec2e420ac2a9f9a9510a02c6808f13", "node_type": "4", "metadata": {"filename": "vectara.md", "author": "LlamaIndex"}, "hash": "d817bc751df8f1360c5d3f13a25808a8db285c478ba8ae9fd296e97fecf56c86", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.vectara\n    options:\n      members:\n        - VectaraIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "015603ff-b935-4164-951f-4f34f504a01c": {"__data__": {"id_": "015603ff-b935-4164-951f-4f34f504a01c", "embedding": null, "metadata": {"filename": "vector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9678fe6b412c9f7d4839065feecc311c6535572c", "node_type": "4", "metadata": {"filename": "vector.md", "author": "LlamaIndex"}, "hash": "aa9d74889b5eff03b28ac28870cbf4c90668b53b63f231fe7be361ba100c712b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices\n    options:\n      members:\n        - VectorStoreIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9d6e56e-fd5c-4dd8-990e-a26c11e7330a": {"__data__": {"id_": "a9d6e56e-fd5c-4dd8-990e-a26c11e7330a", "embedding": null, "metadata": {"filename": "vertexai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c58f355afca9c4d92adbf06bc96472dcc51cc4a", "node_type": "4", "metadata": {"filename": "vertexai.md", "author": "LlamaIndex"}, "hash": "f47fa3878dfb2bfc30c30c9a16fc6e59cff701a08b8b9f89163046696ee5f9cb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.vertexai\n    options:\n      members:\n        - VertexAIIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64702c81-29cf-45c5-a038-0c01dcf435df": {"__data__": {"id_": "64702c81-29cf-45c5-a038-0c01dcf435df", "embedding": null, "metadata": {"filename": "zilliz.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20d8ddd47f8b4eaca063d2777a6e3d929e27fa4c", "node_type": "4", "metadata": {"filename": "zilliz.md", "author": "LlamaIndex"}, "hash": "8865f7f620e2eee82e1380d01ffc005c854c42643a30e59213fb0cf0bdfb28c2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.indices.managed.zilliz\n    options:\n      members:\n        - ZillizCloudPipelineIndex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ede26b2-8282-4353-8bd1-fda2ac1abf91": {"__data__": {"id_": "3ede26b2-8282-4353-8bd1-fda2ac1abf91", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ed6504d8d2fdf8ef54684c4a8f89aa942332b7b", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "ca20df80aa078d5d925344d3a31b4fb55e54547b7dd5a9d7ed7eb9d1c2406766", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.ingestion.pipeline\n    options:\n      members:\n        - IngestionPipeline\n        - DocstoreStrategy", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cde30a2a-91b4-4e6b-854b-8aad4f400cd3": {"__data__": {"id_": "cde30a2a-91b4-4e6b-854b-8aad4f400cd3", "embedding": null, "metadata": {"filename": "event_handlers.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a61305e895dc94bf0152d72eea1433bb572e0b86", "node_type": "4", "metadata": {"filename": "event_handlers.md", "author": "LlamaIndex"}, "hash": "7e8fb423e00b4e34d117296e2cb17469736acacf7a2a00efff9388b64df1ed63", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.instrumentation.event_handlers.base.BaseEventHandler\n    options:\n      show_root_heading: true\n      show_root_full_path: false", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b53e727-aeff-4b2e-b06f-e3597dbfba96": {"__data__": {"id_": "0b53e727-aeff-4b2e-b06f-e3597dbfba96", "embedding": null, "metadata": {"filename": "event_types.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb8ca7a10d21a99bfdcb66e44a54b5590ea7bb41", "node_type": "4", "metadata": {"filename": "event_types.md", "author": "LlamaIndex"}, "hash": "8f5a625b537cf27079336a85e2cc888628a50113c05779da352bae18c2ed019f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.instrumentation.events.base.BaseEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.agent.AgentChatWithStepEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.agent.AgentChatWithStepStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.agent.AgentRunStepEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.agent.AgentRunStepStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.agent.AgentToolCallEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.chat_engine.StreamChatDeltaReceivedEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.chat_engine.StreamChatEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.chat_engine.StreamChatErrorEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.chat_engine.StreamChatStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.embedding.EmbeddingEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.embedding.EmbeddingStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.llm.LLMChatEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.llm.LLMChatStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.llm.LLMCompletionEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.llm.LLMCompletionStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.llm.LLMPredictEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.llm.LLMPredictStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.query.QueryEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.query.QueryStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.retrieval.RetrievalEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.retrieval.RetrievalStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.synthesis.GetResponseEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.synthesis.GetResponseStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.synthesis.SynthesizeEndEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.events.synthesis.SynthesizeStartEvent\n    options:\n      show_root_heading: true\n      show_root_full_path: false", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6377cdd-7455-42bc-9b16-f5ebb38a3240": {"__data__": {"id_": "d6377cdd-7455-42bc-9b16-f5ebb38a3240", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4362e7f9dfdc56d3830bcc4e5367a97e60d74aed", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "04c66f5354c365db4f4850fcc937493bc6db4554250962760bda895924c4d840", "class_name": "RelatedNodeInfo"}}, "text": "# Instrumentation\n\nLlamaIndex contains a simple instrumentation framework that allows you to\nobserve events and spans happening in the framework.\n\n- [Event Handlers](./event_handlers.md)\n- [Event Types](./event_types.md)\n- [Span Handlers](./span_handlers.md)\n- [Span Types](./span_types.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fc025f3-9177-4077-a6c7-cbb755775973": {"__data__": {"id_": "1fc025f3-9177-4077-a6c7-cbb755775973", "embedding": null, "metadata": {"filename": "span_handlers.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe3717d7aec5fae08bd7e851cc20aae92ca64176", "node_type": "4", "metadata": {"filename": "span_handlers.md", "author": "LlamaIndex"}, "hash": "eaa11dd77ab497325b316ba0da754429e2727ca78f4ba802285c8714f328729c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.instrumentation.span_handlers.base.BaseSpanHandler\n    options:\n      show_root_heading: true\n      show_root_full_path: false\n\n::: llama_index.core.instrumentation.span_handlers.simple.SimpleSpanHandler\n    options:\n      show_root_heading: true\n      show_root_full_path: false", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd6beb6c-ca5d-4a06-9718-49c193f15902": {"__data__": {"id_": "dd6beb6c-ca5d-4a06-9718-49c193f15902", "embedding": null, "metadata": {"filename": "span_types.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13cd09784bfa61b79cb1f019da168bd1e44a8d14", "node_type": "4", "metadata": {"filename": "span_types.md", "author": "LlamaIndex"}, "hash": "90a82017cb0b51f1640723a1376c1d8ddf4eafafb04ab2fdde02ca435a4058e6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.instrumentation.span.base.BaseSpan\n    options:\n      show_root_heading: true\n      show_root_full_path: false", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "663ecd12-646e-4170-b4ac-52a2ef0860d1": {"__data__": {"id_": "663ecd12-646e-4170-b4ac-52a2ef0860d1", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84fe4dfe57f76f671e5e82ec192321cf34d88214", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "fc0f72581169b8ffbd6d2f073cdae9e32e7615e1fd72292f6b0ecbc9c16416d6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.llama_dataset", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 34, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c427cf6-18a0-4f04-b628-3051bbf87814": {"__data__": {"id_": "6c427cf6-18a0-4f04-b628-3051bbf87814", "embedding": null, "metadata": {"filename": "ai21.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac29761e696a968494cc57d506078b74d4bf0ac3", "node_type": "4", "metadata": {"filename": "ai21.md", "author": "LlamaIndex"}, "hash": "a51cce5bcba035a63e9b4ec4ba9fd1ea9a5ef5ba371b1d3b609fd9bd8a3f9bbf", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.ai21\n    options:\n      members:\n        - AI21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcee966b-5097-47a9-b0f3-5553ccdd14e8": {"__data__": {"id_": "dcee966b-5097-47a9-b0f3-5553ccdd14e8", "embedding": null, "metadata": {"filename": "alephalpha.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd36c295e59a76c0d9c684d05a537100c7db265c", "node_type": "4", "metadata": {"filename": "alephalpha.md", "author": "LlamaIndex"}, "hash": "0855f8913d6de6ce17c732a606b4ebb3894a683673938df211250894669dfef5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.alephalpha\n    options:\n      members:\n        - AlephAlpha", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2c7effe-379b-4d09-a7e7-cdb93de1d638": {"__data__": {"id_": "b2c7effe-379b-4d09-a7e7-cdb93de1d638", "embedding": null, "metadata": {"filename": "anthropic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60c45f68d54000d2c5fd26fe505ac09ecec66b5c", "node_type": "4", "metadata": {"filename": "anthropic.md", "author": "LlamaIndex"}, "hash": "47c20e849a03a42e24270d5a7aaa281e245e3f4fad41bf5f9499fff7ff5df968", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.anthropic\n    options:\n      members:\n        - Anthropic", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22c94b06-5d07-4f57-bbd1-59eb0fe004e3": {"__data__": {"id_": "22c94b06-5d07-4f57-bbd1-59eb0fe004e3", "embedding": null, "metadata": {"filename": "anyscale.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40be432eecfe185b006b51294a8d122bfb6f7c40", "node_type": "4", "metadata": {"filename": "anyscale.md", "author": "LlamaIndex"}, "hash": "2ac1e68e541d87776b267a12de0b3322d3194826d62e5b56443b65b656628062", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.anyscale\n    options:\n      members:\n        - Anyscale", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dc600b2-ea33-41f6-9969-55f1e73725bb": {"__data__": {"id_": "8dc600b2-ea33-41f6-9969-55f1e73725bb", "embedding": null, "metadata": {"filename": "azure_inference.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "727474cb8d2c5790028569cc69e657261a108a3e", "node_type": "4", "metadata": {"filename": "azure_inference.md", "author": "LlamaIndex"}, "hash": "4f64a45b5c4a9a30036cb2d94b22c3d152d2b2636aebfeb1380a491668e95f07", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.azure_inference\n    options:\n      members:\n        - AzureAICompletionsModel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48f67bf9-2f29-4346-8fc6-ca589c7af96b": {"__data__": {"id_": "48f67bf9-2f29-4346-8fc6-ca589c7af96b", "embedding": null, "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0606ea5f2f66d4530d53b92a948204d63be5191", "node_type": "4", "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}, "hash": "98f56efa9aefbaccf3dc2cd49b9605ed40248a554428ac7970197d36cabc9d5f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.azure_openai\n    options:\n      members:\n        - AsyncAzureOpenAI\n        - AzureOpenAI\n        - SyncAzureOpenAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "517aae2d-df08-4db3-93d0-5b4cb6b9e430": {"__data__": {"id_": "517aae2d-df08-4db3-93d0-5b4cb6b9e430", "embedding": null, "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bf2378d051a3e82f96a6766c5da3bbce6134336", "node_type": "4", "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}, "hash": "a9f0ef8deb73c3e2d4fb6497a7975af4bc0b70d556b5f2526818a1d164f4ec6d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.bedrock\n    options:\n      members:\n        - Bedrock", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ea8545b-90f6-4e49-bc3f-4b1995b8f506": {"__data__": {"id_": "9ea8545b-90f6-4e49-bc3f-4b1995b8f506", "embedding": null, "metadata": {"filename": "bedrock_converse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80ba1182c081a6fe6940f9bbaaf2aca5b760404e", "node_type": "4", "metadata": {"filename": "bedrock_converse.md", "author": "LlamaIndex"}, "hash": "6623f72290d386619ec2591f07fb50f8ae52cb022e84e6ca7b633ef92176ecd7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.bedrock_converse\n    options:\n      members:\n        - BedrockConverse", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0495e7c7-a396-425d-bb5e-e6d22c2a3663": {"__data__": {"id_": "0495e7c7-a396-425d-bb5e-e6d22c2a3663", "embedding": null, "metadata": {"filename": "clarifai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f678f2c6095eee77cbbbfa375d64206c78d8c429", "node_type": "4", "metadata": {"filename": "clarifai.md", "author": "LlamaIndex"}, "hash": "15c387daaf367bc3ad427043dbaed807143c61678df868d1834dfa0ff24d2b6f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.clarifai\n    options:\n      members:\n        - Clarifai", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "863211e7-3760-4a12-8bdc-6707bb3853ae": {"__data__": {"id_": "863211e7-3760-4a12-8bdc-6707bb3853ae", "embedding": null, "metadata": {"filename": "cleanlab.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfac5a69f019d4aa9f7b2a5bc4f158e3a4606a80", "node_type": "4", "metadata": {"filename": "cleanlab.md", "author": "LlamaIndex"}, "hash": "b76d462b0e493c8307878cbefe3a35f72dbe716ff260fdb785eefc6bf90fe22e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.cleanlab\n    options:\n      members:\n        - CleanlabTLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e588059-7b2c-49c7-9ef6-b2c8d4a48d44": {"__data__": {"id_": "0e588059-7b2c-49c7-9ef6-b2c8d4a48d44", "embedding": null, "metadata": {"filename": "cohere.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a33d292f6205ffcc5146e83258da344dda1f9979", "node_type": "4", "metadata": {"filename": "cohere.md", "author": "LlamaIndex"}, "hash": "17ce126551e5659232c0584bdec08bf76d6e3e8cf2084407491a50c2600fa4e6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.cohere\n    options:\n      members:\n        - Cohere", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fba9c1a-3f8d-4d7d-9f47-a292f53d4bfe": {"__data__": {"id_": "9fba9c1a-3f8d-4d7d-9f47-a292f53d4bfe", "embedding": null, "metadata": {"filename": "custom_llm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9099216a7e9e4b4cff2ed38112fc536a7cbd9b94", "node_type": "4", "metadata": {"filename": "custom_llm.md", "author": "LlamaIndex"}, "hash": "605a0ddfc3c4cdf4472a5d9ceae3c58c22ee1170d1cdcc77b26d45898a5e5aa0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.llms.custom.CustomLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 42, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a633f210-fc68-4cc5-b377-def14456eee3": {"__data__": {"id_": "a633f210-fc68-4cc5-b377-def14456eee3", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "067b1fbe41f1ee204ac838682d0eb3b863d81e10", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "599da420dd72503bb3bc40c8ba3d8d70855b11e01e122945609685b2b80f2f88", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.dashscope\n    options:\n      members:\n        - DashScope", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a8f06a9-8d09-4135-8a6f-7000fea0b2c1": {"__data__": {"id_": "3a8f06a9-8d09-4135-8a6f-7000fea0b2c1", "embedding": null, "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c419c3f753f6fd5a03cf5dddb504aefdef996253", "node_type": "4", "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}, "hash": "28902199638897b304cec7d684ceea078ac6aeacf2d7f1fd679a34f65a1ddd47", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.databricks\n    options:\n      members:\n        - DataBricks", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "993e0164-e598-4137-b784-df6e0e466bc8": {"__data__": {"id_": "993e0164-e598-4137-b784-df6e0e466bc8", "embedding": null, "metadata": {"filename": "deepinfra.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46196e28cc71c6406f891b030e62d80ce99e78ad", "node_type": "4", "metadata": {"filename": "deepinfra.md", "author": "LlamaIndex"}, "hash": "96ad1391d13a13466495e903d3606d9335e9bfeb93105975d10b662bafa5abe6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.deepinfra\n    options:\n      members:\n        - DeepInfraLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "941417a8-8510-44a8-b3bb-f0c3679e4063": {"__data__": {"id_": "941417a8-8510-44a8-b3bb-f0c3679e4063", "embedding": null, "metadata": {"filename": "everlyai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08da8bdded2b73a1a4b731dc30454f3e26d5bb0f", "node_type": "4", "metadata": {"filename": "everlyai.md", "author": "LlamaIndex"}, "hash": "4089351d559c4881e3a4b02d0ea81641d1b720279e916de51a997a656ea5b929", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.everlyai\n    options:\n      members:\n        - EverlyAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0105fac0-6144-4707-80fd-1a39856c7e8e": {"__data__": {"id_": "0105fac0-6144-4707-80fd-1a39856c7e8e", "embedding": null, "metadata": {"filename": "fireworks.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9007974d92c18f316e6d7ea38f76c2da7461efee", "node_type": "4", "metadata": {"filename": "fireworks.md", "author": "LlamaIndex"}, "hash": "f9ece4b6b7714c0d80a5a8831403e70c52ff88fc0da47dfaf42e7a8e374ea1ee", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.fireworks\n    options:\n      members:\n        - Fireworks", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69ca6e5e-c50d-4c21-8983-7d53e18a9fdb": {"__data__": {"id_": "69ca6e5e-c50d-4c21-8983-7d53e18a9fdb", "embedding": null, "metadata": {"filename": "friendli.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8091da51f2b03c30974f186babde0e8d1f62309e", "node_type": "4", "metadata": {"filename": "friendli.md", "author": "LlamaIndex"}, "hash": "1353275e840d611872be398e5cb2f23103646bd00686b33e0d691312a386ae3b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.friendli\n    options:\n      members:\n        - Friendli", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3a75f41-494e-46d2-96dc-0ebc31122413": {"__data__": {"id_": "e3a75f41-494e-46d2-96dc-0ebc31122413", "embedding": null, "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84eb198685c264eac4a9183f6906abb4b4a98d2f", "node_type": "4", "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}, "hash": "9f5000da1abb2fcb343cc603bd6d1e5ed9d0d49ec20a6ca8577fbfda0925a5eb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.gemini\n    options:\n      members:\n        - Gemini", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c24bb819-1750-4567-9490-8bf4cf63f437": {"__data__": {"id_": "c24bb819-1750-4567-9490-8bf4cf63f437", "embedding": null, "metadata": {"filename": "gradient.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b2f298c2d1617c2a680f6dec501e9d20c41684f", "node_type": "4", "metadata": {"filename": "gradient.md", "author": "LlamaIndex"}, "hash": "29cc354607b89b574b1dc593fd5b489e224038eacd2aa80ba887dfff4f7f0c3a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.gradient\n    options:\n      members:\n        - GradientBaseModelLLM\n        - GradientModelAdapterLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4b433f4-3b52-4630-a576-a9366d08d3e7": {"__data__": {"id_": "f4b433f4-3b52-4630-a576-a9366d08d3e7", "embedding": null, "metadata": {"filename": "groq.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85e1eac31aa22b348b3aa13155257f6e4c82aa89", "node_type": "4", "metadata": {"filename": "groq.md", "author": "LlamaIndex"}, "hash": "6c7390378e11e385fb34f72061536cce7461c9c889347090a80ae5f64d4dc3e8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.groq\n    options:\n      members:\n        - Groq", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efe53e2e-0c99-46b9-829a-796469662c89": {"__data__": {"id_": "efe53e2e-0c99-46b9-829a-796469662c89", "embedding": null, "metadata": {"filename": "huggingface.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a03d1953b7c015f646be41a483caeb0e103b4220", "node_type": "4", "metadata": {"filename": "huggingface.md", "author": "LlamaIndex"}, "hash": "09843091ad35be583273ea8c982661d8d86bae3874a3ec043f984dc0434cacd9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.huggingface\n    options:\n      members:\n        - HuggingFaceInferenceAPI\n        - HuggingFaceLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fce12447-47a5-40e1-bc60-a8afb45a241b": {"__data__": {"id_": "fce12447-47a5-40e1-bc60-a8afb45a241b", "embedding": null, "metadata": {"filename": "huggingface_api.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33ee697f599b2805b5329823b7ba34a9306f4a2c", "node_type": "4", "metadata": {"filename": "huggingface_api.md", "author": "LlamaIndex"}, "hash": "16fb708a384354eca6c418726eb89fcc1359048b5a7b3b338759fb6394d31a42", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.huggingface_api\n    options:\n      members:\n        - HuggingFaceInferenceAPI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22e8eba6-91a7-4bae-8ae7-a09ac3a7d9b6": {"__data__": {"id_": "22e8eba6-91a7-4bae-8ae7-a09ac3a7d9b6", "embedding": null, "metadata": {"filename": "ibm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c70aa23d57862aaf4bd7ddb3a866ca4cb8af63b", "node_type": "4", "metadata": {"filename": "ibm.md", "author": "LlamaIndex"}, "hash": "ec64851065c6e4e760657d9f1af47fbf6771934d5ca28854f1718f60d7a9ba49", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.ibm\n    options:\n      members:\n        - WatsonxLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 73, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "479911d1-05eb-48a2-be02-c30a1b9cc837": {"__data__": {"id_": "479911d1-05eb-48a2-be02-c30a1b9cc837", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c714342127a6855cd46748d76458f77e6741782", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "7285aa27cb5b342a36c97bde7caead51b6ccf205f52b675b6d6d6c8d7ede6c0a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.llms.llm\n    options:\n      members:\n        - LLM\n      show_source: false\n      inherited_members: true", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "714a03b5-50dc-4a7c-a549-65db96cae3d5": {"__data__": {"id_": "714a03b5-50dc-4a7c-a549-65db96cae3d5", "embedding": null, "metadata": {"filename": "ipex_llm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aec44b11d79026095c37b02c13d5ade2185dc5ee", "node_type": "4", "metadata": {"filename": "ipex_llm.md", "author": "LlamaIndex"}, "hash": "3dffafe19dd13d3747e3beb656516059e5703484a69af730e5acecf52a0674a2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.ipex_llm\n    options:\n      members:\n        - IpexLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d455bcf6-f55b-484e-88ab-f0b1fcdbaaa7": {"__data__": {"id_": "d455bcf6-f55b-484e-88ab-f0b1fcdbaaa7", "embedding": null, "metadata": {"filename": "konko.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af2e07a58fa4e935374b3205f1f576c5d832719b", "node_type": "4", "metadata": {"filename": "konko.md", "author": "LlamaIndex"}, "hash": "5ab087dae9e0f12df29026dae25a91e0616db35871e124a803ddf51ba69903c9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.konko\n    options:\n      members:\n        - Konko", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5142deec-cc76-40fe-a90e-d34d90cbfb8a": {"__data__": {"id_": "5142deec-cc76-40fe-a90e-d34d90cbfb8a", "embedding": null, "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb43ece399d17d5eddd6503382136f699b9461bf", "node_type": "4", "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "hash": "8a5779b0fe651c7ce38542caae72c0c2f585bb9b4af51e5c499394a55afd6d85", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.langchain\n    options:\n      members:\n        - LangChainLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc92ec55-ef92-47a8-b64b-9e6852fe6e19": {"__data__": {"id_": "bc92ec55-ef92-47a8-b64b-9e6852fe6e19", "embedding": null, "metadata": {"filename": "litellm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa1ee2a2b37ce4b2e73d65e50158c8e9d339e622", "node_type": "4", "metadata": {"filename": "litellm.md", "author": "LlamaIndex"}, "hash": "7b04ffebe1965477c259bdde8e7824c9d35b894b0c77fe68ad65d0e72fde9ead", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.litellm\n    options:\n      members:\n        - LiteLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72a4b730-e316-4320-8de5-a8d5294c430c": {"__data__": {"id_": "72a4b730-e316-4320-8de5-a8d5294c430c", "embedding": null, "metadata": {"filename": "llama_api.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fde85ca0e7c587a432998a27657959609b1c5fa1", "node_type": "4", "metadata": {"filename": "llama_api.md", "author": "LlamaIndex"}, "hash": "537c26ba862eb86519b5d30b02305ef282a94af0a47cc2697605bf19b08faeee", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.llama_api\n    options:\n      members:\n        - LlamaAPI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6888fa5-6c81-424b-a57c-e74ce5966f0c": {"__data__": {"id_": "e6888fa5-6c81-424b-a57c-e74ce5966f0c", "embedding": null, "metadata": {"filename": "llama_cpp.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e290fe18a4d1db7a4d05c2bc4f79dccf23e7fad", "node_type": "4", "metadata": {"filename": "llama_cpp.md", "author": "LlamaIndex"}, "hash": "39b5be8caeef205b08dfb9b5dffb41f7d8b0258ed50fec1b3a7e467dbae7d330", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.llama_cpp\n    options:\n      members:\n        - LlamaCPP", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e56f6282-462a-4228-b1c5-2a44adfb946f": {"__data__": {"id_": "e56f6282-462a-4228-b1c5-2a44adfb946f", "embedding": null, "metadata": {"filename": "llamafile.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ac0ff38a07d18a59329c21f89b4f7b11b0941cc", "node_type": "4", "metadata": {"filename": "llamafile.md", "author": "LlamaIndex"}, "hash": "3f2beb12c358a9be76afdefb0a541c5afb3dc71f4b7587a6231f8cc64111527e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.llamafile\n    options:\n      members:\n        - Llamafile", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "352c3bca-963b-4745-b690-51ac62783d37": {"__data__": {"id_": "352c3bca-963b-4745-b690-51ac62783d37", "embedding": null, "metadata": {"filename": "lmstudio.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92a0edc07916d81956005ba900ee646d86c34d56", "node_type": "4", "metadata": {"filename": "lmstudio.md", "author": "LlamaIndex"}, "hash": "98a57844ff09263690f6e8f36d844dc120779ec5c3d250377049cb28af54162d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.lmstudio\n    options:\n      members:\n        - LMStudio", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33b4fb12-7d0e-4f77-a122-e30a8f94f45d": {"__data__": {"id_": "33b4fb12-7d0e-4f77-a122-e30a8f94f45d", "embedding": null, "metadata": {"filename": "localai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bac99c24902a76dd2a1886038b3993f197ad537f", "node_type": "4", "metadata": {"filename": "localai.md", "author": "LlamaIndex"}, "hash": "b1cb1b771f67d43436faf8c40bdcc3e42ed07ebfde789c0c6ae7b1c7fb074f48", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.localai\n    options:\n      members:\n        - LocalAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23aa3911-c09a-46ad-8d63-615e28070d58": {"__data__": {"id_": "23aa3911-c09a-46ad-8d63-615e28070d58", "embedding": null, "metadata": {"filename": "maritalk.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab19c6d419976b8adc3acf90571e36b1307149c6", "node_type": "4", "metadata": {"filename": "maritalk.md", "author": "LlamaIndex"}, "hash": "19e3e37194ac7c26c763da179878ed8c029421ed594cf634790dc74817d0cc53", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.maritalk\n    options:\n      members:\n        - Maritalk", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "563f983a-ba41-4083-8aba-a9ee2b339c46": {"__data__": {"id_": "563f983a-ba41-4083-8aba-a9ee2b339c46", "embedding": null, "metadata": {"filename": "mistral_rs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d95fa07ea28210ed8a2b7f855a2fa1201b3b726", "node_type": "4", "metadata": {"filename": "mistral_rs.md", "author": "LlamaIndex"}, "hash": "06640b50dd0937573ab0dff6f0071abec66cf194a52a8a854c12dc49181ab4db", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.mistral_rs\n    options:\n      members:\n        - MistralRs", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d96a1780-8361-45b9-ac99-a8b712a20bfb": {"__data__": {"id_": "d96a1780-8361-45b9-ac99-a8b712a20bfb", "embedding": null, "metadata": {"filename": "mistralai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72158a169d729fecc167992fdeca176a685829db", "node_type": "4", "metadata": {"filename": "mistralai.md", "author": "LlamaIndex"}, "hash": "c460040570529c61992cedbbbbc6e86dfb93e881c45138c51a5d9ba32028d849", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.mistralai\n    options:\n      members:\n        - MistralAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99b420b8-48e9-4874-b1e9-6a78432ae508": {"__data__": {"id_": "99b420b8-48e9-4874-b1e9-6a78432ae508", "embedding": null, "metadata": {"filename": "mlx.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "961377d4aca7ea1d0274b49daceeaae10137dbdb", "node_type": "4", "metadata": {"filename": "mlx.md", "author": "LlamaIndex"}, "hash": "1034f0d13a5c71c1c0126742148d99f07819dc9bb509fa20df8d5d05c5cdc132", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.mlx\n    options:\n      members:\n        - MLX", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 66, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdfb7534-bf82-4c32-9e59-ed870b05178e": {"__data__": {"id_": "cdfb7534-bf82-4c32-9e59-ed870b05178e", "embedding": null, "metadata": {"filename": "modelscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74ec8e3d5142748a58917e5fa625fccc93cab8e1", "node_type": "4", "metadata": {"filename": "modelscope.md", "author": "LlamaIndex"}, "hash": "38515f636fb75a78e43f1d461f05abe3f4b5828bd9e6e322c13f17e332052342", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.modelscope\n    options:\n      members:\n        - ModelScopeLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9c24c1c-10ee-404d-8d6a-048f5d76cef6": {"__data__": {"id_": "e9c24c1c-10ee-404d-8d6a-048f5d76cef6", "embedding": null, "metadata": {"filename": "monsterapi.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a9a83e64b89a73c86f2f0eb737eb5ccda93a8dd", "node_type": "4", "metadata": {"filename": "monsterapi.md", "author": "LlamaIndex"}, "hash": "695055e52ec9cc5043cb716f55306c13cd1f9024288bb7a08b09daba41328939", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.monsterapi\n    options:\n      members:\n        - MonsterLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d41bcda5-e843-4111-a25f-6f36666b5082": {"__data__": {"id_": "d41bcda5-e843-4111-a25f-6f36666b5082", "embedding": null, "metadata": {"filename": "mymagic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d7dc7c977c1b86d5c7e7aaa12e348bd3826d9de", "node_type": "4", "metadata": {"filename": "mymagic.md", "author": "LlamaIndex"}, "hash": "4ed18c03ab3e1c72c4a815cce73194c3c4f06ed9674b59af3be5258c1963e7ee", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.mymagic\n    options:\n      members:\n        - MyMagicAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9f37820-8422-4414-b7bb-3f46e331ebf5": {"__data__": {"id_": "a9f37820-8422-4414-b7bb-3f46e331ebf5", "embedding": null, "metadata": {"filename": "neutrino.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d04873a942949ae2ddf345444a0e1e470827d06", "node_type": "4", "metadata": {"filename": "neutrino.md", "author": "LlamaIndex"}, "hash": "23ea0982db37cc7005ee066569374af425f9bbf00139752850c469eb482d7f97", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.neutrino\n    options:\n      members:\n        - Neutrino", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09cf7df7-e8d9-4545-88a2-4cf2b5ea6762": {"__data__": {"id_": "09cf7df7-e8d9-4545-88a2-4cf2b5ea6762", "embedding": null, "metadata": {"filename": "nvidia.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86b9e5886e1f7db18e4d0088f116cde432c67bf3", "node_type": "4", "metadata": {"filename": "nvidia.md", "author": "LlamaIndex"}, "hash": "c4c411cefeccd4533362a7e56a576f5226a1d023d7c37e3be9fe8c77c273d70d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.nvidia\n    options:\n      members:\n        - NVIDIA", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ba6c9e6-a9cb-4ca1-b500-0dc1912d9866": {"__data__": {"id_": "1ba6c9e6-a9cb-4ca1-b500-0dc1912d9866", "embedding": null, "metadata": {"filename": "nvidia_tensorrt.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a4d13bcd0a88f856d45915747867fac3162dd3b", "node_type": "4", "metadata": {"filename": "nvidia_tensorrt.md", "author": "LlamaIndex"}, "hash": "3cc15e2fd56380a7574a2ed939ff770d7825ec2829d3125355fa1c9dfc646959", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.nvidia_tensorrt\n    options:\n      members:\n        - LocalTensorRTLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11e7178f-4809-4162-b1fa-a4617c8973ff": {"__data__": {"id_": "11e7178f-4809-4162-b1fa-a4617c8973ff", "embedding": null, "metadata": {"filename": "nvidia_triton.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cbf177abd2f536b86f88adfc1ebbcbf5867ef4c", "node_type": "4", "metadata": {"filename": "nvidia_triton.md", "author": "LlamaIndex"}, "hash": "cebd51f98b0bff512cda6e0714f01f2857b1ca45043929da7e657942b8d65187", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.nvidia_triton\n    options:\n      members:\n        - NvidiaTriton", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d3dc81e-59d2-4d12-b8c2-dc5bed31a5bc": {"__data__": {"id_": "2d3dc81e-59d2-4d12-b8c2-dc5bed31a5bc", "embedding": null, "metadata": {"filename": "oci_genai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83cc20a19d58e1fb839250caf678f884d418b133", "node_type": "4", "metadata": {"filename": "oci_genai.md", "author": "LlamaIndex"}, "hash": "c06444fff6ec5eba674766954e00215fdf0d7ef4e9462c96d7775ef14af2c3d4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.oci_genai\n    options:\n      members:\n        - OCIGenAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96b88442-0862-4781-b1a1-034bed9d5549": {"__data__": {"id_": "96b88442-0862-4781-b1a1-034bed9d5549", "embedding": null, "metadata": {"filename": "octoai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd6b10e038ee5477eb07514c7f0d3124d04613ab", "node_type": "4", "metadata": {"filename": "octoai.md", "author": "LlamaIndex"}, "hash": "33c678b77f98880a13cfe752e0f9ee0b09d3a52722ba8c94c8a12ce318cf34d4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.octoai\n    options:\n      members:\n        - OctoAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dea87571-c91c-4cee-9593-cae94753cce9": {"__data__": {"id_": "dea87571-c91c-4cee-9593-cae94753cce9", "embedding": null, "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bf7d47d60f8e3ca542f394ec46075758a3cc7b0", "node_type": "4", "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}, "hash": "0b18c4d871e41c4ffdff0ca35a3aab5850283e90b6ae20932293c9387b3cd2c0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.ollama\n    options:\n      members:\n        - Ollama", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66d4a40d-93fe-44a3-837f-a7e7e4f5dd13": {"__data__": {"id_": "66d4a40d-93fe-44a3-837f-a7e7e4f5dd13", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac8ac08abfbca7a02503de846750715a164f7c6c", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "769ce2619043a6ba7fb81a747dc320c4b4212d6cfec15628a263a5a222428aef", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.openai\n    options:\n      members:\n        - AsyncOpenAI\n        - OpenAI\n        - SyncOpenAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9baf0a05-12a0-4a67-8252-85938ad5445f": {"__data__": {"id_": "9baf0a05-12a0-4a67-8252-85938ad5445f", "embedding": null, "metadata": {"filename": "openai_like.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2673ecfdc7d512afd9b61e328c5e4b9ca3b5497d", "node_type": "4", "metadata": {"filename": "openai_like.md", "author": "LlamaIndex"}, "hash": "854e617ab77d740277f0801ae92419042a225f8336735f85c0ce4c94a6039167", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.openai_like\n    options:\n      members:\n        - OpenAILike", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78a4cb67-e5a0-49fe-a9c6-d31ba8c2f592": {"__data__": {"id_": "78a4cb67-e5a0-49fe-a9c6-d31ba8c2f592", "embedding": null, "metadata": {"filename": "openllm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0038e82c7610d1aa0d22c4ca4322efa093f37b26", "node_type": "4", "metadata": {"filename": "openllm.md", "author": "LlamaIndex"}, "hash": "95cc9474610eb61a62ec298be9d99aadef5dde39f95c5322811d22b4d97f7f88", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.openllm\n    options:\n      members:\n        - OpenLLM\n        - OpenLLMAPI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b2b6cd6-2d9d-45e8-aa86-6a2f623c7eeb": {"__data__": {"id_": "7b2b6cd6-2d9d-45e8-aa86-6a2f623c7eeb", "embedding": null, "metadata": {"filename": "openrouter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43230982b1d7bbb45ab508fe3ffcae361c9d9bab", "node_type": "4", "metadata": {"filename": "openrouter.md", "author": "LlamaIndex"}, "hash": "db7831002b80582349d8aa32bfbee4081ffd69123a92688fcd3cc308bf279d84", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.openrouter\n    options:\n      members:\n        - OpenRouter", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a514c6e4-70e4-4c90-ba1a-fec248f27079": {"__data__": {"id_": "a514c6e4-70e4-4c90-ba1a-fec248f27079", "embedding": null, "metadata": {"filename": "openvino.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "132a3be5cd821cabd8083bbf35b25748ebdfe5af", "node_type": "4", "metadata": {"filename": "openvino.md", "author": "LlamaIndex"}, "hash": "eefc2ecceea3776e087e506de8d3c687d7907222c93d2e1ddf151eaa20d14d33", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.openvino\n    options:\n      members:\n        - OpenVINOLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fad728c5-f084-48c6-a650-feb6f6fe8c8e": {"__data__": {"id_": "fad728c5-f084-48c6-a650-feb6f6fe8c8e", "embedding": null, "metadata": {"filename": "optimum_intel.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c14c627525dacf03d693369000d48797724d57b", "node_type": "4", "metadata": {"filename": "optimum_intel.md", "author": "LlamaIndex"}, "hash": "201a6784706a25461e5a08bef70aec4e42bf874840becdb67dbbef6977cad720", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.optimum_intel\n    options:\n      members:\n        - OptimumIntelLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c789ce4-5053-48a9-b928-f960d740a5dc": {"__data__": {"id_": "4c789ce4-5053-48a9-b928-f960d740a5dc", "embedding": null, "metadata": {"filename": "palm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "126e9a8727a132922a9e82c9fd564ca5d7a297f9", "node_type": "4", "metadata": {"filename": "palm.md", "author": "LlamaIndex"}, "hash": "97ec075f7aecae927b70d3f32b2969956cae6bf13bff4f0144110d765ee626a5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.palm\n    options:\n      members:\n        - PaLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83bfde80-25fd-408b-aed9-3c1f6d9e0abf": {"__data__": {"id_": "83bfde80-25fd-408b-aed9-3c1f6d9e0abf", "embedding": null, "metadata": {"filename": "perplexity.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f677afd214b768a748dfa165f4159427afebb17c", "node_type": "4", "metadata": {"filename": "perplexity.md", "author": "LlamaIndex"}, "hash": "e203de7aedfebf25a6081066295485b082338d38b5e0d9c1db6b1278ecbe042c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.perplexity\n    options:\n      members:\n        - Perplexity", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95615e5a-19c2-40e2-8dcc-2a306f97eff3": {"__data__": {"id_": "95615e5a-19c2-40e2-8dcc-2a306f97eff3", "embedding": null, "metadata": {"filename": "portkey.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d8a321dd2c45b3da2e9b22615e0ba62ee463c51", "node_type": "4", "metadata": {"filename": "portkey.md", "author": "LlamaIndex"}, "hash": "8db06bc4f3fef75a47331ca1ee7b7c224c154fa642755eabe5fe796f12dd78e3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.portkey\n    options:\n      members:\n        - Portkey", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebfce126-6a66-4521-93e7-d44b1cf8e72c": {"__data__": {"id_": "ebfce126-6a66-4521-93e7-d44b1cf8e72c", "embedding": null, "metadata": {"filename": "predibase.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca134577fd704889716fea8083359bec2079f0ad", "node_type": "4", "metadata": {"filename": "predibase.md", "author": "LlamaIndex"}, "hash": "be4d221b18169ad44eb3932e297ead84dee1a09829f79a7fa569114f626a29e1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.predibase\n    options:\n      members:\n        - PredibaseLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef11c736-40be-4a43-91ba-ea7daf40966a": {"__data__": {"id_": "ef11c736-40be-4a43-91ba-ea7daf40966a", "embedding": null, "metadata": {"filename": "premai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fcd64a416bf306ad31365273975fb4bfa7fa772", "node_type": "4", "metadata": {"filename": "premai.md", "author": "LlamaIndex"}, "hash": "77411f3f0b06bdc4fe156f0c309410b458569fa37c02407c3c701a7b5c7538bf", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.premai\n    options:\n      members:\n        - PremAI", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80cab939-0992-4dc3-b6fe-056e46d23f4d": {"__data__": {"id_": "80cab939-0992-4dc3-b6fe-056e46d23f4d", "embedding": null, "metadata": {"filename": "qianfan.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62b0f1daf00496bca6c57f3a4e193800fea33e97", "node_type": "4", "metadata": {"filename": "qianfan.md", "author": "LlamaIndex"}, "hash": "908c2b0e4c281e7e7ac8a9db210e3ad67136377d2132bb267d1bec5c8b179740", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.qianfan\n    options:\n      members:\n        - Qianfan", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bb51e66-40fa-4283-bdff-48ebec9a9249": {"__data__": {"id_": "8bb51e66-40fa-4283-bdff-48ebec9a9249", "embedding": null, "metadata": {"filename": "replicate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc6639228a4e90854f701cdf6ed6422e2931cf2", "node_type": "4", "metadata": {"filename": "replicate.md", "author": "LlamaIndex"}, "hash": "4e19f085002eafa344835903995194ee5dd10a1eedfaa5604f0915db68978241", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.replicate\n    options:\n      members:\n        - Replicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a23883cc-ac55-4a17-a5c8-883101b57751": {"__data__": {"id_": "a23883cc-ac55-4a17-a5c8-883101b57751", "embedding": null, "metadata": {"filename": "rungpt.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1419f258f124640cc51b5aaa5754a0f6386e5a14", "node_type": "4", "metadata": {"filename": "rungpt.md", "author": "LlamaIndex"}, "hash": "82d838619b21bc026d28833988da1835e8c31a8e0c340436a5e671348982f684", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.rungpt\n    options:\n      members:\n        - RunGptLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6396bfb2-9e0e-453d-b449-51b86e6243de": {"__data__": {"id_": "6396bfb2-9e0e-453d-b449-51b86e6243de", "embedding": null, "metadata": {"filename": "sagemaker_endpoint.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c93fd2351851c49c51f7218ff4750026acff406", "node_type": "4", "metadata": {"filename": "sagemaker_endpoint.md", "author": "LlamaIndex"}, "hash": "d2ae9c2d56801792ab521c85c09a662ade23d8009c8c1dab7ad9ea5159eb42f3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.sagemaker_endpoint\n    options:\n      members:\n        - SageMakerLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de7812e3-a553-4e49-8675-903d900e128a": {"__data__": {"id_": "de7812e3-a553-4e49-8675-903d900e128a", "embedding": null, "metadata": {"filename": "solar.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3a223bee990164c007bdcf1b70051858ce282da", "node_type": "4", "metadata": {"filename": "solar.md", "author": "LlamaIndex"}, "hash": "cd4154cfa95af4a1fcf0695d61da8af85aecf4d0977868e7c1adaa6305113ebd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.solar\n    options:\n      members:\n        - Solar", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56cad37f-9c3c-49dd-84db-d77a4edd42b5": {"__data__": {"id_": "56cad37f-9c3c-49dd-84db-d77a4edd42b5", "embedding": null, "metadata": {"filename": "text_generation_inference.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "afeb9ae9d3543bff4c154b584450c4be4f8e1564", "node_type": "4", "metadata": {"filename": "text_generation_inference.md", "author": "LlamaIndex"}, "hash": "5bee34997c3b5f85d828af7df5e41e4840997124ffe08c750395fe12e3177fdd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.text_generation_inference\n    options:\n      members:\n        - TextGenerationInference", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52b1c710-d85e-4689-ace0-b33b3b416155": {"__data__": {"id_": "52b1c710-d85e-4689-ace0-b33b3b416155", "embedding": null, "metadata": {"filename": "together.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19e3fd8bdd5d4821302a98d0b8a460a2adbb36b0", "node_type": "4", "metadata": {"filename": "together.md", "author": "LlamaIndex"}, "hash": "47ab7285fe8df7a782a222fdcacbe5bf2c2d6876a1a95f8484bf4b182ab5a284", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.together\n    options:\n      members:\n        - TogetherLLM", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33133590-6a40-4593-b23e-0c78ee0d5037": {"__data__": {"id_": "33133590-6a40-4593-b23e-0c78ee0d5037", "embedding": null, "metadata": {"filename": "unify.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b69773c0c9b6ab783a52a49b48ec0962228319ea", "node_type": "4", "metadata": {"filename": "unify.md", "author": "LlamaIndex"}, "hash": "acb6fb79907a2efabef84a5a95f8c2ca85db54f0faf0695fde08436664b97836", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.unify\n    options:\n      members:\n        - Unify", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26cf4073-6d41-466e-a641-a326aded4216": {"__data__": {"id_": "26cf4073-6d41-466e-a641-a326aded4216", "embedding": null, "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea36317231ea6768aacbfd2a370830e34183cafd", "node_type": "4", "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}, "hash": "2c66dbb6716c12c59654c46e60e34c59c27de7d33c1a31fb13292898e0f23a77", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.upstage\n    options:\n      members:\n        - Upstage", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8da56310-a4c8-4b1b-b803-e7b677685dc8": {"__data__": {"id_": "8da56310-a4c8-4b1b-b803-e7b677685dc8", "embedding": null, "metadata": {"filename": "vertex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4722b733cb5aa7afc09c222def98031a1d701d1f", "node_type": "4", "metadata": {"filename": "vertex.md", "author": "LlamaIndex"}, "hash": "791d80b8bbf8b4c6cf4e11fe9ed3eb6ccc1da023e741c33b58825db2a5fe4f5f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.vertex\n    options:\n      members:\n        - Vertex", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d549ae49-1b01-48e6-bf48-a6bf23c9d66f": {"__data__": {"id_": "d549ae49-1b01-48e6-bf48-a6bf23c9d66f", "embedding": null, "metadata": {"filename": "vllm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be0e8188c363e3c62f0267893b30c0164cdb88fa", "node_type": "4", "metadata": {"filename": "vllm.md", "author": "LlamaIndex"}, "hash": "864785e84197b12063e4e234db34b74b22f95a11991122e34a7811ae9b6c6e7c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.vllm\n    options:\n      members:\n        - Vllm\n        - VllmServer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "529a7a27-ba42-44fc-86e4-8f02ea50ac45": {"__data__": {"id_": "529a7a27-ba42-44fc-86e4-8f02ea50ac45", "embedding": null, "metadata": {"filename": "xinference.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7fe8271834efeb9712ab116ffa91a382c6260fda", "node_type": "4", "metadata": {"filename": "xinference.md", "author": "LlamaIndex"}, "hash": "d9dddf99148354625234289c757647df33c8e34d88cedf54c01688fa25e34fda", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.xinference\n    options:\n      members:\n        - Xinference", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b380951f-10b7-4382-a68f-fb506295f819": {"__data__": {"id_": "b380951f-10b7-4382-a68f-fb506295f819", "embedding": null, "metadata": {"filename": "yi.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4018d1f8c35f1fa7f5b2494ee5f17b37dc986fca", "node_type": "4", "metadata": {"filename": "yi.md", "author": "LlamaIndex"}, "hash": "42ea0b1f7d656913c98fd417c4368b675851b3acde4b1f0024592ea9fc09ac8b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.yi\n    options:\n      members:\n        - Yi", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 64, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c1e2698-cef1-4276-947a-fbb5f3c6211a": {"__data__": {"id_": "7c1e2698-cef1-4276-947a-fbb5f3c6211a", "embedding": null, "metadata": {"filename": "you.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7c5ec6375565e856ac2fd7b14868b539db6143e", "node_type": "4", "metadata": {"filename": "you.md", "author": "LlamaIndex"}, "hash": "0bb3f88607e8256ad47b379df37b23b5683260317217b2c01903df27feadc2c7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.llms.you\n    options:\n      members:\n        - You", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 66, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1333a66-d22d-44b5-95ac-45d1b62433be": {"__data__": {"id_": "f1333a66-d22d-44b5-95ac-45d1b62433be", "embedding": null, "metadata": {"filename": "chat_memory_buffer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "334b77b0d28559d24d715a578fc97ad081fcfb06", "node_type": "4", "metadata": {"filename": "chat_memory_buffer.md", "author": "LlamaIndex"}, "hash": "86cdcaa1d277c1afcfdec732faaa7b5e6856e54d3fd2a65d362f4a38f625d225", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.memory.chat_memory_buffer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 46, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e7c0f14-629a-4b1b-a6ad-daa408e2ddf6": {"__data__": {"id_": "8e7c0f14-629a-4b1b-a6ad-daa408e2ddf6", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de1559ae3f3990a07293820122f02cbed8684e1d", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c8d9882054c6da57dae2ef17f700f121bb3955e7260810fb67fb01275f69cd43", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.memory.types", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 33, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab44f382-394d-455b-a062-27e1e011ecc8": {"__data__": {"id_": "ab44f382-394d-455b-a062-27e1e011ecc8", "embedding": null, "metadata": {"filename": "simple_composable_memory.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e85b90c5a9c1db9f27cff6aac94a8077c8453d2", "node_type": "4", "metadata": {"filename": "simple_composable_memory.md", "author": "LlamaIndex"}, "hash": "d97343365c10f4f096454143703338006a9774fae2daf4032fb45d2e68840c87", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.memory.simple_composable_memory", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 52, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d687452c-ed3e-4c48-bcb0-4061880e933f": {"__data__": {"id_": "d687452c-ed3e-4c48-bcb0-4061880e933f", "embedding": null, "metadata": {"filename": "vector_memory.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb50c0ef256dcb735fcf1cc703156ca0530f4f9d", "node_type": "4", "metadata": {"filename": "vector_memory.md", "author": "LlamaIndex"}, "hash": "30b08f7f23f1bce81cab60213f6e97dbaaacfa839696f4c618362b9bb0d56fbf", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.memory.vector_memory", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 41, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ab4f797-4b56-494f-b13a-a4f210cb6165": {"__data__": {"id_": "9ab4f797-4b56-494f-b13a-a4f210cb6165", "embedding": null, "metadata": {"filename": "anthropic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc27895b184fa9134083f8d6b3aad12a3da19eb", "node_type": "4", "metadata": {"filename": "anthropic.md", "author": "LlamaIndex"}, "hash": "faf67772a3fe504ae6a02713a0b161302fde17ba459bbc3e501af3aab0bd8555", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.anthropic\n    options:\n      members:\n        - AnthropicMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96956a5e-3c88-41be-ae2f-0827d05cc577": {"__data__": {"id_": "96956a5e-3c88-41be-ae2f-0827d05cc577", "embedding": null, "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61718ea082b8d18c205059176989104d4132d4a7", "node_type": "4", "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}, "hash": "7f89034f33003774baa3bc8e6eae80f756afc0b7f0f9136ad47ec7474343b4cf", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.azure_openai\n    options:\n      members:\n        - AzureOpenAIMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acb52e51-e830-4b0d-be1a-fadb7d01eb64": {"__data__": {"id_": "acb52e51-e830-4b0d-be1a-fadb7d01eb64", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a70e3d2abb81c179f96cc0c1a6990328e0737e7", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "ee9a87cefe239484c8048924aba7136e6a82ad91f1a9f7a6fc240c9ccc9c840a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.dashscope\n    options:\n      members:\n        - DashScopeMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b3c67ba-3c50-404a-a6a2-4613b10ba823": {"__data__": {"id_": "0b3c67ba-3c50-404a-a6a2-4613b10ba823", "embedding": null, "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4922bce0a754ff907229da3158d4430e674bbc02", "node_type": "4", "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}, "hash": "966379db0b8878fd21fa30048804681858eb400b38f074210145fb2b7694a90b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.gemini\n    options:\n      members:\n        - GeminiMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7f1a673-14df-42e2-b711-5a95bd017761": {"__data__": {"id_": "f7f1a673-14df-42e2-b711-5a95bd017761", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7edc82e3bc649b8becc4edb7f36a4d746e574c", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "040b4733f58f2e9822a7b08c1cea8166523f3cdf561c31a8ea157340aa6eaf7e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.multi_modal_llms.base", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 42, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6882ace-88dc-4357-9326-ca745ba8a349": {"__data__": {"id_": "a6882ace-88dc-4357-9326-ca745ba8a349", "embedding": null, "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a3c55fcc919c971743594e5c74d9c9f20f55d78", "node_type": "4", "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}, "hash": "346d5a0066959905e52722cdc7133ee1235ab13bcc0b1e395e0c84ae01a374a7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.ollama\n    options:\n      members:\n        - OllamaMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1abc96da-fbf3-4dd8-82e5-95d330c75828": {"__data__": {"id_": "1abc96da-fbf3-4dd8-82e5-95d330c75828", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c138dc98f30ea564d0fba065622327cc7848175", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "cfe01329c1c56da8beb1c824c4f69a3fa5adaa0f2da2d3aa2e0e7242071e2187", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.openai\n    options:\n      members:\n        - OpenAIMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d72f0545-0715-4b5f-9d26-beb263362b33": {"__data__": {"id_": "d72f0545-0715-4b5f-9d26-beb263362b33", "embedding": null, "metadata": {"filename": "replicate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b7526a553973a61217093dceabbfcf6c982636f", "node_type": "4", "metadata": {"filename": "replicate.md", "author": "LlamaIndex"}, "hash": "dac0e54f0d5769e34c88b08cb32112b94e01cf98b67844321d81720f4f5d538b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.multi_modal_llms.replicate\n    options:\n      members:\n        - ReplicateMultiModal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a23d2c6-4272-4e0b-8d2c-3e6a3832fcec": {"__data__": {"id_": "7a23d2c6-4272-4e0b-8d2c-3e6a3832fcec", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63eb306018fe2bc1d75b8ec7b8f9d33aa6b93974", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "a82525521e98d3b46bacb8b95a5c11b941a0dca306761a4e69b45ac3a207062e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.node_parser.dashscope\n    options:\n      members:\n        - DashScopeJsonNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ece38c9-4435-4c53-b09e-afd840c5d4a4": {"__data__": {"id_": "6ece38c9-4435-4c53-b09e-afd840c5d4a4", "embedding": null, "metadata": {"filename": "code.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "222cd70b2d034e786c6ba620a876fbf064bceca3", "node_type": "4", "metadata": {"filename": "code.md", "author": "LlamaIndex"}, "hash": "5450fab4d02f7a5246d3ab5d6c8bb7cff1d55b53c6c21052db4963659cd78fe5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - CodeSplitter", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f945ec71-fb43-4b7e-9a63-7cd08d5a0413": {"__data__": {"id_": "f945ec71-fb43-4b7e-9a63-7cd08d5a0413", "embedding": null, "metadata": {"filename": "hierarchical.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5245b851143d3e6d83b4e2cfdd31e63ba67fb480", "node_type": "4", "metadata": {"filename": "hierarchical.md", "author": "LlamaIndex"}, "hash": "08087856c6d4b992ed9d796069219c69bc992c070844345c8255a3db9cbc4783", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - HierarchicalNodeParser\n        - get_leaf_nodes\n        - get_root_nodes\n        - get_child_nodes\n        - get_deeper_nodes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b626086-2cb4-42f6-9a8d-a80cb7b1f8b3": {"__data__": {"id_": "4b626086-2cb4-42f6-9a8d-a80cb7b1f8b3", "embedding": null, "metadata": {"filename": "html.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11a977e9457d77fcc76cbcd4666ba8a6b9756b75", "node_type": "4", "metadata": {"filename": "html.md", "author": "LlamaIndex"}, "hash": "855edd8e5a6ba5550b8a088ccec7eb197012d8565055f07e6633bc1aedbabff3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - HTMLNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "854eb623-3ff2-4c46-9312-52fcecf7efeb": {"__data__": {"id_": "854eb623-3ff2-4c46-9312-52fcecf7efeb", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "359df9a40dd5f8cd94cdcb0927fed773170ba8ac", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "a92af15c6f416fc6b3c421f400c668cdbc117a5439fb222f751c6fa036143ee8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser.interface", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 42, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f680187a-708e-4285-94f0-67312eb056e7": {"__data__": {"id_": "f680187a-708e-4285-94f0-67312eb056e7", "embedding": null, "metadata": {"filename": "json.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eddf2015da7c28e2a652cbee446c3714f89a28de", "node_type": "4", "metadata": {"filename": "json.md", "author": "LlamaIndex"}, "hash": "ef06300b122dbdc5c335d90389d0d7471003e3b298ec51f267e5080696df727e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - JSONNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10226bc0-4dd3-42df-b919-0a11931dc0d0": {"__data__": {"id_": "10226bc0-4dd3-42df-b919-0a11931dc0d0", "embedding": null, "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dcb2bc004da48ba773d9ab5668e90a06d8e2a05", "node_type": "4", "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "hash": "f5a35ea280f1483b64d54b64ea65b64af755b4f74ed2bfd7006938731fd3b455", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - LangchainNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ac97b0f-7589-4b45-a0ef-be5bfe7080f0": {"__data__": {"id_": "1ac97b0f-7589-4b45-a0ef-be5bfe7080f0", "embedding": null, "metadata": {"filename": "markdown.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8a8228790d2b60b2b24085c9c3aedb01d18015b", "node_type": "4", "metadata": {"filename": "markdown.md", "author": "LlamaIndex"}, "hash": "ab8529746346b84e1d4feb2cee53aa545f1607b8256e4d35e6e2fd826637009f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - MarkdownNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79eeeadc-a72e-4a07-a111-ca14ba5499f8": {"__data__": {"id_": "79eeeadc-a72e-4a07-a111-ca14ba5499f8", "embedding": null, "metadata": {"filename": "markdown_element.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "242f3bd61a1e9d9f52c7cb84501c8a39cc99d9e6", "node_type": "4", "metadata": {"filename": "markdown_element.md", "author": "LlamaIndex"}, "hash": "18da7719aa8125361ad07424efa80425f455c09832626f83d1160df1ea679cf7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - MarkdownElementNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d48b9ae-356f-41cf-8cad-fba0a5988bf6": {"__data__": {"id_": "2d48b9ae-356f-41cf-8cad-fba0a5988bf6", "embedding": null, "metadata": {"filename": "semantic_splitter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dbf50ba0570f630b3103fd3794d9a40b193b07b", "node_type": "4", "metadata": {"filename": "semantic_splitter.md", "author": "LlamaIndex"}, "hash": "1fd46519cab0bca62c9ec9790a81fc31c7d41c9fa7556c7f7fffbdb6a7c82f5a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - SemanticSplitterNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dff749fb-6540-4f30-ab48-49511f4a781b": {"__data__": {"id_": "dff749fb-6540-4f30-ab48-49511f4a781b", "embedding": null, "metadata": {"filename": "sentence_splitter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcd78fda211589bb545008bef898bb179cd90802", "node_type": "4", "metadata": {"filename": "sentence_splitter.md", "author": "LlamaIndex"}, "hash": "abc15fe95fd3e60a05ecac49fa15241d2ce579f4400470eea640b8c36c07f607", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - SentenceSplitter", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf7f0be4-5698-4346-8ee1-e0dc9c695bc8": {"__data__": {"id_": "cf7f0be4-5698-4346-8ee1-e0dc9c695bc8", "embedding": null, "metadata": {"filename": "sentence_window.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a77fe2180cc9ed72c294f03c9ceefdf102ee0a9a", "node_type": "4", "metadata": {"filename": "sentence_window.md", "author": "LlamaIndex"}, "hash": "4de70d07e9d91cfba79a2ae844be4f32b20e51b240de7b69dc466152ddc0c773", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - SentenceWindowNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "434fd670-3021-4064-b036-ee9e21a84294": {"__data__": {"id_": "434fd670-3021-4064-b036-ee9e21a84294", "embedding": null, "metadata": {"filename": "token_text_splitter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10c0ff46005471baf20527ed7a2139009e198dc8", "node_type": "4", "metadata": {"filename": "token_text_splitter.md", "author": "LlamaIndex"}, "hash": "dc7b904572ca052450e3ba472eae8f03ae29901625b54782aa6ed7cebca1cf4b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - TokenTextSplitter", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19858768-2da0-4160-818b-b36d9fb6dc42": {"__data__": {"id_": "19858768-2da0-4160-818b-b36d9fb6dc42", "embedding": null, "metadata": {"filename": "unstructured_element.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44da5da1974ed6d5e2e7ad01a11cc9c8c163bfd6", "node_type": "4", "metadata": {"filename": "unstructured_element.md", "author": "LlamaIndex"}, "hash": "d3f16f635d60516cfd4c33cd849bbe8840ef8fe70e0b9d88f72dfae9d525715f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.node_parser\n    options:\n      members:\n        - UnstructuredElementNodeParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ba191da-578b-4d9f-a573-a0e8f1f73b58": {"__data__": {"id_": "0ba191da-578b-4d9f-a573-a0e8f1f73b58", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e596dcae04b94f1219f10ff01a0d4dda85d65c23", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "56b410491f6405dea54951de3f79efd39555d8098f198857c3f7a2bff68ec752", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.objects", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 28, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3f58b1e-cc4e-404c-bc92-1b3e483ebbee": {"__data__": {"id_": "b3f58b1e-cc4e-404c-bc92-1b3e483ebbee", "embedding": null, "metadata": {"filename": "guardrails.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d7b4d3cb217c7a4382bd05a5fc41e40443b6020", "node_type": "4", "metadata": {"filename": "guardrails.md", "author": "LlamaIndex"}, "hash": "461cc5936ba8050e11be791b777d8d0576ff46e6fb880180d5c168a95a843105", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.output_parsers.guardrails\n    options:\n      members:\n        - GuardrailsOutputParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ce33de7-ef99-40f4-b726-71923d469612": {"__data__": {"id_": "1ce33de7-ef99-40f4-b726-71923d469612", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2239a557df3d96794997289b383d0dff8ef4b6af", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "16375791049899004c917b061ab3d6d522eaadbea62bde2a837d26ddee7c3683", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.types\n    options:\n      members:\n        - BaseOutputParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77dfa1bf-83fc-443a-8803-ff2a142b8310": {"__data__": {"id_": "77dfa1bf-83fc-443a-8803-ff2a142b8310", "embedding": null, "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8392d65c118bab5f1976a39c729fb3627aadc376", "node_type": "4", "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}, "hash": "af0c999a2d202ea858915a46390b2410c38759c5e51f63986f07af6aa06a96a3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.output_parsers.langchain\n    options:\n      members:\n        - LangchainOutputParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42f4303f-6272-434c-a4d6-7599305f31ca": {"__data__": {"id_": "42f4303f-6272-434c-a4d6-7599305f31ca", "embedding": null, "metadata": {"filename": "pydantic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76fbd330da97b2236addaafabad0be6dd1e8cc85", "node_type": "4", "metadata": {"filename": "pydantic.md", "author": "LlamaIndex"}, "hash": "2a8df6fe668d286f0d929ebeaac57bf3632435105282e27a5c4b06792621d56f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.output_parsers\n    options:\n      members:\n        - PydanticOutputParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "207e2ea5-a65e-4766-b41c-5efe333cc87a": {"__data__": {"id_": "207e2ea5-a65e-4766-b41c-5efe333cc87a", "embedding": null, "metadata": {"filename": "selection.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "355e093146539a8d2d9b9f88d141cc9affd56567", "node_type": "4", "metadata": {"filename": "selection.md", "author": "LlamaIndex"}, "hash": "024c78f8abf35646e4982b1a4cfa72927d525dcd4db76768ef047c2ae59293cc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.output_parsers\n    options:\n      members:\n        - SelectionOutputParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c72beb0f-2e03-4628-90ad-1a95f19ecad3": {"__data__": {"id_": "c72beb0f-2e03-4628-90ad-1a95f19ecad3", "embedding": null, "metadata": {"filename": "agent_search_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c156a9e31df96226957ee03924deafa6da448422", "node_type": "4", "metadata": {"filename": "agent_search_retriever.md", "author": "LlamaIndex"}, "hash": "4d7012247a82cd9f7631bbdb989c572dc67cc490d2d017b1ca18da29e53cb8fc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.agent_search_retriever\n    options:\n      members:\n        - AgentSearchRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43ba3f65-d918-4f7f-bac8-26f669e60632": {"__data__": {"id_": "43ba3f65-d918-4f7f-bac8-26f669e60632", "embedding": null, "metadata": {"filename": "agents_coa.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5df6de2e4a9ceff5af92c252c4552e9aa64643e1", "node_type": "4", "metadata": {"filename": "agents_coa.md", "author": "LlamaIndex"}, "hash": "87e2d8d49d00fa297a7de29220d4739a867774b6edd06a39b67182d107b09010", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.agents_coa\n    options:\n      members:\n        - CoAAgentPack\n        - CoAAgentWorker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "842b7a21-4728-481d-bda6-7ea689f6f6ee": {"__data__": {"id_": "842b7a21-4728-481d-bda6-7ea689f6f6ee", "embedding": null, "metadata": {"filename": "agents_lats.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec5ce22e00e61d767e1213f66aa5f5c025ed2f26", "node_type": "4", "metadata": {"filename": "agents_lats.md", "author": "LlamaIndex"}, "hash": "1ff4d43f289528fcbe0a09cee99bbc84fa32dca54a13ece6f58bbc715009a876", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.agents_lats\n    options:\n      members:\n        - LATSPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf39f0c8-4ed8-4f35-b13b-92f63a98cc6a": {"__data__": {"id_": "bf39f0c8-4ed8-4f35-b13b-92f63a98cc6a", "embedding": null, "metadata": {"filename": "agents_llm_compiler.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49c5354ccc2221360119cf6af52e2404006d88c3", "node_type": "4", "metadata": {"filename": "agents_llm_compiler.md", "author": "LlamaIndex"}, "hash": "ea34970474c5c35e446f8743e25bea2fce98c79531598b2da999474e65f53b37", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.agents_llm_compiler\n    options:\n      members:\n        - LLMCompilerAgentPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81028195-73b9-42e9-9859-cc3e9d601190": {"__data__": {"id_": "81028195-73b9-42e9-9859-cc3e9d601190", "embedding": null, "metadata": {"filename": "amazon_product_extraction.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8a632ee6fba14c0c09a70d073a08cae0745e0a8", "node_type": "4", "metadata": {"filename": "amazon_product_extraction.md", "author": "LlamaIndex"}, "hash": "d3dce17ac2524272603f821ce4d1f86bbacc85ab027a1fd55aeda9db8ab9b0d4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.amazon_product_extraction\n    options:\n      members:\n        - AmazonProductExtractionPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1ac3472-45f0-46ad-8045-4ad4b771bb5f": {"__data__": {"id_": "c1ac3472-45f0-46ad-8045-4ad4b771bb5f", "embedding": null, "metadata": {"filename": "arize_phoenix_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3a9daca9d86a97cad8ae3d854de485ec09cf905", "node_type": "4", "metadata": {"filename": "arize_phoenix_query_engine.md", "author": "LlamaIndex"}, "hash": "498e9b2c6218ed11bbe454b3eabbdd4ee7cf898b10f45f71430c2166d96274d9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.arize_phoenix_query_engine\n    options:\n      members:\n        - ArizePhoenixQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d8258df-9b9c-46b8-a9c0-339c046bb167": {"__data__": {"id_": "5d8258df-9b9c-46b8-a9c0-339c046bb167", "embedding": null, "metadata": {"filename": "auto_merging_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f8c92242b57cc153978707c0b98e1a65679b77e", "node_type": "4", "metadata": {"filename": "auto_merging_retriever.md", "author": "LlamaIndex"}, "hash": "0b9db0ee8e63a7d7c4f6a61f0650f0c64cdcd00cc1d4891deeb4e54f74068955", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.auto_merging_retriever\n    options:\n      members:\n        - AutoMergingRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b410e342-d907-4cb5-9718-9360a3d01879": {"__data__": {"id_": "b410e342-d907-4cb5-9718-9360a3d01879", "embedding": null, "metadata": {"filename": "chroma_autoretrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ef43db627cea1e9df3dd819c118aaa09f76f209", "node_type": "4", "metadata": {"filename": "chroma_autoretrieval.md", "author": "LlamaIndex"}, "hash": "c139873f48ac513ca6609528891560a212cabe6b41e886123b3d12c7dc14bb2a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.chroma_autoretrieval\n    options:\n      members:\n        - ChromaAutoretrievalPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65c701ec-ba3d-4b7c-a622-aeb114ef1769": {"__data__": {"id_": "65c701ec-ba3d-4b7c-a622-aeb114ef1769", "embedding": null, "metadata": {"filename": "code_hierarchy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "577b2976c1d90adffec2c7d29d78a30f39aebc1c", "node_type": "4", "metadata": {"filename": "code_hierarchy.md", "author": "LlamaIndex"}, "hash": "f25863d35e604288fd1df958083abe3801e41866f6489da65352b48b2d81f8b5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.code_hierarchy\n    options:\n      members:\n        - CodeHierarchyAgentPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "660cf76f-34e8-4f4c-8333-35c0b41ee94a": {"__data__": {"id_": "660cf76f-34e8-4f4c-8333-35c0b41ee94a", "embedding": null, "metadata": {"filename": "cogniswitch_agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a20b99202e671b94db9daa35250050a58bdf410", "node_type": "4", "metadata": {"filename": "cogniswitch_agent.md", "author": "LlamaIndex"}, "hash": "b78b63f4becb1cdfa0a98d5aeebf6ad2d756503f4018f537692e10305d384736", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.cogniswitch_agent\n    options:\n      members:\n        - CogniswitchAgentPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9ce7903-25fc-470d-8aac-2c1ab52337b6": {"__data__": {"id_": "a9ce7903-25fc-470d-8aac-2c1ab52337b6", "embedding": null, "metadata": {"filename": "cohere_citation_chat.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd1d8d64a8aa84a17dc9c72e9ea12907f4d02ca5", "node_type": "4", "metadata": {"filename": "cohere_citation_chat.md", "author": "LlamaIndex"}, "hash": "373b0112a47730c121985d30d3f184160c0fd248aac26d3405914e4222eb2661", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.cohere_citation_chat\n    options:\n      members:\n        - CohereCitationChatEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00b12f47-16f9-49fb-884d-99778331154d": {"__data__": {"id_": "00b12f47-16f9-49fb-884d-99778331154d", "embedding": null, "metadata": {"filename": "corrective_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6822a18ab128390ac1943fea5c83cfdb1a93fdc2", "node_type": "4", "metadata": {"filename": "corrective_rag.md", "author": "LlamaIndex"}, "hash": "70685cb3fa968c90b6de2fd4c0bb84401d3cb4206ac0a39516f942533850feaa", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.corrective_rag\n    options:\n      members:\n        - CorrectiveRAGPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07a78963-2d62-46b7-b931-66314b7eee61": {"__data__": {"id_": "07a78963-2d62-46b7-b931-66314b7eee61", "embedding": null, "metadata": {"filename": "deeplake_deepmemory_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4eb8fd2dfddd56081a971e4f40951f45954eb6e", "node_type": "4", "metadata": {"filename": "deeplake_deepmemory_retriever.md", "author": "LlamaIndex"}, "hash": "5e46fb1d44181625f5a85b8c96c81ed913891a95b05605d84da8d59b8c753335", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.deeplake_deepmemory_retriever\n    options:\n      members:\n        - DeepMemoryRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fd42729-bc86-4de0-ae90-491a69d9b6e1": {"__data__": {"id_": "4fd42729-bc86-4de0-ae90-491a69d9b6e1", "embedding": null, "metadata": {"filename": "deeplake_multimodal_retrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ebd2bebcf4088e27b7f081df6567d34a835fa938", "node_type": "4", "metadata": {"filename": "deeplake_multimodal_retrieval.md", "author": "LlamaIndex"}, "hash": "fc4aadb72917d0b80a82f7170e4c5a5cf2816ab1e19dda28bb15ba2297c63f83", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.deeplake_multimodal_retrieval\n    options:\n      members:\n        - DeepLakeMultimodalRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4296ad33-6fb2-4c25-9bfc-fe19a2fac19a": {"__data__": {"id_": "4296ad33-6fb2-4c25-9bfc-fe19a2fac19a", "embedding": null, "metadata": {"filename": "dense_x_retrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34787b868dd9bd5d50f5101598e0dd67e99d8395", "node_type": "4", "metadata": {"filename": "dense_x_retrieval.md", "author": "LlamaIndex"}, "hash": "d2978f931dc000313f649f20e17fcd1383f70b9f585583568c003479d3ab1d4c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.dense_x_retrieval\n    options:\n      members:\n        - DenseXRetrievalPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b431b915-8cab-40da-a8cd-78a3328ca66a": {"__data__": {"id_": "b431b915-8cab-40da-a8cd-78a3328ca66a", "embedding": null, "metadata": {"filename": "diff_private_simple_dataset.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcec0101e25a088294492ab195fd2bddc134b497", "node_type": "4", "metadata": {"filename": "diff_private_simple_dataset.md", "author": "LlamaIndex"}, "hash": "64b46f4e0b7a2804d39583020e7433854a2629e44988c5d69e6a81f12bcbf576", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.diff_private_simple_dataset\n    options:\n      members:\n        - DiffPrivateSimpleDatasetPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76fee423-40c9-42fc-9335-8901ce2c7245": {"__data__": {"id_": "76fee423-40c9-42fc-9335-8901ce2c7245", "embedding": null, "metadata": {"filename": "docugami_kg_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6776675e95ff5c94832961ec1c75e684ece65c27", "node_type": "4", "metadata": {"filename": "docugami_kg_rag.md", "author": "LlamaIndex"}, "hash": "dcbb92e9a6a21fb75559a102e6e99075ca3b797d0fa03a01058589c5992e2465", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.docugami_kg_rag\n    options:\n      members:\n        - DocugamiKgRagPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e865aac-551e-4c37-adb3-26be34e616b3": {"__data__": {"id_": "1e865aac-551e-4c37-adb3-26be34e616b3", "embedding": null, "metadata": {"filename": "evaluator_benchmarker.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6d1ff5356cd05a25657927ef40774300bd77d15", "node_type": "4", "metadata": {"filename": "evaluator_benchmarker.md", "author": "LlamaIndex"}, "hash": "58a1af15e6e4c1605130758a6e04741ff53b9d828cb416b88f7f1d80222a61be", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.evaluator_benchmarker\n    options:\n      members:\n        - EvaluatorBenchmarkerPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cfcde5e-0ac5-4c7f-a99f-56a528adc113": {"__data__": {"id_": "3cfcde5e-0ac5-4c7f-a99f-56a528adc113", "embedding": null, "metadata": {"filename": "finchat.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41f52a108e151f281202c316417139ebcc5e2c44", "node_type": "4", "metadata": {"filename": "finchat.md", "author": "LlamaIndex"}, "hash": "714009dd3f2f75d7943c6311c3eeeab1db7fc676eacb7d07268b092ebe9a3c51", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.finchat\n    options:\n      members:\n        - FinanceChatPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9027c120-8b25-45e2-9115-0e75bef8c779": {"__data__": {"id_": "9027c120-8b25-45e2-9115-0e75bef8c779", "embedding": null, "metadata": {"filename": "fusion_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "374269e8fc062345b8f37106fd7a5ffdb0301795", "node_type": "4", "metadata": {"filename": "fusion_retriever.md", "author": "LlamaIndex"}, "hash": "ed4931a6ea0ac4c7b80e8ea024b6ec28589bf3756b4bdda50468152a9342e641", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.fusion_retriever\n    options:\n      members:\n        - HybridFusionRetrieverPack\n        - QueryRewritingRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf508a54-a7dc-479e-9805-6a89ab731400": {"__data__": {"id_": "bf508a54-a7dc-479e-9805-6a89ab731400", "embedding": null, "metadata": {"filename": "fuzzy_citation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f9aa415323798473d6ef254f47ee596484744d7", "node_type": "4", "metadata": {"filename": "fuzzy_citation.md", "author": "LlamaIndex"}, "hash": "51bfcaa46ead445cba54d570ad0341c2b61d434e29a598b08b31838418d3b829", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.fuzzy_citation\n    options:\n      members:\n        - FuzzyCitationEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e81ae16a-c957-41f4-864a-9c9e680858e0": {"__data__": {"id_": "e81ae16a-c957-41f4-864a-9c9e680858e0", "embedding": null, "metadata": {"filename": "gmail_openai_agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8a45d4832262f95f27af6892508e9b71326f556", "node_type": "4", "metadata": {"filename": "gmail_openai_agent.md", "author": "LlamaIndex"}, "hash": "7aa80159cde3f45eeeab912d95c23b50810f8bd1cb89f0dc85158b16e8963509", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.gmail_openai_agent\n    options:\n      members:\n        - GmailOpenAIAgentPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "994d9882-b73a-431b-b61b-8eda6c8ab49c": {"__data__": {"id_": "994d9882-b73a-431b-b61b-8eda6c8ab49c", "embedding": null, "metadata": {"filename": "gradio_agent_chat.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "102c575e7127dd460ae30ac75569a83842c89046", "node_type": "4", "metadata": {"filename": "gradio_agent_chat.md", "author": "LlamaIndex"}, "hash": "c2e6740747f958705a1ac7694ec333676134e57e5f79d18c95eb2623db43ac5b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.gradio_agent_chat\n    options:\n      members:\n        - GradioAgentChatPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52ba0fe4-28a4-4a2d-b1f6-105618944b9f": {"__data__": {"id_": "52ba0fe4-28a4-4a2d-b1f6-105618944b9f", "embedding": null, "metadata": {"filename": "gradio_react_agent_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f4ae18efb5a648f5fdeb9d4346aa28fef15baf3", "node_type": "4", "metadata": {"filename": "gradio_react_agent_chatbot.md", "author": "LlamaIndex"}, "hash": "8cff24abf2832ac9f0a18b9552fe238c0840f51a7736a9d3e11e1cedb90ff62b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.gradio_react_agent_chatbot\n    options:\n      members:\n        - GradioReActAgentPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d8c5adc-f88f-4392-8358-5487e4141bda": {"__data__": {"id_": "3d8c5adc-f88f-4392-8358-5487e4141bda", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a71761f9031375e936cd14314061d98a6115cf7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "af4bda2e824a3875235cb35c89886ac178940468ed51345d98947b1e1827ac38", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.llama_pack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb5dd8f5-75ac-494d-a23d-ac616771095d": {"__data__": {"id_": "bb5dd8f5-75ac-494d-a23d-ac616771095d", "embedding": null, "metadata": {"filename": "infer_retrieve_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ffa1d6b392b65a4a5407254ed369a3dbcd7caa0", "node_type": "4", "metadata": {"filename": "infer_retrieve_rerank.md", "author": "LlamaIndex"}, "hash": "b569d6f3ce715306fba502af8043b4d62ac2b13e8e867923b64a0c9f70b942e1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.infer_retrieve_rerank\n    options:\n      members:\n        - InferRetrieveRerankPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3a14fbe-49ed-429d-9914-8dc48b9e9612": {"__data__": {"id_": "d3a14fbe-49ed-429d-9914-8dc48b9e9612", "embedding": null, "metadata": {"filename": "koda_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2e1759e9623871b0328cab4e8c48bee0d8cfff4", "node_type": "4", "metadata": {"filename": "koda_retriever.md", "author": "LlamaIndex"}, "hash": "e19e8f64683a5a2d828965a04bfc572c4cd5d58faafff6318a993d39e7a53d05", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.koda_retriever\n    options:\n      members:\n        - KodaRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82f82075-4388-41ba-8a12-032da77aa173": {"__data__": {"id_": "82f82075-4388-41ba-8a12-032da77aa173", "embedding": null, "metadata": {"filename": "llama_dataset_metadata.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f2819b90f26c11e3dfacecb6d9efb9e0889ab50", "node_type": "4", "metadata": {"filename": "llama_dataset_metadata.md", "author": "LlamaIndex"}, "hash": "38401085958e7bf216eacf38bade5408a167f35d6a8ab5d976feaa9480b15851", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.llama_dataset_metadata\n    options:\n      members:\n        - LlamaDatasetMetadataPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de871bb0-e8da-4713-bbf5-fe63a11b7e1b": {"__data__": {"id_": "de871bb0-e8da-4713-bbf5-fe63a11b7e1b", "embedding": null, "metadata": {"filename": "llama_guard_moderator.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7e5dc0d40cb46cacc6d7cbb3e9f72f57721c33d", "node_type": "4", "metadata": {"filename": "llama_guard_moderator.md", "author": "LlamaIndex"}, "hash": "75805e9718bcf15cef3e097daae0ea8e1c0c58601e6a01f6301f58b5a7f2ee43", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.llama_guard_moderator\n    options:\n      members:\n        - LlamaGuardModeratorPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d89ca5ae-bf3f-4a40-bd24-3c0cc5ea3808": {"__data__": {"id_": "d89ca5ae-bf3f-4a40-bd24-3c0cc5ea3808", "embedding": null, "metadata": {"filename": "llava_completion.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7abb81699d52c5ae12ee43d557aa3f36c393d07", "node_type": "4", "metadata": {"filename": "llava_completion.md", "author": "LlamaIndex"}, "hash": "c378f2235623607c033fe1a7a84c83b958b72f911fe840888551a31aff0ab6cd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.llava_completion\n    options:\n      members:\n        - LlavaCompletionPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad25ef65-2732-4e55-a908-2f9cd27274f0": {"__data__": {"id_": "ad25ef65-2732-4e55-a908-2f9cd27274f0", "embedding": null, "metadata": {"filename": "mixture_of_agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4d9d2d8cce00a7f749ee4913a6305f6caac60a9", "node_type": "4", "metadata": {"filename": "mixture_of_agents.md", "author": "LlamaIndex"}, "hash": "c37f1113abf5dab057d6f4d29f0a3ce85a009a89aa0279bef88fdbd03e47b093", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.mixture_of_agents\n    options:\n      members:\n        - MixtureOfAgentsPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11a99d8f-6a8e-4b68-bfcb-56888a6b711b": {"__data__": {"id_": "11a99d8f-6a8e-4b68-bfcb-56888a6b711b", "embedding": null, "metadata": {"filename": "multi_document_agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99cc50418ecc429fe30d364dd6e7ce6fbf295116", "node_type": "4", "metadata": {"filename": "multi_document_agents.md", "author": "LlamaIndex"}, "hash": "55bb947ba7cbac9b1bdbb7f8999eef0b3f3882adcea9981448cb52a75bf9b897", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.multi_document_agents\n    options:\n      members:\n        - MultiDocumentAgentsPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13b61079-ff1c-4634-aa4d-3a764ef5185e": {"__data__": {"id_": "13b61079-ff1c-4634-aa4d-3a764ef5185e", "embedding": null, "metadata": {"filename": "multi_tenancy_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd705e54bdb157be4215350464bd92cf5f64c710", "node_type": "4", "metadata": {"filename": "multi_tenancy_rag.md", "author": "LlamaIndex"}, "hash": "e062c68631844280de6fe54890abae71f5fcb6968209ec71d3b1cb53415fee1c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.multi_tenancy_rag\n    options:\n      members:\n        - MultiTenancyRAGPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1dcf25f-c16f-4212-b46e-e78c16f6eefc": {"__data__": {"id_": "b1dcf25f-c16f-4212-b46e-e78c16f6eefc", "embedding": null, "metadata": {"filename": "multidoc_autoretrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ba372d831c3f7316ddd28bc5936d3778e61ac8b", "node_type": "4", "metadata": {"filename": "multidoc_autoretrieval.md", "author": "LlamaIndex"}, "hash": "a3bc63fa43bc7f7904e974848749d92bd0fa8a28716145be8d89425ed91f3a40", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.multidoc_autoretrieval\n    options:\n      members:\n        - MultiDocAutoRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21096fd4-ab2a-4d12-9c48-af7c065abf0c": {"__data__": {"id_": "21096fd4-ab2a-4d12-9c48-af7c065abf0c", "embedding": null, "metadata": {"filename": "nebulagraph_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "483d1723e8905308f49f012eb62d0387f4df2546", "node_type": "4", "metadata": {"filename": "nebulagraph_query_engine.md", "author": "LlamaIndex"}, "hash": "882d6e5fb24feebb543f249e68dfec7ace9d62632b6c6a28bd3f5efd1037b24c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.nebulagraph_query_engine\n    options:\n      members:\n        - NebulaGraphQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ba783bc-59d9-43d9-8164-730b31035e75": {"__data__": {"id_": "3ba783bc-59d9-43d9-8164-730b31035e75", "embedding": null, "metadata": {"filename": "neo4j_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53923b68c11fc48b3fbce0a1e9de069ca90625a7", "node_type": "4", "metadata": {"filename": "neo4j_query_engine.md", "author": "LlamaIndex"}, "hash": "c61ada4f7075c9e79ad68915cf715eff3d94c5ff6d8639f02b0b08022c62fc66", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.neo4j_query_engine\n    options:\n      members:\n        - Neo4jQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55ac3ab4-57d8-478f-b8fc-cf0d8de3bbcc": {"__data__": {"id_": "55ac3ab4-57d8-478f-b8fc-cf0d8de3bbcc", "embedding": null, "metadata": {"filename": "node_parser_semantic_chunking.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a146be0fad1c7a822210eb25eb8b0cd174bd0097", "node_type": "4", "metadata": {"filename": "node_parser_semantic_chunking.md", "author": "LlamaIndex"}, "hash": "5a5dfb3a7fa7a0640cf37c22048516b8a387f8b1405e7a55a21609a399162589", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.node_parser_semantic_chunking\n    options:\n      members:\n        - SemanticChunkingQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7c8cabe-649e-49ce-a5c8-771faf3954f5": {"__data__": {"id_": "e7c8cabe-649e-49ce-a5c8-771faf3954f5", "embedding": null, "metadata": {"filename": "ollama_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "897252ff5f80ec87becbf86401829f3aa03e86b5", "node_type": "4", "metadata": {"filename": "ollama_query_engine.md", "author": "LlamaIndex"}, "hash": "d532fa9bfe6e27c5b8018b56c2ebca7ad36598bd589faf6f40e5b6d171c1976f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.ollama_query_engine\n    options:\n      members:\n        - OllamaQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28f2fa26-91b5-45d0-be41-698c5f9bf8f0": {"__data__": {"id_": "28f2fa26-91b5-45d0-be41-698c5f9bf8f0", "embedding": null, "metadata": {"filename": "panel_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "191d68103a91b70d758c4931b225a89aae60177f", "node_type": "4", "metadata": {"filename": "panel_chatbot.md", "author": "LlamaIndex"}, "hash": "ea6c8033227fbf4dd046eb439d8b3416e4be13356fedb3637eea22a9c2dfbce5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.panel_chatbot\n    options:\n      members:\n        - PanelChatPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09b8a0f4-1d4e-4c75-b239-36afeeb50a48": {"__data__": {"id_": "09b8a0f4-1d4e-4c75-b239-36afeeb50a48", "embedding": null, "metadata": {"filename": "query_understanding_agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ebeffed79f54516898b8a0a2ecdcac4053030f79", "node_type": "4", "metadata": {"filename": "query_understanding_agent.md", "author": "LlamaIndex"}, "hash": "7b9c5bb91f6b3d94f1237c584998a57d178421055149d2a3ff2fd34788962dbb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.query_understanding_agent\n    options:\n      members:\n        - QueryUnderstandingPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78e15a8a-58eb-4b8d-a4b5-eb6da0ff70a7": {"__data__": {"id_": "78e15a8a-58eb-4b8d-a4b5-eb6da0ff70a7", "embedding": null, "metadata": {"filename": "raft_dataset.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ece95c86fffa759a334a12d87c2c6e03bdff1ff", "node_type": "4", "metadata": {"filename": "raft_dataset.md", "author": "LlamaIndex"}, "hash": "62a6cfe4b0dc1fb1af0c8880fdbf130194adb2478984ec3300efcb0ab6dfd869", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.raft_dataset\n    options:\n      members:\n        - RAFTDatasetPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f3246b-8a2d-45fb-8a2d-47731b25b226": {"__data__": {"id_": "d7f3246b-8a2d-45fb-8a2d-47731b25b226", "embedding": null, "metadata": {"filename": "rag_cli_local.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d84407d9f70b6ad3926aadeea5e775cb62337070", "node_type": "4", "metadata": {"filename": "rag_cli_local.md", "author": "LlamaIndex"}, "hash": "dc7fd434768897c27f67b6e8bc0b4eeb4213867d34c985842146da41db43efc3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.rag_cli_local\n    options:\n      members:\n        - LocalRAGCLIPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a02ac731-b8be-47bf-acb6-03efb06da6e2": {"__data__": {"id_": "a02ac731-b8be-47bf-acb6-03efb06da6e2", "embedding": null, "metadata": {"filename": "rag_evaluator.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd4e6420a2433e453af8e20fbfe573ced6a2c7ca", "node_type": "4", "metadata": {"filename": "rag_evaluator.md", "author": "LlamaIndex"}, "hash": "83aa13bcf1249b066cb60b229004be9d01e470d92dd7bc51baf4d6a2aee0bc7c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.rag_evaluator\n    options:\n      members:\n        - RagEvaluatorPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "961f8966-03d6-4add-8d92-42b11f21eb25": {"__data__": {"id_": "961f8966-03d6-4add-8d92-42b11f21eb25", "embedding": null, "metadata": {"filename": "rag_fusion_query_pipeline.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35b966ee8ae82cf730ff1c8212d9b01adf75abb3", "node_type": "4", "metadata": {"filename": "rag_fusion_query_pipeline.md", "author": "LlamaIndex"}, "hash": "0b3ea8b8e6688ed02ded73ce4ce748df8271af0730b535a4dad4a4ba5bbd9acf", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.rag_fusion_query_pipeline\n    options:\n      members:\n        - RAGFusionPipelinePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f96294b-d0ba-4ccd-8fec-8ecd7931b506": {"__data__": {"id_": "1f96294b-d0ba-4ccd-8fec-8ecd7931b506", "embedding": null, "metadata": {"filename": "ragatouille_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6366801e42c7d1f97045b3b65a43e33729708399", "node_type": "4", "metadata": {"filename": "ragatouille_retriever.md", "author": "LlamaIndex"}, "hash": "e537c520293162e4a0f3d64dd330ec839b99e6281b7c46922250a57e452ae448", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.ragatouille_retriever\n    options:\n      members:\n        - RAGatouilleRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72b061f4-af16-4bd8-a0cc-98dc94a4fe64": {"__data__": {"id_": "72b061f4-af16-4bd8-a0cc-98dc94a4fe64", "embedding": null, "metadata": {"filename": "raptor.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cc4569ec83408df687be7cc36a6e6afa66f3539", "node_type": "4", "metadata": {"filename": "raptor.md", "author": "LlamaIndex"}, "hash": "7df7ba4023c3d6cdcce23fd46a28188a65b8a18f950fe688f0f5ea794c483033", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.raptor\n    options:\n      members:\n        - RaptorPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcc6a650-5c23-4291-860c-2e6719c7a830": {"__data__": {"id_": "fcc6a650-5c23-4291-860c-2e6719c7a830", "embedding": null, "metadata": {"filename": "recursive_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43980882c896dd55a8628295e0b9bd5ba046e65a", "node_type": "4", "metadata": {"filename": "recursive_retriever.md", "author": "LlamaIndex"}, "hash": "51139994ab4b2dd111096f5cf4bd6b8b9f7e83792b1eb7ce2d098e743601d23a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.recursive_retriever\n    options:\n      members:\n        - EmbeddedTablesUnstructuredRetrieverPack\n        - RecursiveRetrieverSmallToBigPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97e7dc10-26ab-4ea6-b4ff-598ce541a17f": {"__data__": {"id_": "97e7dc10-26ab-4ea6-b4ff-598ce541a17f", "embedding": null, "metadata": {"filename": "redis_ingestion_pipeline.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1fbef21a5d5e2e3be7c51ea575ef215cd73227c5", "node_type": "4", "metadata": {"filename": "redis_ingestion_pipeline.md", "author": "LlamaIndex"}, "hash": "7df6a6e1482bb20579fbd6333838d48ebfbf7ff11e60a4dead3934e9eac1e942", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.redis_ingestion_pipeline\n    options:\n      members:\n        - RedisIngestionPipelinePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cae5264d-4b27-4982-afe4-97be32c2a1b1": {"__data__": {"id_": "cae5264d-4b27-4982-afe4-97be32c2a1b1", "embedding": null, "metadata": {"filename": "resume_screener.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "caaa991886f6640c21314422751369aee412bbcc", "node_type": "4", "metadata": {"filename": "resume_screener.md", "author": "LlamaIndex"}, "hash": "c034415e9f628feb3bca7ddcf6dc794ae697007a596ee6b4238cc3dc19fcc1b3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.resume_screener\n    options:\n      members:\n        - ResumeScreenerPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33879e2f-860b-4fc1-a611-8b045c079a32": {"__data__": {"id_": "33879e2f-860b-4fc1-a611-8b045c079a32", "embedding": null, "metadata": {"filename": "retry_engine_weaviate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48e1a7f49735b33c854551199a96842656f24bf0", "node_type": "4", "metadata": {"filename": "retry_engine_weaviate.md", "author": "LlamaIndex"}, "hash": "0951b46b1e281eac798f9d11c327cfa9718e490eb8ab3ff0038a3d3b2b968982", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.retry_engine_weaviate\n    options:\n      members:\n        - WeaviateRetryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1f40bbc-120c-4b8c-b171-c4953f948074": {"__data__": {"id_": "a1f40bbc-120c-4b8c-b171-c4953f948074", "embedding": null, "metadata": {"filename": "searchain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0ca0f9cc168dbc4800872c6d40639d52d5ac4dbe", "node_type": "4", "metadata": {"filename": "searchain.md", "author": "LlamaIndex"}, "hash": "bd64de7b8e62187f8623a09ade6615cabd48e1ae0f1e82ba5d8ec70158e4b24f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.searchain\n    options:\n      members:\n        - SearChainPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe788021-75d0-492d-935a-658cb87c5801": {"__data__": {"id_": "fe788021-75d0-492d-935a-658cb87c5801", "embedding": null, "metadata": {"filename": "secgpt.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2da0a60c399c630ca77822b0896e2934efee23a3", "node_type": "4", "metadata": {"filename": "secgpt.md", "author": "LlamaIndex"}, "hash": "746a268146072cece1f43ed4aa723f29fac424feb27801cc2a293afc84776ed3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.secgpt\n    options:\n      members:\n        - SecGPTPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "006b8f92-92e1-4c7d-9e48-9ad9eacd22ad": {"__data__": {"id_": "006b8f92-92e1-4c7d-9e48-9ad9eacd22ad", "embedding": null, "metadata": {"filename": "self_discover.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a5ab91077ba8095195a3eb9e5d397d956e9bc30", "node_type": "4", "metadata": {"filename": "self_discover.md", "author": "LlamaIndex"}, "hash": "7ede1426556133dc2377ba88a4df09d6858481cd075d98096d8c14417e1fd20f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.self_discover\n    options:\n      members:\n        - SelfDiscoverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "451cb770-8928-412d-b5ee-d9d792847c9f": {"__data__": {"id_": "451cb770-8928-412d-b5ee-d9d792847c9f", "embedding": null, "metadata": {"filename": "self_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b5d9ffe10fbca2582c979bfba9ea241ab139e71", "node_type": "4", "metadata": {"filename": "self_rag.md", "author": "LlamaIndex"}, "hash": "b5a78060544ebdc46fcf822e00718098be38ac143558153f2abc9ad778b1b680", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.self_rag\n    options:\n      members:\n        - SelfRAGPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf05d549-fc57-4378-a790-987897f5d55c": {"__data__": {"id_": "bf05d549-fc57-4378-a790-987897f5d55c", "embedding": null, "metadata": {"filename": "sentence_window_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505fe0393c3587376191355d48e5797d255ea0f0", "node_type": "4", "metadata": {"filename": "sentence_window_retriever.md", "author": "LlamaIndex"}, "hash": "2bc5dc923fc64bc0113ee95588fa97628a902465bad72bd49ac163a1dc44cc25", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.sentence_window_retriever\n    options:\n      members:\n        - SentenceWindowRetrieverPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "692437fc-cd0f-4054-a2aa-5ec6be96af0e": {"__data__": {"id_": "692437fc-cd0f-4054-a2aa-5ec6be96af0e", "embedding": null, "metadata": {"filename": "snowflake_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92db1008e836731e78a5ff2b84098429cc73373e", "node_type": "4", "metadata": {"filename": "snowflake_query_engine.md", "author": "LlamaIndex"}, "hash": "47906690209f68c75608a45e3abb582a927f8aa963341eee11de6f2804b30c52", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.snowflake_query_engine\n    options:\n      members:\n        - SnowflakeQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b473115d-ad39-40c4-b9fe-688ac3142da8": {"__data__": {"id_": "b473115d-ad39-40c4-b9fe-688ac3142da8", "embedding": null, "metadata": {"filename": "stock_market_data_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0626c02421ca26ff8dafd9bca93b3c517ce7ebbc", "node_type": "4", "metadata": {"filename": "stock_market_data_query_engine.md", "author": "LlamaIndex"}, "hash": "e42f6aaadc3e1a880ec65045eae579b5b93886b30f21ef91dad6b6cf8abf3cdf", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.stock_market_data_query_engine\n    options:\n      members:\n        - StockMarketDataQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b5ee20d-8f91-467b-a613-4f8145d6c9be": {"__data__": {"id_": "0b5ee20d-8f91-467b-a613-4f8145d6c9be", "embedding": null, "metadata": {"filename": "streamlit_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d0968c1800e3b7e35971711ff8838bd5e5fca8d", "node_type": "4", "metadata": {"filename": "streamlit_chatbot.md", "author": "LlamaIndex"}, "hash": "389b676731006acf8db150bd2296b106234e27d6cab3759684ad50ac3609a054", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.streamlit_chatbot\n    options:\n      members:\n        - StreamlitChatPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a76fb8ba-842a-47ca-893d-b2683093500c": {"__data__": {"id_": "a76fb8ba-842a-47ca-893d-b2683093500c", "embedding": null, "metadata": {"filename": "sub_question_weaviate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "317767d77c9e7e5a8f99823e43382b8f34b8cb54", "node_type": "4", "metadata": {"filename": "sub_question_weaviate.md", "author": "LlamaIndex"}, "hash": "a77024089a1965f5666a8eaee1b0d27a42767b37970d13db09c9e5c93e669721", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.sub_question_weaviate\n    options:\n      members:\n        - WeaviateSubQuestionPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "267db816-8b3e-4192-95ab-bb646179b69e": {"__data__": {"id_": "267db816-8b3e-4192-95ab-bb646179b69e", "embedding": null, "metadata": {"filename": "subdoc_summary.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "393009be341e9c0bbfb59c829898b97dcf94c556", "node_type": "4", "metadata": {"filename": "subdoc_summary.md", "author": "LlamaIndex"}, "hash": "167a42c2bed4207bc335be3ecc308aa41d4c194265b60746d3faec3412c31a8a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.subdoc_summary\n    options:\n      members:\n        - SubDocSummaryPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "765614f0-6342-4e11-98dc-75302d0a23fe": {"__data__": {"id_": "765614f0-6342-4e11-98dc-75302d0a23fe", "embedding": null, "metadata": {"filename": "tables.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe3f7f42272dcbf94f5d1e68a84a4e0e6c373c1c", "node_type": "4", "metadata": {"filename": "tables.md", "author": "LlamaIndex"}, "hash": "09f6c7ffb7f4cacc90fd4a0d56359bc29fafd5845138ad0b3dce6dc713a79c2e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.tables\n    options:\n      members:\n        - ChainOfTablePack\n        - MixSelfConsistencyPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a53a3d6e-bb7c-40de-ae0a-c636aee95fad": {"__data__": {"id_": "a53a3d6e-bb7c-40de-ae0a-c636aee95fad", "embedding": null, "metadata": {"filename": "timescale_vector_autoretrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dfcf1ec300035f6570667eba8b3abc78fdee32cb", "node_type": "4", "metadata": {"filename": "timescale_vector_autoretrieval.md", "author": "LlamaIndex"}, "hash": "ffd12837f692798b74f1ed085b06142cc87fb05389088c8113286af4b9b10782", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.timescale_vector_autoretrieval\n    options:\n      members:\n        - TimescaleVectorAutoretrievalPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c83a0ed9-4ce8-4cd9-9ff0-defb25ada350": {"__data__": {"id_": "c83a0ed9-4ce8-4cd9-9ff0-defb25ada350", "embedding": null, "metadata": {"filename": "trulens_eval_packs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c0fd9cbc734220f7844159e2fee6d6628697570", "node_type": "4", "metadata": {"filename": "trulens_eval_packs.md", "author": "LlamaIndex"}, "hash": "c010d9d2f5aff7da56cfa441696fd512424d793dc4b3218637374bf898fd118d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.trulens_eval_packs\n    options:\n      members:\n        - TruLensHarmlessPack\n        - TruLensHelpfulPack\n        - TruLensRAGTriadPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f907ce16-4df2-4d56-8d73-0588d713b382": {"__data__": {"id_": "f907ce16-4df2-4d56-8d73-0588d713b382", "embedding": null, "metadata": {"filename": "vanna.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "639c67c3ab0789ab4fc4e2e0a98d9a4041924d64", "node_type": "4", "metadata": {"filename": "vanna.md", "author": "LlamaIndex"}, "hash": "7b186fbcab588693d1e1cb9599bbcb698e55e599c13865abab9d1bd11fc88892", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.vanna\n    options:\n      members:\n        - VannaPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bab2c2e3-b398-4405-a1c1-6137d61c9cf8": {"__data__": {"id_": "bab2c2e3-b398-4405-a1c1-6137d61c9cf8", "embedding": null, "metadata": {"filename": "vectara_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d3e44c8e90b949827671f8c6e4a2cbb7ab4b6e2", "node_type": "4", "metadata": {"filename": "vectara_rag.md", "author": "LlamaIndex"}, "hash": "789139b98f4ab34f19db94430a4f1b9ae2f8bf20f1d2c415b4f0796dde93705a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.vectara_rag\n    options:\n      members:\n        - VectaraRagPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2548b2e-d510-41e8-a3b4-a7677d9c60ff": {"__data__": {"id_": "f2548b2e-d510-41e8-a3b4-a7677d9c60ff", "embedding": null, "metadata": {"filename": "voyage_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3f1227c67fab6106568d52eef84f3ae17c3213c", "node_type": "4", "metadata": {"filename": "voyage_query_engine.md", "author": "LlamaIndex"}, "hash": "7c63901b9113a14809a3e778be59c680bc1026187ad2bfffd4994dce94627c87", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.voyage_query_engine\n    options:\n      members:\n        - VoyageQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae34db84-f248-41db-b017-b6303d9bf975": {"__data__": {"id_": "ae34db84-f248-41db-b017-b6303d9bf975", "embedding": null, "metadata": {"filename": "zenguard.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95e5145a57017cc88d33a739d9f0b4f465ecde47", "node_type": "4", "metadata": {"filename": "zenguard.md", "author": "LlamaIndex"}, "hash": "c00e4f159cb261f05bd6e338541cb43e812bf60cfe513af0aff4d3be07356092", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.zenguard\n    options:\n      members:\n        - ZenGuardPack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ab6d12a-7b9b-456d-bf2b-8e042f07d346": {"__data__": {"id_": "5ab6d12a-7b9b-456d-bf2b-8e042f07d346", "embedding": null, "metadata": {"filename": "zephyr_query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0ca6558db565f17fad49e07717d3fbd7e2139f93", "node_type": "4", "metadata": {"filename": "zephyr_query_engine.md", "author": "LlamaIndex"}, "hash": "7cd68efd7573c3ff257583abb04830e29074c46a3e51f1e7d875bb8ac719da55", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.packs.zephyr_query_engine\n    options:\n      members:\n        - ZephyrQueryEnginePack", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a3d301f-d7ad-42a8-ad28-24f7f62f2876": {"__data__": {"id_": "7a3d301f-d7ad-42a8-ad28-24f7f62f2876", "embedding": null, "metadata": {"filename": "NER_PII.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30bc917e3f3f08b7c782248291f4a353cbecc0bf", "node_type": "4", "metadata": {"filename": "NER_PII.md", "author": "LlamaIndex"}, "hash": "9823cd3b906fa0bd92fa3701cffd09f9251d8aa63a5e2622e6c2b0734f6bff5e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - NERPIINodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07fc7114-38cf-448e-9cfd-d6223d959c99": {"__data__": {"id_": "07fc7114-38cf-448e-9cfd-d6223d959c99", "embedding": null, "metadata": {"filename": "PII.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38ff0adb180d9f5f605839f3bec5c4c1b22cecfc", "node_type": "4", "metadata": {"filename": "PII.md", "author": "LlamaIndex"}, "hash": "bbd647e30b6223383b39e511c236f6fc1d1404e80bdbbab7e1a08081e8c40736", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - PIINodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "304d9cb9-aa5a-4116-ae94-361bf6bcac37": {"__data__": {"id_": "304d9cb9-aa5a-4116-ae94-361bf6bcac37", "embedding": null, "metadata": {"filename": "auto_prev_next.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a8c2cac0dccbdc3b584f7b1e29bc30107f1a3710", "node_type": "4", "metadata": {"filename": "auto_prev_next.md", "author": "LlamaIndex"}, "hash": "0bd2053d2b48af921ae5f915ef1138fa62b1fa6225ae8f7a0b8e9cb7683841c3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - AutoPrevNextNodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0230e0ba-8103-4241-b8fb-61bb853b8d83": {"__data__": {"id_": "0230e0ba-8103-4241-b8fb-61bb853b8d83", "embedding": null, "metadata": {"filename": "cohere_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6819b3bd28808e56ec8ee4a46896530247e22509", "node_type": "4", "metadata": {"filename": "cohere_rerank.md", "author": "LlamaIndex"}, "hash": "d60a833f36b8354b81bf0882c474f29f1b5bde8016e97247653c1a6c84b97623", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.cohere_rerank\n    options:\n      members:\n        - CohereRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f009f7b5-d927-4bc3-86cc-c155b5403ea7": {"__data__": {"id_": "f009f7b5-d927-4bc3-86cc-c155b5403ea7", "embedding": null, "metadata": {"filename": "colbert_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90548f00f95c74fce851a8cbdf848bf8a70e367f", "node_type": "4", "metadata": {"filename": "colbert_rerank.md", "author": "LlamaIndex"}, "hash": "71b29dfa96195392394888839e49fcce1a246704e543286137ee90b4bd6fca16", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.colbert_rerank\n    options:\n      members:\n        - ColbertRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79a8f70a-c18c-4db6-a1b8-6de92127c12c": {"__data__": {"id_": "79a8f70a-c18c-4db6-a1b8-6de92127c12c", "embedding": null, "metadata": {"filename": "dashscope_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "396de306fb36a381af379e5db478bafc88b9de75", "node_type": "4", "metadata": {"filename": "dashscope_rerank.md", "author": "LlamaIndex"}, "hash": "2be82ef0b3de017e54b4b4bca3540e5f9dd0da210887ce47c1497a4685d7cc23", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.dashscope_rerank\n    options:\n      members:\n        - DashScopeRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dac17d90-b4a2-46f3-8af8-8c57bda3b7ad": {"__data__": {"id_": "dac17d90-b4a2-46f3-8af8-8c57bda3b7ad", "embedding": null, "metadata": {"filename": "embedding_recency.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41161917f7bf81be1efce7133c5a39261a992b02", "node_type": "4", "metadata": {"filename": "embedding_recency.md", "author": "LlamaIndex"}, "hash": "dc3ae29cfa7e76b24f9a431ea63ddabf1854eed830eefe7e3a1f753221485948", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - EmbeddingRecencyPostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62a10a11-7116-4394-b48f-bfda3780eb62": {"__data__": {"id_": "62a10a11-7116-4394-b48f-bfda3780eb62", "embedding": null, "metadata": {"filename": "fixed_recency.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac0c6c932ceb663de686e2bfe6a085b09787c252", "node_type": "4", "metadata": {"filename": "fixed_recency.md", "author": "LlamaIndex"}, "hash": "b3cbbfb2087bcf91673b629e15e0e984b59737e155079af16194b927a38ab629", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - FixedRecencyPostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e793cbf2-5df7-4b52-ae61-93525c823cf7": {"__data__": {"id_": "e793cbf2-5df7-4b52-ae61-93525c823cf7", "embedding": null, "metadata": {"filename": "flag_embedding_reranker.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "096dff23fc3d7557391a9aa53d656f5571b30194", "node_type": "4", "metadata": {"filename": "flag_embedding_reranker.md", "author": "LlamaIndex"}, "hash": "1dfed45656a99dee3c5f7c7639a910ab4cf86197dece617b9a29430091235ee8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.flag_embedding_reranker\n    options:\n      members:\n        - FlagEmbeddingReranker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "275cae37-8ee4-418b-965f-bab2faa7e611": {"__data__": {"id_": "275cae37-8ee4-418b-965f-bab2faa7e611", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fafdec5d9459a0179b06d42b14398a8a74d4dc6", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c52e70402a84bf91eafce66d83b40068de3e37a6f103631f4f4b44b3a36b5cdd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor.types\n    options:\n      members:\n        - BaseNodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a1f1ec9-6237-4b92-af67-7832588e4268": {"__data__": {"id_": "6a1f1ec9-6237-4b92-af67-7832588e4268", "embedding": null, "metadata": {"filename": "jinaai_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4315961ec405da54ef65d95bb4458e691bf90ea", "node_type": "4", "metadata": {"filename": "jinaai_rerank.md", "author": "LlamaIndex"}, "hash": "0391cc10979b289202beb998d4afb9fb53074a88f2dd15ff057e6b5d11e512f7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.jinaai_rerank\n    options:\n      members:\n        - JinaRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ae1eed6-b91f-4cef-a471-5ec0c775342b": {"__data__": {"id_": "0ae1eed6-b91f-4cef-a471-5ec0c775342b", "embedding": null, "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b1669f1c6eea89badbdf01b0460cbef0c37946b", "node_type": "4", "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "hash": "436b23d04d74d77eae7082105aa347286de1099af1db1d9438613d96dce564ea", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - KeywordNodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e0ce863-780f-47a7-842a-3d5a48a762b6": {"__data__": {"id_": "2e0ce863-780f-47a7-842a-3d5a48a762b6", "embedding": null, "metadata": {"filename": "llm_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c5c4603b86f39be53a727ec34543d6a40e2f54b", "node_type": "4", "metadata": {"filename": "llm_rerank.md", "author": "LlamaIndex"}, "hash": "37a996d3d36c0eef7563898e35364fa2a1b0a7af5f0cf4375e1368b7d23d52c6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - LLMRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 82, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16941e8f-e5ea-4f4b-8e22-62abda57c0a3": {"__data__": {"id_": "16941e8f-e5ea-4f4b-8e22-62abda57c0a3", "embedding": null, "metadata": {"filename": "long_context_reorder.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f28d351e804a5c6becab81f09aa50f5a39ac30d", "node_type": "4", "metadata": {"filename": "long_context_reorder.md", "author": "LlamaIndex"}, "hash": "64a762da6e636dcbbda61dca64a0e139707f1b48b6f06634db61713725250d63", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - LongContextReorder", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d15ef93e-093d-4711-b9de-18e8b060bb39": {"__data__": {"id_": "d15ef93e-093d-4711-b9de-18e8b060bb39", "embedding": null, "metadata": {"filename": "longllmlingua.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0987f232209a6c1b5d0496e877848efd3b84631", "node_type": "4", "metadata": {"filename": "longllmlingua.md", "author": "LlamaIndex"}, "hash": "56fcc173a57e1a1c799354ed2e8d09d68054847153b30bcaf0b6c3e0c42f1933", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.longllmlingua\n    options:\n      members:\n        - LongLLMLinguaPostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1c92599-a62f-47cf-bab1-195e90e37112": {"__data__": {"id_": "a1c92599-a62f-47cf-bab1-195e90e37112", "embedding": null, "metadata": {"filename": "metadata_replacement.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2e7f1f832f93a12fedb36f3583a5072bdcb4166", "node_type": "4", "metadata": {"filename": "metadata_replacement.md", "author": "LlamaIndex"}, "hash": "ddd22e75d9dc3eb48f38f3ec5adc82bb70588bd824db9f44040e574211cad37a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - MetadataReplacementPostProcessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "664a460e-579d-45a8-8c02-f6934e2e088d": {"__data__": {"id_": "664a460e-579d-45a8-8c02-f6934e2e088d", "embedding": null, "metadata": {"filename": "mixedbreadai_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2809da0746fbee9aa15b379e0246f83ff324458", "node_type": "4", "metadata": {"filename": "mixedbreadai_rerank.md", "author": "LlamaIndex"}, "hash": "0bb79d0fd81d67fbfe067198ab9c61e496d1d9afb745d55a7414851b202fc163", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.mixedbreadai_rerank\n    options:\n      members:\n        - MixedbreadAIRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfbe5124-247a-44c0-8b53-994a90217100": {"__data__": {"id_": "bfbe5124-247a-44c0-8b53-994a90217100", "embedding": null, "metadata": {"filename": "nvidia_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3758a15fdd4303efb67249f78ca11675f390e915", "node_type": "4", "metadata": {"filename": "nvidia_rerank.md", "author": "LlamaIndex"}, "hash": "d9ce7eb667eca50a919a7700050df61ac03a18437104bd2b4fc7490c845c9e9e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.nvidia_rerank\n    options:\n      members:\n        - NVIDIA", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7da63f46-124c-4647-a330-3797d9d79508": {"__data__": {"id_": "7da63f46-124c-4647-a330-3797d9d79508", "embedding": null, "metadata": {"filename": "openvino_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67d7504d32178700091ac0a81a081d038fb60146", "node_type": "4", "metadata": {"filename": "openvino_rerank.md", "author": "LlamaIndex"}, "hash": "4d97d4ed7ffdaf5087269b74feaf480c19ee40b68ae03f5c633eb211adb212f2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.openvino_rerank\n    options:\n      members:\n        - OpenVINORerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31931501-8218-42b4-9c80-af75e6aaabfb": {"__data__": {"id_": "31931501-8218-42b4-9c80-af75e6aaabfb", "embedding": null, "metadata": {"filename": "presidio.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d11274cc985bb890af3e3fdf8bf7eb05b85346a", "node_type": "4", "metadata": {"filename": "presidio.md", "author": "LlamaIndex"}, "hash": "7f72e01915cd71f1f3cfb7d6864ef7562add8a3f2b0b116c135d4a7d0c023dd7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.presidio\n    options:\n      members:\n        - PresidioPIINodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6af0580-621e-4b86-bfac-f391f76cff44": {"__data__": {"id_": "f6af0580-621e-4b86-bfac-f391f76cff44", "embedding": null, "metadata": {"filename": "prev_next.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aae1c50883e9114c85ea1dc91f90015e17d49a6", "node_type": "4", "metadata": {"filename": "prev_next.md", "author": "LlamaIndex"}, "hash": "5ecfbb83265f5dc8182e6df525ed6dac973ab7fc2b78ca941210d2d9cec33189", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - PrevNextNodePostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c744f41a-ff4e-4cfa-b470-52e44f542b13": {"__data__": {"id_": "c744f41a-ff4e-4cfa-b470-52e44f542b13", "embedding": null, "metadata": {"filename": "rankgpt_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c02591f3c6c772d32e396f9b377dd5fd9fc29af5", "node_type": "4", "metadata": {"filename": "rankgpt_rerank.md", "author": "LlamaIndex"}, "hash": "b83e5724c06e8beb4693cf1714a4d048d127a3cab1872c4a147e915e13f3a014", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.rankgpt_rerank\n    options:\n      members:\n        - RankGPTRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60387b50-0b6d-4c4f-9dab-5a3c41d90218": {"__data__": {"id_": "60387b50-0b6d-4c4f-9dab-5a3c41d90218", "embedding": null, "metadata": {"filename": "rankllm_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cff89b6afdc50d1beca0abdeaa8efd803c498557", "node_type": "4", "metadata": {"filename": "rankllm_rerank.md", "author": "LlamaIndex"}, "hash": "744a1f217d73795e31815ef0efa50aabf4f771eca08d1d79782363c91ae0f6f4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.rankllm_rerank\n    options:\n      members:\n        - CLASS", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7df224f9-8a9f-411b-82ed-53014aaa8d2a": {"__data__": {"id_": "7df224f9-8a9f-411b-82ed-53014aaa8d2a", "embedding": null, "metadata": {"filename": "sbert_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "233cf8942bf7115ba614f20861b2a976e408335a", "node_type": "4", "metadata": {"filename": "sbert_rerank.md", "author": "LlamaIndex"}, "hash": "da0e3b45427b316350c2db89714992e49c88b9a6e04754603e4ea83920a40850", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.sbert_rerank\n    options:\n      members:\n        - SentenceTransformerRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7915051-023b-461a-9dc8-46023991a991": {"__data__": {"id_": "f7915051-023b-461a-9dc8-46023991a991", "embedding": null, "metadata": {"filename": "sentence_optimizer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f511b9ddf67a6125bbedcfa829825980f73e834c", "node_type": "4", "metadata": {"filename": "sentence_optimizer.md", "author": "LlamaIndex"}, "hash": "0d81a0f21c7f3ff349feabf11cbe1b14e5b79c88fe2f2b31af714f5d548d023b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - SentenceEmbeddingOptimizer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c75ee29e-efb6-4be3-99a3-409f6a7194f4": {"__data__": {"id_": "c75ee29e-efb6-4be3-99a3-409f6a7194f4", "embedding": null, "metadata": {"filename": "similarity.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7afbbb4a6aba7af099b56895b45d02dc26c30a0d", "node_type": "4", "metadata": {"filename": "similarity.md", "author": "LlamaIndex"}, "hash": "cdf1c1c91d6b19af9a0afb79c227b1b1e4a954ed93d7b99e53548e51b6636c17", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - SimilarityPostprocessor", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63bb391c-035b-4810-9249-adb329d4997a": {"__data__": {"id_": "63bb391c-035b-4810-9249-adb329d4997a", "embedding": null, "metadata": {"filename": "time_weighted.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b002c919739200212d11beca3c43a08d99d38cd", "node_type": "4", "metadata": {"filename": "time_weighted.md", "author": "LlamaIndex"}, "hash": "2e24cf13d7f8cf8fd095b7dad6ce86e2c6f8356b39080e2f91b9080c9634dd9d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor\n    options:\n      members:\n        - TimeWeightedPostprocessors", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6f03d5f-490c-48d0-b654-dde3743cb3e5": {"__data__": {"id_": "d6f03d5f-490c-48d0-b654-dde3743cb3e5", "embedding": null, "metadata": {"filename": "voyageai_rerank.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b3470d23cb1925a902db320efc9ce2d1e2b1bf1", "node_type": "4", "metadata": {"filename": "voyageai_rerank.md", "author": "LlamaIndex"}, "hash": "2c5a123f21d542921adb43824afd25e8a12d5701d24764749af7a8aacfce2496", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.postprocessor.voyageai_rerank\n    options:\n      members:\n        - VoyageAIRerank", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0e722ac-f530-428d-8810-010481a2d120": {"__data__": {"id_": "f0e722ac-f530-428d-8810-010481a2d120", "embedding": null, "metadata": {"filename": "evaporate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8eb4d213ff1031a254b1e61037194b87ae8f230", "node_type": "4", "metadata": {"filename": "evaporate.md", "author": "LlamaIndex"}, "hash": "70a6261d299207eba354d75617d6eaa48c5eadd990648d0d138765fe9d395652", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.program.evaporate\n    options:\n      members:\n        - DFEvaporateProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b6ef14-0035-45c1-bc27-66be9771fd08": {"__data__": {"id_": "76b6ef14-0035-45c1-bc27-66be9771fd08", "embedding": null, "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ce5ebc5d202901163ba11456068515bc34513e8", "node_type": "4", "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}, "hash": "0861d609469fb65c0a93d97f0dcef141b91b9a9dea447e4bf95e163303489bfc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.program.guidance\n    options:\n      members:\n        - GuidancePydanticProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d20a048-3bb2-49b6-99db-a815d180c8d5": {"__data__": {"id_": "7d20a048-3bb2-49b6-99db-a815d180c8d5", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1469cb0a57f85b992238492309457ba8516133cb", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "97255e7319b53615452b5557ea51cdd0159db19d2695a9d212f6b790a106f3ed", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.types\n    options:\n      members:\n        - BasePydanticProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "659cb26f-1727-4ce5-a6f7-5be747dd4a20": {"__data__": {"id_": "659cb26f-1727-4ce5-a6f7-5be747dd4a20", "embedding": null, "metadata": {"filename": "llm_text_completion.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3506582a83f3ef3444a8aa9b01518d31efacdf40", "node_type": "4", "metadata": {"filename": "llm_text_completion.md", "author": "LlamaIndex"}, "hash": "d4ff5908faa65e60cdf530f13a39d1029ffb2e6df9b08b779ecc3b6dbe6c2825", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.program.llm_program\n    options:\n      members:\n        - LLMTextCompletionProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "016e0094-7c3f-4edd-8738-435a8b46c7d0": {"__data__": {"id_": "016e0094-7c3f-4edd-8738-435a8b46c7d0", "embedding": null, "metadata": {"filename": "lmformatenforcer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9a7e92c697e3eef575351a643d0b1f0cca4496f", "node_type": "4", "metadata": {"filename": "lmformatenforcer.md", "author": "LlamaIndex"}, "hash": "681f9c00c435559a7a0f392c518019cf0e07287dcc593644783dd30d6e361807", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.program.lmformatenforcer\n    options:\n      members:\n        - LMFormatEnforcerPydanticProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ba9d7d0-8580-46c5-ab9a-3bc794167e58": {"__data__": {"id_": "2ba9d7d0-8580-46c5-ab9a-3bc794167e58", "embedding": null, "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28822ec547c0d60887cd4bb270dc2340bd248dd0", "node_type": "4", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "38d68987227e158d46164de0416d2959e4876618a48884eab53ea45fc58273ed", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.program.multi_modal_llm_program\n    options:\n      members:\n        - MultiModalLLMCompletionProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85c43f39-f3d7-42c1-b867-eab9db40f156": {"__data__": {"id_": "85c43f39-f3d7-42c1-b867-eab9db40f156", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41843a9d39987c7b7d5276f021d4ca3bf8e492a5", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "dc6786110f20d88ac2d796c7d1007dbce3203548ef9a8d758ffa407dce8a8724", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.program.openai\n    options:\n      members:\n        - OpenAIPydanticProgram", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94d94985-50de-4244-9a00-97a7e4b6497a": {"__data__": {"id_": "94d94985-50de-4244-9a00-97a7e4b6497a", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e23da97edf8d96d827ce489d54c7d2620d630b08", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "463493f35b4deb249c97f7bcb5daf4ba85911a3fd8068df09b9a85583c0e1894", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.prompts", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 28, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c33e30c4-09cf-4fc1-9ee8-ab0edff53569": {"__data__": {"id_": "c33e30c4-09cf-4fc1-9ee8-ab0edff53569", "embedding": null, "metadata": {"filename": "FLARE.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d36ceaf20d1642bdd253c3dfb0d3640d5ea45735", "node_type": "4", "metadata": {"filename": "FLARE.md", "author": "LlamaIndex"}, "hash": "c7fada78323823ad0ff475aa2d9357caa46668f3ba89a3d5121b53497661fc5d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - FLAREInstructQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a41a1e09-d9e2-44fe-8614-00099bbeea96": {"__data__": {"id_": "a41a1e09-d9e2-44fe-8614-00099bbeea96", "embedding": null, "metadata": {"filename": "JSONalayze.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ebf3f775909cbd58cb433d3f3a0cd88bfd2068d9", "node_type": "4", "metadata": {"filename": "JSONalayze.md", "author": "LlamaIndex"}, "hash": "7f6b36a60ae12c785cd8265ad0e7f3e131dccbf0e8b86bb50676cc9328c39841", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - JSONalyzeQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9065d79-572d-4382-be99-d1d35cc2201b": {"__data__": {"id_": "b9065d79-572d-4382-be99-d1d35cc2201b", "embedding": null, "metadata": {"filename": "NL_SQL_table.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ece1e7b851ab4126a11ac2803f215101664111dd", "node_type": "4", "metadata": {"filename": "NL_SQL_table.md", "author": "LlamaIndex"}, "hash": "e69c3c995e9f20a61e7d8c3241a703a5293df9427f69b6d1720e026b45618f4a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - NLSQLTableQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "272ae961-e2ac-4469-8e15-1d697dad1cf9": {"__data__": {"id_": "272ae961-e2ac-4469-8e15-1d697dad1cf9", "embedding": null, "metadata": {"filename": "PGVector_SQL.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee9fbeafb433e6bdeaec2ef98e03798f79d9c08a", "node_type": "4", "metadata": {"filename": "PGVector_SQL.md", "author": "LlamaIndex"}, "hash": "6ce38e48a66bf7168a1be0f2c0542e5f0554a89d149608c6bbdd7b749ac7de22", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - PGVectorSQLQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53ee1945-333d-4843-a3eb-a5a8d0b7c4cb": {"__data__": {"id_": "53ee1945-333d-4843-a3eb-a5a8d0b7c4cb", "embedding": null, "metadata": {"filename": "SQL_join.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b99b574d83074753a9736ccf866d23821139d13a", "node_type": "4", "metadata": {"filename": "SQL_join.md", "author": "LlamaIndex"}, "hash": "46ca20fae43468afd448f40b75d5f78b67153e1d503d9d525e3f808b58cf38ea", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - SQLJoinQueryEngine\n        - SQLAutoVectorQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "653382e2-67b2-42f7-8c16-f48c2f68a337": {"__data__": {"id_": "653382e2-67b2-42f7-8c16-f48c2f68a337", "embedding": null, "metadata": {"filename": "SQL_table_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde90532c7b6dbde48e037b3eef12ac2c224451b", "node_type": "4", "metadata": {"filename": "SQL_table_retriever.md", "author": "LlamaIndex"}, "hash": "6dc535bed918dccbe67c6ab4461a0b8ed8b39dbd407df204be478a8815ca4a85", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - SQLTableRetrieverQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddc7cc3e-1486-4b10-9db7-fde345e53d57": {"__data__": {"id_": "ddc7cc3e-1486-4b10-9db7-fde345e53d57", "embedding": null, "metadata": {"filename": "citation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f0371a3f9adf27ed86e5242f8cc934d4de4095d", "node_type": "4", "metadata": {"filename": "citation.md", "author": "LlamaIndex"}, "hash": "1086a08630b339da0fec092c1352ef6c73d4c7506ca138d75b5319e2fcb40220", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - CitationQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af6e710a-ce7c-48b9-bf1f-1dcdbc57d268": {"__data__": {"id_": "af6e710a-ce7c-48b9-bf1f-1dcdbc57d268", "embedding": null, "metadata": {"filename": "cogniswitch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ba253eff24b7f8bca06da7da3e98d8e965296ef", "node_type": "4", "metadata": {"filename": "cogniswitch.md", "author": "LlamaIndex"}, "hash": "d690234decbdd10880facb4b4d7e07aa384ec9fd04ed0fd340133dc3795bbdaa", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - CogniswitchQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8ee322c-9e03-4eb5-9c78-10262d6fc473": {"__data__": {"id_": "e8ee322c-9e03-4eb5-9c78-10262d6fc473", "embedding": null, "metadata": {"filename": "custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4f4ebf888e72a6133ebdf9e370a17ccceeac080", "node_type": "4", "metadata": {"filename": "custom.md", "author": "LlamaIndex"}, "hash": "a896148d84f0bf5d17351c4a30a6a09a5dceb99df83ec2b0eee4c40d8bb25b20", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - CustomQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90d9c522-f255-4700-87de-334dbaa8708c": {"__data__": {"id_": "90d9c522-f255-4700-87de-334dbaa8708c", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f8385d02e134bb2edfa5f250c07e8124fad644b", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "64821ad6568d804fe985fda14699de466fc067947c834b9e7e6915e7761199e1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.base.base_query_engine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 43, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c7eb89a-1ac9-4118-8f24-53bd7ed42d2c": {"__data__": {"id_": "3c7eb89a-1ac9-4118-8f24-53bd7ed42d2c", "embedding": null, "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00bc83d779827dc0ccf7504d08b95007ec09e22f", "node_type": "4", "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}, "hash": "c08be8634571e7ccb5b1232d19f1d5f8a2a0a1b1e45bc35678af8d7b812afe5b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - KnowledgeGraphQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aa1d73c-517a-44c2-98d1-b5dc06752c66": {"__data__": {"id_": "3aa1d73c-517a-44c2-98d1-b5dc06752c66", "embedding": null, "metadata": {"filename": "multi_step.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "623bd8e5d53920a5d151cd4cd85cf2c1a2baf1e8", "node_type": "4", "metadata": {"filename": "multi_step.md", "author": "LlamaIndex"}, "hash": "ce8ecc187a80731e6b12293e0067b7922070bc658255defa7c0633744686c452", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - MultiStepQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84b63c72-e310-4271-98cd-ac36c19db6dd": {"__data__": {"id_": "84b63c72-e310-4271-98cd-ac36c19db6dd", "embedding": null, "metadata": {"filename": "pandas.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34c9740e3973eefd944dac92c32747a265ad3798", "node_type": "4", "metadata": {"filename": "pandas.md", "author": "LlamaIndex"}, "hash": "c4b66b5db76d723a19825ad6a06e2b7c966541b2f3234aa5177efc0fc8760fa2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.experimental.query_engine\n    options:\n       members:\n         - PandasQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e63e9c7-697e-4e35-87f4-dd6d00f5c4b4": {"__data__": {"id_": "9e63e9c7-697e-4e35-87f4-dd6d00f5c4b4", "embedding": null, "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bee7cde3d96c38985640d695144c72f19ebd3fa", "node_type": "4", "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}, "hash": "da209b860489d90d3daf7e4092aa87594bd8565ca402d69add39e3007d08fa57", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - RetrieverQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5d3d634-83fc-4ff7-85ce-a0157c97cf31": {"__data__": {"id_": "d5d3d634-83fc-4ff7-85ce-a0157c97cf31", "embedding": null, "metadata": {"filename": "retriever_router.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3111f2357803513407a8e89d48670f4b7df56907", "node_type": "4", "metadata": {"filename": "retriever_router.md", "author": "LlamaIndex"}, "hash": "50d15b827f08c0136d8dfe9eda34325f77db7601b51dffa70eb5e8384ed98dc7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - RetrieverRouterQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41ae5866-59ac-45c7-bb65-7ec85a82356b": {"__data__": {"id_": "41ae5866-59ac-45c7-bb65-7ec85a82356b", "embedding": null, "metadata": {"filename": "retry.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e44d859d84869444c03dbd2cef1d63cbf9c43d4", "node_type": "4", "metadata": {"filename": "retry.md", "author": "LlamaIndex"}, "hash": "0529b3200f9ee1705e52ce4d536648ffe75b8ad3df54de9bb4e4037faa7b0799", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - RetryGuidelineQueryEngine\n        - RetryQueryEngine\n        - RetrySourceQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c090e9dc-4aaa-4f08-a8b8-612db0c27cd8": {"__data__": {"id_": "c090e9dc-4aaa-4f08-a8b8-612db0c27cd8", "embedding": null, "metadata": {"filename": "router.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "463c9401f53097c4b4b188c031506ea053dc6301", "node_type": "4", "metadata": {"filename": "router.md", "author": "LlamaIndex"}, "hash": "f17b2b3c713a5ca135a4e773dd802eef1c1de612290449a70c427ac0f692fda0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - RouterQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a73b319-361c-4e14-9a27-a7bee4dc84d0": {"__data__": {"id_": "3a73b319-361c-4e14-9a27-a7bee4dc84d0", "embedding": null, "metadata": {"filename": "simple_multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26f152ab67d2707afe697c4679a064ab66556e6", "node_type": "4", "metadata": {"filename": "simple_multi_modal.md", "author": "LlamaIndex"}, "hash": "b581508b8d17f1859bbc75f082c03afacbced942cecf37e68a0b12aa44c7ef3d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - SimpleMultiModalQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6723f58d-f951-4a2b-bb2a-60c513660031": {"__data__": {"id_": "6723f58d-f951-4a2b-bb2a-60c513660031", "embedding": null, "metadata": {"filename": "sub_question.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0809d75107799494fb8c16972d51f970de76ad30", "node_type": "4", "metadata": {"filename": "sub_question.md", "author": "LlamaIndex"}, "hash": "b0674eb3258bdecbe104b96297aae03fa87c635f3d3545aff0b04dbff3b2ed7d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - SubQuestionQueryEngine\n        - SubQuestionAnswerPair", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c439405e-e16e-4d61-ba97-e7feafb57390": {"__data__": {"id_": "c439405e-e16e-4d61-ba97-e7feafb57390", "embedding": null, "metadata": {"filename": "tool_retriever_router.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42199ba9aba0cca4a2a26542cb765283597105ed", "node_type": "4", "metadata": {"filename": "tool_retriever_router.md", "author": "LlamaIndex"}, "hash": "9e831f4f9f941cc021fce07dda515d26b11294ef7a9974a12f5f432a7c5e6459", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - ToolRetrieverRouterQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "002c8a0c-48d0-4fac-a87f-9cb0794f0dfa": {"__data__": {"id_": "002c8a0c-48d0-4fac-a87f-9cb0794f0dfa", "embedding": null, "metadata": {"filename": "transform.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85e8b5587b6f31f9b448727975a1e3ff36833238", "node_type": "4", "metadata": {"filename": "transform.md", "author": "LlamaIndex"}, "hash": "d29e9635c88525ba93b49fc9bc51dfb96e2dbda011f27eb177eaf1b5cd3935ed", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_engine\n    options:\n      members:\n        - TransformQueryEngine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c61c742-2d76-4c8d-9c3d-299de43a5ef9": {"__data__": {"id_": "9c61c742-2d76-4c8d-9c3d-299de43a5ef9", "embedding": null, "metadata": {"filename": "agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40dbe0cc13f191c976775c15adf916913fcaacf8", "node_type": "4", "metadata": {"filename": "agent.md", "author": "LlamaIndex"}, "hash": "f8a67e28f9a82b36dc06e91b59e55ae9125aa79f7b0b8af2df0edc6f43325fde", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_pipeline.components.agent.BaseAgentComponent\n\n::: llama_index.core.query_pipeline.components.agent.AgentFnComponent\n\n::: llama_index.core.query_pipeline.components.agent.CustomAgentComponent\n\n::: llama_index.core.query_pipeline.components.agent.AgentInputComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c47dc2db-ecc6-4ba6-b218-10d8882f7108": {"__data__": {"id_": "c47dc2db-ecc6-4ba6-b218-10d8882f7108", "embedding": null, "metadata": {"filename": "arg_pack.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00bfc9d550dc8385671f78f6db81e62b3870dafc", "node_type": "4", "metadata": {"filename": "arg_pack.md", "author": "LlamaIndex"}, "hash": "422b9ea8b140aef269cdd9a5ed4df96a1be03ce8cfd5e5c2c4aad0575e705256", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_pipeline.components.argpacks.ArgPackComponent\n\n::: llama_index.core.query_pipeline.components.argpacks.KwargPackComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5acf6bc9-9f72-4937-843d-5f676e268df8": {"__data__": {"id_": "5acf6bc9-9f72-4937-843d-5f676e268df8", "embedding": null, "metadata": {"filename": "custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f218445a84916cda4ab84d2501af92ef2624fcd", "node_type": "4", "metadata": {"filename": "custom.md", "author": "LlamaIndex"}, "hash": "1b957af586c05d0fc2becf2ee29770b9fdcebc4af13d8d5055df5a921558e809", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.base.query_pipeline.query.CustomQueryComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 67, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f114bf48-435b-46ba-99f3-0172fa3a043b": {"__data__": {"id_": "f114bf48-435b-46ba-99f3-0172fa3a043b", "embedding": null, "metadata": {"filename": "function.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1a2a171beb24aac21c623d9fa3607ea8b292681", "node_type": "4", "metadata": {"filename": "function.md", "author": "LlamaIndex"}, "hash": "cbd20af0d4ca8c0b552b79f47a44bc3501ede803f3215902593bcc33bf20790b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_pipeline.components.function.FnComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 67, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72e55be4-ff14-46b1-86a2-40c762ac08c5": {"__data__": {"id_": "72e55be4-ff14-46b1-86a2-40c762ac08c5", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c4b9b3d5709cbd66b0d1e93f5a51c1aa281c41d", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d6bb75727da27a12b90a361d74d40e39ba13494ff80798e83cd90f47f27f4dda", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.base.query_pipeline.query", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 46, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e907f270-0da5-4deb-b910-c156368a2211": {"__data__": {"id_": "e907f270-0da5-4deb-b910-c156368a2211", "embedding": null, "metadata": {"filename": "input.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "970cafdcfa65c29c86e8a91628282045b764f1d7", "node_type": "4", "metadata": {"filename": "input.md", "author": "LlamaIndex"}, "hash": "646571b25391b7e43d8147527479c56ad7f85768257e7b08673bd992dbd20ceb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_pipeline.components.input.InputComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 67, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29083378-e353-4980-bd49-b957fd91d837": {"__data__": {"id_": "29083378-e353-4980-bd49-b957fd91d837", "embedding": null, "metadata": {"filename": "llm.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b93b7ef6f84fb060e458c7721cf967c6fe28a2cc", "node_type": "4", "metadata": {"filename": "llm.md", "author": "LlamaIndex"}, "hash": "450f0ab6d9ad191bc97460b3da0dfdd6d73924ed3a2939a0504440933b15c6ac", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.llms.llm.BaseLLMComponent\n\n::: llama_index.core.llms.llm.LLMCompleteComponent\n\n::: llama_index.core.llms.llm.LLMChatComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abb1cebb-532d-4000-b868-8563f6d9435f": {"__data__": {"id_": "abb1cebb-532d-4000-b868-8563f6d9435f", "embedding": null, "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb386af040fd28f68ab307e651d57a91ca9b18a0", "node_type": "4", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "c9327c9ed9dab3c717b72effcd6d6bf7612d6580b6d027f30cc5a10d5baefb58", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.multi_modal_llms.base.BaseMultiModalComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 66, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d07278b6-16e7-4214-a27d-44a2f889f0b2": {"__data__": {"id_": "d07278b6-16e7-4214-a27d-44a2f889f0b2", "embedding": null, "metadata": {"filename": "object.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27d4f2754510f1ad3306d9c8469add6501eceeb6", "node_type": "4", "metadata": {"filename": "object.md", "author": "LlamaIndex"}, "hash": "06eb473f14201c0c518348c29f204e3f6f13be552c3fc6dd7974c5cfd00a5dbc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.objects.base.ObjectRetrieverComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 58, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b94bf0f-0dd7-4086-afdd-aa4a618d0e84": {"__data__": {"id_": "3b94bf0f-0dd7-4086-afdd-aa4a618d0e84", "embedding": null, "metadata": {"filename": "output_parser.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a06423a8e321339a51d190da81eb3b294a449648", "node_type": "4", "metadata": {"filename": "output_parser.md", "author": "LlamaIndex"}, "hash": "ad743a8e6465fa0e8fe33194106b9812ab4dab0dfa450695af80c08e1873690e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.output_parsers.base.OutputParserComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 62, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44930d26-079d-4b07-a272-c132e4094f08": {"__data__": {"id_": "44930d26-079d-4b07-a272-c132e4094f08", "embedding": null, "metadata": {"filename": "postprocessor.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b967bc936db6ab9d3c0f3e6f2869bbb27b0ef57c", "node_type": "4", "metadata": {"filename": "postprocessor.md", "author": "LlamaIndex"}, "hash": "32da760680060998e17ec29f12230a4c85a3914662f19ea0bc50813e9f50e2c7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.postprocessor.types.PostprocessorComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 63, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c48762a-3619-47e1-8226-e7c04ff637b4": {"__data__": {"id_": "7c48762a-3619-47e1-8226-e7c04ff637b4", "embedding": null, "metadata": {"filename": "prompt.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eceae1a86e8aa3288cc1a43800c351434f59b415", "node_type": "4", "metadata": {"filename": "prompt.md", "author": "LlamaIndex"}, "hash": "c85ed60c4050a6a6f7177cabcdeb8691c9d6de59a21c8b5e84fb9dd6f21e9b46", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.prompts.base.PromptComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 49, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21f1f5a4-513c-4bf4-aeff-046d068e00bf": {"__data__": {"id_": "21f1f5a4-513c-4bf4-aeff-046d068e00bf", "embedding": null, "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a99bbe6aca306dee217cee196ac8d2a27d0194c", "node_type": "4", "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}, "hash": "374954252fdd6713d04a4602fc893353b80c95b24b102aa5535cf15e5b1d8aab", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.base.base_query_engine.QueryEngineComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 64, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a11074fa-eb19-437f-8b48-72709f9ff8f2": {"__data__": {"id_": "a11074fa-eb19-437f-8b48-72709f9ff8f2", "embedding": null, "metadata": {"filename": "query_transform.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7aedc2af5dccf5e23d70755a09601a8f3b5d7a9", "node_type": "4", "metadata": {"filename": "query_transform.md", "author": "LlamaIndex"}, "hash": "c9f9b22d99b9c8e1cc6ea96fd1fba1d81cc8e6449ae66f894f728cf7b4efd211", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices.query.query_transform.base.QueryTransformComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61446d34-5524-4d32-8dea-6623a4f8ef5f": {"__data__": {"id_": "61446d34-5524-4d32-8dea-6623a4f8ef5f", "embedding": null, "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c026d42ccdda4de172bd6724ba1ab8a6e9126b5", "node_type": "4", "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}, "hash": "f59835f82e1d8c66ab547d0f53820876fef08e842c7272f77982d74082bbcef7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.base.base_retriever.RetrieverComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 59, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2c4037a-4d11-4b22-9e58-21648b51f6dd": {"__data__": {"id_": "e2c4037a-4d11-4b22-9e58-21648b51f6dd", "embedding": null, "metadata": {"filename": "router.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34339929e65ed6d22a4daf2f208094c329b5e136", "node_type": "4", "metadata": {"filename": "router.md", "author": "LlamaIndex"}, "hash": "23fa15b020a4ace01031495bff23bf34f25c0b13aa398bb8db3bbbb9f9bd31e3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_pipeline.components.router.SelectorComponent\n\n::: llama_index.core.query_pipeline.components.router.RouterComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be9861f1-e458-4a92-8aca-d131b796f45c": {"__data__": {"id_": "be9861f1-e458-4a92-8aca-d131b796f45c", "embedding": null, "metadata": {"filename": "synthesizer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8df8dffc9f42e7841f6780d58b35c5aeb8399227", "node_type": "4", "metadata": {"filename": "synthesizer.md", "author": "LlamaIndex"}, "hash": "032edb2f753897d935b9956bfafe9eda1aaae1ed462512b406b77c3e3b980131", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers.base.SynthesizerComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "369f8b4b-1d27-41be-910f-80713dfe4837": {"__data__": {"id_": "369f8b4b-1d27-41be-910f-80713dfe4837", "embedding": null, "metadata": {"filename": "tool_runner.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "416a1c1201b91f1a634947ecf07be73d4b894bc9", "node_type": "4", "metadata": {"filename": "tool_runner.md", "author": "LlamaIndex"}, "hash": "f87451f68c18a3a74b8c17095af67ae109c67a427e0fb9041a0e51c6d4162e32", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.query_pipeline.components.tool_runner.ToolRunnerComponent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d35002ca-f2fb-48d7-973a-55a83190fc6f": {"__data__": {"id_": "d35002ca-f2fb-48d7-973a-55a83190fc6f", "embedding": null, "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cc32ade63f68f6128d28564aae817b8c8211e4e", "node_type": "4", "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}, "hash": "b31cb54ed63e20639767c9b32dbd137203f008a2f8cdd186dae1af1c28376015", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.question_gen.guidance\n    options:\n      members:\n        - GuidanceQuestionGenerator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bbf90e8-b1a3-41b1-af00-a3a2f713c975": {"__data__": {"id_": "2bbf90e8-b1a3-41b1-af00-a3a2f713c975", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f982a315aced401f13bba71aad1b8ed7d0bd5b9", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d1fc9c1131c523f3062e2b0d534b83602b9ea5d0b7306f048c300233dc70d91d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.question_gen.types\n    options:\n      members:\n        - BaseQuestionGenerator\n        - SubQuestionList\n        - SubQuestion", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a28cec98-aa6d-42ef-91e1-20052fc19e4c": {"__data__": {"id_": "a28cec98-aa6d-42ef-91e1-20052fc19e4c", "embedding": null, "metadata": {"filename": "llm_question_gen.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "933712e1d0b9dbaf6149d80991fb7b5dd3b476a6", "node_type": "4", "metadata": {"filename": "llm_question_gen.md", "author": "LlamaIndex"}, "hash": "110d21bf6152f5cc8f4b747914a151042cc69c4a8c7f3392e2f61cfeb5a72dab", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.question_gen\n    options:\n      members:\n        - LLMQuestionGenerator\n        - SubQuestionOutputParser", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95d37e09-d156-4253-9ccd-a7db0eb3d477": {"__data__": {"id_": "95d37e09-d156-4253-9ccd-a7db0eb3d477", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2545f995aecf6fcf4928aee098f8afe13b54888d", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "3bd27c93a9953daa8616af991d88310c4174482282cfb5a13731f5f376fd6eec", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.question_gen.openai\n    options:\n      members:\n        - OpenAIQuestionGenerator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "174c791b-1dbc-49c4-89a5-0aa66bed6ca5": {"__data__": {"id_": "174c791b-1dbc-49c4-89a5-0aa66bed6ca5", "embedding": null, "metadata": {"filename": "agent_search.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "538c2be6d3571518441d74d0ae2fad71ed7ab499", "node_type": "4", "metadata": {"filename": "agent_search.md", "author": "LlamaIndex"}, "hash": "a6ff6cc2aa21dbeab70a1c2779ec6577583900bd7e6a47fcf05ddcf3016ca45e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.agent_search\n    options:\n      members:\n        - AgentSearchReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5d50b55-3a0a-4de0-ab90-bd9e6f56e2c2": {"__data__": {"id_": "e5d50b55-3a0a-4de0-ab90-bd9e6f56e2c2", "embedding": null, "metadata": {"filename": "airbyte_cdk.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e7a9a3fa93ba4f04c3efc76b6d53de8330b159f", "node_type": "4", "metadata": {"filename": "airbyte_cdk.md", "author": "LlamaIndex"}, "hash": "ca5755665f5bf9b9be3cbc2a4373118448eb01d3b0ea74a08dd215425b068f8b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_cdk\n    options:\n      members:\n        - AirbyteCDKReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "640ad0c3-708b-4ac0-a90c-41a76ad5ad60": {"__data__": {"id_": "640ad0c3-708b-4ac0-a90c-41a76ad5ad60", "embedding": null, "metadata": {"filename": "airbyte_gong.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35babd83be1a7c82971424b2c56c07a686cb265f", "node_type": "4", "metadata": {"filename": "airbyte_gong.md", "author": "LlamaIndex"}, "hash": "62768c4504c1e06b9e619846de90a791c97fa04261665277f03e78cf5eaa9497", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_gong\n    options:\n      members:\n        - AirbyteGongReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab894481-83c6-47a8-bed0-2ccd98a5564e": {"__data__": {"id_": "ab894481-83c6-47a8-bed0-2ccd98a5564e", "embedding": null, "metadata": {"filename": "airbyte_hubspot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f8a67c04dec932bd7f9bf2e0865d7c9cace065a", "node_type": "4", "metadata": {"filename": "airbyte_hubspot.md", "author": "LlamaIndex"}, "hash": "e5a2238dec016054a87c464f2feecfeb1e302e769198d41c68360293cb4b99af", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_hubspot\n    options:\n      members:\n        - AirbyteHubspotReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7ce1ff6-02f0-4e89-9c02-3dd8d1127cfe": {"__data__": {"id_": "f7ce1ff6-02f0-4e89-9c02-3dd8d1127cfe", "embedding": null, "metadata": {"filename": "airbyte_salesforce.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc6ad63a8fdb1acf9ca5633a804c96bb185871cd", "node_type": "4", "metadata": {"filename": "airbyte_salesforce.md", "author": "LlamaIndex"}, "hash": "9bf4454900e5e60241e0dc6f47296191877e6f94823418486c42904e79ea8e50", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_salesforce\n    options:\n      members:\n        - AirbyteSalesforceReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0784f31-ad9f-408e-be2c-8b865d59ec71": {"__data__": {"id_": "c0784f31-ad9f-408e-be2c-8b865d59ec71", "embedding": null, "metadata": {"filename": "airbyte_shopify.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f97f4831dd726aa459b6ad3e4918ad5b235bc74", "node_type": "4", "metadata": {"filename": "airbyte_shopify.md", "author": "LlamaIndex"}, "hash": "5fd274327726aec0babbc788fcd1de3cb8a37d8f0674142e059f907a234c86ee", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_shopify\n    options:\n      members:\n        - AirbyteShopifyReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5894634-07ca-4b94-aa9f-4c1d2656409e": {"__data__": {"id_": "b5894634-07ca-4b94-aa9f-4c1d2656409e", "embedding": null, "metadata": {"filename": "airbyte_stripe.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4219aacd5917410ada7578371c2438fc75044ae7", "node_type": "4", "metadata": {"filename": "airbyte_stripe.md", "author": "LlamaIndex"}, "hash": "1a421a56c4dbe9f5f0d33ee021b19317ba3bec4c9be07c4c03c9d3df37f4ec97", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_stripe\n    options:\n      members:\n        - AirbyteStripeReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6522a31-8585-4fdc-bc26-10f905e99217": {"__data__": {"id_": "d6522a31-8585-4fdc-bc26-10f905e99217", "embedding": null, "metadata": {"filename": "airbyte_typeform.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b0f086bcb57c5d72780c8577ad9cb4cca0aa91b", "node_type": "4", "metadata": {"filename": "airbyte_typeform.md", "author": "LlamaIndex"}, "hash": "091ae60bff5802df15f791baa269602d2d53998f8ab89b0f16f6bcff7033308d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_typeform\n    options:\n      members:\n        - AirbyteTypeformReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a53ba005-6004-4e98-bb39-09e445a041a0": {"__data__": {"id_": "a53ba005-6004-4e98-bb39-09e445a041a0", "embedding": null, "metadata": {"filename": "airbyte_zendesk_support.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73931cf84544f3d0ad1e93e5c2ded7b3fea84d35", "node_type": "4", "metadata": {"filename": "airbyte_zendesk_support.md", "author": "LlamaIndex"}, "hash": "1f1d0a79602c1eb9ecb421a6ae83ae95aa037a07bd982ef8c001bd9862fb7069", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airbyte_zendesk_support\n    options:\n      members:\n        - AirbyteZendeskSupportReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "183e17e5-4445-464b-9ee9-eb86ba7b7849": {"__data__": {"id_": "183e17e5-4445-464b-9ee9-eb86ba7b7849", "embedding": null, "metadata": {"filename": "airtable.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d30b811d03d45da9374a82e644f8bf366128c8be", "node_type": "4", "metadata": {"filename": "airtable.md", "author": "LlamaIndex"}, "hash": "1e2722e3cda2475a3e1e492d43bdbb5b66a1f9baf1226642f507014f3eec4dce", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.airtable\n    options:\n      members:\n        - AirtableReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67876a23-b885-4ad6-bd8d-d7266d9aaf85": {"__data__": {"id_": "67876a23-b885-4ad6-bd8d-d7266d9aaf85", "embedding": null, "metadata": {"filename": "apify.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c87419376264fe68e6822fc06605da7dfd799a2f", "node_type": "4", "metadata": {"filename": "apify.md", "author": "LlamaIndex"}, "hash": "41bd6176768973ed982aa8c74670eddf28a82a5b0a369e577ed98fa0459369f0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.apify\n    options:\n      members:\n        - ApifyActor\n        - ApifyDataset", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "487d1587-42e8-4de9-abc5-b1b63fe3b023": {"__data__": {"id_": "487d1587-42e8-4de9-abc5-b1b63fe3b023", "embedding": null, "metadata": {"filename": "arango_db.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08b0614909cd1811603a919ec2a8e032aa1d6e26", "node_type": "4", "metadata": {"filename": "arango_db.md", "author": "LlamaIndex"}, "hash": "5d2e962502e446f32955d0722203178cbf91fc6928224fdccf7f3478c67ba7a6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.arango_db\n    options:\n      members:\n        - SimpleArangoDBReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32597eac-33c1-40f0-8032-1d98be800345": {"__data__": {"id_": "32597eac-33c1-40f0-8032-1d98be800345", "embedding": null, "metadata": {"filename": "papers.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48d0f51a27058ddbe8230a955175d2d6354077b2", "node_type": "4", "metadata": {"filename": "arxiv.md", "author": "LlamaIndex"}, "hash": "3c498d2231ba3e5b07837f376b02504bd1674fd8876df965be2e51d3eff4d118", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.papers\n    options:\n      members:\n        - ArxivReader\n        - PubmedReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfdc0185-83f1-43b4-a9b3-33781bc68ae1": {"__data__": {"id_": "cfdc0185-83f1-43b4-a9b3-33781bc68ae1", "embedding": null, "metadata": {"filename": "asana.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb9d97ae9b9f049307b05ae99655a52f8bf5d21b", "node_type": "4", "metadata": {"filename": "asana.md", "author": "LlamaIndex"}, "hash": "bc1d425cc5a2931d80ad0792f0171db414997cbb604b741bdc3d876ab1de6884", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.asana\n    options:\n      members:\n        - AsanaReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ff5aae7-1b9c-43bf-bcfb-e5913152153f": {"__data__": {"id_": "0ff5aae7-1b9c-43bf-bcfb-e5913152153f", "embedding": null, "metadata": {"filename": "assemblyai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a795673c5a71f4f4b4d089cf08871330b34cbeb5", "node_type": "4", "metadata": {"filename": "assemblyai.md", "author": "LlamaIndex"}, "hash": "8ebf981562a85002bf2f13de9326e2acb86d7595d571b9ac9b1dedefdc2a0148", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.assemblyai\n    options:\n      members:\n        - AssemblyAIAudioTranscriptReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecdd2a2d-801d-4588-8f56-59979a5ad5ef": {"__data__": {"id_": "ecdd2a2d-801d-4588-8f56-59979a5ad5ef", "embedding": null, "metadata": {"filename": "astra_db.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8228cdd00f4751c0846e191773965c648940be90", "node_type": "4", "metadata": {"filename": "astra_db.md", "author": "LlamaIndex"}, "hash": "231335d04e000b2e138140cabc7881da9dd15052e611fd67eba563f7a4581eae", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.astra_db\n    options:\n      members:\n        - AstraDBReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13594da6-17d7-4bc7-9bfd-076888d5b71a": {"__data__": {"id_": "13594da6-17d7-4bc7-9bfd-076888d5b71a", "embedding": null, "metadata": {"filename": "athena.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e725e989a7664f0830c50a86b13829393b2065e", "node_type": "4", "metadata": {"filename": "athena.md", "author": "LlamaIndex"}, "hash": "f37b0d7f45d405ccb014ca900cb117f8b07ae2c239ef5fab55b73aed1f5d81bb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.athena\n    options:\n      members:\n        - AthenaReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d40925ec-542b-4eac-bac6-5d0306603efc": {"__data__": {"id_": "d40925ec-542b-4eac-bac6-5d0306603efc", "embedding": null, "metadata": {"filename": "awadb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9438e50dbe92161cf658710ab70d72ac14791148", "node_type": "4", "metadata": {"filename": "awadb.md", "author": "LlamaIndex"}, "hash": "e4a9db0ccdf8add172d0f6dc9631ccc0c9c4024ab7a9b4a61c9e2ce0fc0c411d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.awadb\n    options:\n      members:\n        - AwadbReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8af8e66-d204-41ad-bb4e-a0e4076dd3c1": {"__data__": {"id_": "b8af8e66-d204-41ad-bb4e-a0e4076dd3c1", "embedding": null, "metadata": {"filename": "azcognitive_search.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65e229d914eaaeafee90b74aae91502f96f48eb5", "node_type": "4", "metadata": {"filename": "azcognitive_search.md", "author": "LlamaIndex"}, "hash": "07b75dea600f6409bd4f3c594306c66de3024abf6e47acd0afd86966ee78e5d0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.azcognitive_search\n    options:\n      members:\n        - AzCognitiveSearchReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9aad2c6-2128-4b6c-9e6e-f5a15dade429": {"__data__": {"id_": "b9aad2c6-2128-4b6c-9e6e-f5a15dade429", "embedding": null, "metadata": {"filename": "azstorage_blob.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55d139eb9f2b47278624b8779915f2bda2dc6292", "node_type": "4", "metadata": {"filename": "azstorage_blob.md", "author": "LlamaIndex"}, "hash": "fd8b67d78dfe02f35dc85dda70795b78a0d34ce43f268699dc19e5a2fab634e4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.azstorage_blob\n    options:\n      members:\n        - AzStorageBlobReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed4ff34e-306c-4cd7-bdef-efda21b31137": {"__data__": {"id_": "ed4ff34e-306c-4cd7-bdef-efda21b31137", "embedding": null, "metadata": {"filename": "azure_devops.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "968e69eea53249b40a963414dafaf630bf731fb4", "node_type": "4", "metadata": {"filename": "azure_devops.md", "author": "LlamaIndex"}, "hash": "551813c1b1ebdcf8c5165c92c2bd84ec58e6d6a6994d875bac83c76663652c77", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.azure_devops\n    options:\n      members:\n        - AzureDevopsReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7b3f4b3-d591-40bc-bce4-c1ea9c352404": {"__data__": {"id_": "d7b3f4b3-d591-40bc-bce4-c1ea9c352404", "embedding": null, "metadata": {"filename": "bagel.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3f581c0e175c5d94f3d3190860e2dc718d8b4e7", "node_type": "4", "metadata": {"filename": "bagel.md", "author": "LlamaIndex"}, "hash": "c5e58afe32b0c233b8813a3419a40353ad595c37dc02c69a9d652e2fe47fcea1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.bagel\n    options:\n      members:\n        - BagelReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be2481e3-940e-41f0-a3d1-f509a8d6874b": {"__data__": {"id_": "be2481e3-940e-41f0-a3d1-f509a8d6874b", "embedding": null, "metadata": {"filename": "bilibili.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40ec2cdb9e53bc33ccf7d0b5713ce71c5f0a1ab2", "node_type": "4", "metadata": {"filename": "bilibili.md", "author": "LlamaIndex"}, "hash": "9bca908620c3726fea270950f0545e56baad347124e6ad987ce552040a31f9ea", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.bilibili\n    options:\n      members:\n        - BilibiliTranscriptReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bef5a0fb-8b49-48d3-819e-a3974fd701e7": {"__data__": {"id_": "bef5a0fb-8b49-48d3-819e-a3974fd701e7", "embedding": null, "metadata": {"filename": "bitbucket.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ef2eba5f69e07e083e3bede7802ddd7dd82c3e7", "node_type": "4", "metadata": {"filename": "bitbucket.md", "author": "LlamaIndex"}, "hash": "c780254af860921050d90b4c1c3a411395722c998573c51ea9ac5da81a55188f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.bitbucket\n    options:\n      members:\n        - BitbucketReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b15527d-2f7f-4588-a98f-54f3d83fdbd1": {"__data__": {"id_": "9b15527d-2f7f-4588-a98f-54f3d83fdbd1", "embedding": null, "metadata": {"filename": "boarddocs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c961591ca4316d537c54ae366b105188209f93a", "node_type": "4", "metadata": {"filename": "boarddocs.md", "author": "LlamaIndex"}, "hash": "e4a67da703b0ad7a7c75069b6fd186a0b1e4d8072d6ff5cee312f1f6754a5efc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.boarddocs\n    options:\n      members:\n        - BoardDocsReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b46c8ca4-db84-4131-aa7e-262eb17a1072": {"__data__": {"id_": "b46c8ca4-db84-4131-aa7e-262eb17a1072", "embedding": null, "metadata": {"filename": "box.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5de35fdc046688c1ecc4dc6844bdbfff2f7dd0ed", "node_type": "4", "metadata": {"filename": "box.md", "author": "LlamaIndex"}, "hash": "df54e3b49a1e77e2ff0b9e9f87e0b7a1e01cffc17466ac0aa116f40f77ac914a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.box\n    options:\n      members:\n        - BoxReader\n        - BoxReaderAIExtract\n        - BoxReaderAIPrompt\n        - BoxReaderTextExtraction", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86f9a39e-e126-4ae6-a7ca-f5f637b48ad1": {"__data__": {"id_": "86f9a39e-e126-4ae6-a7ca-f5f637b48ad1", "embedding": null, "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bfde8aaf3caadc721be5257d4353637b6a925e5", "node_type": "4", "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}, "hash": "557aeaccad9b844cd6bc72a9155b2f9437275893ad83aee7b9ca0469762c8eb2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.chatgpt_plugin\n    options:\n      members:\n        - ChatGPTRetrievalPluginReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "884d5999-3bd9-41bf-8ee6-62a669ba60d7": {"__data__": {"id_": "884d5999-3bd9-41bf-8ee6-62a669ba60d7", "embedding": null, "metadata": {"filename": "chroma.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95f9ac48e5ab26a68d0419e7e512eb9e56f116bd", "node_type": "4", "metadata": {"filename": "chroma.md", "author": "LlamaIndex"}, "hash": "1f6591b7744c6c96d2beaae06cedabb97a81f25beb86852202a5182f714d2bdb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.chroma\n    options:\n      members:\n        - ChromaReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61b9f278-0538-41da-8720-44e039e3c008": {"__data__": {"id_": "61b9f278-0538-41da-8720-44e039e3c008", "embedding": null, "metadata": {"filename": "clickhouse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3b2ab411b4b812a9e117b7408bf55e2e5498abd", "node_type": "4", "metadata": {"filename": "clickhouse.md", "author": "LlamaIndex"}, "hash": "e5468a78b81ade68193717a70574f5c3ab8aca7bb2081cdc14ef175c9a355880", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.clickhouse\n    options:\n      members:\n        - ClickHouseReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99424056-e37c-4744-8c5c-c31c994d1234": {"__data__": {"id_": "99424056-e37c-4744-8c5c-c31c994d1234", "embedding": null, "metadata": {"filename": "confluence.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28cca30fc708b5aa6dd3ef2352afbd6a66f4ab94", "node_type": "4", "metadata": {"filename": "confluence.md", "author": "LlamaIndex"}, "hash": "ed6d8f64c437c525f2627ba62d5adff45ab821a09fe970c68de4990e4ee042ae", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.confluence\n    options:\n      members:\n        - ConfluenceReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a543bf0-d4d5-41d0-a8e9-2932339f948c": {"__data__": {"id_": "0a543bf0-d4d5-41d0-a8e9-2932339f948c", "embedding": null, "metadata": {"filename": "couchbase.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0edf0980ee6fbadb26c2da2d97d01712898ffe3b", "node_type": "4", "metadata": {"filename": "couchbase.md", "author": "LlamaIndex"}, "hash": "b997a8cad5b35df4ffcf9782ea3a14c6e3f9a140c10319c58166af305d998715", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.couchbase\n    options:\n      members:\n        - CouchbaseReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22d163a0-74a7-4438-b7be-9cf478136169": {"__data__": {"id_": "22d163a0-74a7-4438-b7be-9cf478136169", "embedding": null, "metadata": {"filename": "couchdb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "708c39b8c2d11d81d2feaab86b27262bc9451cc7", "node_type": "4", "metadata": {"filename": "couchdb.md", "author": "LlamaIndex"}, "hash": "23e0f3ace63df06018f91de19fabe7bdb553fefe47025b18e40a7bb65012bd65", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.couchdb\n    options:\n      members:\n        - SimpleCouchDBReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aaad9ed-b8fe-422d-8042-914ebd9057c6": {"__data__": {"id_": "6aaad9ed-b8fe-422d-8042-914ebd9057c6", "embedding": null, "metadata": {"filename": "dad_jokes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9deb167eca77eed0f121233a821264fc4b5dc9ef", "node_type": "4", "metadata": {"filename": "dad_jokes.md", "author": "LlamaIndex"}, "hash": "54f4511cab3ede8fd41283681da55ba4f39c30e2fa9a6b7267edb30defb79bb6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.dad_jokes\n    options:\n      members:\n        - DadJokesReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6235c7f8-ef2d-44c4-8ace-691cccd24c17": {"__data__": {"id_": "6235c7f8-ef2d-44c4-8ace-691cccd24c17", "embedding": null, "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "111d9451854da0e93fa67400322cd3b49660e216", "node_type": "4", "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}, "hash": "2b9192d94f3f0771fd65cfc84531f38810ed739c41d3adc19c33baab78323444", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.dashscope\n    options:\n      members:\n        - DashScopeParse", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8186450-cbbe-4aa7-80ea-301eda1f331d": {"__data__": {"id_": "e8186450-cbbe-4aa7-80ea-301eda1f331d", "embedding": null, "metadata": {"filename": "dashvector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5703a1f04551ed71420c36a24482d9056274a71b", "node_type": "4", "metadata": {"filename": "dashvector.md", "author": "LlamaIndex"}, "hash": "8f151c997f7c6e1a037e1fc7318ee40ceadf21ad032344e09396f1bc6016ee2b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.dashvector\n    options:\n      members:\n        - DashVectorReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52371d23-bf75-41c4-b8bc-323624cfe161": {"__data__": {"id_": "52371d23-bf75-41c4-b8bc-323624cfe161", "embedding": null, "metadata": {"filename": "database.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e65818ed0ceaf046428028d829514d5c444ea6e4", "node_type": "4", "metadata": {"filename": "database.md", "author": "LlamaIndex"}, "hash": "63de13a6ca30a20e34dcff8e07c56d6b68f2997a7758c1d5aad22e00d08d86ce", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.database\n    options:\n      members:\n        - DatabaseReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11690fe9-013a-48b6-99f5-808b88573a71": {"__data__": {"id_": "11690fe9-013a-48b6-99f5-808b88573a71", "embedding": null, "metadata": {"filename": "deeplake.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20d04acf6ee1912b7349417483d003f68c9669d6", "node_type": "4", "metadata": {"filename": "deeplake.md", "author": "LlamaIndex"}, "hash": "052fc0219a605216806872e8739e9a6e4f2e01e6e507cfcdbb107377df6feca0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.deeplake\n    options:\n      members:\n        - DeepLakeReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df76d07d-ebe8-45e1-8a26-023ca1f300ab": {"__data__": {"id_": "df76d07d-ebe8-45e1-8a26-023ca1f300ab", "embedding": null, "metadata": {"filename": "discord.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af7601407b5960fe3c30a3f924fed10d5f9b0026", "node_type": "4", "metadata": {"filename": "discord.md", "author": "LlamaIndex"}, "hash": "0faa043767545fda1132bcd86e1e965d970be84c20b51530295d0e5d95a9f638", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.discord\n    options:\n      members:\n        - DiscordReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bda83c5-87e2-4603-a775-6abe3b931f7b": {"__data__": {"id_": "7bda83c5-87e2-4603-a775-6abe3b931f7b", "embedding": null, "metadata": {"filename": "docstring_walker.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fda42eaa673ff97b26a1e7a0b8f165665a24875", "node_type": "4", "metadata": {"filename": "docstring_walker.md", "author": "LlamaIndex"}, "hash": "bbb032ef1a24fc1ce841e9a3141e4458a416e86db5bd056c2299a3cc270b6eca", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.docstring_walker\n    options:\n      members:\n        - DocstringWalker", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebc91640-fb5c-4f3d-b624-8ac75c0f8a2d": {"__data__": {"id_": "ebc91640-fb5c-4f3d-b624-8ac75c0f8a2d", "embedding": null, "metadata": {"filename": "docugami.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56678f5674fcdb8e57d41ea4cde22589cca58404", "node_type": "4", "metadata": {"filename": "docugami.md", "author": "LlamaIndex"}, "hash": "9cbe47d1841c44019967ebdd1a4f1ad9adc55f1d1be43720c0435f22c06d7207", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.docugami\n    options:\n      members:\n        - DocugamiReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7cbea75-8cae-4445-a055-d6f763b86a22": {"__data__": {"id_": "f7cbea75-8cae-4445-a055-d6f763b86a22", "embedding": null, "metadata": {"filename": "earnings_call_transcript.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a35b120c851acd7e9ddf9a21638a9d9420db70b5", "node_type": "4", "metadata": {"filename": "earnings_call_transcript.md", "author": "LlamaIndex"}, "hash": "243dc5100e7ccb3bdf38c1d717dd963fa9bb0eaed131fb309b7cddcb285f866e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.earnings_call_transcript\n    options:\n      members:\n        - EarningsCallTranscript", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ee98647-bb51-4cae-ae27-efb6a9e9831c": {"__data__": {"id_": "3ee98647-bb51-4cae-ae27-efb6a9e9831c", "embedding": null, "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b23649d2c29d5758bec9ec05d75035849e5d857", "node_type": "4", "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "hash": "babc35de211cd70da2178632c7b757324702997d9eae340803136d7a398524a6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.elasticsearch\n    options:\n      members:\n        - ElasticsearchReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10b1f377-3be6-45aa-a356-356ea64f4460": {"__data__": {"id_": "10b1f377-3be6-45aa-a356-356ea64f4460", "embedding": null, "metadata": {"filename": "faiss.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1cf93672363b8d2ca62c2c44249419dc4cd41b", "node_type": "4", "metadata": {"filename": "faiss.md", "author": "LlamaIndex"}, "hash": "19047e546067f1406ccc4b2654c807ebbfb57a2d8b4ec95cc81ad8b8573c51a9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.faiss\n    options:\n      members:\n        - FaissReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b0b843d-557c-4c9b-9b10-cca4f7960128": {"__data__": {"id_": "0b0b843d-557c-4c9b-9b10-cca4f7960128", "embedding": null, "metadata": {"filename": "feedly_rss.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "073c0d1e0d48079d6a431f59105f9fe0692c129f", "node_type": "4", "metadata": {"filename": "feedly_rss.md", "author": "LlamaIndex"}, "hash": "224828a38988e6604baafd6af9a3c1ceceb6cb9cb0e8fdb87f147d13ffbe4f00", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.feedly_rss\n    options:\n      members:\n        - FeedlyRssReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8916022c-2a20-4b7d-b036-ad836c93ebe6": {"__data__": {"id_": "8916022c-2a20-4b7d-b036-ad836c93ebe6", "embedding": null, "metadata": {"filename": "feishu_docs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1f3b9613f634c5a7ca44acba9580820e4bf6384", "node_type": "4", "metadata": {"filename": "feishu_docs.md", "author": "LlamaIndex"}, "hash": "4b360474dc71a09a10e47677dab91882a3b5b829b5a4009a212fa94db88b7e3f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.feishu_docs\n    options:\n      members:\n        - FeishuDocsReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f7b80ac-8520-4bd9-89e7-e3fb55036895": {"__data__": {"id_": "1f7b80ac-8520-4bd9-89e7-e3fb55036895", "embedding": null, "metadata": {"filename": "feishu_wiki.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe649b280ad28ebaa911b5f19160c46aab707892", "node_type": "4", "metadata": {"filename": "feishu_wiki.md", "author": "LlamaIndex"}, "hash": "0086247914e6438ad45ceb54df40914a7dc4cb824ab496879a838a42f70481d8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.feishu_wiki\n    options:\n      members:\n        - FeishuWikiReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b63572d9-8359-43b2-be38-6ab88b7191da": {"__data__": {"id_": "b63572d9-8359-43b2-be38-6ab88b7191da", "embedding": null, "metadata": {"filename": "file.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "585f57f1747011659453ae04fc61bbeaba9a9e86", "node_type": "4", "metadata": {"filename": "file.md", "author": "LlamaIndex"}, "hash": "f72a9c3e139e21a532ce750b3ec8db4b23c164b4942dc1dabf009f7fc5a4c5d3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.file\n    options:\n      members:\n        - CSVReader\n        - DocxReader\n        - EpubReader\n        - FlatReader\n        - HTMLTagReader\n        - HWPReader\n        - IPYNBReader\n        - ImageCaptionReader\n        - ImageReader\n        - ImageTabularChartReader\n        - ImageVisionLLMReader\n        - MarkdownReader\n        - MboxReader\n        - PDFReader\n        - PagedCSVReader\n        - PandasCSVReader\n        - PandasExcelReader\n        - PptxReader\n        - PyMuPDFReader\n        - RTFReader\n        - UnstructuredReader\n        - VideoAudioReader\n        - XMLReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1667c08e-07f6-453d-80e1-d5eea464cb36": {"__data__": {"id_": "1667c08e-07f6-453d-80e1-d5eea464cb36", "embedding": null, "metadata": {"filename": "firebase_realtimedb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93a0fd585f76e379efa0f711177fec7ba0441a88", "node_type": "4", "metadata": {"filename": "firebase_realtimedb.md", "author": "LlamaIndex"}, "hash": "0ad9abfab50f32f8be3de929ee871904776daafd7b91e831d4720bdd2f24ed0e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.firebase_realtimedb\n    options:\n      members:\n        - FirebaseRealtimeDatabaseReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "863c1935-d225-4bc5-a98b-50fe1e4f720c": {"__data__": {"id_": "863c1935-d225-4bc5-a98b-50fe1e4f720c", "embedding": null, "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af0889ad3ceef79b60a37d986f7ff5cd9812ac9a", "node_type": "4", "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "hash": "dc01199e1494346dbe8a9bb49b65066fa60d61194908c9d1fa451aa663802d4b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.firestore\n    options:\n      members:\n        - FirestoreReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52da75af-87f0-4706-8b92-afd4dfcb4982": {"__data__": {"id_": "52da75af-87f0-4706-8b92-afd4dfcb4982", "embedding": null, "metadata": {"filename": "gcs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3b28830cb8554237936a9c82b51a2450739512a", "node_type": "4", "metadata": {"filename": "gcs.md", "author": "LlamaIndex"}, "hash": "49bd0d8ca2d76b262eafea12c5275f04899309b93dee5fc526c91b2ec460ce80", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.gcs\n    options:\n      members:\n        - GCSReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "933b7fbe-5397-4e88-a7a7-57be1d0ff893": {"__data__": {"id_": "933b7fbe-5397-4e88-a7a7-57be1d0ff893", "embedding": null, "metadata": {"filename": "genius.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b71e34d84d9579104d0cd97051ee7e2f76ba99e5", "node_type": "4", "metadata": {"filename": "genius.md", "author": "LlamaIndex"}, "hash": "68369fa6fd8a92e0d8ae9943245bbca6f7d4e8c452bff2c00d9a399f7095bfa6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.genius\n    options:\n      members:\n        - GeniusReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40ea3c87-7fec-4e8c-9465-5ef58c5c76ba": {"__data__": {"id_": "40ea3c87-7fec-4e8c-9465-5ef58c5c76ba", "embedding": null, "metadata": {"filename": "github.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2942924bc485616364cf0689e9bef63f49a7fa02", "node_type": "4", "metadata": {"filename": "github.md", "author": "LlamaIndex"}, "hash": "a86d0cb2e705bba02427b04b27ab68bc3725f5731e6b9b5f5894e4703dbf4a5c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.github\n    options:\n      members:\n        - GitHubRepositoryCollaboratorsReader\n        - GitHubRepositoryIssuesReader\n        - GithubRepositoryReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fbd5341-ce71-4292-a4af-84451e7941e8": {"__data__": {"id_": "2fbd5341-ce71-4292-a4af-84451e7941e8", "embedding": null, "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6494818b45eba0391bd0945864bf46d4bfb6463b", "node_type": "4", "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "hash": "eb0db464f1d038abc1af241b73e2f6677cdbee18eaaf836d6961a08febbdb027", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.google\n    options:\n      members:\n        - GmailReader\n        - GoogleCalendarReader\n        - GoogleChatReader\n        - GoogleDocsReader\n        - GoogleDriveReader\n        - GoogleKeepReader\n        - GoogleMapsTextSearchReader\n        - GoogleSheetsReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4815a023-2ee0-4ab4-b80a-a354c6e34af9": {"__data__": {"id_": "4815a023-2ee0-4ab4-b80a-a354c6e34af9", "embedding": null, "metadata": {"filename": "gpt_repo.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf705843796aa56369cc4e3113d93ad33b8d3baf", "node_type": "4", "metadata": {"filename": "gpt_repo.md", "author": "LlamaIndex"}, "hash": "b399c752cfe79d946bc2ec85d8fd5e0a1cd1b1c095ec3d4bb6c481370e4819f2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.gpt_repo\n    options:\n      members:\n        - GPTRepoReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51ea4b04-2c80-442f-ac5d-424dab6b0f2e": {"__data__": {"id_": "51ea4b04-2c80-442f-ac5d-424dab6b0f2e", "embedding": null, "metadata": {"filename": "graphdb_cypher.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6864421bb639bb70c83e141fe8861f57da90663e", "node_type": "4", "metadata": {"filename": "graphdb_cypher.md", "author": "LlamaIndex"}, "hash": "1eeb663b9845ed29311e88c9a455eaf1aeeaea9b42dcf7e8d713e4128f88751a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.graphdb_cypher\n    options:\n      members:\n        - GraphDBCypherReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "680d0299-32bf-4976-ab70-3287c207a9e6": {"__data__": {"id_": "680d0299-32bf-4976-ab70-3287c207a9e6", "embedding": null, "metadata": {"filename": "graphql.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd5fc89e05a6085337a5db2aafc7418e763e0e2a", "node_type": "4", "metadata": {"filename": "graphql.md", "author": "LlamaIndex"}, "hash": "b9d8dac511a30ccfaa625384f623aa1786ac3dc80524de4705aacd5c9d20407f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.graphql\n    options:\n      members:\n        - GraphQLReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0223e76-6475-494c-8f3e-620aeb17ff14": {"__data__": {"id_": "c0223e76-6475-494c-8f3e-620aeb17ff14", "embedding": null, "metadata": {"filename": "guru.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99591f15e438aec1cdb1969b8d299caf6af56f6d", "node_type": "4", "metadata": {"filename": "guru.md", "author": "LlamaIndex"}, "hash": "7faa3410ecf2deb19036e259d6b12f3fe9f0fc8467729e705697855902417625", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.guru\n    options:\n      members:\n        - GuruReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76991441-812d-4103-8e53-032f96bd247d": {"__data__": {"id_": "76991441-812d-4103-8e53-032f96bd247d", "embedding": null, "metadata": {"filename": "hatena_blog.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "470674882f0425673e271285fc98f788f314cb82", "node_type": "4", "metadata": {"filename": "hatena_blog.md", "author": "LlamaIndex"}, "hash": "af0b71e2ac00ffd9e4fb36cd7bf23199b9a7334c03381d46799d4e8be5af1a47", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.hatena_blog\n    options:\n      members:\n        - HatenaBlogReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fb223d6-9fd1-4d11-8815-9037c6853926": {"__data__": {"id_": "4fb223d6-9fd1-4d11-8815-9037c6853926", "embedding": null, "metadata": {"filename": "hive.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17dc6f68884f3c5fd83d2bf86d0886b008a2b120", "node_type": "4", "metadata": {"filename": "hive.md", "author": "LlamaIndex"}, "hash": "6b7319b4aee91abbe0a356984a17e2d6b5f2433a4e65abfdb72adaa640bfb751", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.hive\n    options:\n      members:\n        - HiveReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "395917b7-c3e6-47b7-8aef-48af468f417e": {"__data__": {"id_": "395917b7-c3e6-47b7-8aef-48af468f417e", "embedding": null, "metadata": {"filename": "hubspot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3c793394089374271bf4d22d350d63345c0c709", "node_type": "4", "metadata": {"filename": "hubspot.md", "author": "LlamaIndex"}, "hash": "7b886e5e6216b2cd4b4b89ddb239321c0f9f7e2569a6e6afb69107cbcd6bc726", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.hubspot\n    options:\n      members:\n        - HubspotReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "247c10ff-3366-457e-b13a-d2aeba4b74c2": {"__data__": {"id_": "247c10ff-3366-457e-b13a-d2aeba4b74c2", "embedding": null, "metadata": {"filename": "huggingface_fs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85deed719c2bf7221170c5ca440ee7745ae62e0f", "node_type": "4", "metadata": {"filename": "huggingface_fs.md", "author": "LlamaIndex"}, "hash": "8a405f0b6f8250f70b189c1301a201e3ac4d3a60d0a6941f39e694120f4485c0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.huggingface_fs\n    options:\n      members:\n        - HuggingFaceFSReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13d310f7-c6a1-4920-8357-31232f639912": {"__data__": {"id_": "13d310f7-c6a1-4920-8357-31232f639912", "embedding": null, "metadata": {"filename": "hwp.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1466a864334e90a545eb87c8b9b89b20700bb634", "node_type": "4", "metadata": {"filename": "hwp.md", "author": "LlamaIndex"}, "hash": "5586b057151bd8a3b53a427c04afc90597072ee1b32872818e2f973529dd542b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.hwp\n    options:\n      members:\n        - HWPReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "294ca400-783e-4c95-bd2b-dbfb0ee8f8f3": {"__data__": {"id_": "294ca400-783e-4c95-bd2b-dbfb0ee8f8f3", "embedding": null, "metadata": {"filename": "iceberg.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eea962e0a1fc0db744a84a85d1f805688dea56c0", "node_type": "4", "metadata": {"filename": "iceberg.md", "author": "LlamaIndex"}, "hash": "bd0c1f6549d962c153e5958e2556fd3c9b924d5c4567a879f4803f0f86b35d86", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.iceberg\n    options:\n      members:\n        - IcebergReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb59511c-ad04-4909-8a08-7a45e3a4664a": {"__data__": {"id_": "cb59511c-ad04-4909-8a08-7a45e3a4664a", "embedding": null, "metadata": {"filename": "imdb_review.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e120cdb8ede6f4c518289399e9434407d6d08a5", "node_type": "4", "metadata": {"filename": "imdb_review.md", "author": "LlamaIndex"}, "hash": "59524a1c25680f8f2be07729c269af3453492b7d7b58392c199ca00cec742048", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.imdb_review\n    options:\n      members:\n        - IMDBReviews", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b578903-836e-4b6c-b0ab-64e858fa5b46": {"__data__": {"id_": "9b578903-836e-4b6c-b0ab-64e858fa5b46", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93c5abc5d6f40ca768eff15129ab1dc28b1a8ddc", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "75a11ee74139a51c252ac2d94c28883ad0f4ff2b1400b642f2b85c89e6ac1aa1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.readers.base\n    options:\n      members:\n        - BaseReader\n        - BasePydanticReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "819b4f68-56de-41ea-8ec7-ade5f09997ee": {"__data__": {"id_": "819b4f68-56de-41ea-8ec7-ade5f09997ee", "embedding": null, "metadata": {"filename": "intercom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f61df34cc7535506442d9020dfce6f4a9c3769a8", "node_type": "4", "metadata": {"filename": "intercom.md", "author": "LlamaIndex"}, "hash": "d5c934893e6d3c749e6dc0538ff2722619898580156aa28b18701c37264e153c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.intercom\n    options:\n      members:\n        - IntercomReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c98edb3-39e5-4882-9d98-c34b2e325dbf": {"__data__": {"id_": "2c98edb3-39e5-4882-9d98-c34b2e325dbf", "embedding": null, "metadata": {"filename": "jaguar.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "812a79b09c3be2e63b7b03b2a5a2579e4fe872db", "node_type": "4", "metadata": {"filename": "jaguar.md", "author": "LlamaIndex"}, "hash": "10496cb947322631dee21c7c00740918afd97b49cfa510434eea6fe079bd568c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.jaguar\n    options:\n      members:\n        - JaguarReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90a79192-b11e-4b07-9f5d-1a50bcc3038e": {"__data__": {"id_": "90a79192-b11e-4b07-9f5d-1a50bcc3038e", "embedding": null, "metadata": {"filename": "jira.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a355ac6f950a5c186a92d10be74f16773eb0aa45", "node_type": "4", "metadata": {"filename": "jira.md", "author": "LlamaIndex"}, "hash": "67a2cbe4107f99707758046e571c0a2f2d4b492bcd1803b2ace6d64e7bb66a34", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.jira\n    options:\n      members:\n        - JiraReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d74f94d-b1d7-4bfd-8f74-5770dc5e9ede": {"__data__": {"id_": "9d74f94d-b1d7-4bfd-8f74-5770dc5e9ede", "embedding": null, "metadata": {"filename": "joplin.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35d731eb08fd9cfb35c2d282f7afc4c86484e26d", "node_type": "4", "metadata": {"filename": "joplin.md", "author": "LlamaIndex"}, "hash": "ff741852adc27f10e055168ca9425b1240de19db5e985ae165a59f4ae4f6da6d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.joplin\n    options:\n      members:\n        - JoplinReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36104a2e-48a8-4407-b123-ca749ae44aec": {"__data__": {"id_": "36104a2e-48a8-4407-b123-ca749ae44aec", "embedding": null, "metadata": {"filename": "json.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9670c55b69450b1cc1077e085201371e803f7c56", "node_type": "4", "metadata": {"filename": "json.md", "author": "LlamaIndex"}, "hash": "02268ca3bde9a3d37ec1902ed1c9ea674e0891caf655bd6abf04bfa35164eaea", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.json\n    options:\n      members:\n        - JSONReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4be2015e-690b-4ca8-95ac-a3d0a352f2ac": {"__data__": {"id_": "4be2015e-690b-4ca8-95ac-a3d0a352f2ac", "embedding": null, "metadata": {"filename": "kaltura_esearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0bd37e6c3c91515ae46dc5f7dada9bb8bf073255", "node_type": "4", "metadata": {"filename": "kaltura_esearch.md", "author": "LlamaIndex"}, "hash": "4b5a33e5f6e177314c7cd2a890c6cdf001fbbb2c3ae2ad94988a2c4a443cc216", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.kaltura_esearch\n    options:\n      members:\n        - KalturaESearchReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0bc7711-05a1-4613-a43d-f4ec927a83fd": {"__data__": {"id_": "a0bc7711-05a1-4613-a43d-f4ec927a83fd", "embedding": null, "metadata": {"filename": "kibela.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "146b90c79821009d243072444742da5904f7ddf8", "node_type": "4", "metadata": {"filename": "kibela.md", "author": "LlamaIndex"}, "hash": "d7c47f09a86f7df9c1758d4fbd85b04e1f78dc13cfe83e211ac22fc3aac96388", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.kibela\n    options:\n      members:\n        - KibelaReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "749a830a-e84c-4a98-8dc2-f82e5571d2da": {"__data__": {"id_": "749a830a-e84c-4a98-8dc2-f82e5571d2da", "embedding": null, "metadata": {"filename": "lilac.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f5e47a62b2956b5d7761868ca27bc304b607ba3", "node_type": "4", "metadata": {"filename": "lilac.md", "author": "LlamaIndex"}, "hash": "fd00af9532bba39893fcd48ee6dbd189f63d4711d617aaf214ae40ebc70ccbfa", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.lilac\n    options:\n      members:\n        - LilacReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3d54bd6-d359-4104-a8bd-fe3e42a786fc": {"__data__": {"id_": "a3d54bd6-d359-4104-a8bd-fe3e42a786fc", "embedding": null, "metadata": {"filename": "linear.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a153a76d1a863a089b008119426634326c33670", "node_type": "4", "metadata": {"filename": "linear.md", "author": "LlamaIndex"}, "hash": "1c88771932f8cf4077e0f25558b618e5d219a9898558a173a252ab0852af1414", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.linear\n    options:\n      members:\n        - LinearReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39a1b6e0-ad30-4160-951b-b09da91b5082": {"__data__": {"id_": "39a1b6e0-ad30-4160-951b-b09da91b5082", "embedding": null, "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88c4aa106ca92abd7ddd89bda068f25b232dbfdd", "node_type": "4", "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}, "hash": "eb1bddce99988998867845a6f8945f161cf1fec09b95e25bb7e2f06bac110625", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.llama_parse\n    options:\n      members:\n        - LlamaParse", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a17a333-3d96-4861-87c2-a51fbf4ebb5f": {"__data__": {"id_": "4a17a333-3d96-4861-87c2-a51fbf4ebb5f", "embedding": null, "metadata": {"filename": "macrometa_gdn.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f4f18f9ff3683c22d2099c48117953600d2804c", "node_type": "4", "metadata": {"filename": "macrometa_gdn.md", "author": "LlamaIndex"}, "hash": "ad6da4ced5b72bd5b1eec6954dc844868ab352108d21ce3642c30ec6c3f72aee", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.macrometa_gdn\n    options:\n      members:\n        - MacrometaGDNReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "715cf458-1d34-4915-8516-578cc98dc090": {"__data__": {"id_": "715cf458-1d34-4915-8516-578cc98dc090", "embedding": null, "metadata": {"filename": "make_com.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a266ef18e03fc2400a105c0ecc6f8b9e5a50748a", "node_type": "4", "metadata": {"filename": "make_com.md", "author": "LlamaIndex"}, "hash": "871ac924944be5da2e859e7a9581bb63fbdde9724669b3fbe9dfbd471e5973eb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.make_com\n    options:\n      members:\n        - MakeWrapper", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 82, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d5321cc-c330-4e4b-be5c-caac05445ed8": {"__data__": {"id_": "9d5321cc-c330-4e4b-be5c-caac05445ed8", "embedding": null, "metadata": {"filename": "mangadex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4011658728dca3169d4e10c488ee9964de3fdfeb", "node_type": "4", "metadata": {"filename": "mangadex.md", "author": "LlamaIndex"}, "hash": "d0a7ce522baebab26895744c3e0d9c1b9a994230993e4de84b813344d5dd7d1a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.mangadex\n    options:\n      members:\n        - MangaDexReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3441db2a-45e2-436e-a2a1-030b9a1286c9": {"__data__": {"id_": "3441db2a-45e2-436e-a2a1-030b9a1286c9", "embedding": null, "metadata": {"filename": "mangoapps_guides.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e9eb28eddac9513bf7441b93ff4331ace44d5bc", "node_type": "4", "metadata": {"filename": "mangoapps_guides.md", "author": "LlamaIndex"}, "hash": "b13e2889234d378e0561832a512f4f0a85f92806b26a679e24b7595cab3ed66a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.mangoapps_guides\n    options:\n      members:\n        - MangoppsGuidesReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63d5548f-bf77-4a2b-9d6f-b8ba5d17568c": {"__data__": {"id_": "63d5548f-bf77-4a2b-9d6f-b8ba5d17568c", "embedding": null, "metadata": {"filename": "maps.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca7dfb98a4b2ec8a5a431eb03399c25aea19b3a4", "node_type": "4", "metadata": {"filename": "maps.md", "author": "LlamaIndex"}, "hash": "ca0eb4daaccfc39f97135185e80675626e77e296d32521262bff53edcdc92723", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.maps\n    options:\n      members:\n        - OpenMap", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7646ee70-3251-4179-a1e5-32b14dcd8bda": {"__data__": {"id_": "7646ee70-3251-4179-a1e5-32b14dcd8bda", "embedding": null, "metadata": {"filename": "mbox.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7491957cafb40efc1f27ec60fd37677de4fb47f6", "node_type": "4", "metadata": {"filename": "mbox.md", "author": "LlamaIndex"}, "hash": "d1e9ed0883188b8e771bf3f33ba3ca0bb87a0b44b5220df3761d87a961726f37", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.mbox\n    options:\n      members:\n        - MboxReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7868ea1-95ca-421b-a690-d375c5755ed4": {"__data__": {"id_": "c7868ea1-95ca-421b-a690-d375c5755ed4", "embedding": null, "metadata": {"filename": "memos.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "693177d604653663d10afccad72362b93a6dec8e", "node_type": "4", "metadata": {"filename": "memos.md", "author": "LlamaIndex"}, "hash": "8aa95bdce39b21e4709545a52231c36ee63b53096a768ddf0698d703f38437ce", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.memos\n    options:\n      members:\n        - MemosReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51e5e9f6-04d7-4a64-94b5-3fc66466d4cd": {"__data__": {"id_": "51e5e9f6-04d7-4a64-94b5-3fc66466d4cd", "embedding": null, "metadata": {"filename": "metal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40a7fa6a6cacaedf05d04a7f5ea68a71527b044f", "node_type": "4", "metadata": {"filename": "metal.md", "author": "LlamaIndex"}, "hash": "9c5ab79711effe96e2c37e998e443e6c0c268fc831055fe5dbb5b2d6d78594ba", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.metal\n    options:\n      members:\n        - MetalReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e77fd714-c5d5-40fb-b45a-bde01b978701": {"__data__": {"id_": "e77fd714-c5d5-40fb-b45a-bde01b978701", "embedding": null, "metadata": {"filename": "microsoft_onedrive.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2d46e9f195d857f3f9da7e417300e9b8ec72490", "node_type": "4", "metadata": {"filename": "microsoft_onedrive.md", "author": "LlamaIndex"}, "hash": "d4b8e0922a5380b9d136fc2fd0ddc483343c52ba323eb38bfa9b5f0c1168a936", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.microsoft_onedrive\n    options:\n      members:\n        - OneDriveReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f228ee35-ac64-45bd-8dc1-27d942fcde82": {"__data__": {"id_": "f228ee35-ac64-45bd-8dc1-27d942fcde82", "embedding": null, "metadata": {"filename": "microsoft_outlook.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e2e4c253d0d9d75df540cf0a1748e6a6a1dbaa4", "node_type": "4", "metadata": {"filename": "microsoft_outlook.md", "author": "LlamaIndex"}, "hash": "8fc627030876827f6eb0e344de15968e202339dc93b505d154b5c2b0db794626", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.microsoft_outlook\n    options:\n      members:\n        - OutlookLocalCalendarReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c6f0da1-15ea-4462-9f6d-7f2a0a4b2c71": {"__data__": {"id_": "9c6f0da1-15ea-4462-9f6d-7f2a0a4b2c71", "embedding": null, "metadata": {"filename": "microsoft_sharepoint.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f82a29aee5662e7f900228428dfb3fe176e67bb2", "node_type": "4", "metadata": {"filename": "microsoft_sharepoint.md", "author": "LlamaIndex"}, "hash": "d232ef0573e0f12b144ef49b23c37eb392a16d3cfbffb6e20b8f8c0f27957224", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.microsoft_sharepoint\n    options:\n      members:\n        - SharePointReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2122aa37-814f-4fca-a5a9-549a22d8d2f6": {"__data__": {"id_": "2122aa37-814f-4fca-a5a9-549a22d8d2f6", "embedding": null, "metadata": {"filename": "milvus.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5affbb64582c3dc9d8a2d395a77786107939a1c9", "node_type": "4", "metadata": {"filename": "milvus.md", "author": "LlamaIndex"}, "hash": "5ea5e0a6ac3306829e4016364a202886c792988f28208e07b22d507557ac1dd6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.milvus\n    options:\n      members:\n        - MilvusReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9977898f-d7e3-4781-a007-0c23e2db0af4": {"__data__": {"id_": "9977898f-d7e3-4781-a007-0c23e2db0af4", "embedding": null, "metadata": {"filename": "minio.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4bd28dc0d85d77931eec041337df2e7cf9e97113", "node_type": "4", "metadata": {"filename": "minio.md", "author": "LlamaIndex"}, "hash": "821dc66a73ccd6e2aee5795e29e80d0dc327aa6cf1ac063fcc0c185d0dcf5973", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.minio\n    options:\n      members:\n        - BotoMinioReader\n        - MinioReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b70d1870-8033-413b-bcab-4f1c37d25906": {"__data__": {"id_": "b70d1870-8033-413b-bcab-4f1c37d25906", "embedding": null, "metadata": {"filename": "mondaydotcom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b36b4029bec0d03c0b330100ef29a6f1caa7847", "node_type": "4", "metadata": {"filename": "mondaydotcom.md", "author": "LlamaIndex"}, "hash": "8ab43ea66f2e895c4ba612f819eed28c2549219ecb0c331e1828b54648cf1301", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.mondaydotcom\n    options:\n      members:\n        - MondayReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46a07493-1694-4638-8771-9f32206884ac": {"__data__": {"id_": "46a07493-1694-4638-8771-9f32206884ac", "embedding": null, "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a6ca314971e60a1936935700957fda0bf89e806", "node_type": "4", "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "hash": "cea51b5acf5c570668c1e70ca08acbd2048a8ccb4597366f046cffa46d8c094e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.mongodb\n    options:\n      members:\n        - SimpleMongoReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06ddef22-f3be-448a-bbd4-50cd633bba0a": {"__data__": {"id_": "06ddef22-f3be-448a-bbd4-50cd633bba0a", "embedding": null, "metadata": {"filename": "myscale.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8624f784c10d51511f4a718960f23c092e6052d3", "node_type": "4", "metadata": {"filename": "myscale.md", "author": "LlamaIndex"}, "hash": "e310d0aa92a5d34d53fbb2bb4f3598c178edea4e6fb11d861b3bf827df57d2cd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.myscale\n    options:\n      members:\n        - MyScaleReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebea690f-e5d9-4ea0-897b-cbcfbc3de85a": {"__data__": {"id_": "ebea690f-e5d9-4ea0-897b-cbcfbc3de85a", "embedding": null, "metadata": {"filename": "notion.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ebf340c0b9bc8b8306258e61c9c170e5462ca86", "node_type": "4", "metadata": {"filename": "notion.md", "author": "LlamaIndex"}, "hash": "338a23ae564dec24c8ffedbb7ebe449f5774b1ec7401ac9dbf0cb36c0d83aa37", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.notion\n    options:\n      members:\n        - NotionPageReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecb51072-0d60-4723-a948-00a280931726": {"__data__": {"id_": "ecb51072-0d60-4723-a948-00a280931726", "embedding": null, "metadata": {"filename": "nougat_ocr.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6c43da91f698137800ce66f5830b2d0f9a9367c", "node_type": "4", "metadata": {"filename": "nougat_ocr.md", "author": "LlamaIndex"}, "hash": "d40ddd410389f6ff9fb66594e38f47a18d734c50c3f1aa98d8a9e8a37ca4bb59", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.nougat_ocr\n    options:\n      members:\n        - PDFNougatOCR", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd9a6192-ec7f-4b35-9cc7-c42aa8a12a90": {"__data__": {"id_": "cd9a6192-ec7f-4b35-9cc7-c42aa8a12a90", "embedding": null, "metadata": {"filename": "obsidian.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3eb70c5518ee54be5c2f78b8b5c860c3b0343d19", "node_type": "4", "metadata": {"filename": "obsidian.md", "author": "LlamaIndex"}, "hash": "657307380d48dc6207050ec37f57395b3215e5ab0ce33359539496b2ebee6b14", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.obsidian\n    options:\n      members:\n        - ObsidianReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61edeba7-76fb-40c9-b104-650fcfa85abf": {"__data__": {"id_": "61edeba7-76fb-40c9-b104-650fcfa85abf", "embedding": null, "metadata": {"filename": "openalex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c7fd19c8a42353e861e3afbf09fc6bf2b2f8c42", "node_type": "4", "metadata": {"filename": "openalex.md", "author": "LlamaIndex"}, "hash": "a540c9115177ab1f4d3b064a11f07945d1278fd14a41514839d08bd1765a4e39", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.openalex\n    options:\n      members:\n        - OpenAlexReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cd617e1-8362-4c93-a752-e5f58540ced7": {"__data__": {"id_": "9cd617e1-8362-4c93-a752-e5f58540ced7", "embedding": null, "metadata": {"filename": "openapi.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e3d7754b23c84aaaf95ad6226cf6fd752005977", "node_type": "4", "metadata": {"filename": "openapi.md", "author": "LlamaIndex"}, "hash": "483db07b321a016f01cadf9447f6c88fa06bf2ec810b60d48a5b176e9e113bbb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.openapi\n    options:\n      members:\n        - OpenAPIReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b865f83-856c-4fc1-b1ac-05d6d8736d62": {"__data__": {"id_": "9b865f83-856c-4fc1-b1ac-05d6d8736d62", "embedding": null, "metadata": {"filename": "opendal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fae51af416a3b67c5de7b7d7bf94a4cf728160b2", "node_type": "4", "metadata": {"filename": "opendal.md", "author": "LlamaIndex"}, "hash": "b7848cfb53e7988215c29949faa5436f2ee0826886c6909ab2c4ebc980fedcbe", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.opendal\n    options:\n      members:\n        - OpendalAzblobReader\n        - OpendalGcsReader\n        - OpendalReader\n        - OpendalS3Reader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2078d050-a8b6-46fc-8a50-c93cd60aa67f": {"__data__": {"id_": "2078d050-a8b6-46fc-8a50-c93cd60aa67f", "embedding": null, "metadata": {"filename": "opensearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "833436604a77440b19eb68f0c374727630692910", "node_type": "4", "metadata": {"filename": "opensearch.md", "author": "LlamaIndex"}, "hash": "030ab0d03933e5b71d5b5e8ef8a876e27fef683e0f3c2b2d2d852424b3288f8e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.opensearch\n    options:\n      members:\n        - OpensearchReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cb9aa63-03ab-468f-8ef2-4f668631d72a": {"__data__": {"id_": "2cb9aa63-03ab-468f-8ef2-4f668631d72a", "embedding": null, "metadata": {"filename": "pandas_ai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "910cd15c371d6413fd00b0c63396920a5c4ff81b", "node_type": "4", "metadata": {"filename": "pandas_ai.md", "author": "LlamaIndex"}, "hash": "e96f490e4b5c21c2a3ab35c2490ab5994924ea5b4805d53f9ff61179ba8cf887", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.pandas_ai\n    options:\n      members:\n        - PandasAIReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68d34a40-00e5-4cd3-b151-57c0b93858ed": {"__data__": {"id_": "68d34a40-00e5-4cd3-b151-57c0b93858ed", "embedding": null, "metadata": {"filename": "papers.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48d0f51a27058ddbe8230a955175d2d6354077b2", "node_type": "4", "metadata": {"filename": "papers.md", "author": "LlamaIndex"}, "hash": "cc000c4786b756000ffa2870264c82287e4bd3ce6802fab190e52540549ba15a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.papers\n    options:\n      members:\n        - ArxivReader\n        - PubmedReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f24e4521-2a60-482d-926e-5c7452925323": {"__data__": {"id_": "f24e4521-2a60-482d-926e-5c7452925323", "embedding": null, "metadata": {"filename": "patentsview.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de64246483ae8c6df49eb973eef177f136d36c05", "node_type": "4", "metadata": {"filename": "patentsview.md", "author": "LlamaIndex"}, "hash": "2be00683c0c65598dfc1727ab0c213d0aa71bc57e0bc11322964cc1cc14d421e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.patentsview\n    options:\n      members:\n        - PatentsviewReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a0df90f-47aa-4e83-9bf1-ab30d799fd1c": {"__data__": {"id_": "7a0df90f-47aa-4e83-9bf1-ab30d799fd1c", "embedding": null, "metadata": {"filename": "pathway.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93036c25eea447b615e6a9d0da5bd3ed72f5312b", "node_type": "4", "metadata": {"filename": "pathway.md", "author": "LlamaIndex"}, "hash": "fc78ada17a23d5173e1d8ab61452b11836f7b9e9abc17c24a5377c2a044dac42", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.pathway\n    options:\n      members:\n        - PathwayReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "002b4dd4-2881-4a6d-adc5-cae1031197e9": {"__data__": {"id_": "002b4dd4-2881-4a6d-adc5-cae1031197e9", "embedding": null, "metadata": {"filename": "pdb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e37c240e3711d44c3deef8958510693c6ebc1d9b", "node_type": "4", "metadata": {"filename": "pdb.md", "author": "LlamaIndex"}, "hash": "8b14c5072b20c59c69530866047a366661d076e9a9f2afa75b17426649c89197", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.pdb\n    options:\n      members:\n        - PdbAbstractReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81ac9299-1c7d-4a94-9637-d38099e7ec59": {"__data__": {"id_": "81ac9299-1c7d-4a94-9637-d38099e7ec59", "embedding": null, "metadata": {"filename": "pdf_marker.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "022e849b38330614d62f493fd9d2674a1367b60d", "node_type": "4", "metadata": {"filename": "pdf_marker.md", "author": "LlamaIndex"}, "hash": "f9b2871ecdec668f9cf267122d42fed7e8f6bd417583f8b5d140e2afae04ea26", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.pdf_marker\n    options:\n      members:\n        - PDFMarkerReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e885a90c-656e-4917-aa42-2e3d919f1603": {"__data__": {"id_": "e885a90c-656e-4917-aa42-2e3d919f1603", "embedding": null, "metadata": {"filename": "pdf_table.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "740507ae894011157ea7cbd6c215ce87e40e44b5", "node_type": "4", "metadata": {"filename": "pdf_table.md", "author": "LlamaIndex"}, "hash": "ac5857c3457e93433712e2f09885f1cb4bb85cd96f2a0a03fdc2d653cf6265b2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.pdf_table\n    options:\n      members:\n        - PDFTableReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38921181-cfed-4044-9a86-9edc979ae941": {"__data__": {"id_": "38921181-cfed-4044-9a86-9edc979ae941", "embedding": null, "metadata": {"filename": "pebblo.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85c66da61db9e08f0b92e444d4ec4d7d036d3679", "node_type": "4", "metadata": {"filename": "pebblo.md", "author": "LlamaIndex"}, "hash": "13993f41bc435da63b2af586fd971e03a8f1e4bb377e55c53bd5542282566d0c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.pebblo\n    options:\n      members:\n        - PebbloReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39d6aea4-9204-417d-b9ee-53be2faba1bc": {"__data__": {"id_": "39d6aea4-9204-417d-b9ee-53be2faba1bc", "embedding": null, "metadata": {"filename": "preprocess.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "408bc9de18eb9c1f945d66cb68772ecc2ad786b9", "node_type": "4", "metadata": {"filename": "preprocess.md", "author": "LlamaIndex"}, "hash": "d50d675282fb438e9d8774f078c5095d915203fcb090c4e86ff1e5d4f2b38ce6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.preprocess\n    options:\n      members:\n        - PreprocessReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25900ba9-4290-495a-af61-ed9887c017c1": {"__data__": {"id_": "25900ba9-4290-495a-af61-ed9887c017c1", "embedding": null, "metadata": {"filename": "psychic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "914e511e523bf748d78568d41cbca3dfd69613e7", "node_type": "4", "metadata": {"filename": "psychic.md", "author": "LlamaIndex"}, "hash": "4c445628329a083c401560e5b472c5029cb67bef866a64e5e947258689ae6244", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.psychic\n    options:\n      members:\n        - PsychicReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "daac4a1b-2828-4b15-95ba-dd3a4cd12e87": {"__data__": {"id_": "daac4a1b-2828-4b15-95ba-dd3a4cd12e87", "embedding": null, "metadata": {"filename": "qdrant.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31e933c95b0e910c41fd5fcabba64cc17bebfdd9", "node_type": "4", "metadata": {"filename": "qdrant.md", "author": "LlamaIndex"}, "hash": "08b45c5a7fe8aff470f78b2cfbc74fbdb65f7992babbc76fcdeb0afdef436b98", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.qdrant\n    options:\n      members:\n        - QdrantReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8afb54b7-8c71-490d-9a76-8ff8a67c3306": {"__data__": {"id_": "8afb54b7-8c71-490d-9a76-8ff8a67c3306", "embedding": null, "metadata": {"filename": "rayyan.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b0db256d641b3fe2b03ef491f9e4efbc7ca2cfb", "node_type": "4", "metadata": {"filename": "rayyan.md", "author": "LlamaIndex"}, "hash": "7ffadcf6f902230ab5b77a3ea5cc3bedd6ba6da194d0b8b5cee78cc3350b8bea", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.rayyan\n    options:\n      members:\n        - RayyanReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd6f30f0-b0c2-48f9-898b-02046761f37f": {"__data__": {"id_": "dd6f30f0-b0c2-48f9-898b-02046761f37f", "embedding": null, "metadata": {"filename": "readme.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0280cc1899d33902c0cd5c9d4b2faa1c58624ba0", "node_type": "4", "metadata": {"filename": "readme.md", "author": "LlamaIndex"}, "hash": "ac461e58862c31463a3697804dd2787d94f5eb860fe05c5b2325f45b8e45f8fb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.readme\n    options:\n      members:\n        - ReadmeReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbe401e7-09e5-46a7-a193-0db90f1cdaa8": {"__data__": {"id_": "fbe401e7-09e5-46a7-a193-0db90f1cdaa8", "embedding": null, "metadata": {"filename": "readwise.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b3029dd9921cc46763915be9e4e890297a07c51", "node_type": "4", "metadata": {"filename": "readwise.md", "author": "LlamaIndex"}, "hash": "f8a84f5b16bffaee9031a340fc1341d63227b32b813e9426d7bf6853b5b0d4f7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.readwise\n    options:\n      members:\n        - ReadwiseReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dbbb499-a357-4f65-b825-d1301c2385af": {"__data__": {"id_": "8dbbb499-a357-4f65-b825-d1301c2385af", "embedding": null, "metadata": {"filename": "reddit.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "734d6fce64418d0bfa0f062909198ee98aef37fd", "node_type": "4", "metadata": {"filename": "reddit.md", "author": "LlamaIndex"}, "hash": "2c08667c9ccb14fee49c1d213d1d4b31721d3dfd0bca7e14160f68e1b131c65b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.reddit\n    options:\n      members:\n        - RedditReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfde5c17-fda0-45fb-a940-dd55b072b24c": {"__data__": {"id_": "bfde5c17-fda0-45fb-a940-dd55b072b24c", "embedding": null, "metadata": {"filename": "remote.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "586cc46d5c8f7b1f8b69c72c27f21f4e7db849b6", "node_type": "4", "metadata": {"filename": "remote.md", "author": "LlamaIndex"}, "hash": "e1dc44613fdbb589231b95867e0fadb2767d296ae5618062cd94465106554324", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.remote\n    options:\n      members:\n        - RemoteReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1566b61-3f98-4ca9-936a-8d8da644649b": {"__data__": {"id_": "e1566b61-3f98-4ca9-936a-8d8da644649b", "embedding": null, "metadata": {"filename": "remote_depth.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d5f8ec36d72cab49b56a8dc8df806a92af1a72e", "node_type": "4", "metadata": {"filename": "remote_depth.md", "author": "LlamaIndex"}, "hash": "3b950f8fffa231ecab8788955c4e4e295b267ec280c3eaa9f10812067b95ced4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.remote_depth\n    options:\n      members:\n        - RemoteDepthReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a312fd72-0774-4933-9e3b-486571f34d04": {"__data__": {"id_": "a312fd72-0774-4933-9e3b-486571f34d04", "embedding": null, "metadata": {"filename": "s3.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89fc49ef34fb7491d7c81318f97efe260beda8f1", "node_type": "4", "metadata": {"filename": "s3.md", "author": "LlamaIndex"}, "hash": "bedf20f45216b8bdc4df2c6a310f21fc682a43011fa5db8261c8bc1300d4c16d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.s3\n    options:\n      members:\n        - S3Reader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 73, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a4dcee6-6760-497f-a98e-28287bb021cd": {"__data__": {"id_": "9a4dcee6-6760-497f-a98e-28287bb021cd", "embedding": null, "metadata": {"filename": "sec_filings.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "357809ebfb6641a4be757a9f4a6e3d1f644a7839", "node_type": "4", "metadata": {"filename": "sec_filings.md", "author": "LlamaIndex"}, "hash": "95252e04f2bfcabe960993d03d32376b838b648983fd1058562c6728e302155e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.sec_filings\n    options:\n      members:\n        - SECFilingsLoader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4fe2c9c-979c-4e80-9655-abbd2fab1efc": {"__data__": {"id_": "f4fe2c9c-979c-4e80-9655-abbd2fab1efc", "embedding": null, "metadata": {"filename": "semanticscholar.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "890be0da4b482be0ea867018dafd66c08ec2d0bd", "node_type": "4", "metadata": {"filename": "semanticscholar.md", "author": "LlamaIndex"}, "hash": "d6412af3efb85869b19dbe19d0869bdf07e4cdf9c78af8687cc0fc487879be4e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.semanticscholar\n    options:\n      members:\n        - SemanticScholarReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8fb5fd44-a5d1-4bb0-a702-a228aef2353f": {"__data__": {"id_": "8fb5fd44-a5d1-4bb0-a702-a228aef2353f", "embedding": null, "metadata": {"filename": "simple_directory_reader.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d8ade8f37253419b9ba2c7b3ac738dbc098a3b1", "node_type": "4", "metadata": {"filename": "simple_directory_reader.md", "author": "LlamaIndex"}, "hash": "ec74a2793022246966c6e44ae4580845d5907ccfc538370d2cfbaf65a87db1ab", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.readers.file.base\n    options:\n      members:\n        - SimpleDirectoryReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e5d82df-9016-41c5-bea7-40ea92a16537": {"__data__": {"id_": "6e5d82df-9016-41c5-bea7-40ea92a16537", "embedding": null, "metadata": {"filename": "singlestore.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26752d4865eaef7fa41439e9aaf99317dd556a1a", "node_type": "4", "metadata": {"filename": "singlestore.md", "author": "LlamaIndex"}, "hash": "73f5cf53a8665797de7fecdceee724951a255528d1037f9e338a0cea142857cb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.singlestore\n    options:\n      members:\n        - SingleStoreReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d8adfd0-51d5-4d4d-9539-337e89876d9b": {"__data__": {"id_": "5d8adfd0-51d5-4d4d-9539-337e89876d9b", "embedding": null, "metadata": {"filename": "slack.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db20c905e943a6d0170a5b66992865814602a6b6", "node_type": "4", "metadata": {"filename": "slack.md", "author": "LlamaIndex"}, "hash": "502ce897c2306aa3538d6d5e9bc689cbf79dee9f698fe800ee07985ac4690422", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.slack\n    options:\n      members:\n        - SlackReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5a31247-0f5a-4c81-af50-7295fd2cc50c": {"__data__": {"id_": "b5a31247-0f5a-4c81-af50-7295fd2cc50c", "embedding": null, "metadata": {"filename": "smart_pdf_loader.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b89eea42594b57e31d87954a2339bf21676a4291", "node_type": "4", "metadata": {"filename": "smart_pdf_loader.md", "author": "LlamaIndex"}, "hash": "96ace5473355294dfb56ab7056c50855387b696f23d839d86c41899d393cf7c2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.smart_pdf_loader\n    options:\n      members:\n        - SmartPDFLoader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78864713-c31d-4b5c-a75f-2e35224836fe": {"__data__": {"id_": "78864713-c31d-4b5c-a75f-2e35224836fe", "embedding": null, "metadata": {"filename": "snowflake.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b7f2acf072cea75f33acb439f9a25b7dfeed062", "node_type": "4", "metadata": {"filename": "snowflake.md", "author": "LlamaIndex"}, "hash": "c8186cd0e145705b103debfa775b6faa479c12c35e7697ba3b521f333ce6fa68", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.snowflake\n    options:\n      members:\n        - SnowflakeReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1c3fc12-d7a8-440e-9720-178022885e6d": {"__data__": {"id_": "c1c3fc12-d7a8-440e-9720-178022885e6d", "embedding": null, "metadata": {"filename": "snscrape_twitter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b7096a2c2ead5ae7d3a4f0478433226e54d2c16", "node_type": "4", "metadata": {"filename": "snscrape_twitter.md", "author": "LlamaIndex"}, "hash": "ce73b238fbd1854da3a91209e2c8e19c9d3603e76aac434b21042ecec48674c6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.snscrape_twitter\n    options:\n      members:\n        - SnscrapeTwitterReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "929deb35-19cc-4fa3-afd0-9623c4cccac5": {"__data__": {"id_": "929deb35-19cc-4fa3-afd0-9623c4cccac5", "embedding": null, "metadata": {"filename": "spotify.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30452628318def79ad377ea4cca17da1a3877b9c", "node_type": "4", "metadata": {"filename": "spotify.md", "author": "LlamaIndex"}, "hash": "7b87b3c341ebf736d5fab8841dc63c9cffb508469f29bee546883c33627c41f2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.spotify\n    options:\n      members:\n        - SpotifyReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16839a38-f5eb-4b32-bac8-15a918740a1c": {"__data__": {"id_": "16839a38-f5eb-4b32-bac8-15a918740a1c", "embedding": null, "metadata": {"filename": "stackoverflow.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "031e51db9756578edb7f513446e090197f061c80", "node_type": "4", "metadata": {"filename": "stackoverflow.md", "author": "LlamaIndex"}, "hash": "d225b84449033d942e8289232874a9dc35859053b788a05613fe9e4a71224bac", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.stackoverflow\n    options:\n      members:\n        - StackoverflowReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d755120-f481-407c-9a43-1c5948b941bc": {"__data__": {"id_": "6d755120-f481-407c-9a43-1c5948b941bc", "embedding": null, "metadata": {"filename": "steamship.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61d8bf6faeea49ad098334c547ede763dbdf3069", "node_type": "4", "metadata": {"filename": "steamship.md", "author": "LlamaIndex"}, "hash": "075714c78d162f98207bbf412110a48edce97c2335ceab8c6cbaa07e5bca18ae", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.steamship\n    options:\n      members:\n        - SteamshipFileReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a997189-0663-4709-8666-787982f28fe4": {"__data__": {"id_": "4a997189-0663-4709-8666-787982f28fe4", "embedding": null, "metadata": {"filename": "string_iterable.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbd39b1fcd7d617c9be60de9ef4a1584e2a48dc4", "node_type": "4", "metadata": {"filename": "string_iterable.md", "author": "LlamaIndex"}, "hash": "2d981dc9d06a879dc6a413af54661a4a04e69a963fbcdd2915c0e7e4889ee084", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.string_iterable\n    options:\n      members:\n        - StringIterableReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cc0992a-76e7-4f44-b308-08b0ef886f0c": {"__data__": {"id_": "2cc0992a-76e7-4f44-b308-08b0ef886f0c", "embedding": null, "metadata": {"filename": "stripe_docs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56318c43b0a532c109df5990766409115b864d41", "node_type": "4", "metadata": {"filename": "stripe_docs.md", "author": "LlamaIndex"}, "hash": "b8e9043aed68bc1522b14d463ba757043cb8548adc534897640f848af747245b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.stripe_docs\n    options:\n      members:\n        - StripeDocsReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02043eb4-0375-4ce6-b095-a49f4649a197": {"__data__": {"id_": "02043eb4-0375-4ce6-b095-a49f4649a197", "embedding": null, "metadata": {"filename": "structured_data.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46dd9dbac596eb7265dd0ff431dda3cefc2cef7c", "node_type": "4", "metadata": {"filename": "structured_data.md", "author": "LlamaIndex"}, "hash": "d8067c0edbaafaced8879bcbbb8f0ed3bf7868d5b946369ef0c3f460842eeabd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.structured_data\n    options:\n      members:\n        - DashScopeAgent", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea04c740-5b74-40cb-a2f8-284358e3907b": {"__data__": {"id_": "ea04c740-5b74-40cb-a2f8-284358e3907b", "embedding": null, "metadata": {"filename": "telegram.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f9ae643520832a6886ca0b483bf6d24f6205aae", "node_type": "4", "metadata": {"filename": "telegram.md", "author": "LlamaIndex"}, "hash": "d549cd84a5a459386a7f9b498ece9a0cec3d102c429adced352704dc6c52daf3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.telegram\n    options:\n      members:\n        - TelegramReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b24eef26-dee3-4cf0-bf36-cbfd8eb0f7fe": {"__data__": {"id_": "b24eef26-dee3-4cf0-bf36-cbfd8eb0f7fe", "embedding": null, "metadata": {"filename": "toggl.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc46978ee1045df6d5d89fc3f55617970ee7a3ff", "node_type": "4", "metadata": {"filename": "toggl.md", "author": "LlamaIndex"}, "hash": "8e2ebff8f406827fc770f55ddf93e05f701f483c55ba72103e4444a065449d84", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.toggl\n    options:\n      members:\n        - TogglReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3126cd00-c833-40b8-b3e0-f6f66cb76234": {"__data__": {"id_": "3126cd00-c833-40b8-b3e0-f6f66cb76234", "embedding": null, "metadata": {"filename": "trello.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8ae1db630dc30634d8404eb8c1365bafb99716c", "node_type": "4", "metadata": {"filename": "trello.md", "author": "LlamaIndex"}, "hash": "87ea0b33f59ddf39da35de7c3de93cec55a477a82d7043a99497206888414b5c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.trello\n    options:\n      members:\n        - TrelloReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c4d91f7-891e-4730-a246-bede2c6a2f4c": {"__data__": {"id_": "3c4d91f7-891e-4730-a246-bede2c6a2f4c", "embedding": null, "metadata": {"filename": "twitter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e038987c90f829c6fc21ec1f541abebd926d0686", "node_type": "4", "metadata": {"filename": "twitter.md", "author": "LlamaIndex"}, "hash": "199f040c055431a6ebc46b9075dded3f63133b5ceb82a2efa0fb6bd626904642", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.twitter\n    options:\n      members:\n        - TwitterTweetReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d880d95f-78f1-4ebe-8b8b-1635255841d0": {"__data__": {"id_": "d880d95f-78f1-4ebe-8b8b-1635255841d0", "embedding": null, "metadata": {"filename": "txtai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1fb219e1b1af75202a166d2e0ebac17a2f3d6739", "node_type": "4", "metadata": {"filename": "txtai.md", "author": "LlamaIndex"}, "hash": "46672a1eda6b1be0f9d3c367957cbca5664fb4c5f5384c8d73cc0662877e371a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.txtai\n    options:\n      members:\n        - TxtaiReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22debcf5-b223-4b78-86d6-14259a6b1328": {"__data__": {"id_": "22debcf5-b223-4b78-86d6-14259a6b1328", "embedding": null, "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbda070e4bba3c7b07f6886c357b2c4d0e721b22", "node_type": "4", "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}, "hash": "03a9db5150209861570de52550074270dcf05df70a0186b7afeceabd350fed2e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.upstage\n    options:\n      members:\n        - UpstageDocumentReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a9aa7bb-69ed-48a3-a5cc-b949ce753bbd": {"__data__": {"id_": "3a9aa7bb-69ed-48a3-a5cc-b949ce753bbd", "embedding": null, "metadata": {"filename": "weather.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7ee67e48f8c0c95618dc39221f5a27a34ec829f", "node_type": "4", "metadata": {"filename": "weather.md", "author": "LlamaIndex"}, "hash": "3a336a265ba559d1c20afced91f17e1a95ba02be2a81bff63b0c9f8ba90786c4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.weather\n    options:\n      members:\n        - WeatherReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1ad5dc5-8922-4552-8f85-8eac211f1896": {"__data__": {"id_": "c1ad5dc5-8922-4552-8f85-8eac211f1896", "embedding": null, "metadata": {"filename": "weaviate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75481ec81e17040fe2ad0d03bd5d3dc928687a07", "node_type": "4", "metadata": {"filename": "weaviate.md", "author": "LlamaIndex"}, "hash": "f843907283348e37c9b72fea8e7f60577b74c8c0a78781262a81516011b4a808", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.weaviate\n    options:\n      members:\n        - WeaviateReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca9d25ae-725a-4949-a514-bcc16d700e0d": {"__data__": {"id_": "ca9d25ae-725a-4949-a514-bcc16d700e0d", "embedding": null, "metadata": {"filename": "web.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7c763148fe6a90ad34d1628b4197d9432dea81c", "node_type": "4", "metadata": {"filename": "web.md", "author": "LlamaIndex"}, "hash": "0a4d4bf3e4180719b934398c0e4bad0f072deaaca7889a21c509c6ed7e229000", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.web\n    options:\n      members:\n        - AsyncWebPageReader\n        - BeautifulSoupWebReader\n        - BrowserbaseWebReader\n        - FireCrawlWebReader\n        - KnowledgeBaseWebReader\n        - MainContentExtractorReader\n        - NewsArticleReader\n        - ReadabilityWebPageReader\n        - RssNewsReader\n        - RssReader\n        - ScrapflyReader\n        - SimpleWebPageReader\n        - SitemapReader\n        - SpiderReader\n        - TrafilaturaWebReader\n        - UnstructuredURLLoader\n        - WholeSiteReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7446438-ebca-48cc-86e7-bc9b927518df": {"__data__": {"id_": "f7446438-ebca-48cc-86e7-bc9b927518df", "embedding": null, "metadata": {"filename": "whatsapp.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "886d77cd229608205776525bb114a6444ead4aef", "node_type": "4", "metadata": {"filename": "whatsapp.md", "author": "LlamaIndex"}, "hash": "981f434b01811416cf5300d491e80df161459da69768e6ff169cc3990d20d1c7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.whatsapp\n    options:\n      members:\n        - WhatsappChatLoader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f08ea7b2-4ad2-4065-8e7d-bb04336b1785": {"__data__": {"id_": "f08ea7b2-4ad2-4065-8e7d-bb04336b1785", "embedding": null, "metadata": {"filename": "wikipedia.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d86bdd5b6e515127e5cbf105b1aa8f4ee0dc659f", "node_type": "4", "metadata": {"filename": "wikipedia.md", "author": "LlamaIndex"}, "hash": "423e3c5b81b94e6b9dcb95d6e39d1869edb4cf43857e78550db733d77903e7a4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.wikipedia\n    options:\n      members:\n        - WikipediaReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74fbe73b-b9d7-4cd4-9615-3eba7820d8db": {"__data__": {"id_": "74fbe73b-b9d7-4cd4-9615-3eba7820d8db", "embedding": null, "metadata": {"filename": "wordlift.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12af139e1de3404bef178badec24959cc81d6419", "node_type": "4", "metadata": {"filename": "wordlift.md", "author": "LlamaIndex"}, "hash": "ff8611cce959be94267fb354713dfe25cad5ec3d4fb29f4d44d7f21417925dc6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.wordlift\n    options:\n      members:\n        - WordLiftLoader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "378f5d30-e333-4928-be19-0d3c93abc2f4": {"__data__": {"id_": "378f5d30-e333-4928-be19-0d3c93abc2f4", "embedding": null, "metadata": {"filename": "wordpress.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99a1a37a467216a08a0cb98e469faa8e2cbfb01a", "node_type": "4", "metadata": {"filename": "wordpress.md", "author": "LlamaIndex"}, "hash": "bd857f7ca6031003f2176f07a9cc9f9361473be4281a2d34faf33b8e87ac1965", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.wordpress\n    options:\n      members:\n        - WordpressReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6176453-1f33-4dad-b714-8bd3cd08379d": {"__data__": {"id_": "d6176453-1f33-4dad-b714-8bd3cd08379d", "embedding": null, "metadata": {"filename": "youtube_metadata.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a8a7fd8d00ed3497217062af2fe4618463558839", "node_type": "4", "metadata": {"filename": "youtube_metadata.md", "author": "LlamaIndex"}, "hash": "13fe28d913ba5ce1f0757f5c81c897b992c093a4b904adfbce53fa9bff2a68eb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.youtube_metadata\n    options:\n      members:\n        - YoutubeTranscriptReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4b2530a-2bfb-4fa9-b962-bc1ca017ff01": {"__data__": {"id_": "a4b2530a-2bfb-4fa9-b962-bc1ca017ff01", "embedding": null, "metadata": {"filename": "youtube_transcript.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66ed75cc0f93c38031e833ac9b63b8736eb8c317", "node_type": "4", "metadata": {"filename": "youtube_transcript.md", "author": "LlamaIndex"}, "hash": "06f88e28f6feef7eb15389860fd9f12036785351e840b6c2237cb58385694b6a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.youtube_transcript\n    options:\n      members:\n        - YoutubeTranscriptReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b83612-9b9d-48c9-9bc4-3c0f4442dafe": {"__data__": {"id_": "76b83612-9b9d-48c9-9bc4-3c0f4442dafe", "embedding": null, "metadata": {"filename": "zendesk.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e83205e9973e1635664456e9b37a316ff55571d9", "node_type": "4", "metadata": {"filename": "zendesk.md", "author": "LlamaIndex"}, "hash": "665f05a349511c7860d454bf2f95c5d1fa72130627478f668ff1274f7620bd02", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.zendesk\n    options:\n      members:\n        - ZendeskReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d58b485d-d61d-4301-a18a-8ea259cf2a70": {"__data__": {"id_": "d58b485d-d61d-4301-a18a-8ea259cf2a70", "embedding": null, "metadata": {"filename": "zep.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9d2c6fcb1f11f8a89234ed887632f0a625821e3", "node_type": "4", "metadata": {"filename": "zep.md", "author": "LlamaIndex"}, "hash": "26afcecf807be3127e53dba084de919d9b48ef7cc9f0e978d8c60af258bd9391", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.zep\n    options:\n      members:\n        - ZepReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b41a84d8-69dd-4efb-b5cb-3cea6e71ef09": {"__data__": {"id_": "b41a84d8-69dd-4efb-b5cb-3cea6e71ef09", "embedding": null, "metadata": {"filename": "zulip.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e80b6aed37a70227ddf23fcce6724c295f2703e6", "node_type": "4", "metadata": {"filename": "zulip.md", "author": "LlamaIndex"}, "hash": "b6f42739c2198ec57108655c0d82186c2af83ede18d095a8556a2ee82c6a19d0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.readers.zulip\n    options:\n      members:\n        - ZulipReader", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb7de2be-19d9-458a-a1af-bc6e056459b8": {"__data__": {"id_": "fb7de2be-19d9-458a-a1af-bc6e056459b8", "embedding": null, "metadata": {"filename": "accumulate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c07fb19215997328f6329fb50d4884f384a25ace", "node_type": "4", "metadata": {"filename": "accumulate.md", "author": "LlamaIndex"}, "hash": "81e3d69975e157b5284ebc34a384088b0b8188fe1e0dae5430ecbaa2679ff052", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers\n    options:\n      members:\n        - Accumulate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57a5e2e5-688e-4b9e-9a59-89abc195fefd": {"__data__": {"id_": "57a5e2e5-688e-4b9e-9a59-89abc195fefd", "embedding": null, "metadata": {"filename": "compact_accumulate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec4480f32b0bc57c291ec034219105a53f1134ad", "node_type": "4", "metadata": {"filename": "compact_accumulate.md", "author": "LlamaIndex"}, "hash": "38e0956d935c7f9afb2adf889e85f5db328bdddc75d03d6b45fdd31766c90963", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers.compact_and_accumulate\n    options:\n      members:\n        - CompactAndAccumulate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4290f30-c836-4472-8e09-cfca109c1624": {"__data__": {"id_": "c4290f30-c836-4472-8e09-cfca109c1624", "embedding": null, "metadata": {"filename": "compact_and_refine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1da13086404288c96ea3cbd4f305f0c44e63d837", "node_type": "4", "metadata": {"filename": "compact_and_refine.md", "author": "LlamaIndex"}, "hash": "f2e00af83bf3e0787fce54e640c71d51609356b867dfeb51a9c1f27f0d16984c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers\n    options:\n      members:\n        - CompactAndRefine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f3ea72e-34d6-428f-af3c-df8c3c46cae8": {"__data__": {"id_": "8f3ea72e-34d6-428f-af3c-df8c3c46cae8", "embedding": null, "metadata": {"filename": "generation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcbe227332a9b52d9dc36db4704cc284a4c3ca74", "node_type": "4", "metadata": {"filename": "generation.md", "author": "LlamaIndex"}, "hash": "82df6d590403a58f4ef02473b1978e8dadaea038324e1f6cefade8ad18ac16e0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers\n    options:\n      members:\n        - Generation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77ff3670-5771-4768-9894-01cdb058a559": {"__data__": {"id_": "77ff3670-5771-4768-9894-01cdb058a559", "embedding": null, "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f63a55d6923cfe8aeabe86ec140570a0ad82d2cb", "node_type": "4", "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "hash": "3b420f7dc1903da0a73c690204ffcd3ad6cf524ce2695635175bdc0b97fd0f80", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.response_synthesizers.google\n    options:\n      members:\n        - GoogleTextSynthesizer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4f80904-63c6-4c9b-bf2f-4fd5f3f72e07": {"__data__": {"id_": "c4f80904-63c6-4c9b-bf2f-4fd5f3f72e07", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b8b541953a0f33ec63398f62b2a70f703a1d428", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "82d24687cb9134edd49863b3e1b1b6d283a3c5984020587f7d9cfff3b086fcf6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers.base\n    options:\n      members:\n        - BaseSynthesizer\n\n::: llama_index.core.response_synthesizers.factory\n    options:\n      members:\n        - get_response_synthesizer\n\n::: llama_index.core.response_synthesizers.type\n    options:\n      members:\n        - ResponseMode", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba0e5786-cf32-4cda-ad15-484f03a297c1": {"__data__": {"id_": "ba0e5786-cf32-4cda-ad15-484f03a297c1", "embedding": null, "metadata": {"filename": "refine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a69de484874bbab04cba05602a2a554cea3cc7a5", "node_type": "4", "metadata": {"filename": "refine.md", "author": "LlamaIndex"}, "hash": "987dcbdde08ceeac56ddfcc7d903b549526eeb1a9330b775385ebec1042d2c9e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers\n    options:\n      members:\n        - Refine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85822257-d65d-47bf-ad8b-78ae0827028f": {"__data__": {"id_": "85822257-d65d-47bf-ad8b-78ae0827028f", "embedding": null, "metadata": {"filename": "simple_summarize.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a4c3bc095b3bb31b754fa53904d845d0d0fd60f", "node_type": "4", "metadata": {"filename": "simple_summarize.md", "author": "LlamaIndex"}, "hash": "62aab083eb22ea9a8925b23000331492481637f471ee2175b66b1e57c370b674", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers\n    options:\n      members:\n        - SimpleSummarize", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27a6b5a2-d57c-4b86-a3bf-9cfe48d26b35": {"__data__": {"id_": "27a6b5a2-d57c-4b86-a3bf-9cfe48d26b35", "embedding": null, "metadata": {"filename": "tree_summarize.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0544a715d0d5b6af091dddc84b2df11ad1a8cf7", "node_type": "4", "metadata": {"filename": "tree_summarize.md", "author": "LlamaIndex"}, "hash": "f2ca5fa876d65935d3ef9567b88ddd0379592ebfc76dc63f392bc0d29bfbb3ab", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.response_synthesizers\n    options:\n      members:\n        - TreeSummarize", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4042e84d-9eb5-46b8-9bc3-29152f924a23": {"__data__": {"id_": "4042e84d-9eb5-46b8-9bc3-29152f924a23", "embedding": null, "metadata": {"filename": "auto_merging.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dea41f8e24efed91986d3032c78842346b2378d", "node_type": "4", "metadata": {"filename": "auto_merging.md", "author": "LlamaIndex"}, "hash": "01858a5868b18a3892b65be16ff99d9faeaac83ff713621899bc253ac4b8bb67", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - AutoMergingRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9dbc23f2-ab86-4442-9039-460b0b9a6324": {"__data__": {"id_": "9dbc23f2-ab86-4442-9039-460b0b9a6324", "embedding": null, "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d98fdf3dad795fff9e2e214644bb22ecc5b34364", "node_type": "4", "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}, "hash": "d4923ab32fc059814e5b9b2397af328e31655c63657e0fbafdd464e58e510db7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.bedrock\n    options:\n      members:\n        - AmazonKnowledgeBasesRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43f0da14-6f32-4312-8f63-6b9939717239": {"__data__": {"id_": "43f0da14-6f32-4312-8f63-6b9939717239", "embedding": null, "metadata": {"filename": "bm25.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8040efda72877bd805ef7d3a73a0780948350c6", "node_type": "4", "metadata": {"filename": "bm25.md", "author": "LlamaIndex"}, "hash": "ee5ee1d50b5727e09f4107aad7cc94ef1514e90767219c1215523225386ca05e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.bm25\n    options:\n      members:\n        - BM25Retriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf3a1721-66b1-4f82-82e8-bc8c825e9b74": {"__data__": {"id_": "cf3a1721-66b1-4f82-82e8-bc8c825e9b74", "embedding": null, "metadata": {"filename": "duckdb_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2eb6050b73d017c151d38f50f463322537859eed", "node_type": "4", "metadata": {"filename": "duckdb_retriever.md", "author": "LlamaIndex"}, "hash": "ce90a18cb30b328bc3aa08de5415007d855f959a0cdcf1efff6bc96900ae9b84", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.duckdb_retriever\n    options:\n      members:\n        - DuckDBRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa39c52a-cb73-4da5-ad92-a49aabb6680d": {"__data__": {"id_": "aa39c52a-cb73-4da5-ad92-a49aabb6680d", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "407afaa3eb4b69e39c60127c639b9b372b391e29", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "17d35eb76a907b47f49d780ec227eea40b7f93691d3ceee377a8e42dd9e6728b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.base.base_retriever\n    options:\n      members:\n        - BaseRetriever\n\n::: llama_index.core.image_retriever\n    options:\n      members:\n        - BaseImageRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64805582-a164-4b41-97c4-5890e1d0c311": {"__data__": {"id_": "64805582-a164-4b41-97c4-5890e1d0c311", "embedding": null, "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "663b4c0e043b116c3777b158246345ca89b77a05", "node_type": "4", "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}, "hash": "cd05369982e96f94128ebad27bea850eb2abc85ba8c9cac5eddda04bbb4f38e7", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.indices.keyword_table.retrievers\n    options:\n      members:\n        - BaseKeywordTableRetriever\n        - KeywordTableGPTRetriever\n        - KeywordTableSimpleRetriever\n        - KeywordTableRAKERetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d7fe055-404a-4bda-8464-53142d7d526d": {"__data__": {"id_": "0d7fe055-404a-4bda-8464-53142d7d526d", "embedding": null, "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a97a5f79f3afc397269781615d15197ddca96952", "node_type": "4", "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}, "hash": "92119d3d977299d966e088bdce8d34942c5b94119dbe701124eb74959248fa8a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - KGTableRetriever\n        - KnowledgeGraphRAGRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd813165-dba1-4d50-97fd-1d3cba7f14ff": {"__data__": {"id_": "fd813165-dba1-4d50-97fd-1d3cba7f14ff", "embedding": null, "metadata": {"filename": "mongodb_atlas_bm25_retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10acecc248c980944d096d2ec407e5455d6d8497", "node_type": "4", "metadata": {"filename": "mongodb_atlas_bm25_retriever.md", "author": "LlamaIndex"}, "hash": "8d33074aff0cc0eedbf85287353812f39e7d3921028fdc889b54cd27aff4e3d0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.mongodb_atlas_bm25_retriever\n    options:\n      members:\n        - MongoDBAtlasBM25Retriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dddc7395-e6e7-4e62-83a7-0f453160505c": {"__data__": {"id_": "dddc7395-e6e7-4e62-83a7-0f453160505c", "embedding": null, "metadata": {"filename": "pathway.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e2855ae06a12108ec939943dff84ad094a50ae0", "node_type": "4", "metadata": {"filename": "pathway.md", "author": "LlamaIndex"}, "hash": "036ece1c3732b9f62bf71c5967896513772d310cc88bf9ffeafed7d4b540529c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.pathway\n    options:\n      members:\n        - PathwayRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "484513f4-5da2-4769-b17b-986593a52707": {"__data__": {"id_": "484513f4-5da2-4769-b17b-986593a52707", "embedding": null, "metadata": {"filename": "query_fusion.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5594011622b0bd3c18889073299e9376e67cc24", "node_type": "4", "metadata": {"filename": "query_fusion.md", "author": "LlamaIndex"}, "hash": "f40ea4a4688a3a49ff2c774fcc8e5c41364bae4155c29711414c9944749c4cf3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - QueryFusionRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ad7a89b-994a-4afa-9a6b-0a1d4ac0d5c1": {"__data__": {"id_": "1ad7a89b-994a-4afa-9a6b-0a1d4ac0d5c1", "embedding": null, "metadata": {"filename": "recursive.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e6c3bf9596f03184005614b05008628bc7f7ff8", "node_type": "4", "metadata": {"filename": "recursive.md", "author": "LlamaIndex"}, "hash": "42c1582385973ac38419e997253c0fea84e706c39527bafe11b6396e512a23b3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - RecursiveRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e37b9d6-5600-4793-b58d-e0c2502dbab1": {"__data__": {"id_": "4e37b9d6-5600-4793-b58d-e0c2502dbab1", "embedding": null, "metadata": {"filename": "router.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f9732dc505c9a49a933d20dacf000be167648b4", "node_type": "4", "metadata": {"filename": "router.md", "author": "LlamaIndex"}, "hash": "3aad5105b55a2c2a22b6521efc3913e3cf319a26d466c59c3bb0e754cef424ff", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - RouterRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "127a515b-4631-41de-8396-e623f308e22a": {"__data__": {"id_": "127a515b-4631-41de-8396-e623f308e22a", "embedding": null, "metadata": {"filename": "sql.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a306ee8aeac99c9023623e5ed8bf7576e40c832", "node_type": "4", "metadata": {"filename": "sql.md", "author": "LlamaIndex"}, "hash": "c90e31997f478ec4efe09c0054f51be90ca7c45c133e203c11e52471316671fa", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - NLSQLRetriever\n        - SQLParserMode\n        - SQLRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9c66e8c-4f8b-4736-9abe-61923a1bb331": {"__data__": {"id_": "b9c66e8c-4f8b-4736-9abe-61923a1bb331", "embedding": null, "metadata": {"filename": "summary.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b300c65a0e069a78d9e6cce53f15e86651fc94f9", "node_type": "4", "metadata": {"filename": "summary.md", "author": "LlamaIndex"}, "hash": "65257a056ce88a2a5d1c96cfc85ae50267b076bb67ec3931700f4a5a26d19ce1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - SummaryIndexRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d209f22f-2f1e-4a25-81e0-08b993f97d73": {"__data__": {"id_": "d209f22f-2f1e-4a25-81e0-08b993f97d73", "embedding": null, "metadata": {"filename": "transform.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5aae897ec710accfb4ce1785058c8ce90e51f12c", "node_type": "4", "metadata": {"filename": "transform.md", "author": "LlamaIndex"}, "hash": "0cdf1b18f4a956a4686afba259405e2bb08911759a73a81e75b68d947b447471", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - TransformRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c899d932-d5f2-4053-a7fc-29389606d912": {"__data__": {"id_": "c899d932-d5f2-4053-a7fc-29389606d912", "embedding": null, "metadata": {"filename": "tree.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10279885bbd31a672544267253b384908ca09331", "node_type": "4", "metadata": {"filename": "tree.md", "author": "LlamaIndex"}, "hash": "193e7a531e785d6cb8ff858738be5e4382312a7a0d61719897b61be6fc0c2d22", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - TreeAllLeafRetriever\n        - TreeSelectLeafEmbeddingRetriever\n        - TreeSelectLeafRetriever\n        - TreeRootRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1acd3ec-7877-4d2b-bf2e-09aabb025820": {"__data__": {"id_": "b1acd3ec-7877-4d2b-bf2e-09aabb025820", "embedding": null, "metadata": {"filename": "vector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc79b814af4864c553cfdf575c1bb6725bf7cfa8", "node_type": "4", "metadata": {"filename": "vector.md", "author": "LlamaIndex"}, "hash": "934504a65a5a129a77869cf64677140beff0666ac205df45b789267dad9d258e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.retrievers\n    options:\n      members:\n        - VectorIndexRetriever\n        - VectorIndexAutoRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac3ddaf0-bad6-4be7-ade5-0493d244ed54": {"__data__": {"id_": "ac3ddaf0-bad6-4be7-ade5-0493d244ed54", "embedding": null, "metadata": {"filename": "videodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e896c313b100dc5b23ffee9a80ff61430bf1008a", "node_type": "4", "metadata": {"filename": "videodb.md", "author": "LlamaIndex"}, "hash": "84613cc2ded955eaba1ee03838847ee901bc1feacd30c9654c75e4e3f3ab379e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.videodb\n    options:\n      members:\n        - VideoDBRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8af98b70-df77-4035-959d-35aa28457212": {"__data__": {"id_": "8af98b70-df77-4035-959d-35aa28457212", "embedding": null, "metadata": {"filename": "you.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5e26fd088717b776c077c9a26ae44752bd7a7b3", "node_type": "4", "metadata": {"filename": "you.md", "author": "LlamaIndex"}, "hash": "fa890e745ea398fb2df139161f65bcd38c63d37bc0974e831b002b2fb21eea30", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.retrievers.you\n    options:\n      members:\n        - YouRetriever", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30c97429-409b-4a99-a5cb-d3f911d02969": {"__data__": {"id_": "30c97429-409b-4a99-a5cb-d3f911d02969", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c214862b81646beae9750041b319d2e7d840da07", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c49af403dcb06c6793cacd29c01e5b0d69c6a79a5902389b4f4d27dd6cc43ee9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.schema", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 27, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed2dc0cd-c08b-4b4a-8772-0fed8e74664e": {"__data__": {"id_": "ed2dc0cd-c08b-4b4a-8772-0fed8e74664e", "embedding": null, "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "371e0170c4fe3af69f4976ab7d79b5a8ec035128", "node_type": "4", "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "hash": "e4ddccec40f7644d781de63ca07f17380069aa45f21f7dd575f965cd79dab77f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.chat_store.azure\n    options:\n      members:\n        - AzureChatStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35258239-163d-464c-8cd7-6ccd9afd3581": {"__data__": {"id_": "35258239-163d-464c-8cd7-6ccd9afd3581", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76d26756aea7f5970b9df86db5cd9a2d3dd44df1", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "b8012f5091ef38e9a96718c56f87d8de266cba7dbefbf3a8969a0eb7d0c54079", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.chat_store.base\n    options:\n      members:\n        - BaseChatStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28ed0b4e-6ea5-496c-b20a-67392f6490ad": {"__data__": {"id_": "28ed0b4e-6ea5-496c-b20a-67392f6490ad", "embedding": null, "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c55ba01c3e62081cfa69a01a20958c447e33f12", "node_type": "4", "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "hash": "98c7170f4be26125252a204945892475ec3b162b223d8ac4005ec78d2b06fef9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.chat_store.redis\n    options:\n      members:\n        - RedisChatStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a30e75bc-6821-4b97-b7a9-1fbead056471": {"__data__": {"id_": "a30e75bc-6821-4b97-b7a9-1fbead056471", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d49e91b9eb4f8dda620bd048f1bdb80aaabeb95", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "786db17293f745231d70340dbef5d5731cd208b0f30b9e20cf8dfe2e60d3401b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.chat_store.simple_chat_store\n    options:\n      members:\n        - SimpleChatStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09231805-90de-4b0b-87d5-0bb27982db95": {"__data__": {"id_": "09231805-90de-4b0b-87d5-0bb27982db95", "embedding": null, "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "670d8ac915cfdefef43c356afcba29adeb56d0a3", "node_type": "4", "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "hash": "741d7037940efd2db2735c95cd9bb1d314c2a83cefc159cb08589a99e1227dbe", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.azure\n    options:\n      members:\n        - AzureDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff1b4e8f-78ed-4787-886e-ae7fbf3514cf": {"__data__": {"id_": "ff1b4e8f-78ed-4787-886e-ae7fbf3514cf", "embedding": null, "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4996bd48dfe4e17bafd7fff506641920c39f0efc", "node_type": "4", "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "hash": "286cc8af68eb60d682ff2159494b47c3fcfc566ca2a6d9bbf9f93f30584edd24", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.dynamodb\n    options:\n      members:\n        - DynamoDBDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97c0c400-3127-4d0f-a40e-85fbfdf7e04a": {"__data__": {"id_": "97c0c400-3127-4d0f-a40e-85fbfdf7e04a", "embedding": null, "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02494a2d5e625e26c55cdb7e32b9a61d63cfddc7", "node_type": "4", "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "hash": "c3a22a746417a226859186969b170f0f5be2ab5060a49cc03f93d1cf2b9457ba", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.elasticsearch\n    options:\n      members:\n        - ElasticsearchDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99af2be7-3cf4-4ac1-9e97-77265242b4a6": {"__data__": {"id_": "99af2be7-3cf4-4ac1-9e97-77265242b4a6", "embedding": null, "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7b8e3eee3a696326daeb646aa0c48368bfcdcff", "node_type": "4", "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "hash": "c045ad1817ad1604b342ce2688a4d31f9e4f7244b3a5f304d08f9a66a6a3bffb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.firestore\n    options:\n      members:\n        - FirestoreDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59ce7ca1-00be-4ee3-8b95-833d7512d844": {"__data__": {"id_": "59ce7ca1-00be-4ee3-8b95-833d7512d844", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "607d0cfcf290d6cc1be543431d31abdf16c3b351", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "7a55e4a4da61937dbf834504a1a206f5bbcf11a187558538698bc5aa7c244ac3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.docstore.types", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 43, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4231905d-0e05-43a0-9653-93172c089305": {"__data__": {"id_": "4231905d-0e05-43a0-9653-93172c089305", "embedding": null, "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1feb56dd0624fd7712e5c3d9dfdbb04488bf861", "node_type": "4", "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "hash": "98ea6b19e5d7935cabdf96477f93b1acafb0b2ec2fe052a854e4c6bbbc3ca4ca", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.mongodb\n    options:\n      members:\n        - MongoDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb483ef3-3bda-47aa-8376-27878613187f": {"__data__": {"id_": "bb483ef3-3bda-47aa-8376-27878613187f", "embedding": null, "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49ca31e540c7071afb3356e611410e178463ff92", "node_type": "4", "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "hash": "0d34c6b7abc4049696dbf7f1a49500927cb8a14e2128cff5d6fcce9c7998ba43", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.postgres\n    options:\n      members:\n        - PostgresDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "194bdd24-6d15-4379-9d5c-b64bc4a27fae": {"__data__": {"id_": "194bdd24-6d15-4379-9d5c-b64bc4a27fae", "embedding": null, "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b68e2c3d86b8f398e85e23caaafedb1dd9002315", "node_type": "4", "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "hash": "eb45bb368b75e55e7eaeb288c99457359a91d2b3a0c838f6dc907668bf363c43", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.docstore.redis\n    options:\n      members:\n        - RedisDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c5ac026-e90c-42e0-9ea6-bad8f2771f33": {"__data__": {"id_": "9c5ac026-e90c-42e0-9ea6-bad8f2771f33", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfe822056fcba34b0701800668bb2eb19a7488ce", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "0c962e2082e7535b56446fc3905af5221313bbaa95832328b6d01c8710a8926c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.docstore\n    options:\n      members:\n        - SimpleDocumentStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b125ebb-320b-4ac0-8d40-d45fe3832c0a": {"__data__": {"id_": "1b125ebb-320b-4ac0-8d40-d45fe3832c0a", "embedding": null, "metadata": {"filename": "falkordb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12906a65f9a48692c714a887bb32d057e7e05259", "node_type": "4", "metadata": {"filename": "falkordb.md", "author": "LlamaIndex"}, "hash": "a9fc541aff14b14656a1c150258cc7488ab22600b4d805c63043c016b2412ec2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.graph_stores.falkordb\n    options:\n      members:\n        - FalkorDBGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0caaa87c-90f0-4303-8755-e9e235464353": {"__data__": {"id_": "0caaa87c-90f0-4303-8755-e9e235464353", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6e0160d9595777106da177d7f67f7320713653a", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "ebb56505fc0117d86acb3bf1a3d369926b94a4e149f68cac52c1ebbd91b80b66", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.graph_stores.types\n    options:\n      members:\n        - GraphStore\n        - DEFAULT_PERSIST_DIR\n        - DEFAULT_PERSIST_FNAME", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a09193c4-d601-46c7-9629-98fb2f3da823": {"__data__": {"id_": "a09193c4-d601-46c7-9629-98fb2f3da823", "embedding": null, "metadata": {"filename": "kuzu.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a19d228e43df1d13ae4600bc942cd8d65dc807f7", "node_type": "4", "metadata": {"filename": "kuzu.md", "author": "LlamaIndex"}, "hash": "c761ede8d5a0b1d3c98dbd54d601da7547351a47a423708e2815c651e6411d24", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.graph_stores.kuzu\n    options:\n      members:\n        - KuzuGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6059110-c9aa-4dee-a4f7-030268cc61eb": {"__data__": {"id_": "b6059110-c9aa-4dee-a4f7-030268cc61eb", "embedding": null, "metadata": {"filename": "nebula.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f01f3f46df5440243807f85939d147b0a030d6f", "node_type": "4", "metadata": {"filename": "nebula.md", "author": "LlamaIndex"}, "hash": "2cf223c6cb9f01a1bc4015a937e5742792fc7bd213c472abceaf1b406742d7c0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.graph_stores.nebula\n    options:\n      members:\n        - NebulaGraphStore\n        - NebulaPropertyGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "558faf48-0604-48cf-a0b9-4fb2058e12d7": {"__data__": {"id_": "558faf48-0604-48cf-a0b9-4fb2058e12d7", "embedding": null, "metadata": {"filename": "neo4j.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0957d156917f8cbba31d6ad54ca9442b165ad55b", "node_type": "4", "metadata": {"filename": "neo4j.md", "author": "LlamaIndex"}, "hash": "ca1164e6d88f81eaa3bc153954fb81b832979a6031701d689764d10c6b90451a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.graph_stores.neo4j\n    options:\n      members:\n        - Neo4jGraphStore\n        - Neo4jPropertyGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "768d520a-6033-487e-b820-878c6fc0e2c8": {"__data__": {"id_": "768d520a-6033-487e-b820-878c6fc0e2c8", "embedding": null, "metadata": {"filename": "neptune.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8be441642f4cb39603b91d41b37953e2d4d2f35f", "node_type": "4", "metadata": {"filename": "neptune.md", "author": "LlamaIndex"}, "hash": "1410ca12bdd84098d9ea4f896a7b9706189667476fc39bf1ebc0f6dafb1914b8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.graph_stores.neptune\n    options:\n      members:\n        - NeptuneAnalyticsGraphStore\n        - NeptuneDatabaseGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75856f2e-176a-4c1f-9b1c-adaa178b6a5b": {"__data__": {"id_": "75856f2e-176a-4c1f-9b1c-adaa178b6a5b", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c725b57f5fd2d75a3ea881c5684802e8f01b095", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "fb2e14fd34f7effe068c821980aac58ec034d68a00fd93f8bbd7c26103bbed00", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.graph_stores.simple\n    options:\n      members:\n        - SimpleGraphStore\n        - SimplePropertyGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33a7c7f5-ef53-40ba-8a15-a266b16190b9": {"__data__": {"id_": "33a7c7f5-ef53-40ba-8a15-a266b16190b9", "embedding": null, "metadata": {"filename": "tidb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6688e22be048a39850c496f2b862f5393ae9307b", "node_type": "4", "metadata": {"filename": "tidb.md", "author": "LlamaIndex"}, "hash": "3788f03aa413e8aa8c00d0b9edbb293054919f6435f486531c9e98472345486a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.graph_stores.tidb\n    options:\n      members:\n        - TiDBGraphStore\n        - TiDBPropertyGraphStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c2039c8-7e87-4f44-900a-3943e23cae22": {"__data__": {"id_": "6c2039c8-7e87-4f44-900a-3943e23cae22", "embedding": null, "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0627cf498dca4bf5ffa730dcf7794114a796079a", "node_type": "4", "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "hash": "6a4af1a13f2ee623008cf4b1475264bfb6eecfae9c5e81c952930a9dc26e4fb0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.azure\n    options:\n      members:\n        - AzureIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "806004c5-6aff-418e-8ffa-c5ab86443309": {"__data__": {"id_": "806004c5-6aff-418e-8ffa-c5ab86443309", "embedding": null, "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "787d85fe604d459eb8d701822189317931a7351c", "node_type": "4", "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "hash": "e665fb572b5190964344e2a35c319e9e0dc8b596b7414ab27a78f11953170ba0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.dynamodb\n    options:\n      members:\n        - DynamoDBIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21a064b2-5451-4da2-9b64-68fca1e19d86": {"__data__": {"id_": "21a064b2-5451-4da2-9b64-68fca1e19d86", "embedding": null, "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d60e2e025dd8b3e2934fec4dc9454493a40de692", "node_type": "4", "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "hash": "df152f2d00f9b26601dca2e7fedff343bbf1fda0628e283cf1e052ee3c1dd1e2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.elasticsearch\n    options:\n      members:\n        - ElasticsearchIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36f05abc-4090-4610-80c9-78853b6fb116": {"__data__": {"id_": "36f05abc-4090-4610-80c9-78853b6fb116", "embedding": null, "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "248d183b455a8698b2f33e7a761aa4d265c598d6", "node_type": "4", "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "hash": "36d4064046de1c63d7856883be65d635e3351701a558c3421c3f5ee48a8b6ab5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.firestore\n    options:\n      members:\n        - FirestoreIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "375d233a-3249-4a1c-a0d4-2ed8a455328d": {"__data__": {"id_": "375d233a-3249-4a1c-a0d4-2ed8a455328d", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5882e3c415a9bd436f9938faf7dbfa219a87165", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "fb514dff4c5d57123cecd671afd951a75feabdb27e9692afec06f4c0d3fa5bf5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.index_store.types\n\n::: llama_index.core.storage.index_store\n    options:\n      members:\n        - KVIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "831d6c7d-2cd5-4688-b99d-bcfeba52d8a7": {"__data__": {"id_": "831d6c7d-2cd5-4688-b99d-bcfeba52d8a7", "embedding": null, "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63efb122574d65025688e1a907992915dca4b7a3", "node_type": "4", "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "hash": "2d11cce65bdbec5db689a3fb84b51967e262aa739e380a7a2d04989c894746b4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.mongodb\n    options:\n      members:\n        - MongoIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "512aa655-a0c1-466a-a6ac-17a4bcfcd7e2": {"__data__": {"id_": "512aa655-a0c1-466a-a6ac-17a4bcfcd7e2", "embedding": null, "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de08126f1174e7be0e25e1aa7f925b6c667a9450", "node_type": "4", "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "hash": "db86d2efc254ceef98818c0683b32b30bdff5cd4382483d50263c4b5d347cbe0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.postgres\n    options:\n      members:\n        - PostgresIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c88ce24-694a-4d42-a170-87dfa7066b67": {"__data__": {"id_": "9c88ce24-694a-4d42-a170-87dfa7066b67", "embedding": null, "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4b4e1574388385e3ac26e03a09e6cf7f83c2d8d", "node_type": "4", "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "hash": "31d9d488e328ae2d04bacc81b07577a19fc24c6513c9abb086b88c5e463d4424", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.index_store.redis\n    options:\n      members:\n        - RedisIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b84ac2d9-3aaf-45d7-81f2-195683198b2a": {"__data__": {"id_": "b84ac2d9-3aaf-45d7-81f2-195683198b2a", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb4785a2b2c175d5a08789e35f0f15bb271b385d", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "696f0456ac1baf7ffe04e8fec748892dc8fb867fe19d0597d43323a4cf6a8234", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.index_store\n    options:\n      members:\n        - SimpleIndexStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94bfb0ec-ead7-4a53-9a35-9d17eb294488": {"__data__": {"id_": "94bfb0ec-ead7-4a53-9a35-9d17eb294488", "embedding": null, "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b841797df11b7f8afeeaa248503cceb137258f43", "node_type": "4", "metadata": {"filename": "azure.md", "author": "LlamaIndex"}, "hash": "f69ebb77db7f250021464ae1e3ba9ba76505d31f29609b89e4e041ca1503c951", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.azure\n    options:\n      members:\n        - AzureKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e273b43-8d3f-46f7-9aff-0a09a2b654b4": {"__data__": {"id_": "3e273b43-8d3f-46f7-9aff-0a09a2b654b4", "embedding": null, "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ddcca0436cd2835a84cbbdf8f60afa019d12d39", "node_type": "4", "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "hash": "594c2361a5225c8d105fd59ee74916da450a2be9cf6e524a77653f3180d081df", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.dynamodb\n    options:\n      members:\n        - DynamoDBKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c72cbea-dbee-4208-b66a-5f075a7cfb8e": {"__data__": {"id_": "8c72cbea-dbee-4208-b66a-5f075a7cfb8e", "embedding": null, "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a90fc658257dd07a5d033e0b8aa6d67c59f534b", "node_type": "4", "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "hash": "b741086b23271ec1907d407652da7fd274f1fdac31c9d3a681b8ff8f298cf73e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.elasticsearch\n    options:\n      members:\n        - ElasticsearchKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "793c8ed3-14e3-4585-b7b7-15003595deea": {"__data__": {"id_": "793c8ed3-14e3-4585-b7b7-15003595deea", "embedding": null, "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be0c036c100e39b8be69830850c0650299627b2e", "node_type": "4", "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "hash": "7c8da50d5d2037051d44bba77c974abde11d54433103549f8260a4dc7917f9cd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.firestore\n    options:\n      members:\n        - FirestoreKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "010f397c-af09-474e-a8f8-04df085eaae6": {"__data__": {"id_": "010f397c-af09-474e-a8f8-04df085eaae6", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a6bd621bfd10e80c467c4f51742663d4db4f1b4", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "53ebb72cf036ead4143791cd993a7b6c0d7c8b064ae47e8f3cfaf486d6c6516e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.kvstore.types", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 42, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9faec05-e494-4e0e-b16d-d8e1fc973848": {"__data__": {"id_": "c9faec05-e494-4e0e-b16d-d8e1fc973848", "embedding": null, "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40d22577b82504e8a7a79f2243f186ae3e80b8f3", "node_type": "4", "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "hash": "88dd13671dd59434aecfe6304c850c96a864da7d1c9b1a7cf77fa8eb2b28728b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.mongodb\n    options:\n      members:\n        - MongoDBKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c277181a-1787-48de-b9fd-74b5850d48e4": {"__data__": {"id_": "c277181a-1787-48de-b9fd-74b5850d48e4", "embedding": null, "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58c4d9506b715838d23c4080080e0287d0cbd4f2", "node_type": "4", "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "hash": "256c318d2de380b629a4bc2259e60beb14839f6de10b8a0672e06b48eb6fbff2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.postgres\n    options:\n      members:\n        - PostgresKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b69d7541-17a3-4d7f-82bf-aa048bc36222": {"__data__": {"id_": "b69d7541-17a3-4d7f-82bf-aa048bc36222", "embedding": null, "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d26b914dab720218475d97931dbbd716a46f6b4", "node_type": "4", "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "hash": "c70a272081444a0a26b074ec95b141dc88a92798371c2b58e8956ddf1783649d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.redis\n    options:\n      members:\n        - RedisKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5aa74481-7ebd-457b-95e8-018977e5f191": {"__data__": {"id_": "5aa74481-7ebd-457b-95e8-018977e5f191", "embedding": null, "metadata": {"filename": "s3.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2013198a78917e0ec6b303d6641d7cff6d026f5", "node_type": "4", "metadata": {"filename": "s3.md", "author": "LlamaIndex"}, "hash": "dbe039e768ad1f2a0260ce29646b1162e649c4f13df0e7fdd2637dac6482dc10", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.storage.kvstore.s3\n    options:\n      members:\n        - S3DBKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80d7d37b-d124-473a-a790-b215b9ba30fb": {"__data__": {"id_": "80d7d37b-d124-473a-a790-b215b9ba30fb", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "687787325c1484d6202c85754c9b28261432d8b5", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "983c459dba5ed0b4caf3a94166e5309b6a826266711e76819cfcc4103b3dbd71", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.kvstore\n    options:\n      members:\n        - SimpleKVStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e637ebb-628a-49d9-8edf-5dbfc26215d2": {"__data__": {"id_": "4e637ebb-628a-49d9-8edf-5dbfc26215d2", "embedding": null, "metadata": {"filename": "storage_context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca0f6e260c988c093738f5ca5c5e50d2211f5301", "node_type": "4", "metadata": {"filename": "storage_context.md", "author": "LlamaIndex"}, "hash": "f5c224ee1ec41781fb6f45b7b4b4f4d333be5f7d758d2237ed360ad97ada2275", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.storage.storage_context\n    options:\n      members:\n        - StorageContext\n\n::: llama_index.core\n    options:\n      members:\n        - load_index_from_storage\n        - load_indices_from_storage", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "226f32a7-2db5-4f0e-9ac4-f94ceb3435b4": {"__data__": {"id_": "226f32a7-2db5-4f0e-9ac4-f94ceb3435b4", "embedding": null, "metadata": {"filename": "alibabacloud_opensearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8333c9678e05f23879715e9ee3ef0b5eda62ce17", "node_type": "4", "metadata": {"filename": "alibabacloud_opensearch.md", "author": "LlamaIndex"}, "hash": "3091eefbfa9753c82727666ea879e32a642b99a44c760a9be9f762cc3a307504", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.alibabacloud_opensearch\n    options:\n      members:\n        - AlibabaCloudOpenSearch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14b092f1-14c6-4a2a-8518-b3014cb01c7d": {"__data__": {"id_": "14b092f1-14c6-4a2a-8518-b3014cb01c7d", "embedding": null, "metadata": {"filename": "analyticdb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9652e5f424891c692db6c172ca4e6d7dda6d697", "node_type": "4", "metadata": {"filename": "analyticdb.md", "author": "LlamaIndex"}, "hash": "68e2dd84b37c6c622282e5532b129c511f48328b29eefc1dbfc1a42c1b0d364f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.analyticdb\n    options:\n      members:\n        - AnalyticDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11eea536-1df7-4f48-9767-b3f5b44c7c4f": {"__data__": {"id_": "11eea536-1df7-4f48-9767-b3f5b44c7c4f", "embedding": null, "metadata": {"filename": "astra_db.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc2a34a3601b85df46b81895ae1dad043b8e116a", "node_type": "4", "metadata": {"filename": "astra_db.md", "author": "LlamaIndex"}, "hash": "d3b036f1870faa74f7201d6464b91ef06149e9c73d44fade01f079bdcac17435", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.astra_db\n    options:\n      members:\n        - AstraDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "319b29c8-77b4-4dd5-9b32-78403002b385": {"__data__": {"id_": "319b29c8-77b4-4dd5-9b32-78403002b385", "embedding": null, "metadata": {"filename": "awadb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18c6d661141a680cb3f296e36c68176d31347dd2", "node_type": "4", "metadata": {"filename": "awadb.md", "author": "LlamaIndex"}, "hash": "5dfcecb3e11a254021ea4b322555b7a60f616d97b180b54d77a169b550db16f2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.awadb\n    options:\n      members:\n        - AwaDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7096f605-b37d-4d46-ba65-8ff293460899": {"__data__": {"id_": "7096f605-b37d-4d46-ba65-8ff293460899", "embedding": null, "metadata": {"filename": "awsdocdb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0eb48bb8ddbb48be83773877275d9c7412f52b4", "node_type": "4", "metadata": {"filename": "awsdocdb.md", "author": "LlamaIndex"}, "hash": "139f26f8e39b0702c649454427d65d4fa98f17259ca40b593bcbe5368bd06b8f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.awsdocdb\n    options:\n      members:\n        - AWSDocDbVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86043378-1a60-4cdc-845c-97d47a44645a": {"__data__": {"id_": "86043378-1a60-4cdc-845c-97d47a44645a", "embedding": null, "metadata": {"filename": "azureaisearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1acd7a0db42fdd0b1fb85e19766a17f8c884cf5", "node_type": "4", "metadata": {"filename": "azureaisearch.md", "author": "LlamaIndex"}, "hash": "0b7645430bd5321cdc744a9d8cff62dd0c790403f6fe15d0ff652e89d9e50d2d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.azureaisearch\n    options:\n      members:\n        - AzureAISearchVectorStore\n        - CognitiveSearchVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfa72da0-5c5e-4351-a7b1-e206464c5413": {"__data__": {"id_": "bfa72da0-5c5e-4351-a7b1-e206464c5413", "embedding": null, "metadata": {"filename": "azurecosmosmongo.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3208fc943b3ea51088f4d3e785abec789f43b84", "node_type": "4", "metadata": {"filename": "azurecosmosmongo.md", "author": "LlamaIndex"}, "hash": "5a00cebd974cc842f9a5c90a25ad524ff3d201eccc4671c8d3eacf92c60a4fac", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.azurecosmosmongo\n    options:\n      members:\n        - AzureCosmosDBMongoDBVectorSearch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37b28f43-6dbc-470f-9406-c2f1345a3210": {"__data__": {"id_": "37b28f43-6dbc-470f-9406-c2f1345a3210", "embedding": null, "metadata": {"filename": "bagel.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "439d94ba03ab39cbb75c59ffa30edecf4cb193f8", "node_type": "4", "metadata": {"filename": "bagel.md", "author": "LlamaIndex"}, "hash": "c615a265443dd0be691f2f5cfa06f9212e93d2948fc8d42daf9f32897bacbb16", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.bagel\n    options:\n      members:\n        - BagelVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75ca0cb7-2297-4fc4-ae29-25180bef664e": {"__data__": {"id_": "75ca0cb7-2297-4fc4-ae29-25180bef664e", "embedding": null, "metadata": {"filename": "baiduvectordb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1db993e7176bbdf18529febf41278e148c0972a5", "node_type": "4", "metadata": {"filename": "baiduvectordb.md", "author": "LlamaIndex"}, "hash": "e8db873ffdafe47b806d8abd453fef237b3471caf645a0e784fd81d88f475ba8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.baiduvectordb\n    options:\n      members:\n        - BaiduVectorDB", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9f4c44f-a239-416a-b664-c474b4011708": {"__data__": {"id_": "a9f4c44f-a239-416a-b664-c474b4011708", "embedding": null, "metadata": {"filename": "cassandra.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d340a58d8ba5237037a3ee74e87a2f8ca8f7a00f", "node_type": "4", "metadata": {"filename": "cassandra.md", "author": "LlamaIndex"}, "hash": "a84b44076531567567cf3fc2a201d3c929c827ba7017a4f0e1ddc6eb1f1da5b0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.cassandra\n    options:\n      members:\n        - CassandraVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67cbe419-72b2-4a98-99c0-64200f904c13": {"__data__": {"id_": "67cbe419-72b2-4a98-99c0-64200f904c13", "embedding": null, "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76a756c480190a3eaef63bbf41e265fc24e35103", "node_type": "4", "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}, "hash": "676b4718dbc2691b4e9b94cb2a30e24521c77c55b85740cbdbaa6aeff0a96da9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.chatgpt_plugin\n    options:\n      members:\n        - ChatGPTRetrievalPluginClient", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8282ce5d-9667-4409-832a-46d88a9b6388": {"__data__": {"id_": "8282ce5d-9667-4409-832a-46d88a9b6388", "embedding": null, "metadata": {"filename": "chroma.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "808aeb5f89693308bec1e838703ae74cad8e261c", "node_type": "4", "metadata": {"filename": "chroma.md", "author": "LlamaIndex"}, "hash": "40c72619afdb30a45555f5af8b2a17240d117d284f9d849e8801292bd0929e8d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.chroma\n    options:\n      members:\n        - ChromaVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24544fd9-7b81-4b7c-ad90-2c2fea80a0a7": {"__data__": {"id_": "24544fd9-7b81-4b7c-ad90-2c2fea80a0a7", "embedding": null, "metadata": {"filename": "clickhouse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95060b04c5679701bcfd61ab2207cd94877949af", "node_type": "4", "metadata": {"filename": "clickhouse.md", "author": "LlamaIndex"}, "hash": "1c12a7f8b457c484cb18233e3b0435f9d4296b36fe1a6cc7e2819a0125f8a0fd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.clickhouse\n    options:\n      members:\n        - ClickHouseVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1120ddac-d410-4378-8d16-f6bf3906a0ca": {"__data__": {"id_": "1120ddac-d410-4378-8d16-f6bf3906a0ca", "embedding": null, "metadata": {"filename": "couchbase.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86516421e1454770de5799541478fcf17790116d", "node_type": "4", "metadata": {"filename": "couchbase.md", "author": "LlamaIndex"}, "hash": "4223922a67c7725537ddb89a625f9f5e965e615cce2959e9e60c1a527f41ff08", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.couchbase\n    options:\n      members:\n        - CouchbaseVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a595a12-802e-4576-8cdd-926152f2d439": {"__data__": {"id_": "7a595a12-802e-4576-8cdd-926152f2d439", "embedding": null, "metadata": {"filename": "dashvector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07170183cfb921c22440563582c084d7d5e6dd77", "node_type": "4", "metadata": {"filename": "dashvector.md", "author": "LlamaIndex"}, "hash": "21d1094f4639120cd430f25d8ee15d5a17604a3e2cba18f156cee97c564b8eb9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.dashvector\n    options:\n      members:\n        - DashVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "070f3a8a-072e-4dc9-aa0a-bc2a42292380": {"__data__": {"id_": "070f3a8a-072e-4dc9-aa0a-bc2a42292380", "embedding": null, "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28806b8812ef1b644dc04237edf9d138b5d4ba20", "node_type": "4", "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}, "hash": "f024b2b95c24a21df59f112513352b8933f5a9772e784ba55e590c029ec1e2ba", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.databricks\n    options:\n      members:\n        - DatabricksVectorSearch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55ff537d-0ee7-49ed-a9f2-dfae42ea4593": {"__data__": {"id_": "55ff537d-0ee7-49ed-a9f2-dfae42ea4593", "embedding": null, "metadata": {"filename": "deeplake.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2b05684b9db920793a0dc780850eb5d00e8e6f4", "node_type": "4", "metadata": {"filename": "deeplake.md", "author": "LlamaIndex"}, "hash": "d2ea69d2588620aeff7071ed2c67db51664f06e18ea03b268681c12ba422c78c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.deeplake\n    options:\n      members:\n        - DeepLakeVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bdf83bf-eb4c-4bf3-8b18-74701839ba23": {"__data__": {"id_": "0bdf83bf-eb4c-4bf3-8b18-74701839ba23", "embedding": null, "metadata": {"filename": "docarray.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0ad4a194f26680b8c109f5f9ff8c4375d3f561ba", "node_type": "4", "metadata": {"filename": "docarray.md", "author": "LlamaIndex"}, "hash": "8e1c1ba1922ebd0e680d3055f575a10e4d2f263754632c14357b7ae79a7e9a75", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.docarray\n    options:\n      members:\n        - DocArrayHnswVectorStore\n        - DocArrayInMemoryVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77dabd47-c47f-4b37-a88c-4535d34a9953": {"__data__": {"id_": "77dabd47-c47f-4b37-a88c-4535d34a9953", "embedding": null, "metadata": {"filename": "duckdb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4027451bcf03e4e37ec1cb57328d4268f575ff71", "node_type": "4", "metadata": {"filename": "duckdb.md", "author": "LlamaIndex"}, "hash": "cc0276a2b8d41197f03b2b3bab414ba6acc5bf2f8658258e841386ed83378f87", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.duckdb\n    options:\n      members:\n        - DuckDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "caf74100-f71f-40cd-98e6-5bcbac6525f1": {"__data__": {"id_": "caf74100-f71f-40cd-98e6-5bcbac6525f1", "embedding": null, "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "803443512a8d0db713ab54ce30daf529c1a5065e", "node_type": "4", "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}, "hash": "eef3b99345de959b1a460c893c0314de0d2f1532ec1fb6e6f73353dbae86c2a1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.dynamodb\n    options:\n      members:\n        - DynamoDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a1a42ff-56dc-4a6d-b92b-dfb7791d824a": {"__data__": {"id_": "7a1a42ff-56dc-4a6d-b92b-dfb7791d824a", "embedding": null, "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a6e63b8cf7dfeaeb6915069e417ff32562e9d00", "node_type": "4", "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}, "hash": "ea6a99dc079ae960abf9103043869acdf974af0dec02b12a929dffd7ffeeb674", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.elasticsearch\n    options:\n      members:\n        - ElasticsearchStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe93cfd2-fe63-4b92-b996-d1f4acbc5fde": {"__data__": {"id_": "fe93cfd2-fe63-4b92-b996-d1f4acbc5fde", "embedding": null, "metadata": {"filename": "epsilla.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ef6af0468ba5c83ad8616fd047305b4213227c2", "node_type": "4", "metadata": {"filename": "epsilla.md", "author": "LlamaIndex"}, "hash": "970ee6f007ec4a710b32723126097b15627f108c7035ed788d6b29d2d0ae37e9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.epsilla\n    options:\n      members:\n        - EpsillaVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e14b8ee2-b91b-4858-8953-1c0041f4cdc8": {"__data__": {"id_": "e14b8ee2-b91b-4858-8953-1c0041f4cdc8", "embedding": null, "metadata": {"filename": "faiss.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "506b92e5e454cd2d09b924f620dc9b22f89a9e3e", "node_type": "4", "metadata": {"filename": "faiss.md", "author": "LlamaIndex"}, "hash": "5fbb2a6036379548d1ed7588becfe8e44d2882ca663ad2c824766ec5e5094e59", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.faiss\n    options:\n      members:\n        - FaissVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9566af8-6bbf-446f-8267-546fa702b12d": {"__data__": {"id_": "b9566af8-6bbf-446f-8267-546fa702b12d", "embedding": null, "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5f1f3cfd2e443207c801c5131ffafbf775309ba", "node_type": "4", "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}, "hash": "18c7e6093d8a0f057ff1f5e1e80d65a2cc3d5475133dbf1a36429e50539be12c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.firestore\n    options:\n      members:\n        - FirestoreVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8654ce25-2581-4174-8f49-e998e0de68c0": {"__data__": {"id_": "8654ce25-2581-4174-8f49-e998e0de68c0", "embedding": null, "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7df120054cd53f159a9196a9132aa7b1c5c66eeb", "node_type": "4", "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "hash": "03207735b9a567862193ff4bb9434a0f55191d32d29534dab6669af84deac106", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.google\n    options:\n      members:\n        - GoogleVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "631ad8ee-72f7-4dbe-b01c-4f3bf4b50cc6": {"__data__": {"id_": "631ad8ee-72f7-4dbe-b01c-4f3bf4b50cc6", "embedding": null, "metadata": {"filename": "hologres.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f87712504009f42009fdbc7a2363b6e5419dc7e", "node_type": "4", "metadata": {"filename": "hologres.md", "author": "LlamaIndex"}, "hash": "fbef861abcf7d01329b74e2b8f539c6bfc62ad73ca8ca52dd600d92ca2e676f5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.hologres\n    options:\n      members:\n        - HologresVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1bf0c83-d82d-48d7-acbc-ecc74fc23fc9": {"__data__": {"id_": "f1bf0c83-d82d-48d7-acbc-ecc74fc23fc9", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3788d660894b98e477bb25c8f4e9ce36d4adab91", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "712d403289fcf214cf8eb2cd23ef32edad7354c2e5b2b5fbe6c97c8fe14ee1a2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.vector_stores.types", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 40, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8841b879-49b3-41d5-92ac-cd7735ddee83": {"__data__": {"id_": "8841b879-49b3-41d5-92ac-cd7735ddee83", "embedding": null, "metadata": {"filename": "jaguar.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3172e0bc5c79c2938ae36e2dc7a3858f7c08d23c", "node_type": "4", "metadata": {"filename": "jaguar.md", "author": "LlamaIndex"}, "hash": "4800a2b82ab37b99829f28f5a65b8a6fb7ab6e874fd6f11470a635dc42d23266", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.jaguar\n    options:\n      members:\n        - JaguarVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "232f6b0b-662e-4255-a6e3-c92253dc6097": {"__data__": {"id_": "232f6b0b-662e-4255-a6e3-c92253dc6097", "embedding": null, "metadata": {"filename": "kdbai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2bddfc7e89764b1d4ee5676c9283ba0fe28afc2", "node_type": "4", "metadata": {"filename": "kdbai.md", "author": "LlamaIndex"}, "hash": "8086e2a6dc01b859776d758d65d1e11aea7c83783f12b36ccec24b391ec4d53b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.kdbai\n    options:\n      members:\n        - KDBAIVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c663bfaf-9c97-46b2-bde1-7ce5c16d04be": {"__data__": {"id_": "c663bfaf-9c97-46b2-bde1-7ce5c16d04be", "embedding": null, "metadata": {"filename": "lancedb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61e97f04829c81c62ff5872a3b457bac48185766", "node_type": "4", "metadata": {"filename": "lancedb.md", "author": "LlamaIndex"}, "hash": "2706f8295b26f5ba2e4f2aee761d80a7ed641befec72538f5b034ef5ffcb4b57", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.lancedb\n    options:\n      members:\n        - LanceDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ed42e14-59b8-446b-ab8e-be1da4641135": {"__data__": {"id_": "9ed42e14-59b8-446b-ab8e-be1da4641135", "embedding": null, "metadata": {"filename": "lantern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4648d8f9b42fdc6d4e673a96f2315a0ccb274592", "node_type": "4", "metadata": {"filename": "lantern.md", "author": "LlamaIndex"}, "hash": "92e033996d1b5bd5a43da87b1ace0d993fd6a8f27e07849ed882968a3322149e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.lantern\n    options:\n      members:\n        - LanternVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f268372-ead7-410d-8302-be5ff5e8977d": {"__data__": {"id_": "2f268372-ead7-410d-8302-be5ff5e8977d", "embedding": null, "metadata": {"filename": "metal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d68ffa95fc716fe5f74d71a3e9a8175f5dc7927", "node_type": "4", "metadata": {"filename": "metal.md", "author": "LlamaIndex"}, "hash": "2ea4457a478d32f852900cf818e2e2311d18c963db2a64dba2ee68c8055c8d64", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.metal\n    options:\n      members:\n        - MetalVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1996aa83-1ac4-405f-9342-07ad989996ba": {"__data__": {"id_": "1996aa83-1ac4-405f-9342-07ad989996ba", "embedding": null, "metadata": {"filename": "milvus.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b0eca2344dbf2f17da0cab04601db9f32d8882b", "node_type": "4", "metadata": {"filename": "milvus.md", "author": "LlamaIndex"}, "hash": "6e26beed250814903c2d4410a072807fed7e449e60110a59cca91bc198d924e6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.milvus\n    options:\n      members:\n        - MilvusVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a9e948d-6200-4ea2-aec9-9230e46e45f2": {"__data__": {"id_": "4a9e948d-6200-4ea2-aec9-9230e46e45f2", "embedding": null, "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd90659e92e62339e45e814e4dfe6aa8cc1eb171", "node_type": "4", "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}, "hash": "e67b88c5fbddba09b411bb54b0689d13beec1654c19e9c60567cb108be560cc4", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.mongodb\n    options:\n      members:\n        - MongoDBAtlasVectorSearch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13ad0d37-77d5-49ca-9d4f-0aaf6ae3a615": {"__data__": {"id_": "13ad0d37-77d5-49ca-9d4f-0aaf6ae3a615", "embedding": null, "metadata": {"filename": "myscale.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "605595d2f0969942898003c5b8e44000a29540bb", "node_type": "4", "metadata": {"filename": "myscale.md", "author": "LlamaIndex"}, "hash": "57b9dc906755abc299691d0fcd75a8cce6671f06661a63302490ab1c45b62e44", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.myscale\n    options:\n      members:\n        - MyScaleVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11e787e0-62e8-4511-8557-ff5e2b569398": {"__data__": {"id_": "11e787e0-62e8-4511-8557-ff5e2b569398", "embedding": null, "metadata": {"filename": "neo4jvector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "554d048e98a3ca0268668636b9c59006464b6d81", "node_type": "4", "metadata": {"filename": "neo4jvector.md", "author": "LlamaIndex"}, "hash": "1057965b5ed28ce6b11290979621cad03b2acbe04efdc430a2da88c955936feb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.neo4jvector\n    options:\n      members:\n        - Neo4jVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "473fe75f-af2e-414e-9dab-b645bc359f13": {"__data__": {"id_": "473fe75f-af2e-414e-9dab-b645bc359f13", "embedding": null, "metadata": {"filename": "neptune.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26b57e6987b8353385d4a5b17747660ee8a108f0", "node_type": "4", "metadata": {"filename": "neptune.md", "author": "LlamaIndex"}, "hash": "fda9520509e82270da0d48cee60689f995cb4d84b28c0c473d8a1f0e184bb128", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.neptune\n    options:\n      members:\n        - NeptuneAnalyticsVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0121c8c-949c-4e63-804d-62f8bdb99bd4": {"__data__": {"id_": "d0121c8c-949c-4e63-804d-62f8bdb99bd4", "embedding": null, "metadata": {"filename": "opensearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e4b44ec6341dd51d58750aeafc652125870f289", "node_type": "4", "metadata": {"filename": "opensearch.md", "author": "LlamaIndex"}, "hash": "6b5f953fcf16871533de4d08c262877213a3ae416f7bf3b6cd1f0fe40d567954", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.opensearch\n    options:\n      members:\n        - OpensearchVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53d457a5-5104-4276-ada5-07ff7220f2e1": {"__data__": {"id_": "53d457a5-5104-4276-ada5-07ff7220f2e1", "embedding": null, "metadata": {"filename": "pgvecto_rs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b79974b6688f3972cf1cf3729a6442f735961484", "node_type": "4", "metadata": {"filename": "pgvecto_rs.md", "author": "LlamaIndex"}, "hash": "2334e82d255c24390d2eb2758d17e02c6a5bc6aa15311609116908dc8d14befb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.pgvecto_rs\n    options:\n      members:\n        - PGVectoRsStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1728e364-cbcd-42f6-bb07-da14d606cf00": {"__data__": {"id_": "1728e364-cbcd-42f6-bb07-da14d606cf00", "embedding": null, "metadata": {"filename": "pinecone.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eab5be5bb35894a3bbf43ea70fa2fb127b5b06fc", "node_type": "4", "metadata": {"filename": "pinecone.md", "author": "LlamaIndex"}, "hash": "1b29a0b1de05690dd791845396e916eb8335adfd8aba06eb68f9771659550ab9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.pinecone\n    options:\n      members:\n        - PineconeVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af8f08bb-5153-4964-aed7-82cb33682a4e": {"__data__": {"id_": "af8f08bb-5153-4964-aed7-82cb33682a4e", "embedding": null, "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f29d4590fb52378bce5e54c338eec94fd7ab1d32", "node_type": "4", "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}, "hash": "1f5cd252fa804c9d0991534ea3298401b8fac548ea18f17dd526b290696015be", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.postgres\n    options:\n      members:\n        - PGVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "153f1623-ea76-4890-a5a3-e108b9777ba2": {"__data__": {"id_": "153f1623-ea76-4890-a5a3-e108b9777ba2", "embedding": null, "metadata": {"filename": "qdrant.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46568006c34d31a56e87db66b658249b4c228b83", "node_type": "4", "metadata": {"filename": "qdrant.md", "author": "LlamaIndex"}, "hash": "5e215e8e7157de560d6a49cc1b9bc2fd7461ee4d98d31e1bda9f60db10b57f41", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.qdrant\n    options:\n      members:\n        - QdrantVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64c354d2-0bfe-4592-af71-1cc7ca1eb177": {"__data__": {"id_": "64c354d2-0bfe-4592-af71-1cc7ca1eb177", "embedding": null, "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92e937765c2976bba8a932c68b3888f7c90757fa", "node_type": "4", "metadata": {"filename": "redis.md", "author": "LlamaIndex"}, "hash": "32328192c9c53581cf30e5f07afff9f2592cbd5290a3d1195971635246a5889e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.redis\n    options:\n      members:\n        - RedisVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4354adc7-ba37-4b59-b14c-339e7c04d34f": {"__data__": {"id_": "4354adc7-ba37-4b59-b14c-339e7c04d34f", "embedding": null, "metadata": {"filename": "relyt.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b097cf514f5e3880a331c1e2cc04a12965e68b0", "node_type": "4", "metadata": {"filename": "relyt.md", "author": "LlamaIndex"}, "hash": "ac224a6cb2fad34e331b450463a92525bb9d36b7450ce9811661b3ac7941ac0a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.relyt\n    options:\n      members:\n        - RelytVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26019d6e-4307-4954-9029-1a6e5c9f31ea": {"__data__": {"id_": "26019d6e-4307-4954-9029-1a6e5c9f31ea", "embedding": null, "metadata": {"filename": "rocksetdb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "365a4700e223b622add53e6f2027ff39675894e7", "node_type": "4", "metadata": {"filename": "rocksetdb.md", "author": "LlamaIndex"}, "hash": "4bb63161db68b3fe7db4231a8f9926a129f2850229a05b394d18bafc3795315b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.rocksetdb\n    options:\n      members:\n        - RocksetVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f767aae3-538e-47d4-ab53-ad493925e9b5": {"__data__": {"id_": "f767aae3-538e-47d4-ab53-ad493925e9b5", "embedding": null, "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa4dbadcd55c85e01c59e5436fc6cdf8fe47f068", "node_type": "4", "metadata": {"filename": "simple.md", "author": "LlamaIndex"}, "hash": "e5d52da2bd20d2166938e40b257ef37dbb49383472377108e75a24bef4234497", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.vector_stores.simple\n    options:\n      members:\n        - SimpleVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 97, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "deb48aef-b743-455f-b406-f1b108dc9cdb": {"__data__": {"id_": "deb48aef-b743-455f-b406-f1b108dc9cdb", "embedding": null, "metadata": {"filename": "singlestoredb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e320c4588c63c842c5aa47a3102599daf7826f1", "node_type": "4", "metadata": {"filename": "singlestoredb.md", "author": "LlamaIndex"}, "hash": "051736ac5e6043bc3e34792ebd9d3b257d7a8c8edc4c549233780376e27074b0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.singlestoredb\n    options:\n      members:\n        - SingleStoreVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dce786da-1673-42f9-9588-d1b4b101e2b4": {"__data__": {"id_": "dce786da-1673-42f9-9588-d1b4b101e2b4", "embedding": null, "metadata": {"filename": "supabase.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dacd27281e38a51183d1afc83ab5888030f354b0", "node_type": "4", "metadata": {"filename": "supabase.md", "author": "LlamaIndex"}, "hash": "4fe8ca1640712583a7eed8ba1cc9b303e075524e94ea946a5ea5e430cb72a6b0", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.supabase\n    options:\n      members:\n        - SupabaseVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6072ae69-8089-42d0-93e2-51ceb9919424": {"__data__": {"id_": "6072ae69-8089-42d0-93e2-51ceb9919424", "embedding": null, "metadata": {"filename": "tair.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4931c57e26726934cc7ae7e8824d8a1dd2b4c26", "node_type": "4", "metadata": {"filename": "tair.md", "author": "LlamaIndex"}, "hash": "ef945ab7cdbf148d4075c5f045bd493e111329cbbb3b2d1acda6cecc9c885020", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.tair\n    options:\n      members:\n        - TairVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96753138-f47b-4849-8860-ca567be9017c": {"__data__": {"id_": "96753138-f47b-4849-8860-ca567be9017c", "embedding": null, "metadata": {"filename": "tencentvectordb.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71b6f40101c4f42bbf4109f7f67e247463d452d0", "node_type": "4", "metadata": {"filename": "tencentvectordb.md", "author": "LlamaIndex"}, "hash": "159c98bd1714a9431698064471cf00a3a3185a04b2b3e4ee56707913d7d73056", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.tencentvectordb\n    options:\n      members:\n        - TencentVectorDB", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be740b43-06f2-43b2-8cc7-bdc7faeb3ef5": {"__data__": {"id_": "be740b43-06f2-43b2-8cc7-bdc7faeb3ef5", "embedding": null, "metadata": {"filename": "tidbvector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f091bb33b841ae45b35a0760e114bf1a89cd1e6", "node_type": "4", "metadata": {"filename": "tidbvector.md", "author": "LlamaIndex"}, "hash": "c59141a84bc725bfae57274a785712abe09c8a332cedd068c931f758a322f490", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.tidbvector\n    options:\n      members:\n        - TiDBVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61896bf3-20a9-4827-8a55-eed0ac4804ec": {"__data__": {"id_": "61896bf3-20a9-4827-8a55-eed0ac4804ec", "embedding": null, "metadata": {"filename": "timescalevector.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d63e36d36fdec2f24b4e612b49e0acd3124bae93", "node_type": "4", "metadata": {"filename": "timescalevector.md", "author": "LlamaIndex"}, "hash": "f091c96a99426c138ebcce281ac02c4c85391b769f92239abf5883146cb5babb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.timescalevector\n    options:\n      members:\n        - TimescaleVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d562cd8-accc-4bd3-a49d-d4df85dff903": {"__data__": {"id_": "9d562cd8-accc-4bd3-a49d-d4df85dff903", "embedding": null, "metadata": {"filename": "txtai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc03ae558d691e71ded8ce6be00440f0e0cc82ea", "node_type": "4", "metadata": {"filename": "txtai.md", "author": "LlamaIndex"}, "hash": "f10fe33e0dc994a6603382f1147c65c6c38556ee67eb5a16836d611fffbad113", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.txtai\n    options:\n      members:\n        - TxtaiVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebb4cc0e-a293-448d-9ca7-dd78f36c11ec": {"__data__": {"id_": "ebb4cc0e-a293-448d-9ca7-dd78f36c11ec", "embedding": null, "metadata": {"filename": "typesense.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fd22331dfbb5aedcb66650611a212328d9f30b8", "node_type": "4", "metadata": {"filename": "typesense.md", "author": "LlamaIndex"}, "hash": "c932dc06cb4f07b90a245907031a8e63a499af7692b04b814dfddf073aa53f69", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.typesense\n    options:\n      members:\n        - TypesenseVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d73a4bb-6564-4640-9f72-e1b93cb697ce": {"__data__": {"id_": "5d73a4bb-6564-4640-9f72-e1b93cb697ce", "embedding": null, "metadata": {"filename": "upstash.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f9be5197348b83a091314dfdbe2b0ea17783fd1", "node_type": "4", "metadata": {"filename": "upstash.md", "author": "LlamaIndex"}, "hash": "9ff98bc14869c86c5893f99db68159222df866cf5efa10ecfbfc8fab2fd05929", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.upstash\n    options:\n      members:\n        - UpstashVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c6d08ce-85e3-4a32-a582-b77d98fee3e1": {"__data__": {"id_": "2c6d08ce-85e3-4a32-a582-b77d98fee3e1", "embedding": null, "metadata": {"filename": "vearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd572c3108df403ede670e66a9fe1dfe77105ccb", "node_type": "4", "metadata": {"filename": "vearch.md", "author": "LlamaIndex"}, "hash": "cdcc3a75dcc5a5b998c35df5eda811a6aba0b00d03dd110f0685e00f4894ffce", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.vearch\n    options:\n      members:\n        - VearchVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cfbd38c-09b7-48ee-9998-3c748045b144": {"__data__": {"id_": "6cfbd38c-09b7-48ee-9998-3c748045b144", "embedding": null, "metadata": {"filename": "vertexaivectorsearch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "542bf85fb706637da54ddea9e8f5a25f7cf8d73a", "node_type": "4", "metadata": {"filename": "vertexaivectorsearch.md", "author": "LlamaIndex"}, "hash": "72aee79e31d9447554106efb75dedcb377d75fee247002f9d696656bcf2d7531", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.vertexaivectorsearch\n    options:\n      members:\n        - VertexAIVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5345a1ef-26be-4e9e-9732-f5df28501c18": {"__data__": {"id_": "5345a1ef-26be-4e9e-9732-f5df28501c18", "embedding": null, "metadata": {"filename": "vespa.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1727c5dcf1859ade43c55464441430347ec33a10", "node_type": "4", "metadata": {"filename": "vespa.md", "author": "LlamaIndex"}, "hash": "7d5c6d31cb45c0db3e1dc90bca45b465931bbc1c0b0f36b67c8b8a593f10b5ff", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.vespa\n    options:\n      members:\n        - VespaVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23188879-d580-4d78-ac38-662f1fef9267": {"__data__": {"id_": "23188879-d580-4d78-ac38-662f1fef9267", "embedding": null, "metadata": {"filename": "weaviate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba6220c4d1135e18526b108ac742322b1014217d", "node_type": "4", "metadata": {"filename": "weaviate.md", "author": "LlamaIndex"}, "hash": "a71e7b2c8d28751a188952a69bebac57da68dd30bbcd7c62aa2e2c7fedbe4f9e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.weaviate\n    options:\n      members:\n        - WeaviateVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2956c786-241c-48b9-9db1-3043e1b5943c": {"__data__": {"id_": "2956c786-241c-48b9-9db1-3043e1b5943c", "embedding": null, "metadata": {"filename": "wordlift.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be9a7e4fefe33933328aef0d5cf5dfcf2b4dffda", "node_type": "4", "metadata": {"filename": "wordlift.md", "author": "LlamaIndex"}, "hash": "ee6e62f6c0791dbd49bfe6960e0fbc02bce3a097e36c75757e3ccb554ac78089", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.wordlift\n    options:\n      members:\n        - WordliftVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f9265d3-13f6-4cad-90f4-ee5c76789d09": {"__data__": {"id_": "0f9265d3-13f6-4cad-90f4-ee5c76789d09", "embedding": null, "metadata": {"filename": "zep.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffcdba15338dfc47c9283667b90c8f66a5c9b8a9", "node_type": "4", "metadata": {"filename": "zep.md", "author": "LlamaIndex"}, "hash": "a985528815cabe8d6215eeb7da144e4aaf82cd2074d8ca3dfae02b594d9098f3", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.vector_stores.zep\n    options:\n      members:\n        - ZepVectorStore", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76c91302-7080-434b-9d4d-d62a08560487": {"__data__": {"id_": "76c91302-7080-434b-9d4d-d62a08560487", "embedding": null, "metadata": {"filename": "arxiv.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b4a3fc749f4d048c2ea6204c892209ebea3eb98", "node_type": "4", "metadata": {"filename": "arxiv.md", "author": "LlamaIndex"}, "hash": "f4f13af8cc37e14762e7705abe68a29db678f76222b0b5a43051cc080b0aab35", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.arxiv\n    options:\n      members:\n        - ArxivToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "922b9b4d-8561-4fda-acb3-861af6969d67": {"__data__": {"id_": "922b9b4d-8561-4fda-acb3-861af6969d67", "embedding": null, "metadata": {"filename": "azure_code_interpreter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e2f71de0f7d7a9a07ea5e648d7452e048734181", "node_type": "4", "metadata": {"filename": "azure_code_interpreter.md", "author": "LlamaIndex"}, "hash": "34a3d82c049fc31ca5a733b9bcc24f5c288483dd0be9afea7860df38c1126dea", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.azure_code_interpreter\n    options:\n      members:\n        - AzureCodeInterpreterToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3390ca43-b087-48cf-a2b3-cbb2e37ae46b": {"__data__": {"id_": "3390ca43-b087-48cf-a2b3-cbb2e37ae46b", "embedding": null, "metadata": {"filename": "azure_cv.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fb4214c7ed2b451df9586d4583e5165db32258c", "node_type": "4", "metadata": {"filename": "azure_cv.md", "author": "LlamaIndex"}, "hash": "8f5687fbdea4b6c228f3824045f11788a933f1df9050a974f54804761c548b07", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.azure_cv\n    options:\n      members:\n        - AzureCVToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15c7affd-7942-47d9-9295-45a239eaaeb1": {"__data__": {"id_": "15c7affd-7942-47d9-9295-45a239eaaeb1", "embedding": null, "metadata": {"filename": "azure_speech.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1a080368716b8002a0030cddc06c896fb148aae", "node_type": "4", "metadata": {"filename": "azure_speech.md", "author": "LlamaIndex"}, "hash": "f95b394f2221873e0a3f83cd390b824b1059a64bf5a2a66f32f57d93d8aae62a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.azure_speech\n    options:\n      members:\n        - AzureSpeechToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf142090-7adf-4ccf-8a13-c18990c4cc04": {"__data__": {"id_": "cf142090-7adf-4ccf-8a13-c18990c4cc04", "embedding": null, "metadata": {"filename": "azure_translate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9293f065b3de98115a89c139d3cd943c59689e71", "node_type": "4", "metadata": {"filename": "azure_translate.md", "author": "LlamaIndex"}, "hash": "deaf8ebed97faaa85c2826dab26e04680e50c15ecd02d67bca9ccf4eb7bd058a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.azure_translate\n    options:\n      members:\n        - AzureTranslateToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ace5b223-3c3f-492d-b05a-f8f1dee1d297": {"__data__": {"id_": "ace5b223-3c3f-492d-b05a-f8f1dee1d297", "embedding": null, "metadata": {"filename": "bing_search.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db9f58b998e05a0ae5b4bbd1f1e71943bd1302be", "node_type": "4", "metadata": {"filename": "bing_search.md", "author": "LlamaIndex"}, "hash": "b2eb292284f4b6437f306458b536021cdc3497d11ad6c33e0f6a3932ca7ca440", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.bing_search\n    options:\n      members:\n        - BingSearchToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "008aff95-6183-4df3-9e87-e2ce758d8627": {"__data__": {"id_": "008aff95-6183-4df3-9e87-e2ce758d8627", "embedding": null, "metadata": {"filename": "brave_search.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64466a160145bf1459ca20e9229319793f4a4b4d", "node_type": "4", "metadata": {"filename": "brave_search.md", "author": "LlamaIndex"}, "hash": "4e64b2c960222ce8d7ad1bfa62de1dc428defbdaef9ae31c8f059fb6496c032a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.brave_search\n    options:\n      members:\n        - BraveSearchToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bb63174-ff12-4157-85d2-3ca28772639f": {"__data__": {"id_": "8bb63174-ff12-4157-85d2-3ca28772639f", "embedding": null, "metadata": {"filename": "cassandra.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3f3fea719d34c164af774dac1057541e9664843", "node_type": "4", "metadata": {"filename": "cassandra.md", "author": "LlamaIndex"}, "hash": "24c9782d9796f57735f6926f37300f90bf95a2aa644b5ae6ee991fbdc3fce897", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.cassandra\n    options:\n      members:\n        - CassandraDatabaseToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1996a62f-99ac-406a-b16e-15efcae0611f": {"__data__": {"id_": "1996a62f-99ac-406a-b16e-15efcae0611f", "embedding": null, "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "355baa269b5f919f13276d6e1e850850e8a6acb8", "node_type": "4", "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}, "hash": "cfda1d2d1fdedb92a120b4cf21044dbe45ca4b185eeb39742448745f8ecfc529", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.chatgpt_plugin\n    options:\n      members:\n        - ChatGPTPluginToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8577d57-b362-4376-a01f-6c553296fb56": {"__data__": {"id_": "e8577d57-b362-4376-a01f-6c553296fb56", "embedding": null, "metadata": {"filename": "code_interpreter.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3fa46ba542a11f7e57d6be7b26f163b5dca796c", "node_type": "4", "metadata": {"filename": "code_interpreter.md", "author": "LlamaIndex"}, "hash": "8036c8ec4b7acede2d0ddde2033d6c0901521b7cfcafb46cd220b1618f2f63c8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.code_interpreter\n    options:\n      members:\n        - CodeInterpreterToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7b597a8-a824-42f4-98e7-f2a7fc1e0658": {"__data__": {"id_": "f7b597a8-a824-42f4-98e7-f2a7fc1e0658", "embedding": null, "metadata": {"filename": "cogniswitch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "709c7ef9e4dd22946d2fb1d95828bbb1bb963e64", "node_type": "4", "metadata": {"filename": "cogniswitch.md", "author": "LlamaIndex"}, "hash": "fe92277ee92636e75bcd7f0368c0ec686badf3f2c67303adbd706412ca0fbdf5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.cogniswitch\n    options:\n      members:\n        - CogniswitchToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "055e416e-1f0f-4559-afce-b30035ecd35b": {"__data__": {"id_": "055e416e-1f0f-4559-afce-b30035ecd35b", "embedding": null, "metadata": {"filename": "database.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07504dcad16ce033453209dae761505727fcd499", "node_type": "4", "metadata": {"filename": "database.md", "author": "LlamaIndex"}, "hash": "9721a17294ca567839d06a1860510b96c37e9aa7775c19baa220b13949e8d11e", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.database\n    options:\n      members:\n        - DatabaseToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b28e076f-6e66-4d50-9882-5db330c1c922": {"__data__": {"id_": "b28e076f-6e66-4d50-9882-5db330c1c922", "embedding": null, "metadata": {"filename": "duckduckgo.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d48014c22505992c57c236c0a706f823adf94aa", "node_type": "4", "metadata": {"filename": "duckduckgo.md", "author": "LlamaIndex"}, "hash": "d0b1d91a1e1fd511899f7832c700c60438cc24ddc38d76d18c9128f2e4958bc9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.duckduckgo\n    options:\n      members:\n        - DuckDuckGoSearchToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 95, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2cbba05-82ba-4def-86a5-341404c0981a": {"__data__": {"id_": "c2cbba05-82ba-4def-86a5-341404c0981a", "embedding": null, "metadata": {"filename": "exa.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4aa40d5e2d4d5142e21e83d58a1ca1d78255f8f", "node_type": "4", "metadata": {"filename": "exa.md", "author": "LlamaIndex"}, "hash": "4bad5bde56a68ecfeab8f7956c89faccd83db5850b9451825df38ac0f34a7bb5", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.exa\n    options:\n      members:\n        - ExaToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0792ec0b-7959-441f-9dc4-736923e194ae": {"__data__": {"id_": "0792ec0b-7959-441f-9dc4-736923e194ae", "embedding": null, "metadata": {"filename": "finance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "185d816feff18846072b1907c15972a1882847cf", "node_type": "4", "metadata": {"filename": "finance.md", "author": "LlamaIndex"}, "hash": "ce707763af46725aff900b2c6695d6988bee7f7aaf7d10b067bcfe7082933345", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.finance\n    options:\n      members:\n        - FinanceAgentToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b137793-b111-47e5-826c-c607a0bf5fad": {"__data__": {"id_": "0b137793-b111-47e5-826c-c607a0bf5fad", "embedding": null, "metadata": {"filename": "function.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "116e4227ef84b8c40d1b29ae0741163409e2e7bb", "node_type": "4", "metadata": {"filename": "function.md", "author": "LlamaIndex"}, "hash": "e2f8f56af2550d6faec4008bc978810949f9c496e767af87e144fc528c441432", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.function_tool\n    options:\n      members:\n        - FunctionTool", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 91, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac89a504-3734-4ef4-8604-8a4a1448b954": {"__data__": {"id_": "ac89a504-3734-4ef4-8604-8a4a1448b954", "embedding": null, "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d854217c50954078a94f89a6e00e9314c689130", "node_type": "4", "metadata": {"filename": "google.md", "author": "LlamaIndex"}, "hash": "74d5e4860ff758c6f5e08c6cf21bb642b53abb7cbc0035ab7b0ddf08c2ff3291", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.google\n    options:\n      members:\n        - GmailToolSpec\n        - GoogleCalendarToolSpec\n        - GoogleSearchToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2f5b22f-8a2a-43ba-bc5d-7ee6f9d6e1c9": {"__data__": {"id_": "b2f5b22f-8a2a-43ba-bc5d-7ee6f9d6e1c9", "embedding": null, "metadata": {"filename": "graphql.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7d3b6f27c74f7f395a2673bcf18dc12795608db", "node_type": "4", "metadata": {"filename": "graphql.md", "author": "LlamaIndex"}, "hash": "d7208aac44015d5e9941612d4bc2cce16747a95d73a369736e2a5d56276e293d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.graphql\n    options:\n      members:\n        - GraphQLToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8589cb46-714d-4cd6-9d9c-f2724f2318a7": {"__data__": {"id_": "8589cb46-714d-4cd6-9d9c-f2724f2318a7", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2034149b2f552493715eee753be92de74c900e6b", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e63f68f819027e9e3ee56ef5f449a4818bc0d6f6c2cad014058a0486c5a0f0b1", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.types\n    options:\n      members:\n        - AsyncBaseTool\n        - BaseToolAsyncAdapter\n        - BaseTool\n        - ToolMetadata\n        - ToolOutput", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa58bebe-d887-4832-92a5-18e1ea195165": {"__data__": {"id_": "aa58bebe-d887-4832-92a5-18e1ea195165", "embedding": null, "metadata": {"filename": "ionic_shopping.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4317cb97eb1283f0b9a9f29b93bcf92af4e5036a", "node_type": "4", "metadata": {"filename": "ionic_shopping.md", "author": "LlamaIndex"}, "hash": "cca872188455e8c950eed30f9cdb2ec9d9c37e6c2a5eef4aeff0e2e31c60a312", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.ionic_shopping\n    options:\n      members:\n        - IonicShoppingToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e372d18-974f-43f1-aa9f-a14938f381b5": {"__data__": {"id_": "4e372d18-974f-43f1-aa9f-a14938f381b5", "embedding": null, "metadata": {"filename": "jina.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc284055768ce117814504803eed3b6f274f792d", "node_type": "4", "metadata": {"filename": "jina.md", "author": "LlamaIndex"}, "hash": "beca2cd34385b43427b665daffb11161d3ab4124e5079d8a7fd487bca73e1dcd", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.jina\n    options:\n      members:\n        - JinaToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c8303b5-7b84-4353-9493-b6a1a4f05525": {"__data__": {"id_": "2c8303b5-7b84-4353-9493-b6a1a4f05525", "embedding": null, "metadata": {"filename": "load_and_search.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a8e9cfa2d224b5ff0c47542732693b077077076c", "node_type": "4", "metadata": {"filename": "load_and_search.md", "author": "LlamaIndex"}, "hash": "ee6915112898e16b90806d02df04857defa47bae359b296f827749e31e9985a6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.tool_spec.load_and_search.base\n    options:\n      members:\n        - LoadAndSearchToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ad931c6-3a7c-4b35-ba4d-b0be8ae7d975": {"__data__": {"id_": "6ad931c6-3a7c-4b35-ba4d-b0be8ae7d975", "embedding": null, "metadata": {"filename": "metaphor.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46f7195d9d60fd3c04da10c193bd535e3a765603", "node_type": "4", "metadata": {"filename": "metaphor.md", "author": "LlamaIndex"}, "hash": "77e2f2e28b21c195c9ac0ee5310adf3c9508b22d005689c61735cb012324fcd9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.metaphor\n    options:\n      members:\n        - MetaphorToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d177ff11-7f57-47e4-90a8-8fa778bc6374": {"__data__": {"id_": "d177ff11-7f57-47e4-90a8-8fa778bc6374", "embedding": null, "metadata": {"filename": "multion.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ef82f7852fe9bde8dc29e657aeba9183e0206c5", "node_type": "4", "metadata": {"filename": "multion.md", "author": "LlamaIndex"}, "hash": "3ea4220c18301311b41cfda28fafd07f8b3a2822c03ccb03a869dc1927f749ad", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.multion\n    options:\n      members:\n        - MultionToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c438d13d-beef-4565-bce9-0d99dfbfa37d": {"__data__": {"id_": "c438d13d-beef-4565-bce9-0d99dfbfa37d", "embedding": null, "metadata": {"filename": "neo4j.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fd2b81cf0ad3f5097092f074b46c83b480c51ff", "node_type": "4", "metadata": {"filename": "neo4j.md", "author": "LlamaIndex"}, "hash": "7e052649d0eccf45ab27eaaf58598485c33d294266d77ed428882e446146bcfc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.neo4j\n    options:\n      members:\n        - Neo4jQueryToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 84, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "938632da-d05f-4c8e-82e0-36918dbe9996": {"__data__": {"id_": "938632da-d05f-4c8e-82e0-36918dbe9996", "embedding": null, "metadata": {"filename": "notion.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abdccd6c70178d88056d5c3f95f5e682c01552d2", "node_type": "4", "metadata": {"filename": "notion.md", "author": "LlamaIndex"}, "hash": "58b837c51d3b20137dd9e30eb62e092e063593e6181c8cbd7a08abc7aad7614d", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.notion\n    options:\n      members:\n        - NotionToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fc305b6-71f9-476a-8f45-aca460aed43f": {"__data__": {"id_": "0fc305b6-71f9-476a-8f45-aca460aed43f", "embedding": null, "metadata": {"filename": "ondemand_loader.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71ab9b855e9fe249152b08e3a31bbfa2d94f2ef4", "node_type": "4", "metadata": {"filename": "ondemand_loader.md", "author": "LlamaIndex"}, "hash": "29a22b4107ef2dee0e20151683b16226a3d02f84ad92739b95705470e1dc1229", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.ondemand_loader_tool\n    options:\n      members:\n        - OnDemandLoaderTool", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "debaa7e3-422d-4d07-9edd-755e731bfdea": {"__data__": {"id_": "debaa7e3-422d-4d07-9edd-755e731bfdea", "embedding": null, "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "333555e6837acfc985ee503aff3738b02e235a83", "node_type": "4", "metadata": {"filename": "openai.md", "author": "LlamaIndex"}, "hash": "35fef86a1650a12ed3cfefe2f8dc340ebf600daaeb34faf6da389b5ec60eda6c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.openai\n    options:\n      members:\n        - OpenAIImageGenerationToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 96, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c683e8f5-8c15-4e71-8511-410674b7f919": {"__data__": {"id_": "c683e8f5-8c15-4e71-8511-410674b7f919", "embedding": null, "metadata": {"filename": "openapi.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e7bd1f42fb9f323c6aaf5b7b0cb850e085edd39", "node_type": "4", "metadata": {"filename": "openapi.md", "author": "LlamaIndex"}, "hash": "3020222ee2faa421ee2d8488efc8f76ee1fc92e1dd14baa37ca82d664efb6f53", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.openapi\n    options:\n      members:\n        - OpenAPIToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd30374a-e820-4eb9-9167-fd480e30924d": {"__data__": {"id_": "fd30374a-e820-4eb9-9167-fd480e30924d", "embedding": null, "metadata": {"filename": "passio_nutrition_ai.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "322464bc0ed19531e31cc13022d897b39ac35590", "node_type": "4", "metadata": {"filename": "passio_nutrition_ai.md", "author": "LlamaIndex"}, "hash": "c8f4326c3d1b791ef47b81b9cd3338f7cb07181215cd5dce711ec8289a9d4513", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.passio_nutrition_ai\n    options:\n      members:\n        - NutritionAIToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55b573e0-3f17-4b46-834e-cd7edf3604cb": {"__data__": {"id_": "55b573e0-3f17-4b46-834e-cd7edf3604cb", "embedding": null, "metadata": {"filename": "playgrounds.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f4491da62a9777a1a33c7ce82d443ab3e6647ff", "node_type": "4", "metadata": {"filename": "playgrounds.md", "author": "LlamaIndex"}, "hash": "8e939e05a8a22c79fe441f95bb1b77c0e3afc187fb36d6967bbbc64f8121f636", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.playgrounds\n    options:\n      members:\n        - PlaygroundsSubgraphConnectorToolSpec\n        - PlaygroundsSubgraphInspectorToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09a5b943-1426-4988-89c5-7c2e4090c038": {"__data__": {"id_": "09a5b943-1426-4988-89c5-7c2e4090c038", "embedding": null, "metadata": {"filename": "python_file.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0d6b2562242f80ba727b940ac329a95deca78b2", "node_type": "4", "metadata": {"filename": "python_file.md", "author": "LlamaIndex"}, "hash": "a7b093cdfd0a6c85c738cb52a7bd68d04429df7ef1d5225ab24fc3ba33ba9704", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.python_file\n    options:\n      members:\n        - PythonFileToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "592f800c-b972-4cdf-9133-4af9227a713b": {"__data__": {"id_": "592f800c-b972-4cdf-9133-4af9227a713b", "embedding": null, "metadata": {"filename": "query_plan.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78435b150a662a66178e45f86288bda056ec9c0f", "node_type": "4", "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}, "hash": "b7ac94773467dd78e5b6ade5237295cab1b52583ea8da4a0f8825106be4c2b8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5760e3b-f2f5-4bf4-a8c0-a5025df82f9d", "node_type": "1", "metadata": {}, "hash": "aaf7428f0d1eb0c2621b11a764ee4e7f8d5178193e27643dddb44590879331bb", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.query_engine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 39, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5760e3b-f2f5-4bf4-a8c0-a5025df82f9d": {"__data__": {"id_": "f5760e3b-f2f5-4bf4-a8c0-a5025df82f9d", "embedding": null, "metadata": {"filename": "query_plan.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78435b150a662a66178e45f86288bda056ec9c0f", "node_type": "4", "metadata": {"filename": "query_plan.md", "author": "LlamaIndex"}, "hash": "02fa26a1660e2268e595054ee3f4fb03e3c5578e15036f1fb365a5281fe896de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "592f800c-b972-4cdf-9133-4af9227a713b", "node_type": "1", "metadata": {"filename": "query_plan.md", "author": "LlamaIndex"}, "hash": "968b106aa2ea1940a3965c71635c18d0a6c3a56ff1effae4db0a6d40b6eb39c6", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.query_engine", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 39, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c59e0268-97f2-4726-b376-5e65cf68c40b": {"__data__": {"id_": "c59e0268-97f2-4726-b376-5e65cf68c40b", "embedding": null, "metadata": {"filename": "requests.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de6f92bf433429fd8b435bb1722fdb02989e28a9", "node_type": "4", "metadata": {"filename": "requests.md", "author": "LlamaIndex"}, "hash": "5072e950850914d7a991149cedc3dd833fe0c25a71b17758c09e8d6bdf872aca", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.requests\n    options:\n      members:\n        - RequestsToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9f6b55b-7a90-45b7-bf75-4583a2fa1124": {"__data__": {"id_": "c9f6b55b-7a90-45b7-bf75-4583a2fa1124", "embedding": null, "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de3f185942a3ac31c1cfe13013090ad661047dc", "node_type": "4", "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}, "hash": "904a5c628be20101a8707e46d7bbe5d34d897d687cfa60be0ebff884f8d6b8a2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.retriever_tool", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 41, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a105becf-315d-4b1f-81d8-a38ff6255bc6": {"__data__": {"id_": "a105becf-315d-4b1f-81d8-a38ff6255bc6", "embedding": null, "metadata": {"filename": "salesforce.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdabc731c10cbc98d779251698cb8708675bac6a", "node_type": "4", "metadata": {"filename": "salesforce.md", "author": "LlamaIndex"}, "hash": "e32f9dcdf6dc6b3729fe1b35ad5d5da5f945c3ec320a5776c5f40c3090bb677b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.salesforce\n    options:\n      members:\n        - SalesforceToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5758ba7e-24e0-4aea-bf1a-cbef907491b9": {"__data__": {"id_": "5758ba7e-24e0-4aea-bf1a-cbef907491b9", "embedding": null, "metadata": {"filename": "shopify.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db09301091bb16ac12c1b067d7d40b6bad9cdd58", "node_type": "4", "metadata": {"filename": "shopify.md", "author": "LlamaIndex"}, "hash": "2f621cdbb659256a293ab76096a14e15440fa8c199d6d8c91b5a5029387f11cc", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.shopify\n    options:\n      members:\n        - ShopifyToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7657528-a354-45c4-ac97-61baa19f3158": {"__data__": {"id_": "f7657528-a354-45c4-ac97-61baa19f3158", "embedding": null, "metadata": {"filename": "slack.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "523be4b94a0d269e9a6cd3d6da72901750e0f104", "node_type": "4", "metadata": {"filename": "slack.md", "author": "LlamaIndex"}, "hash": "5b58bac883c25c87dee8a2c54e2170249c19be9c6124710b199ac1b867720937", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.slack\n    options:\n      members:\n        - SlackToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3940a82e-afd8-4901-ac8a-1e3945ac8c80": {"__data__": {"id_": "3940a82e-afd8-4901-ac8a-1e3945ac8c80", "embedding": null, "metadata": {"filename": "tavily_research.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f325adc1c1e0eda52891815d6ec3b9a952e76e97", "node_type": "4", "metadata": {"filename": "tavily_research.md", "author": "LlamaIndex"}, "hash": "f0c4064061106412987f4dec8b697464c1d8e81ddc45f701e41086a13f22683b", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.tavily_research\n    options:\n      members:\n        - TavilyToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34878d76-7c56-4264-8b52-7b4fc1a7598f": {"__data__": {"id_": "34878d76-7c56-4264-8b52-7b4fc1a7598f", "embedding": null, "metadata": {"filename": "text_to_image.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "477869049c2a71e82df47e8a89bf134a5beac7ac", "node_type": "4", "metadata": {"filename": "text_to_image.md", "author": "LlamaIndex"}, "hash": "fe558bc6c7f1b9f711f80a968434a4155c63eb0365d1f4271a10b21066221aa2", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.text_to_image\n    options:\n      members:\n        - TextToImageToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 93, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e65927b-ef8a-4f66-8867-49e0a51c20e7": {"__data__": {"id_": "9e65927b-ef8a-4f66-8867-49e0a51c20e7", "embedding": null, "metadata": {"filename": "tool_spec.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6ea9f728754d2956e82560cbd9108bf82389c30", "node_type": "4", "metadata": {"filename": "tool_spec.md", "author": "LlamaIndex"}, "hash": "20ad17d9cb234e99f75e608ae934c2d2beacbb0390efd0283c47c4c32949b05c", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.core.tools.tool_spec.base\n    options:\n      members:\n        - BaseToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 92, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afaa2348-da83-4c89-86cb-bd8aad8781d5": {"__data__": {"id_": "afaa2348-da83-4c89-86cb-bd8aad8781d5", "embedding": null, "metadata": {"filename": "vector_db.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79c5528e23c27c710fa527deb8f06b1f6523bb0c", "node_type": "4", "metadata": {"filename": "vector_db.md", "author": "LlamaIndex"}, "hash": "5d3f3550d3dd8d06972ffba9659bb4ff80e9938c21a22fa265da92c895681991", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.vector_db\n    options:\n      members:\n        - VectorDBToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 86, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58f02a33-44f3-48e2-b746-b051cc4bda79": {"__data__": {"id_": "58f02a33-44f3-48e2-b746-b051cc4bda79", "embedding": null, "metadata": {"filename": "waii.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cba2508ffe2fa6002ad3ce3cdddfb09d82d57dc", "node_type": "4", "metadata": {"filename": "waii.md", "author": "LlamaIndex"}, "hash": "22266f7589362133d7434e05c891921c7be5ef7b2c9336cb1da9560bd5561a1f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.waii\n    options:\n      members:\n        - WaiiToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa0302f7-f023-4246-8726-817e374aa6b3": {"__data__": {"id_": "aa0302f7-f023-4246-8726-817e374aa6b3", "embedding": null, "metadata": {"filename": "weather.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab67ca65cd70b806625d9e2ca7b36affaae86440", "node_type": "4", "metadata": {"filename": "weather.md", "author": "LlamaIndex"}, "hash": "30c4e09c092f853b7491c3ba47e0ea82b5c02c92dcfeb9a6979075375174f4c8", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.weather\n    options:\n      members:\n        - OpenWeatherMapToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "445ce08e-85dc-4214-8d57-704488610acf": {"__data__": {"id_": "445ce08e-85dc-4214-8d57-704488610acf", "embedding": null, "metadata": {"filename": "wikipedia.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5236ea3806a6f74a0f39a8f8f5e6d81738863a1f", "node_type": "4", "metadata": {"filename": "wikipedia.md", "author": "LlamaIndex"}, "hash": "e2ebf9765961cd6d7d5eb9b98e358ca6c258682a1343c474f0dca30c2e3c9008", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.wikipedia\n    options:\n      members:\n        - WikipediaToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce779173-1b03-4fe6-a456-655532dc0897": {"__data__": {"id_": "ce779173-1b03-4fe6-a456-655532dc0897", "embedding": null, "metadata": {"filename": "wolfram_alpha.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9289f3e605d97cf186122f19631d36af404a4716", "node_type": "4", "metadata": {"filename": "wolfram_alpha.md", "author": "LlamaIndex"}, "hash": "417aa3c8daea3a972e4b49dcf884592b7809597a7a33b4863be03ddb3e47e2f9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.wolfram_alpha\n    options:\n      members:\n        - WolframAlphaToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8699425-f21a-4ad9-89f5-4fe4732afcf9": {"__data__": {"id_": "a8699425-f21a-4ad9-89f5-4fe4732afcf9", "embedding": null, "metadata": {"filename": "yahoo_finance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53c3b8875410015ffd75bc751ef3aab7db685e16", "node_type": "4", "metadata": {"filename": "yahoo_finance.md", "author": "LlamaIndex"}, "hash": "aa106a69abab6fdecc36cb141a14782af4fb60387f31b2390fec8b9ee24e017a", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.yahoo_finance\n    options:\n      members:\n        - YahooFinanceToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8977ccd-79c7-4522-b554-73bf1df21832": {"__data__": {"id_": "d8977ccd-79c7-4522-b554-73bf1df21832", "embedding": null, "metadata": {"filename": "yelp.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a70b8dbca4493642a51af7ab7dfba62be404c6a", "node_type": "4", "metadata": {"filename": "yelp.md", "author": "LlamaIndex"}, "hash": "ee673f22ae5115814d1801d92748f4f243b1d3032c3503a66573a005a72e15e9", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.yelp\n    options:\n      members:\n        - YelpToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1407522f-1913-4615-b6a5-a60c3339aef0": {"__data__": {"id_": "1407522f-1913-4615-b6a5-a60c3339aef0", "embedding": null, "metadata": {"filename": "zapier.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7415d36f570f9b2733da56309398b61852dff424", "node_type": "4", "metadata": {"filename": "zapier.md", "author": "LlamaIndex"}, "hash": "4e950317e803455eb1e0bd0f454a0d978aa878be49f5189579e8dde48b76239f", "class_name": "RelatedNodeInfo"}}, "text": "::: llama_index.tools.zapier\n    options:\n      members:\n        - ZapierToolSpec", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baffdb5b-10a8-433b-ab32-722caf695fc4": {"__data__": {"id_": "baffdb5b-10a8-433b-ab32-722caf695fc4", "embedding": null, "metadata": {"filename": "deprecated_terms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0b157307b096ffca8d28c528806272969883a9c", "node_type": "4", "metadata": {"filename": "deprecated_terms.md", "author": "LlamaIndex"}, "hash": "74d809ed0ee079a54dda6d12f212a110408309ad0ef2c9c52d3aa46f8a8986e9", "class_name": "RelatedNodeInfo"}}, "text": "# Deprecated Terms\n\nAs LlamaIndex continues to evolve, many class names and APIs have been adjusted, improved, and deprecated.\n\nThe following is a list of previously popular terms that have been deprecated, with links to their replacements.\n\n## GPTSimpleVectorIndex\n\nThis has been renamed to `VectorStoreIndex`, as well as unifying all vector indexes to a single unified interface. You can integrate with various vector databases by modifying the underlying `vector_store`.\n\nPlease see the following links for more details on usage.\n\n- [Index Usage Pattern](../module_guides/evaluating/usage_pattern.md)\n- [Vector Store Guide](../module_guides/indexing/vector_store_guide.ipynb)\n- [Vector Store Integrations](../community/integrations/vector_stores.md)\n\n## GPTVectorStoreIndex\n\nThis has been renamed to `VectorStoreIndex`, but it is only a cosmetic change. Please see the following links for more details on usage.\n\n- [Index Usage Pattern](../module_guides/evaluating/usage_pattern.md)\n- [Vector Store Guide](../module_guides/indexing/vector_store_guide.ipynb)\n- [Vector Store Integrations](../community/integrations/vector_stores.md)\n\n## LLMPredictor\n\nThe `LLMPredictor` object is no longer intended to be used by users. Instead, you can setup an LLM directly and pass it into the `Settings` or the interface using the LLM. The `LLM` class itself has similar attributes and methods as the `LLMPredictor`.\n\n- [LLMs in LlamaIndex](../module_guides/models/llms.md)\n- [Setting LLMs in the Settings](../module_guides/supporting_modules/settings.md)\n\n## PromptHelper and max_input_size/\n\nThe `max_input_size` parameter for the prompt helper has since been replaced with `context_window`.\n\nThe `PromptHelper` in general has been deprecated in favour of specifying parameters directly in the `service_context` and `node_parser`.\n\nSee the following links for more details.\n\n- [Configuring settings in the Settings](../module_guides/supporting_modules/settings.md)\n- [Parsing Documents into Nodes](../module_guides/loading/node_parsers/index.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2036, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3d7e1e3-edbf-4fee-be3b-5e771d7a799d": {"__data__": {"id_": "b3d7e1e3-edbf-4fee-be3b-5e771d7a799d", "embedding": null, "metadata": {"filename": "chat_engines.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61b233f691ee83fd88e7f7199de862d110f01985", "node_type": "4", "metadata": {"filename": "chat_engines.md", "author": "LlamaIndex"}, "hash": "851810e133eef5da10858188f58b5712a91a72fae89a601b844df04e17483f0b", "class_name": "RelatedNodeInfo"}}, "text": "# Chat Engines\n\n##### FAQ\n\n1. [How to make bot retain context while answering, Can I do that with LlamaIndex?](#1-how-to-make-bot-retain-context-while-answering-can-i-do-that-with-llamaindex)\n2. [How to use Data Agent with Chat engine?](#2-how-to-use-data-agent-with-chat-engine)\n\n---\n\n##### 1. How to make bot retain context while answering, Can I do that with LlamaIndex?\n\nYes you can, Llamaindex provides chat engines that you can use to retain context and answer as per the context. You can find more here [Chat Engines](../../module_guides/deploying/chat_engines/index.md).\n\n---\n\n##### 2. How to use Data Agent with Chat engine?\n\nTo use data Agents with Chat engine you have to set the chat mode while initializing the chat engine. Find more here [ Data Agents with Chat Engine](../../module_guides/deploying/chat_engines/usage_pattern.md#available-chat-modes)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23b06cd4-b1bc-4eaa-a8d1-f699060821c6": {"__data__": {"id_": "23b06cd4-b1bc-4eaa-a8d1-f699060821c6", "embedding": null, "metadata": {"filename": "documents_and_nodes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ebff447280ac4fe02791afad8900922d7d1c9a2", "node_type": "4", "metadata": {"filename": "documents_and_nodes.md", "author": "LlamaIndex"}, "hash": "f875581a8a6152ebda86a1ea49781834f58e3418b94fc432d4d994df69626c61", "class_name": "RelatedNodeInfo"}}, "text": "# Documents and Nodes\n\n##### FAQ\n\n1. [What is the default `chunk_size` of a Node object?](#1-what-is-the-default-chunk_size-of-a-node-object)\n2. [How to add information like name, url in a `Document` object?](#2-how-to-add-information-like-name-url-in-a-document-object)\n3. [How to update existing document in an Index?](#3-how-to-update-existing-document-in-an-index)\n\n---\n\n##### 1. What is the default `chunk_size` of a Node object?\n\nIt's 1024 by default. If you want to customize the `chunk_size`, You can follow [Customizing Node](../../module_guides/loading/node_parsers/index.md#customization)\n\n---\n\n##### 2. How to add information like name, url in a `Document` object?\n\nYou can customize the Document object and add extra info in the form of metadata. To know more on this follow [Customize Document](../../module_guides/loading/documents_and_nodes/usage_documents.md#customizing-documents).\n\n---\n\n##### 3. How to update existing document in an Index?\n\nYou can update/delete existing document in an Index with the help of `doc_id`. You can add new document to an existing Index too. To know more check [Document Management](../../module_guides/indexing/document_management.md)\n\n---", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d75ea97a-5bd0-4e3a-974c-d56342fc8162": {"__data__": {"id_": "d75ea97a-5bd0-4e3a-974c-d56342fc8162", "embedding": null, "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a53735a48d5c8a415373181e7e97b83e2982ffe6", "node_type": "4", "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "hash": "ec312e4090122d88b3a0ccca7493447d271b078c01b62b80c24153bd8a4513e2", "class_name": "RelatedNodeInfo"}}, "text": "# Embeddings\n\n##### FAQ\n\n1. [How to use a custom/local embedding model?](#1-how-to-use-a-customlocal-embedding-model)\n2. [How to use a local hugging face embedding model?](#2-how-to-use-a-local-hugging-face-embedding-model)\n3. [How to use embedding model to generate embeddings for text?](#3-how-to-use-embedding-model-to-generate-embeddings-for-text)\n4. [How to use Huggingface Text-Embedding Inference with LlamaIndex?](#4-how-to-use-huggingface-text-embedding-inference-with-llamaindex)\n\n---\n\n##### 1. How to use a custom/local embedding model?\n\nTo create your customized embedding class you can follow [Custom Embeddings](../../examples/embeddings/custom_embeddings.ipynb) guide.\n\n---\n\n##### 2. How to use a local hugging face embedding model?\n\nTo use a local HuggingFace embedding model you can follow [Local Embeddings with HuggingFace](../../examples/embeddings/huggingface.ipynb) guide.\n\n---\n\n##### 3. How to use embedding model to generate embeddings for text?\n\nYou can generate embeddings for texts with the following piece of code.\n\n```py\ntext_embedding = embed_model.get_text_embedding(\"YOUR_TEXT\")\n```\n\n---\n\n##### 4. How to use Huggingface Text-Embedding Inference with LlamaIndex?\n\nTo use HuggingFace Text-Embedding Inference you can follow [Text-Embedding-Inference](../../examples/embeddings/text_embedding_inference.ipynb) tutorial.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2471f739-df7b-4d34-bf72-57bee1305c11": {"__data__": {"id_": "2471f739-df7b-4d34-bf72-57bee1305c11", "embedding": null, "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "625b12aadf4db855f603ab991d12ed43dd7dff19", "node_type": "4", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "e13763e4c2f287e11c17337c8c516e254dcb2247b9ce69a69d38db6a74bcff3c", "class_name": "RelatedNodeInfo"}}, "text": "# Large Language Models\n\n##### FAQ\n\n1. [How to use a custom/local embedding model?](#1-how-to-define-a-custom-llm)\n2. [How to use a local hugging face embedding model?](#2-how-to-use-a-different-openai-model)\n3. [How can I customize my prompt](#3-how-can-i-customize-my-prompt)\n4. [Is it required to fine-tune my model?](#4-is-it-required-to-fine-tune-my-model)\n5. [I want to the LLM answer in Chinese/Italian/French but only answers in English, how to proceed?](#5-i-want-to-the-llm-answer-in-chineseitalianfrench-but-only-answers-in-english-how-to-proceed)\n6. [Is LlamaIndex GPU accelerated?](#6-is-llamaindex-gpu-accelerated)\n\n---\n\n##### 1. How to define a custom LLM?\n\nYou can access [Usage Custom](../../module_guides/models/llms/usage_custom.md#example-using-a-custom-llm-model---advanced) to define a custom LLM.\n\n---\n\n##### 2. How to use a different OpenAI model?\n\nTo use a different OpenAI model you can access [Configure Model](../../examples/llm/openai.ipynb) to set your own custom model.\n\n---\n\n##### 3. How can I customize my prompt?\n\nYou can access [Prompts](../../module_guides/models/prompts/index.md) to learn how to customize your prompts.\n\n---\n\n##### 4. Is it required to fine-tune my model?\n\nNo. there's isolated modules which might provide better results, but isn't required, you can use llamaindex without needing to fine-tune the model.\n\n---\n\n##### 5. I want to the LLM answer in Chinese/Italian/French but only answers in English, how to proceed?\n\nTo the LLM answer in another language more accurate you can update the prompts to enforce more the output language.\n\n```py\nresponse = query_engine.query(\"Rest of your query... \\nRespond in Italian\")\n```\n\nAlternatively:\n\n```py\nfrom llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(system_prompt=\"Always respond in Italian.\")\n\n# set a global llm\nSettings.llm = llm\n\nquery_engine = load_index_from_storage(\n    storage_context,\n).as_query_engine()\n```\n\n---\n\n##### 6. Is LlamaIndex GPU accelerated?\n\nYes, you can run a language model (LLM) on a GPU when running it locally. You can find an example of setting up LLMs with GPU support in the [llama2 setup](../../examples/vector_stores/SimpleIndexDemoLlama-Local.ipynb) documentation.\n\n---", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd501d63-f773-47f8-b9c0-b7f2518fb8d6": {"__data__": {"id_": "dd501d63-f773-47f8-b9c0-b7f2518fb8d6", "embedding": null, "metadata": {"filename": "query_engines.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e7f93f9bb78878e74f9a5599bc9486ddd378819", "node_type": "4", "metadata": {"filename": "query_engines.md", "author": "LlamaIndex"}, "hash": "7d417173cebc5cb039cc8aeb2046fe0b02984bd475bf54842012aa2d55fa21f0", "class_name": "RelatedNodeInfo"}}, "text": "# Query Engines", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 15, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "020aea8b-1da4-43d3-bbc7-3c934e1b8b3e": {"__data__": {"id_": "020aea8b-1da4-43d3-bbc7-3c934e1b8b3e", "embedding": null, "metadata": {"filename": "vector_database.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff91e1a6eb875f799bb32c8fa58eaa54d04fe6e1", "node_type": "4", "metadata": {"filename": "vector_database.md", "author": "LlamaIndex"}, "hash": "bbecca3bcd01b26696b7059e4384931f3d8099b0f6493949dbfd8b33d7c48fde", "class_name": "RelatedNodeInfo"}}, "text": "# Vector Database\n\n##### FAQ\n\n1. [Do I need to use a vector database?](#1-do-i-need-to-use-a-vector-database)\n2. [What's the difference between the vector databases?](#2-whats-the-difference-between-the-vector-databases)\n\n---\n\n##### 1. Do I need to use a vector database?\n\nLlamaIndex provides a in-memory vector database allowing you to run it locally, when you have a large amount of documents vector databases provides more features and better scalability and less memory constraints depending of your hardware.\n\n---\n\n##### 2. What's the difference between the vector databases?\n\nTo check the difference between the vector databases, you can check at [Vector Store Options & Feature Support](../../module_guides/storing/vector_stores.md#vector-store-options--feature-support).\n\n---", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "945e1359-4010-4118-b2ab-850389660ab5": {"__data__": {"id_": "945e1359-4010-4118-b2ab-850389660ab5", "embedding": null, "metadata": {"filename": "frequently_asked_questions.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71b6ef509a2ab7c2175c22180fe1900b60f5a0f6", "node_type": "4", "metadata": {"filename": "frequently_asked_questions.md", "author": "LlamaIndex"}, "hash": "69b413a17627a20f2d3eac89533bfb424da39aadcb6e2aa768ea55ff28daedaf", "class_name": "RelatedNodeInfo"}}, "text": "# Frequently Asked Questions (FAQ)\n\n---\n\n##### [Large Language Models (LLM)](./faq/llms.md)\n\nDiscover how to tailor LLMs, explore available models, understand cost implications, and switch between languages.\n\n---\n\n##### [Embeddings](./faq/embeddings.md)\n\nHow to customize the embedding, Which embeddings model choose, their pros and cons\n\n---\n\n##### [Vector Database](./faq/vector_database.md)\n\nGet insights on personalizing vector databases, delve into database options\n\n---\n\n##### [Query Engines](./faq/query_engines.md)\n\nKnow more about query engines and their possibilities\n\n---\n\n##### [Chat Engines](./faq/chat_engines.md)\n\nKnow more about chat engines and their possibilities\n\n---\n\n##### [Documents and Nodes](./faq/documents_and_nodes.md)\n\nKnow more about documents and nodes and their possibilities.\n\n---", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "292179f6-10e8-444c-9f94-39e89c3c0b22": {"__data__": {"id_": "292179f6-10e8-444c-9f94-39e89c3c0b22", "embedding": null, "metadata": {"filename": "full_stack_projects.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c76904866ae5040ec1441258d3543405e3b54537", "node_type": "4", "metadata": {"filename": "full_stack_projects.md", "author": "LlamaIndex"}, "hash": "eb534b9aca3329f52348d64b8e4a987d5e4f066ee7d71fe37f6501ec0dd00d39", "class_name": "RelatedNodeInfo"}}, "text": "# Full-Stack Projects\n\nWe've created both tooling and a variety of example projects (all open-source) to help you get started building a full-stack LLM application.\n\n## create-llama\n\n`create-llama` is a command-line tool that will generate a full-stack application template for you. It supports both FastAPI, Vercel, and Node backends. This is one of the easiest ways to get started!\n\nResources:\n\n- [create-llama Blog](https://blog.llamaindex.ai/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191)\n- [create-llama Repo](https://github.com/run-llama/LlamaIndexTS/tree/main/packages/create-llama)\n- [create-llama Additional Templates](https://github.com/jerryjliu/create_llama_projects)\n\n## Full-Stack Applications\n\nThe LlamaIndex team has also built some in-house projects - all of them open-sourced with MIT license - that you can use out of the box, or use as a template to kickstart your own project.\n\nCheck them out below.\n\n### SEC Insights\n\n- [SEC Insights App](https://secinsights.ai/)\n- [SEC Insights Repo](https://github.com/run-llama/sec-insights>)\n\n### Chat LlamaIndex\n\n- [Chat LlamaIndex App](https://chat-llamaindex.vercel.app/)\n- [Chat LlamaIndex Repo](https://github.com/run-llama/chat-llamaindex)\n\n### RAGs\n\n[RAGs Repo](https://github.com/run-llama/rags)\n\n### RAG CLI\n\n[RAG CLI](../use_cases/q_and_a/rag_cli.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51de17b2-66f0-46c5-80dd-e4d6320f87bc": {"__data__": {"id_": "51de17b2-66f0-46c5-80dd-e4d6320f87bc", "embedding": null, "metadata": {"filename": "integrations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a72cb856d4806600a20aebfe2269318aaf606d2", "node_type": "4", "metadata": {"filename": "integrations.md", "author": "LlamaIndex"}, "hash": "441be209e78c1eb3760adf2a7b32d2e3258f65788bf5610ec3c322e344f8fe5a", "class_name": "RelatedNodeInfo"}}, "text": "# Integrations\n\nLlamaIndex has a number of community integrations, from vector stores, to prompt trackers, tracers, and more!\n\n## LlamaPacks -- Code Templates\n\nLlamaHub hosts a full suite of LlamaPacks -- templates for features that you can download, edit, and try out! This offers a quick way to learn about new features and try new techniques.\n\nThe full set of LlamaPacks is available on [LlamaHub](https://llamahub.ai/). Check out our [dedicated page](llama_packs/index.md).\n\n## Data Loaders\n\nThe full set of data loaders are found on [LlamaHub](https://llamahub.ai/)\n\n## Agent Tools\n\nThe full set of agent tools are found on [LlamaHub](https://llamahub.ai/)\n\n## LLMs\n\nWe support [a huge number of LLMs](../module_guides/models/llms/modules.md).\n\n## Observability/Tracing/Evaluation\n\nCheck out our [one-click observability](../module_guides/observability/observability.md) page\nfor full tracing integrations.\n\n## Structured Outputs\n\n- [Guidance](integrations/guidance.md)\n- [LLM Format Enforcer](integrations/lmformatenforcer.md)\n- [Guardrails](../examples/output_parsing/GuardrailsDemo.ipynb)\n- [OpenAI Function Calling](../examples/output_parsing/openai_pydantic_program.ipynb)\n\n## Storage and Managed Indexes\n\n- [Vector Stores](integrations/vector_stores.md)\n- [Managed Indices](integrations/managed_indices.md)\n\n## Application Frameworks\n\n- [Streamlit](https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/)\n- [Chainlit](https://docs.chainlit.io/integrations/llama-index)\n\n## Distributed Compute\n\n- [LlamaIndex + Ray](https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray)\n\n## Other\n\n- [ChatGPT Plugins](integrations/chatgpt_plugins.md)\n- [Poe](https://github.com/poe-platform/poe-protocol/tree/main/llama_poe)\n- [Airbyte](https://airbyte.com/tutorials/airbyte-and-llamaindex-elt-and-chat-with-your-data-warehouse-without-writing-sql)\n- [Fleet](integrations/fleet_libraries_context.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d43b1aa6-4a86-44ca-9ef8-a4aa1acade23": {"__data__": {"id_": "d43b1aa6-4a86-44ca-9ef8-a4aa1acade23", "embedding": null, "metadata": {"filename": "chatgpt_plugins.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "816ae0aacc09beb27107f1a46864e8bf70945e7b", "node_type": "4", "metadata": {"filename": "chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "e1a70dfde18960591b0918415fda1c71cc5d8421affc1b94d5b84d4ed2faef9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e8bda47-2ea0-458f-9cd3-d0a05b9d569c", "node_type": "1", "metadata": {}, "hash": "d920f77ad684d39942f87b3d614e1c3535c30010460d0c306b895f44cbb91b3a", "class_name": "RelatedNodeInfo"}}, "text": "# ChatGPT Plugin Integrations\n\n**NOTE**: This is a work-in-progress, stay tuned for more exciting updates on this front!\n\n## ChatGPT Retrieval Plugin Integrations\n\nThe [OpenAI ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin)\noffers a centralized API specification for any document storage system to interact\nwith ChatGPT. Since this can be deployed on any service, this means that more and more\ndocument retrieval services will implement this spec; this allows them to not only\ninteract with ChatGPT, but also interact with any LLM toolkit that may use\na retrieval service.\n\nLlamaIndex provides a variety of integrations with the ChatGPT Retrieval Plugin.\n\n### Loading Data from LlamaHub into the ChatGPT Retrieval Plugin\n\nThe ChatGPT Retrieval Plugin defines an `/upsert` endpoint for users to load\ndocuments. This offers a natural integration point with LlamaHub, which offers\nover 65 data loaders from various API's and document formats.\n\nHere is a sample code snippet of showing how to load a document from LlamaHub\ninto the JSON format that `/upsert` expects:\n\n```python\nfrom llama_index.core import download_loader, Document\nfrom typing import Dict, List\nimport json\n\n# download loader, load documents\nfrom llama_index.readers.web import SimpleWebPageReader\n\nloader = SimpleWebPageReader(html_to_text=True)\nurl = \"http://www.paulgraham.com/worked.html\"\ndocuments = loader.load_data(urls=[url])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e8bda47-2ea0-458f-9cd3-d0a05b9d569c": {"__data__": {"id_": "9e8bda47-2ea0-458f-9cd3-d0a05b9d569c", "embedding": null, "metadata": {"filename": "chatgpt_plugins.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "816ae0aacc09beb27107f1a46864e8bf70945e7b", "node_type": "4", "metadata": {"filename": "chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "e1a70dfde18960591b0918415fda1c71cc5d8421affc1b94d5b84d4ed2faef9b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d43b1aa6-4a86-44ca-9ef8-a4aa1acade23", "node_type": "1", "metadata": {"filename": "chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "0c6fe0cc500ecac37290d7a374e7dea61012258ab256e383bb4b76624a55f0b2", "class_name": "RelatedNodeInfo"}}, "text": "# Convert LlamaIndex Documents to JSON format\ndef dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:\n    \"\"\"Convert LlamaIndex Documents to JSON format and save it.\"\"\"\n    result_json = []\n    for doc in documents:\n        cur_dict = {\n            \"text\": doc.get_text(),\n            \"id\": doc.get_doc_id(),\n            # NOTE: feel free to customize the other fields as you wish\n            # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage\n            # \"source\": ...,\n            # \"source_id\": ...,\n            # \"url\": url,\n            # \"created_at\": ...,\n            # \"author\": \"Paul Graham\",\n        }\n        result_json.append(cur_dict)\n\n    json.dump(result_json, open(out_path, \"w\"))\n```\n\nFor more details, check out the [full example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatgpt_plugin/ChatGPT_Retrieval_Plugin_Upload.ipynb).\n\n### ChatGPT Retrieval Plugin Data Loader\n\nThe ChatGPT Retrieval Plugin data loader [can be accessed on LlamaHub](https://llamahub.ai/l/chatgpt_plugin).\n\nIt allows you to easily load data from any docstore that implements the plugin API, into a LlamaIndex data structure.\n\nExample code:\n\n```python\nfrom llama_index.readers.chatgpt_plugin import ChatGPTRetrievalPluginReader\nimport os\n\n# load documents\nbearer_token = os.getenv(\"BEARER_TOKEN\")\nreader = ChatGPTRetrievalPluginReader(\n    endpoint_url=\"http://localhost:8000\", bearer_token=bearer_token\n)\ndocuments = reader.load_data(\"What did the author do growing up?\")\n\n# build and query index\nfrom llama_index.core import SummaryIndex\n\nindex = SummaryIndex.from_documents(documents)\n# set Logging to DEBUG for more detailed outputs\nquery_engine = vector_index.as_query_engine(response_mode=\"compact\")\nresponse = query_engine.query(\n    \"Summarize the retrieved content and describe what the author did growing up\",\n)\n```\n\nFor more details, check out the [full example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatgpt_plugin/ChatGPTRetrievalPluginReaderDemo.ipynb).\n\n### ChatGPT Retrieval Plugin Index\n\nThe ChatGPT Retrieval Plugin Index allows you to easily build a vector index over any documents, with storage backed by a document store implementing the\nChatGPT endpoint.\n\nNote: this index is a vector index, allowing top-k retrieval.\n\nExample code:\n\n```python\nfrom llama_index.core.indices.vector_store import ChatGPTRetrievalPluginIndex\nfrom llama_index.core import SimpleDirectoryReader\nimport os\n\n# load documents\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\n\n# build index\nbearer_token = os.getenv(\"BEARER_TOKEN\")\n# initialize without metadata filter\nindex = ChatGPTRetrievalPluginIndex(\n    documents,\n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token,\n)\n\n# query index\nquery_engine = vector_index.as_query_engine(\n    similarity_top_k=3,\n    response_mode=\"compact\",\n)\nresponse = query_engine.query(\"What did the author do growing up?\")\n```\n\nFor more details, check out the [full example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatgpt_plugin/ChatGPTRetrievalPluginIndexDemo.ipynb).", "mimetype": "text/plain", "start_char_idx": 1431, "end_char_idx": 4633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef08b63d-1ff5-4281-81dc-e05059f0da88": {"__data__": {"id_": "ef08b63d-1ff5-4281-81dc-e05059f0da88", "embedding": null, "metadata": {"filename": "deepeval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ee13d2045ec53fe785be09e8395f3e5a7aa31bd", "node_type": "4", "metadata": {"filename": "deepeval.md", "author": "LlamaIndex"}, "hash": "cc426ba69977cf13cea4215cafcfbc94f2bf4ac456cf0509269d1110bb4319b9", "class_name": "RelatedNodeInfo"}}, "text": "# Unit Testing LLMs/RAG With DeepEval\n\n[DeepEval](https://github.com/confident-ai/deepeval) provides unit testing for AI agents and LLM-powered applications. It provides a really simple interface for LlamaIndex users to write tests for LLM outputs and helps developers catch breaking changes in production.\n\nDeepEval provides an opinionated framework to measure responses and is completely open-source.\n\n### Installation and Setup\n\nAdding [DeepEval](https://github.com/confident-ai/deepeval) is simple and requires 0 setup. To install:\n\n```sh\npip install -U deepeval\n# Optional step: Login to get a nice dashboard for your tests later!\ndeepeval login\n```\n\nOnce installed, you can create a `test_rag.py` start writing tests.\n\n```python title=\"test_rag.py\"\nimport pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\n\ndef test_case():\n    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n    test_case = LLMTestCase(\n        input=\"What if these shoes don't fit?\",\n        # Replace this with the actual output from your LLM application\n        actual_output=\"We offer a 30-day full refund at no extra costs.\",\n        retrieval_context=[\n            \"All customers are eligible for a 30 day full refund at no extra costs.\"\n        ],\n    )\n    assert_test(test_case, [answer_relevancy_metric])\n```\n\nYou can then run tests as such:\n\n```bash\ndeepeval test run test_rag.py\n```\n\nIf you're logged in, you'll be able to analyze evaluation results on `deepeval`'s dashboard:\n\n![Sample dashboard](https://d2lsxfc3p6r9rv.cloudfront.net/confident-test-cases.png)\n\n## Types of Metrics\n\nDeepEval presents an opinionated framework for unit testing RAG applications. It breaks down evaluations into test cases, and offers a range of evaluation metrics that you can freely evaluate for each test case, including:\n\n- G-Eval\n- Summarization\n- Answer Relevancy\n- Faithfulness\n- Contextual Recall\n- Contextual Precision\n- Contextual Relevancy\n- RAGAS\n- Hallucination\n- Bias\n- Toxicity\n\n[DeepEval](https://github.com/confident-ai/deepeval) incorporates the latest research into its evaluation metrics. You can learn more about the full list of metrics and how they are calculated [here.](https://docs.confident-ai.com/docs/metrics-introduction)\n\n## Evaluating RAG for Your LlamaIndex Application\n\nDeepEval integrates nicely with LlamaIndex's `BaseEvaluator` class. Below is an example usage of DeepEval's evaluation metrics in the form of a LlamaIndex evaluator.\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom deepeval.integrations.llama_index import DeepEvalAnswerRelevancyEvaluator\n\n# Read LlamaIndex's quickstart on more details\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nrag_application = index.as_query_engine()\n\n# An example input to your RAG application\nuser_input = \"What is LlamaIndex?\"\n\n# LlamaIndex returns a response object that contains\n# both the output string and retrieved nodes\nresponse_object = rag_application.query(user_input)\n\nevaluator = DeepEvalAnswerRelevancyEvaluator()\n```\n\nYou can then evaluate as such:\n\n```python\nevaluation_result = evaluator.evaluate_response(\n    query=user_input, response=response_object\n)\nprint(evaluation_result)\n```\n\n### Full List of Evaluators\n\nHere is how you can import all 6 evaluators from `deepeval`:\n\n```python\nfrom deepeval.integrations.llama_index import (\n    DeepEvalAnswerRelevancyEvaluator,\n    DeepEvalFaithfulnessEvaluator,\n    DeepEvalContextualRelevancyEvaluator,\n    DeepEvalSummarizationEvaluator,\n    DeepEvalBiasEvaluator,\n    DeepEvalToxicityEvaluator,\n)\n```\n\nFor all evaluator definitions and to understand how it integrates with DeepEval's testing suite, [click here.](https://docs.confident-ai.com/docs/integrations-llamaindex)\n\n### Useful Links\n\n- [DeepEval Quickstart](https://docs.confident-ai.com/docs/getting-started)\n- [Everything you need to know about LLM evaluation metrics](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85fa35a9-6d8c-4a8b-b6b5-2101e643907a": {"__data__": {"id_": "85fa35a9-6d8c-4a8b-b6b5-2101e643907a", "embedding": null, "metadata": {"filename": "fleet_libraries_context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59b15025d2314a7d91b792d39f42fec3ad8d9928", "node_type": "4", "metadata": {"filename": "fleet_libraries_context.md", "author": "LlamaIndex"}, "hash": "62bae7955166497e720f3fd68b0ddaa3bb2444ef8b5b8effdc31ac1973c841a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab2a1282-0a49-4fa2-ac77-4d914dfc5244", "node_type": "1", "metadata": {}, "hash": "e86a068fd81d03f07b0b7d1759b406b8b73a2affae21acf2fcf42a84d1203394", "class_name": "RelatedNodeInfo"}}, "text": "# Fleet Context Embeddings - Building a Hybrid Search Engine for the Llamaindex Library\n\nIn this guide, we will be using Fleet Context to download the embeddings for LlamaIndex's documentation and build a hybrid dense/sparse vector retrieval engine on top of it.\n\n<br><br>\n\n## Pre-requisites\n\n```\n!pip install llama-index\n!pip install --upgrade fleet-context\n```\n\n```\nimport os\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # add your API key here!\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n```\n\n<br><br>\n\n## Download Embeddings from Fleet Context\n\nWe will be using Fleet Context to download the embeddings for the\nentirety of LlamaIndex\\'s documentation (\\~12k chunks, \\~100mb of\ncontent). You can download for any of the top 1220 libraries by\nspecifying the library name as a parameter. You can view the full list\nof supported libraries [here](https://fleet.so/context) at the bottom of\nthe page.\n\nWe do this because Fleet has built a embeddings pipeline that preserves\na lot of important information that will make the retrieval and\ngeneration better including position on page (for re-ranking), chunk\ntype (class/function/attribute/etc), the parent section, and more. You\ncan read more about this on their [Github\npage](https://github.com/fleet-ai/context/tree/main).\n\n```python\nfrom context import download_embeddings\n\ndf = download_embeddings(\"llamaindex\")\n```\n\n**Output**:\n\n```shell\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 83.7M/83.7M [00:03<00:00, 27.4MiB/s]\n                                         id  \\\n    0  e268e2a1-9193-4e7b-bb9b-7a4cb88fc735\n    1  e495514b-1378-4696-aaf9-44af948de1a1\n    2  e804f616-7db0-4455-9a06-49dd275f3139\n    3  eb85c854-78f1-4116-ae08-53b2a2a9fa41\n    4  edfc116e-cf58-4118-bad4-c4bc0ca1495e\n```\n\n```python\n# Show some examples of the metadata\ndf[\"metadata\"][0]\ndisplay(Markdown(f\"{df['metadata'][8000]['text']}\"))\n```\n\n**Output**:\n\n```shell\nclassmethod from_dict(data: Dict[str, Any], kwargs: Any) \u2192 Self\uf0c1 classmethod from_json(data_str: str, kwargs: Any) \u2192 Self\uf0c1 classmethod from_orm(obj: Any) \u2192 Model\uf0c1 json(, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True*, dumps_kwargs: Any) \u2192 unicode\uf0c1 Generate a JSON representation of the model, include and exclude arguments as per dict().\n```\n\n<br><br>\n\n## Create Pinecone Index for Hybrid Search in LlamaIndex\n\nWe\\'re going to create a Pinecone index and upsert our vectors there so\nthat we can do hybrid retrieval with both sparse vectors and dense\nvectors. Make sure you have a [Pinecone account](https://pinecone.io)\nbefore you proceed.\n\n```python\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().handlers = []\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n```\n\n```python\nimport pinecone\n\napi_key = \"...\"  # Add your Pinecone API key here\npinecone.init(\n    api_key=api_key, environment=\"us-east-1-aws\"\n)  # Add your db region here\n```\n\n```python\n# Fleet Context uses the text-embedding-ada-002 model from OpenAI with 1536 dimensions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab2a1282-0a49-4fa2-ac77-4d914dfc5244": {"__data__": {"id_": "ab2a1282-0a49-4fa2-ac77-4d914dfc5244", "embedding": null, "metadata": {"filename": "fleet_libraries_context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59b15025d2314a7d91b792d39f42fec3ad8d9928", "node_type": "4", "metadata": {"filename": "fleet_libraries_context.md", "author": "LlamaIndex"}, "hash": "62bae7955166497e720f3fd68b0ddaa3bb2444ef8b5b8effdc31ac1973c841a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85fa35a9-6d8c-4a8b-b6b5-2101e643907a", "node_type": "1", "metadata": {"filename": "fleet_libraries_context.md", "author": "LlamaIndex"}, "hash": "f92460a33c34e48337650602b888e8be9f6c1669961a10d06245d67757a728cb", "class_name": "RelatedNodeInfo"}}, "text": "# NOTE: Pinecone requires dotproduct similarity for hybrid search\npinecone.create_index(\n    \"quickstart-fleet-context\",\n    dimension=1536,\n    metric=\"dotproduct\",\n    pod_type=\"p1\",\n)\n\npinecone.describe_index(\n    \"quickstart-fleet-context\"\n)  # Make sure you create an index in pinecone\n```\n\n<br>\n\n```python\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\n\npinecone_index = pinecone.Index(\"quickstart-fleet-context\")\nvector_store = PineconeVectorStore(pinecone_index, add_sparse_vector=True)\n```\n\n<br><br>\n\n## Batch upsert vectors into Pinecone\n\nPinecone recommends upserting 100 vectors at a time. We\\'re going to do that after we modify the format of the data a bit.\n\n```python\nimport random\nimport itertools\n\n\ndef chunks(iterable, batch_size=100):\n    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n\n\n# generator that generates many (id, vector, metadata, sparse_values) pairs\ndata_generator = map(\n    lambda row: {\n        \"id\": row[1][\"id\"],\n        \"values\": row[1][\"values\"],\n        \"metadata\": row[1][\"metadata\"],\n        \"sparse_values\": row[1][\"sparse_values\"],\n    },\n    df.iterrows(),\n)\n\n# Upsert data with 1000 vectors per upsert request\nfor ids_vectors_chunk in chunks(data_generator, batch_size=100):\n    print(f\"Upserting {len(ids_vectors_chunk)} vectors...\")\n    pinecone_index.upsert(vectors=ids_vectors_chunk)\n```\n\n<br><br>\n\n## Build Pinecone Vector Store in LlamaIndex\n\nFinally, we\\'re going to build the Pinecone vector store via LlamaIndex\nand query it to get results.\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom IPython.display import Markdown, display\n```\n\n```python\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n```\n\n<br><br>\n\n## Query Your Index!\n\n```python\nquery_engine = index.as_query_engine(\n    vector_store_query_mode=\"hybrid\", similarity_top_k=8\n)\nresponse = query_engine.query(\"How do I use llama_index SimpleDirectoryReader\")\n```\n\n```python\ndisplay(Markdown(f\"<b>{response}</b>\"))\n```\n\n**Output**:\n\n```shell\n<b>To use the SimpleDirectoryReader in llama_index, you need to import it from the llama_index library. Once imported, you can create an instance of the SimpleDirectoryReader class by providing the directory path as an argument. Then, you can use the `load_data()` method on the SimpleDirectoryReader instance to load the documents from the specified directory.</b>\n```", "mimetype": "text/plain", "start_char_idx": 3322, "end_char_idx": 5901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52efb7ed-147f-4041-b745-4298216dc14a": {"__data__": {"id_": "52efb7ed-147f-4041-b745-4298216dc14a", "embedding": null, "metadata": {"filename": "graph_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5404ce4620cc5b83879a69b31999055595dfcecb", "node_type": "4", "metadata": {"filename": "graph_stores.md", "author": "LlamaIndex"}, "hash": "e760dadc26221bfb0b422540d9f0c930be99d0169f6df1514a41728f3843c16c", "class_name": "RelatedNodeInfo"}}, "text": "# Using Graph Stores\n\n## `Neo4jGraphStore`\n\n`Neo4j` is supported as a graph store integration. You can persist, visualize, and query graphs using LlamaIndex and Neo4j. Furthermore, existing Neo4j graphs are directly supported using `text2cypher` and the `KnowledgeGraphQueryEngine`.\n\nIf you've never used Neo4j before, you can download the desktop client [here](https://neo4j.com/download/).\n\nOnce you open the client, create a new project and install the `apoc` integration. Full instructions [here](https://neo4j.com/labs/apoc/4.1/installation/). Just click on your project, select `Plugins` on the left side menu, install APOC and restart your server.\n\nSee the example of using the [Neo4j Graph Store](../../examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.ipynb).\n\n## `NebulaGraphStore`\n\nWe support a `NebulaGraphStore` integration, for persisting graphs directly in Nebula! Furthermore, you can generate cypher queries and return natural language responses for your Nebula graphs using the `KnowledgeGraphQueryEngine`.\n\nSee the associated guides below:\n\n- [Nebula Graph Store](../../examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.ipynb)\n- [Knowledge Graph Query Engine](../../examples/query_engine/knowledge_graph_query_engine.ipynb)\n\n## `KuzuGraphStore`\n\nWe support a `KuzuGraphStore` integration, for persisting graphs directly in [Kuzu](https://kuzudb.com).\n\nSee the associated guides below:\n\n- [Kuzu Graph Store](../../examples/index_structs/knowledge_graph/KuzuGraphDemo.ipynb)\n\n## `FalkorDBGraphStore`\n\nWe support a `FalkorDBGraphStore` integration, for persisting graphs directly in FalkorDB! Furthermore, you can generate cypher queries and return natural language responses for your FalkorDB graphs using the `KnowledgeGraphQueryEngine`.\n\nSee the associated guides below:\n\n- [FalkorDB Graph Store](../../examples/index_structs/knowledge_graph/FalkorDBGraphDemo.ipynb)\n\n## `Amazon Neptune Graph Stores`\n\nWe support `Amazon Neptune` integrations for both [Neptune Database](https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview.html) and [Neptune Analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html) as a graph store integration.\n\nSee the associated guides below:\n\n- [Amazon Neptune Graph Store](../../examples/index_structs/knowledge_graph/NeptuneDatabaseKGIndexDemo).\n\n\n## `TiDB Graph Store`\n\nWe support a `TiDBGraphStore` integration, for persisting graphs directly in [TiDB](https://docs.pingcap.com/tidb/stable/overview)!\n\nSee the associated guides below:\n\n- [TiDB Graph Store](../../examples/index_structs/knowledge_graph/TiDBKGIndexDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c3f7c90-ae57-4a94-93e8-67ed3cde8a2f": {"__data__": {"id_": "7c3f7c90-ae57-4a94-93e8-67ed3cde8a2f", "embedding": null, "metadata": {"filename": "graphsignal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51fd3ad759e55dda2ab917c0de14aa3a85241481", "node_type": "4", "metadata": {"filename": "graphsignal.md", "author": "LlamaIndex"}, "hash": "b132fc698b032c405307b15c9090c4b532a07110d00aebeb7512788d6a9d4838", "class_name": "RelatedNodeInfo"}}, "text": "# Tracing with Graphsignal\n\n[Graphsignal](https://graphsignal.com/) provides observability for AI agents and LLM-powered applications. It helps developers ensure AI applications run as expected and users have the best experience.\n\nGraphsignal **automatically** traces and monitors LlamaIndex. Traces and metrics provide execution details for query, retrieval, and index operations. These insights include **prompts**, **completions**, **embedding statistics**, **retrieved nodes**, **parameters**, **latency**, and **exceptions**.\n\nWhen OpenAI APIs are used, Graphsignal provides additional insights such as **token counts** and **costs** per deployment, model or any context.\n\n### Installation and Setup\n\nAdding [Graphsignal tracer](https://github.com/graphsignal/graphsignal-python) is simple, just install and configure it:\n\n```sh\npip install graphsignal\n```\n\n```python\nimport graphsignal\n\n# Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable\ngraphsignal.configure(\n    api_key=\"my-api-key\", deployment=\"my-llama-index-app-prod\"\n)\n```\n\nYou can get an API key [here](https://app.graphsignal.com/).\n\nSee the [Quick Start guide](https://graphsignal.com/docs/guides/quick-start/), [Integration guide](https://graphsignal.com/docs/integrations/llama-index/), and an [example app](https://github.com/graphsignal/examples/blob/main/llama-index-app/main.py) for more information.\n\n### Tracing Other Functions\n\nTo additionally trace any function or code, you can use a decorator or a context manager:\n\n```python\nwith graphsignal.start_trace(\"load-external-data\"):\n    reader.load_data()\n```\n\nSee [Python API Reference](https://graphsignal.com/docs/reference/python-api/) for complete instructions.\n\n### Useful Links\n\n- [Tracing and Monitoring LlamaIndex Applications](https://graphsignal.com/blog/tracing-and-monitoring-llama-index-applications/)\n- [Monitor OpenAI API Latency, Tokens, Rate Limits, and More](https://graphsignal.com/blog/monitor-open-ai-api-latency-tokens-rate-limits-and-more/)\n- [OpenAI API Cost Tracking: Analyzing Expenses by Model, Deployment, and Context](https://graphsignal.com/blog/open-ai-api-cost-tracking-analyzing-expenses-by-model-deployment-and-context/)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7eec17f9-6748-4cde-bb55-0ef1096cb92c": {"__data__": {"id_": "7eec17f9-6748-4cde-bb55-0ef1096cb92c", "embedding": null, "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10715a649a423561966a0e92d4b8b73dc8f759fb", "node_type": "4", "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}, "hash": "cf5ce0ac70c46b7193b4ceb3d14b861bf45b392c7628f7e2a7efb24675f95235", "class_name": "RelatedNodeInfo"}}, "text": "# Guidance\n\n[Guidance](https://github.com/microsoft/guidance) is a guidance language for controlling large language models developed by Microsoft.\n\nGuidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text.\n\n## Structured Output\n\nOne particularly exciting aspect of guidance is the ability to output structured objects (think JSON following a specific schema, or a pydantic object). Instead of just \"suggesting\" the desired output structure to the LLM, guidance can actually \"force\" the LLM output to follow the desired schema. This allows the LLM to focus on the content rather than the syntax, and completely eliminate the possibility of output parsing issues.\n\nThis is particularly powerful for weaker LLMs which be smaller in parameter count, and not trained on sufficient source code data to be able to reliably produce well-formed, hierarchical structured output.\n\n### Creating a guidance program to generate pydantic objects\n\nIn LlamaIndex, we provide an initial integration with guidance, to make it super easy for generating structured output (more specifically pydantic objects).\n\nFor example, if we want to generate an album of songs, with the following schema:\n\n```python\nclass Song(BaseModel):\n    title: str\n    length_seconds: int\n\n\nclass Album(BaseModel):\n    name: str\n    artist: str\n    songs: List[Song]\n```\n\nIt's as simple as creating a `GuidancePydanticProgram`, specifying our desired pydantic class `Album`,\nand supplying a suitable prompt template.\n\n> Note: guidance uses handlebars-style templates, which uses double braces for variable substitution, and single braces for literal braces. This is the opposite convention of Python format strings.\n\nfrom llama_index.core.prompts.guidance_utils import convert_to_handlebars` that can convert from the Python format string style template to guidance handlebars-style template.\n\n```python\nprogram = GuidancePydanticProgram(\n    output_cls=Album,\n    prompt_template_str=\"Generate an example album, with an artist and a list of songs. Using the movie {{movie_name}} as inspiration\",\n    guidance_llm=OpenAI(\"text-davinci-003\"),\n    verbose=True,\n)\n```\n\nNow we can run the program by calling it with additional user input.\nHere let's go for something spooky and create an album inspired by the Shining.\n\n```python\noutput = program(movie_name=\"The Shining\")\n```\n\nWe have our pydantic object:\n\n```python\nAlbum(\n    name=\"The Shining\",\n    artist=\"Jack Torrance\",\n    songs=[\n        Song(title=\"All Work and No Play\", length_seconds=180),\n        Song(title=\"The Overlook Hotel\", length_seconds=240),\n        Song(title=\"The Shining\", length_seconds=210),\n    ],\n)\n```\n\nYou can play with [this notebook](../../examples/output_parsing/guidance_pydantic_program.ipynb) for more details.\n\n### Using guidance to improve the robustness of our sub-question query engine.\n\nLlamaIndex provides a toolkit of advanced query engines for tackling different use-cases.\nSeveral rely on structured output in intermediate steps.\nWe can use guidance to improve the robustness of these query engines, by making sure the\nintermediate response has the expected structure (so that they can be parsed correctly to a structured object).\n\nAs an example, we implement a `GuidanceQuestionGenerator` that can be plugged into a `SubQuestionQueryEngine` to make it more robust than using the default setting.\n\n```python\nfrom llama_index.question_gen.guidance import GuidanceQuestionGenerator\nfrom guidance.llms import OpenAI as GuidanceOpenAI\n\n# define guidance based question generator\nquestion_gen = GuidanceQuestionGenerator.from_defaults(\n    guidance_llm=GuidanceOpenAI(\"text-davinci-003\"), verbose=False\n)\n\n# define query engine tools\nquery_engine_tools = ...\n\n# construct sub-question query engine\ns_engine = SubQuestionQueryEngine.from_defaults(\n    question_gen=question_gen,  # use guidance based question_gen defined above\n    query_engine_tools=query_engine_tools,\n)\n```\n\nSee [this notebook](../../examples/output_parsing/guidance_sub_question.ipynb) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "068fdf37-cffb-4ec8-aa88-804d885d395d": {"__data__": {"id_": "068fdf37-cffb-4ec8-aa88-804d885d395d", "embedding": null, "metadata": {"filename": "lmformatenforcer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff51ec2aaec9e9b17d6bd2eeccf865480c1ef8e6", "node_type": "4", "metadata": {"filename": "lmformatenforcer.md", "author": "LlamaIndex"}, "hash": "941df7297ee81bcb7d2bec2e9bd00fbf2f8612c4b74df6b69ff37482ae0bd9a8", "class_name": "RelatedNodeInfo"}}, "text": "# LM Format Enforcer\n\n[LM Format Enforcer](https://github.com/noamgat/lm-format-enforcer) is a library that enforces the output format (JSON Schema, Regex etc) of a language model. Instead of just \"suggesting\" the desired output structure to the LLM, LM Format Enforcer can actually \"force\" the LLM output to follow the desired schema.\n\n![image](https://raw.githubusercontent.com/noamgat/lm-format-enforcer/main/docs/Intro.webp)\n\nLM Format Enforcer works with local LLMs (currently supports `LlamaCPP` and `HuggingfaceLLM` backends), and operates only by processing the output logits of the LLM. This enables it to support advanced generation methods like beam search and batching, unlike other solutions that modify the generation loop itself. See the comparison table in the [LM Format Enforcer page](https://github.com/noamgat/lm-format-enforcer) for more details.\n\n## JSON Schema Output\n\nIn LlamaIndex, we provide an initial integration with LM Format Enforcer, to make it super easy for generating structured output (more specifically pydantic objects).\n\nFor example, if we want to generate an album of songs, with the following schema:\n\n```python\nclass Song(BaseModel):\n    title: str\n    length_seconds: int\n\n\nclass Album(BaseModel):\n    name: str\n    artist: str\n    songs: List[Song]\n```\n\nIt's as simple as creating a `LMFormatEnforcerPydanticProgram`, specifying our desired pydantic class `Album`,\nand supplying a suitable prompt template.\n\n> Note: `LMFormatEnforcerPydanticProgram` automatically fills in the json schema of the pydantic class in the optional `{json_schema}` parameter of the prompt template. This can help the LLM naturally generate the correct JSON and reduce the interference aggression of the format enforcer, increasing output quality.\n\n```python\nprogram = LMFormatEnforcerPydanticProgram(\n    output_cls=Album,\n    prompt_template_str=\"Generate an example album, with an artist and a list of songs. Using the movie {movie_name} as inspiration. You must answer according to the following schema: \\n{json_schema}\\n\",\n    llm=LlamaCPP(),\n    verbose=True,\n)\n```\n\nNow we can run the program by calling it with additional user input.\nHere let's go for something spooky and create an album inspired by the Shining.\n\n```python\noutput = program(movie_name=\"The Shining\")\n```\n\nWe have our pydantic object:\n\n```python\nAlbum(\n    name=\"The Shining: A Musical Journey Through the Haunted Halls of the Overlook Hotel\",\n    artist=\"The Shining Choir\",\n    songs=[\n        Song(title=\"Redrum\", length_seconds=300),\n        Song(\n            title=\"All Work and No Play Makes Jack a Dull Boy\",\n            length_seconds=240,\n        ),\n        Song(title=\"Heeeeere's Johnny!\", length_seconds=180),\n    ],\n)\n```\n\nYou can play with [this notebook](../../examples/output_parsing/lmformatenforcer_pydantic_program.ipynb) for more details.\n\n## Regular Expression Output\n\nLM Format Enforcer also supports regex output. Since there is no existing abstraction for regular expressions in LlamaIndex, we will use the LLM directly, after injecting the LM Format Generator in it.\n\n```python\nregex = r'\"Hello, my name is (?P<name>[a-zA-Z]*)\\. I was born in (?P<hometown>[a-zA-Z]*). Nice to meet you!\"'\nprompt = \"Here is a way to present myself, if my name was John and I born in Boston: \"\n\nllm = LlamaCPP()\nregex_parser = lmformatenforcer.RegexParser(regex)\nlm_format_enforcer_fn = build_lm_format_enforcer_function(llm, regex_parser)\nwith activate_lm_format_enforcer(llm, lm_format_enforcer_fn):\n    output = llm.complete(prompt)\n```\n\nThis will cause the LLM to generate output in the regular expression format that we specified. We can also parse the output to get the named groups:\n\n```python\nprint(output)\n# \"Hello, my name is John. I was born in Boston, Nice to meet you!\"\nprint(re.match(regex, output.text).groupdict())\n# {'name': 'John', 'hometown': 'Boston'}\n```\n\nSee [this notebook](../../examples/output_parsing/lmformatenforcer_regular_expressions.ipynb) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1db8893-6947-4ad1-8efe-868d4db5a704": {"__data__": {"id_": "d1db8893-6947-4ad1-8efe-868d4db5a704", "embedding": null, "metadata": {"filename": "managed_indices.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7abc011a97485e5f645b91b8f5dc07b71b8fa8ef", "node_type": "4", "metadata": {"filename": "managed_indices.md", "author": "LlamaIndex"}, "hash": "8b1304765372fa01b27a06c100cfa72007decf7f352342474ae464f64c5244ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f14098-8699-43d2-b987-20acbb011b0a", "node_type": "1", "metadata": {}, "hash": "64db84c7b1df2db3e5e177c7efdd8906cfa482f007d53442e2c958205e6105e3", "class_name": "RelatedNodeInfo"}}, "text": "# Using Managed Indices\n\nLlamaIndex offers multiple integration points with Managed Indices. A managed index is a special type of index that is not managed locally as part of LlamaIndex but instead is managed via an API, such as [Vectara](https://vectara.com).\n\n## Using a Managed Index\n\nSimilar to any other index within LlamaIndex (tree, keyword table, list), any `ManagedIndex` can be constructed with a collection\nof documents. Once constructed, the index can be used for querying.\n\nIf the Index has been previously populated with documents - it can also be used directly for querying.\n\n## Google Generative Language Semantic Retriever\n\nGoogle's Semantic Retrieve provides both querying and retrieval capabilities. Create a managed index, insert documents, and use a query engine or retriever anywhere in LlamaIndex!\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.indices.managed.google import GoogleIndex\n\n# Create a corpus\nindex = GoogleIndex.create_corpus(display_name=\"My first corpus!\")\nprint(f\"Newly created corpus ID is {index.corpus_id}.\")\n\n# Ingestion\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex.insert_documents(documents)\n\n# Querying\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n\n# Retrieving\nretriever = index.as_retriever()\nsource_nodes = retriever.retrieve(\"What did the author do growing up?\")\n```\n\nSee the [notebook guide](../../examples/managed/GoogleDemo.ipynb) for full details.\n\n## Vectara\n\nFirst, [sign up](https://vectara.com/integrations/llama_index) and use the Vectara Console to create a corpus (aka Index), and add an API key for access.\nThen put the customer id, corpus id, and API key in your environment.\n\nThen construct the Vectara Index and query it as follows:\n\n```python\nfrom llama_index.core import ManagedIndex, SimpleDirectoryReade\nfrom llama_index.indices.managed.vectara import VectaraIndex\n\n# Load documents and build index\nvectara_customer_id = os.environ.get(\"VECTARA_CUSTOMER_ID\")\nvectara_corpus_id = os.environ.get(\"VECTARA_CORPUS_ID\")\nvectara_api_key = os.environ.get(\"VECTARA_API_KEY\")\n\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\nindex = VectaraIndex.from_documents(\n    documents,\n    vectara_customer_id=vectara_customer_id,\n    vectara_corpus_id=vectara_corpus_id,\n    vectara_api_key=vectara_api_key,\n)\n```\n\nNotes:\n* If the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY` are in the environment already, you do not have to explicitly specify them in your call and the VectaraIndex class will read them from the environment.\n* To connect to multiple Vectara corpora, you can set `VECTARA_CORPUS_ID` to a comma-separated list, for example: `12,51` would connect to corpus `12` and corpus `51`.\n\nIf you already have documents in your corpus, you can just access the data directly by constructing the `VectaraIndex` as follows:\n\n```python\nindex = VectaraIndex()\n```\n\nAnd the index will connect to the existing corpus without loading any new documents.\n\nTo query the index, simply construct a query engine as follows:\n\n```python\nquery_engine = index.as_query_engine(summary_enabled=True)\nprint(query_engine.query(\"What did the author do growing up?\"))\n```\n\nOr you can use the chat functionality:\n\n```python\nchat_engine = index.as_chat_engine()\nprint(chat_engine.chat(\"What did the author do growing up?\").response)\n```\n\nChat works as you expect where subsequent `chat` calls maintain a conversation history. All of this is done on the Vectara platform so you don't have to add any additional logic.\n\nFor more examples - please see below:\n\n- [Vectara Demo](../../examples/managed/vectaraDemo.ipynb)\n- [Vectara AutoRetriever](../../examples/retrievers/vectara_auto_retriever.ipynb)\n\n## Vertex AI RAG (LlamaIndex on Vertex AI)\n\n[LlamaIndex on Vertex AI for RAG](https://cloud.google.com/vertex-ai/generative-ai/docs/llamaindex-on-vertexai) is a managed RAG index on Google Cloud Vertex AI.\n\nFirst, [create a Google Cloud project and enable the Vertex AI API](https://cloud.google.com/vertex-ai/docs/start/cloud-environment). Then run the following code to create a managed index.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69f14098-8699-43d2-b987-20acbb011b0a": {"__data__": {"id_": "69f14098-8699-43d2-b987-20acbb011b0a", "embedding": null, "metadata": {"filename": "managed_indices.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7abc011a97485e5f645b91b8f5dc07b71b8fa8ef", "node_type": "4", "metadata": {"filename": "managed_indices.md", "author": "LlamaIndex"}, "hash": "8b1304765372fa01b27a06c100cfa72007decf7f352342474ae464f64c5244ff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1db8893-6947-4ad1-8efe-868d4db5a704", "node_type": "1", "metadata": {"filename": "managed_indices.md", "author": "LlamaIndex"}, "hash": "0b27fc5f4c54529cf5d5c9172fc16573ec6200a8e90961384dbffa2263f42f09", "class_name": "RelatedNodeInfo"}}, "text": "Then run the following code to create a managed index.\n\n```python\nfrom llama_index.indices.managed.vertexai import VertexAIIndex\n\n# TODO(developer): Replace these values with your project information\nproject_id = \"YOUR_PROJECT_ID\"\nlocation = \"us-central1\"\n\n# Optional: If using an existing corpus\ncorpus_id = \"YOUR_CORPUS_ID\"\n\n# Optional: If creating a new corpus\ncorpus_display_name = \"my-corpus\"\ncorpus_description = \"Vertex AI Corpus for LlamaIndex\"\n\n# Create a corpus or provide an existing corpus ID\nindex = VertexAIIndex(\n    project_id,\n    location,\n    corpus_display_name=corpus_display_name,\n    corpus_description=corpus_description,\n)\nprint(f\"Newly created corpus name is {index.corpus_name}.\")\n\n# Import files from Google Cloud Storage or Google Drive\nindex.import_files(\n    uris=[\"https://drive.google.com/file/123\", \"gs://my_bucket/my_files_dir\"],\n    chunk_size=512,  # Optional\n    chunk_overlap=100,  # Optional\n)\n\n# Upload local file\nindex.insert_file(\n    file_path=\"my_file.txt\",\n    metadata={\"display_name\": \"my_file.txt\", \"description\": \"My file\"},\n)\n\n# Querying\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is RAG and why it is helpful?\")\n\n# Retrieving\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"What is RAG and why it is helpful?\")\n```\n\nSee the [notebook guide](../../examples/managed/VertexAIDemo.ipynb) for full details.\n\n## Zilliz\n\nFirst, set up your [Zilliz Cloud](https://cloud.zilliz.com/signup?utm_source=twitter&utm_medium=social%20&utm_campaign=2023-12-22_social_pipeline-llamaindex_twitter) account and create a free serverless cluster.\nThen copy the Project ID, Cluster ID and API Key from your account.\n\nNow you can construct `ZillizCloudPipelineIndex` to index docs and query as follows:\n\n```python\nimport os\n\nfrom llama_index.core import ManagedIndex\nfrom llama_index.indices.managed.zilliz import ZillizCloudPipelineIndex\n\n# Load documents from url and build document index\nzcp_index = ZillizCloudPipelineIndex.from_document_url(\n    url=\"https://publicdataset.zillizcloud.com/milvus_doc.md\",\n    project_id=\"<YOUR_ZILLIZ_PROJECT_ID>\",\n    cluster_id=\"<YOUR_ZILLIZ_CLUSTER_ID>\",\n    token=\"<YOUR_ZILLIZ_API_KEY>\",\n    metadata={\"version\": \"2.3\"},  # optional\n)\n\n# Insert more docs into index, eg. a Milvus v2.2 document\nzcp_index.insert_doc_url(\n    url=\"https://publicdataset.zillizcloud.com/milvus_doc_22.md\",\n    metadata={\"version\": \"2.2\"},\n)\n\n# Query index\nfrom llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n\nquery_engine_milvus23 = zcp_index.as_query_engine(\n    search_top_k=3,\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(key=\"version\", value=\"2.3\")\n        ]  # version == \"2.3\"\n    ),\n    output_metadata=[\"version\"],\n)\n\nquestion = \"Can users delete entities by complex boolean expressions?\"\n# Retrieving\nretrieval_result = query_engine_with_filters.retrieve(question)\n# Querying\nanswer = query_engine_with_filters.query(question)\n```\n\n- [Zilliz Example Notebook](../../examples/managed/zcpDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 4152, "end_char_idx": 7211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b3ac817-4e6a-4d82-9174-143c408338df": {"__data__": {"id_": "8b3ac817-4e6a-4d82-9174-143c408338df", "embedding": null, "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453fd7e59647e91d21ed790a04b0abb9438daf1f", "node_type": "4", "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "hash": "d25ab4c70f52eba917359326ecbf9f249689223c5e14717d10f766faecaf11a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "374a059b-5a51-4cd4-b599-4f0be4d84033", "node_type": "1", "metadata": {}, "hash": "318a53a46c33f8bbe81d84f839d5424f87fb54cf0da7bc810bf9c8f98e3dcbb4", "class_name": "RelatedNodeInfo"}}, "text": "# Tonic Validate\n\n## What is Tonic Validate\n\nTonic Validate is a tool for people developing retrieval augmented generation (RAG) systems to evaluate the performance of their system. You can use Tonic Validate for one-off spot checks of your LlamaIndex setup's performance or you can even use it inside an existing CI/CD system like Github Actions. There are two parts to Tonic Validate\n\n1. The Open-Source SDK\n2. [The Web UI](https://validate.tonic.ai/)\n\nYou can use the SDK without using the Web UI if you prefer. The SDK includes all of the tools needed to evaluate your RAG system. The purpose of the web UI is to provide a layer on top of the SDK for visualizing your results. This allows you to get a better sense of your system's performance as opposed to viewing only raw numbers.\n\nIf you want to use the web UI you can go to [here](https://validate.tonic.ai/) to sign up for a free account.\n\n## How to use Tonic Validate\n\n### Setting Up Tonic Validate\n\nYou can install Tonic Validate via the following command\n\n```\npip install tonic-validate\n```\n\nTo use Tonic Validate, you need to provide an OpenAI key as the score calculations use an LLM on the backend. You can set an OpenAI key via setting the `OPENAI_API_KEY` environmental variable to your OpenAI API key.\n\n```python\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"put-your-openai-api-key-here\"\n```\n\nIf you are uploading your results to the UI, also make sure to set your Tonic Validate API key you received during the account set up for the [web UI](https://validate.tonic.ai/). If you have not already set up your account on the web UI you can do so [here](https://validate.tonic.ai/). Once you have the API Key you can set it via the `TONIC_VALIDATE_API_KEY` environment variable.\n\n```python\nimport os\n\nos.environ[\"TONIC_VALIDATE_API_KEY\"] = \"put-your-validate-api-key-here\"\n```\n\n### One Question Usage Example\n\nFor this example, we have an example of a question with a reference correct answer that does not match the LLM response answer. There are two retrieved context chunks, of which one of them has the correct answer.\n\n```python\nquestion = \"What makes Sam Altman a good founder?\"\nreference_answer = \"He is smart and has a great force of will.\"\nllm_answer = \"He is a good founder because he is smart.\"\nretrieved_context_list = [\n    \"Sam Altman is a good founder. He is very smart.\",\n    \"What makes Sam Altman such a good founder is his great force of will.\",\n]\n```\n\nThe answer similarity score is a score between 0 and 5 that scores how well the LLM answer matches the reference answer. In this case, they do not match perfectly, so the answer similarity score is not a perfect 5.\n\n```python\nanswer_similarity_evaluator = AnswerSimilarityEvaluator()\nscore = await answer_similarity_evaluator.aevaluate(\n    question,\n    llm_answer,\n    retrieved_context_list,\n    reference_response=reference_answer,\n)\nprint(score)\n# >> EvaluationResult(query='What makes Sam Altman a good founder?', contexts=['Sam Altman is a good founder. He is very smart.', 'What makes Sam Altman such a good founder is his great force of will.'], response='He is a good founder because he is smart.', passing=None, feedback=None, score=4.0, pairwise_source=None, invalid_result=False, invalid_reason=None)\n```\n\nThe answer consistency score is between 0.0 and 1.0, and measure whether the answer has information that does not appear in the retrieved context. In this case, the answer does appear in the retrieved context, so the score is 1.\n\n```python\nanswer_consistency_evaluator = AnswerConsistencyEvaluator()\n\n\nscore = await answer_consistency_evaluator.aevaluate(\n    question, llm_answer, retrieved_context_list\n)\nprint(score)\n# >> EvaluationResult(query='What makes Sam Altman a good founder?', contexts=['Sam Altman is a good founder. He is very smart.', 'What makes Sam Altman such a good founder is his great force of will.'], response='He is a good founder because he is smart.', passing=None, feedback=None, score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)\n```\n\nAugmentation accuracy measures the percentage of the retrieved context that is in the answer. In this case, one of the retrieved contexts is in the answer, so this score is 0.5.\n\n```python\naugmentation_accuracy_evaluator = AugmentationAccuracyEvaluator()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "374a059b-5a51-4cd4-b599-4f0be4d84033": {"__data__": {"id_": "374a059b-5a51-4cd4-b599-4f0be4d84033", "embedding": null, "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453fd7e59647e91d21ed790a04b0abb9438daf1f", "node_type": "4", "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "hash": "d25ab4c70f52eba917359326ecbf9f249689223c5e14717d10f766faecaf11a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b3ac817-4e6a-4d82-9174-143c408338df", "node_type": "1", "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "hash": "c81650951ec32a91bedf6b45370abdf1da517f659a3a82d0b47e7b7b8f6c59de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d8e296b-b279-4ba6-8b45-6a6817bfbffa", "node_type": "1", "metadata": {}, "hash": "c0217a719a7b3de709a45f78196180a74af9cabb9a415a0ae008f340a7fefee8", "class_name": "RelatedNodeInfo"}}, "text": "score = await augmentation_accuracy_evaluator.aevaluate(\n    question, llm_answer, retrieved_context_list\n)\nprint(score)\n# >> EvaluationResult(query='What makes Sam Altman a good founder?', contexts=['Sam Altman is a good founder. He is very smart.', 'What makes Sam Altman such a good founder is his great force of will.'], response='He is a good founder because he is smart.', passing=None, feedback=None, score=0.5, pairwise_source=None, invalid_result=False, invalid_reason=None)\n```\n\nAugmentation precision measures whether the relevant retrieved context makes it into the answer. Both of the retrieved contexts are relevant, but only one makes it into the answer. For that reason, this score is 0.5.\n\n```python\naugmentation_precision_evaluator = AugmentationPrecisionEvaluator()\n\n\nscore = await augmentation_precision_evaluator.aevaluate(\n    question, llm_answer, retrieved_context_list\n)\nprint(score)\n# >> EvaluationResult(query='What makes Sam Altman a good founder?', contexts=['Sam Altman is a good founder. He is very smart.', 'What makes Sam Altman such a good founder is his great force of will.'], response='He is a good founder because he is smart.', passing=None, feedback=None, score=0.5, pairwise_source=None, invalid_result=False, invalid_reason=None)\n```\n\nRetrieval precision measures the percentage of retrieved context is relevant to answer the question. In this case, both of the retrieved contexts are relevant to answer the question, so the score is 1.0.\n\n```python\nretrieval_precision_evaluator = RetrievalPrecisionEvaluator()\n\n\nscore = await retrieval_precision_evaluator.aevaluate(\n    question, llm_answer, retrieved_context_list\n)\nprint(score)\n# >> EvaluationResult(query='What makes Sam Altman a good founder?', contexts=['Sam Altman is a good founder. He is very smart.', 'What makes Sam Altman such a good founder is his great force of will.'], response='He is a good founder because he is smart.', passing=None, feedback=None, score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)\n```\n\nThe TonicValidateEvaluator can calculate all of Tonic Validate's metrics at once.\n\n```python\ntonic_validate_evaluator = TonicValidateEvaluator()\n\n\nscores = await tonic_validate_evaluator.aevaluate(\n    question,\n    llm_answer,\n    retrieved_context_list,\n    reference_response=reference_answer,\n)\nprint(scores.score_dict)\n# >> {\n#     'answer_consistency': 1.0,\n#     'answer_similarity': 4.0,\n#     'augmentation_accuracy': 0.5,\n#     'augmentation_precision': 0.5,\n#     'retrieval_precision': 1.0\n# }\n```\n\n### Evaluating multiple questions at once\n\nYou can also evaluate more than one query and response at once using TonicValidateEvaluator, and return a tonic_validate Run object that can be logged to the [Tonic Validate UI](https://validate.tonic.ai).\n\nTo do this, you put the questions, LLM answers, retrieved context lists, and reference answers into lists and call evaluate_run.\n\n```python\nquestions = [\"What is the capital of France?\", \"What is the capital of Spain?\"]\nreference_answers = [\"Paris\", \"Madrid\"]\nllm_answer = [\"Paris\", \"Madrid\"]\nretrieved_context_lists = [\n    [\n        \"Paris is the capital and most populous city of France.\",\n        \"Paris, France's capital, is a major European city and a global center for art, fashion, gastronomy and culture.\",\n    ],\n    [\n        \"Madrid is the capital and largest city of Spain.\",\n        \"Madrid, Spain's central capital, is a city of elegant boulevards and expansive, manicured parks such as the Buen Retiro.\",\n    ],\n]\n\n\ntonic_validate_evaluator = TonicValidateEvaluator()", "mimetype": "text/plain", "start_char_idx": 4301, "end_char_idx": 7888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d8e296b-b279-4ba6-8b45-6a6817bfbffa": {"__data__": {"id_": "9d8e296b-b279-4ba6-8b45-6a6817bfbffa", "embedding": null, "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "453fd7e59647e91d21ed790a04b0abb9438daf1f", "node_type": "4", "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "hash": "d25ab4c70f52eba917359326ecbf9f249689223c5e14717d10f766faecaf11a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "374a059b-5a51-4cd4-b599-4f0be4d84033", "node_type": "1", "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}, "hash": "14d18e1394f63ab37ea841e7da7fb0f3b65738ab3397e0d0008db0d4ca5fc60c", "class_name": "RelatedNodeInfo"}}, "text": "tonic_validate_evaluator = TonicValidateEvaluator()\n\n\nscores = await tonic_validate_evaluator.aevaluate_run(\n    [questions], [llm_answers], [retrieved_context_lists], [reference_answers]\n)\nprint(scores.run_data[0].scores)\n# >> {\n#     'answer_consistency': 1.0,\n#     'answer_similarity': 3.0,\n#     'augmentation_accuracy': 0.5,\n#     'augmentation_precision': 0.5,\n#     'retrieval_precision': 1.0\n# }\n```\n\n### Uploading Results to the UI\n\nIf you want to upload your scores to the UI, then you can use the Tonic Validate API. Before doing so, make sure you have `TONIC_VALIDATE_API_KEY` set as described in the [Setting Up Tonic Validate](#setting-up-tonic-validate) section. You also need to make sure you have a project created in the Tonic Validate UI and that you have copied the project id. After the API Key and project are set up, you can initialize the Validate API and upload the results.\n\n```python\nvalidate_api = ValidateApi()\nproject_id = \"your-project-id\"\nvalidate_api.upload_run(project_id, scores)\n```\n\nNow you can see your results in the Tonic Validate UI!\n\n![Tonic Validate Graph](../../_static/integrations/TonicValidate-Graph.png)\n\n### End to End Example\n\nHere we will show you how to use Tonic Validate End To End with Llama Index. First, let's download a dataset for Llama Index to run on using the Llama Index CLI.\n\n```bash\nllamaindex-cli download-llamadataset EvaluatingLlmSurveyPaperDataset --download-dir ./data\n```\n\nNow, we can create a python file called `llama.py` and put the following code in it.\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import VectorStoreIndex\n\n\ndocuments = SimpleDirectoryReader(input_dir=\"./data/source_files\").load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n```\n\nThis code essentially just loads in the dataset files and then initializes Llama Index.\n\nLlama Index's CLI also downloads a list of questions and answers you can use for testing on their example dataset. If you want to use these questions and answers, you can use the code below.\n\n```python\nfrom llama_index.core.llama_dataset import LabelledRagDataset\n\nrag_dataset = LabelledRagDataset.from_json(\"./data/rag_dataset.json\")\n\n\n# We are only going to do 10 questions as running through the full data set takes too long\nquestions = [item.query for item in rag_dataset.examples][:10]\nreference_answers = [item.reference_answer for item in rag_dataset.examples][\n    :10\n]\n```\n\nNow we can query for the responses from Llama Index.\n\n```python\nllm_answers = []\nretrieved_context_lists = []\nfor question in questions:\n    response = query_engine.query(question)\n    context_list = [x.text for x in response.source_nodes]\n    retrieved_context_lists.append(context_list)\n    llm_answers.append(response.response)\n```\n\nNow to score it, we can do the following\n\n```\nfrom tonic_validate.metrics import AnswerSimilarityMetric\nfrom llama_index.evaluation.tonic_validate import TonicValidateEvaluator\n\n\ntonic_validate_evaluator = TonicValidateEvaluator(\n    metrics=[AnswerSimilarityMetric()], model_evaluator=\"gpt-4-1106-preview\"\n)\n\nscores = tonic_validate_evaluator.evaluate_run(\n    questions, retrieved_context_lists, reference_answers, llm_answers\n)\nprint(scores.overall_scores)\n```\n\nIf you want to upload your scores to the UI, then you can use the Tonic Validate API. Before doing so, make sure you have `TONIC_VALIDATE_API_KEY` set as described in the [Setting Up Tonic Validate](#setting-up-tonic-validate) section. You also need to make sure you have a project created in the Tonic Validate UI and that you have copied the project id. After the API Key and project are set up, you can initialize the Validate API and upload the results.\n\n```python\nvalidate_api = ValidateApi()\nproject_id = \"your-project-id\"\nvalidate_api.upload_run(project_id, run)\n```\n\n## More Documentation\n\nIn addition to the documentation here, you can also visit [Tonic Validate's Github page](https://github.com/TonicAI/tonic_validate) for more documentation about how to interact with our API for uploading results.", "mimetype": "text/plain", "start_char_idx": 7837, "end_char_idx": 11945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "871f9e51-41b7-4c13-b5b0-58cd55c8bae0": {"__data__": {"id_": "871f9e51-41b7-4c13-b5b0-58cd55c8bae0", "embedding": null, "metadata": {"filename": "trulens.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e0236302a716f81414e0841e20dbcf67e7a4510", "node_type": "4", "metadata": {"filename": "trulens.md", "author": "LlamaIndex"}, "hash": "6e46b37db8b059918a8ddee587335bcedb259a6f26efded46ca06ff826e882d5", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluating and Tracking with TruLens\n\nThis page covers how to use [TruLens](https://trulens.org) to evaluate and track LLM apps built on Llama-Index.\n\n## What is TruLens?\n\nTruLens is an [opensource](https://github.com/truera/trulens) package that provides instrumentation and evaluation tools for large language model (LLM) based applications. This includes feedback function evaluations of relevance, sentiment and more, plus in-depth tracing including cost and latency.\n\n![TruLens Architecture](https://www.trulens.org/Assets/image/TruLens_Architecture.png)\n\nAs you iterate on new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the app metadata for each record.\n\n### Installation and Setup\n\nAdding TruLens is simple, just install it from pypi!\n\n```sh\npip install trulens-eval\n```\n\n```python\nfrom trulens_eval import TruLlama\n```\n\n## Try it out!\n\n[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/trulens-eval-0.20.3/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)\n\n## Read more\n\n- [Build and Evaluate LLM Apps with LlamaIndex and TruLens](https://medium.com/llamaindex-blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c)\n- [More examples](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/llama_index)\n- [trulens.org](https://www.trulens.org/)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ae40079-a15c-4620-944f-afdcc08cc6d5": {"__data__": {"id_": "9ae40079-a15c-4620-944f-afdcc08cc6d5", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04c517d5-22f8-40e9-9628-0df073b446d8", "node_type": "1", "metadata": {}, "hash": "229d6a968831b155878ee44f0bc5c4a885386658edbd057e8b28723521c5510a", "class_name": "RelatedNodeInfo"}}, "text": "# Perform Evaluations on LlamaIndex with UpTrain\n\n**Overview**: In this example, we will see how to use UpTrain with LlamaIndex. UpTrain ([github](https://github.com/uptrain-ai/uptrain) || [website](https://github.com/uptrain-ai/uptrain/) || [docs](https://docs.uptrain.ai/)) is an open-source platform to evaluate and improve GenAI applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analysis on failure cases and gives insights on how to resolve them. More details on UpTrain's evaluations can be found [here](https://github.com/uptrain-ai/uptrain?tab=readme-ov-file#pre-built-evaluations-we-offer-).\n\n**Problem**: As an increasing number of companies are graduating their LLM prototypes to production-ready applications, their RAG pipelines are also getting complex. Developers are utilising modules like QueryRewrite, Context ReRank, etc., to enhance the accuracy of their RAG systems.\n\nWith increasing complexity comes more points of failure.\n\n1. Advanced Evals are needed to evaluate the quality of these newer modules and determine if they actually improve the system's accuracy.\n2. A robust experimentation framework is needed to systematically test different modules and make data-driven decisions.\n\n**Solution**: UpTrain helps to solve for both:\n\n1. UpTrain provides a series of checks to evaluate the quality of generated response, retrieved-context as well as all the interim steps. The relevant checks are ContextRelevance, SubQueryCompleteness, ContextReranking, ContextConciseness, FactualAccuracy, ContextUtilization, ResponseCompleteness, ResponseConciseness, etc.\n2. UpTrain also allows you to experiment with different embedding models as well as have an \"evaluate_experiments\" method to compare different RAG configurations.\n\n# How to go about it?\n\nThere are two ways you can use UpTrain with LlamaIndex:\n\n1. **Using the UpTrain Callback Handler**: This method allows you to seamlessly integrate UpTrain with LlamaIndex. You can simply add UpTrainCallbackHandler to your existing LlamaIndex pipeline and it will evaluate all components of your RAG pipeline. This is the recommended method as it is the easiest to use and provides you with dashboards and insights with minimal effort.\n\n2. **Using UpTrain's EvalLlamaIndex**: This method allows you to use UpTrain to perform evaluations on the generated responses. You can use the EvalLlamaIndex object to generate responses for the queries and then perform evaluations on the responses. You can find a detailed tutorial on how to do this below. This method offers more flexibility and control over the evaluations, but requires more effort to set up and use.\n\n# 1. Using the UpTrain Callback Handler <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/callbacks/UpTrainCallback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\nThe following three demonstrations explain how you can use UpTrain Callback Handler to evaluate different components of your RAG pipelines.\n\n## 1. **RAG Query Engine Evaluations**:\n\nThe RAG query engine plays a crucial role in retrieving context and generating responses. To ensure its performance and response quality, we conduct the following evaluations:\n\n- **[Context Relevance](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance)**: Determines if the retrieved context has sufficient information to answer the user query or not.\n- **[Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy)**: Assesses if the LLM's response can be verified via the retrieved context.\n- **[Response Completeness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness)**: Checks if the response contains all the information required to answer the user query comprehensively.\n\n## 2. **Sub-Question Query Generation Evaluation**:\n\nThe SubQuestionQueryGeneration operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. To measure it's accuracy, we use:\n\n- **[Sub Query Completeness](https://docs.uptrain.ai/predefined-evaluations/query-quality/sub-query-completeness)**: Assures that the sub-questions accurately and comprehensively cover the original query.\n\n## 3. **Re-Ranking Evaluations**:\n\nRe-ranking involves reordering nodes based on relevance to the query and choosing the top nodes. Different evaluations are performed based on the number of nodes returned after re-ranking.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04c517d5-22f8-40e9-9628-0df073b446d8": {"__data__": {"id_": "04c517d5-22f8-40e9-9628-0df073b446d8", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ae40079-a15c-4620-944f-afdcc08cc6d5", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "c7c832a6bb3fe37f5c9634ae2e0cf1e774d5ec150cc547b8e0a6437530926859", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90a3483c-3681-4dee-adb8-14fd194c0a9a", "node_type": "1", "metadata": {}, "hash": "fa2037c797d9cd35232d9a07eb38f5b2d31de7e261efebfa502b47537ef17544", "class_name": "RelatedNodeInfo"}}, "text": "Different evaluations are performed based on the number of nodes returned after re-ranking.\n\na. Same Number of Nodes\n\n- **[Context Reranking](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking)**: Checks if the order of re-ranked nodes is more relevant to the query than the original order.\n\nb. Different Number of Nodes:\n\n- **[Context Conciseness](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness)**: Examines whether the reduced number of nodes still provides all the required information.\n\nThese evaluations collectively ensure the robustness and effectiveness of the RAG query engine, SubQuestionQueryGeneration operator, and the re-ranking process in the LlamaIndex pipeline.\n\n#### **Note:**\n\n- We have performed evaluations using a basic RAG query engine; the same evaluations can be performed using the advanced RAG query engine as well.\n- Same is true for Re-Ranking evaluations, we have performed evaluations using SentenceTransformerRerank, the same evaluations can be performed using other re-rankers as well.\n\n## Install Dependencies and Import Libraries\n\nInstall notebook dependencies.\n\n```bash\n%pip install llama-index-readers-web\n%pip install llama-index-callbacks-uptrain\n%pip install -q html2text llama-index pandas tqdm uptrain torch sentence-transformers\n```\n\nImport libraries.\n\n```python\nfrom getpass import getpass\n\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.readers.web import SimpleWebPageReader\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.callbacks.uptrain.base import UpTrainCallbackHandler\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\nfrom llama_index.llms.openai import OpenAI\n\nimport os\n```\n\n## Setup\n\nUpTrain provides you with:\n1. Dashboards with advanced drill-down and filtering options\n1. Insights and common topics among failing cases\n1. Observability and real-time monitoring of production data\n1. Regression testing via seamless integration with your CI/CD pipelines\n\nYou can choose between the following options for evaluating using UpTrain:\n### 1. **UpTrain's Open-Source Software (OSS)**:\nYou can use the open-source evaluation service to evaluate your model. In this case, you will need to provide an OpenAI API key. You can get yours [here](https://platform.openai.com/account/api-keys).\n\nIn order to view your evaluations in the UpTrain dashboard, you will need to set it up by running the following commands in your terminal:\n\n```bash\ngit clone https://github.com/uptrain-ai/uptrain\ncd uptrain\nbash run_uptrain.sh\n```\n\nThis will start the UpTrain dashboard on your local machine. You can access it at `http://localhost:3000/dashboard`.\n\nParameters:\n- key_type=\"openai\"\n- api_key=\"OPENAI_API_KEY\"\n- project_name=\"PROJECT_NAME\"", "mimetype": "text/plain", "start_char_idx": 4529, "end_char_idx": 7534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90a3483c-3681-4dee-adb8-14fd194c0a9a": {"__data__": {"id_": "90a3483c-3681-4dee-adb8-14fd194c0a9a", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04c517d5-22f8-40e9-9628-0df073b446d8", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "d2f8cefcb4c8e1d1ed871c812415931b0e51d1e021b3f78154e91aca1ef031ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a451b0e3-04d3-4f66-afc7-6680ea984f2d", "node_type": "1", "metadata": {}, "hash": "3ad48fb4f9c8b71425fa34b9c4d1328b4433f8352bf08a06cdba87ceef264a7d", "class_name": "RelatedNodeInfo"}}, "text": "Parameters:\n- key_type=\"openai\"\n- api_key=\"OPENAI_API_KEY\"\n- project_name=\"PROJECT_NAME\"\n\n\n### 2. **UpTrain Managed Service and Dashboards**:\nAlternatively, you can use UpTrain's managed service to evaluate your model. You can create a free UpTrain account [here](https://uptrain.ai/) and get free trial credits. If you want more trial credits, [book a call with the maintainers of UpTrain here](https://calendly.com/uptrain-sourabh/30min).\n\nThe benefits of using the managed service are:\n1. No need to set up the UpTrain dashboard on your local machine.\n1. Access to many LLMs without needing their API keys.\n\nOnce you perform the evaluations, you can view them in the UpTrain dashboard at `https://dashboard.uptrain.ai/dashboard`\n\nParameters:\n- key_type=\"uptrain\"\n- api_key=\"UPTRAIN_API_KEY\"\n- project_name=\"PROJECT_NAME\"\n\n**Note:** The `project_name` will be the project name under which the evaluations performed will be shown in the UpTrain dashboard.\n\n## Create the UpTrain Callback Handler\n\n```python\nos.environ[\"OPENAI_API_KEY\"] = getpass()\n\ncallback_handler = UpTrainCallbackHandler(\n    key_type=\"openai\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    project_name=\"uptrain_llamaindex\",\n)\n\nSettings.callback_manager = CallbackManager([callback_handler])\n```\n\n## Load and Parse Documents\n\nLoad documents from Paul Graham's essay \"What I Worked On\".\n\n```python\ndocuments = SimpleWebPageReader().load_data(\n    [\n        \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n    ]\n)\n```\n\nParse the document into nodes.\n\n```python\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n```\n\n# 1. RAG Query Engine Evaluation\n\nUpTrain callback handler will automatically capture the query, context and response once generated and will run the following three evaluations _(Graded from 0 to 1)_ on the response:\n\n- **[Context Relevance](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance)**: Determines if the retrieved context has sufficient information to answer the user query or not.\n- **[Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy)**: Assesses if the LLM's response can be verified via the retrieved context.\n- **[Response Completeness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness)**: Checks if the response contains all the information required to answer the user query comprehensively.\n\n```python\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\nquery_engine = index.as_query_engine()\n\nmax_characters_per_line = 80\nqueries = [\n    \"What did Paul Graham do growing up?\",\n    \"When and how did Paul Graham's mother die?\",\n    \"What, in Paul Graham's opinion, is the most distinctive thing about YC?\",\n    \"When and how did Paul Graham meet Jessica Livingston?\",\n    \"What is Bel, and when and where was it written?\",\n]\nfor query in queries:\n    response = query_engine.query(query)\n```\n\n    Question: What did Paul Graham do growing up?\n    Response: Paul Graham wrote short stories and started programming on the IBM 1401 in 9th grade using an early version of Fortran. Later, he convinced his father to buy a TRS-80, where he wrote simple games, a program to predict rocket heights, and a word processor.\n\n    Context Relevance Score: 0.0\n    Factual Accuracy Score: 1.0\n    Response Completeness Score: 1.0\n\n\n    Question: When and how did Paul Graham's mother die?\n    Response: Paul Graham's mother died when he was 18 years old, from a brain tumor.\n\n    Context Relevance Score: 0.0\n    Factual Accuracy Score: 0.0\n    Response Completeness Score: 1.0\n\n\n    Question: What, in Paul Graham's opinion, is the most distinctive thing about YC?\n    Response: The most distinctive thing about Y Combinator, according to Paul Graham, is that instead of deciding for himself what to work on, the problems come to him. Every 6 months, a new batch of startups brings their problems, which then become the focus of YC's work.\n\n    Context Relevance Score: 0.0\n    Factual Accuracy Score: 0.5\n    Response Completeness Score: 1.0", "mimetype": "text/plain", "start_char_idx": 7446, "end_char_idx": 11608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a451b0e3-04d3-4f66-afc7-6680ea984f2d": {"__data__": {"id_": "a451b0e3-04d3-4f66-afc7-6680ea984f2d", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90a3483c-3681-4dee-adb8-14fd194c0a9a", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "3328a53314504bec8f1006a78bb91e24482b36054fc58ddb864c2f981571b95e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12ce2c8c-66b0-41d1-88c6-e33210581848", "node_type": "1", "metadata": {}, "hash": "0a639d7ee0ee87828317cf8dabf0963c36bbb95d569d0291165bab2cd9356e34", "class_name": "RelatedNodeInfo"}}, "text": "Question: When and how did Paul Graham meet Jessica Livingston?\n    Response: Paul Graham met Jessica Livingston at a big party at his house in October 2003.\n\n    Context Relevance Score: 1.0\n    Factual Accuracy Score: 0.5\n    Response Completeness Score: 1.0\n\n\n    Question: What is Bel, and when and where was it written?\n    Response: Bel is a new Lisp that was written in Arc. It was developed over a period of 4 years, from March 26, 2015 to October 12, 2019. Most of the work on Bel was done in England, where the author had moved to in the summer of 2016.\n\n    Context Relevance Score: 1.0\n    Factual Accuracy Score: 1.0\n    Response Completeness Score: 1.0\n\nHere's an example of the dashboard showing how you can filter and drill down to the failing cases and get insights on the failing cases:\n![image-2.png](https://uptrain-assets.s3.ap-south-1.amazonaws.com/images/llamaindex/image-2.png)\n\n# 2. Sub-Question Query Engine Evaluation\n\nThe **sub-question query engine** is used to tackle the problem of answering a complex query using multiple data sources. It first breaks down the complex query into sub-questions for each relevant data source, then gathers all the intermediate responses and synthesizes a final response.\n\nUpTrain callback handler will automatically capture the sub-question and the responses for each of them once generated and will run the following three evaluations _(Graded from 0 to 1)_ on the response:\n\n- **[Context Relevance](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance)**: Determines if the retrieved context has sufficient information to answer the user query or not.\n- **[Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy)**: Assesses if the LLM's response can be verified via the retrieved context.\n- **[Response Completeness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness)**: Checks if the response contains all the information required to answer the user query comprehensively.\n\nIn addition to the above evaluations, the callback handler will also run the following evaluation:\n\n- **[Sub Query Completeness](https://docs.uptrain.ai/predefined-evaluations/query-quality/sub-query-completeness)**: Assures that the sub-questions accurately and comprehensively cover the original query.\n\n```python\n# build index and query engine\nvector_query_engine = VectorStoreIndex.from_documents(\n    documents=documents,\n    use_async=True,\n).as_query_engine()\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=vector_query_engine,\n        metadata=ToolMetadata(\n            name=\"documents\",\n            description=\"Paul Graham essay on What I Worked On\",\n        ),\n    ),\n]\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=query_engine_tools,\n    use_async=True,\n)\n\nresponse = query_engine.query(\n    \"How was Paul Grahams life different before, during, and after YC?\"\n)\n```\n\n    Generated 3 sub questions.\n    \u001b[1;3;38;2;237;90;200m[documents] Q: What did Paul Graham work on before Y Combinator?\n    \u001b[0m\u001b[1;3;38;2;90;149;237m[documents] Q: What did Paul Graham work on during Y Combinator?\n    \u001b[0m\u001b[1;3;38;2;11;159;203m[documents] Q: What did Paul Graham work on after Y Combinator?\n    \u001b[0m\u001b[1;3;38;2;11;159;203m[documents] A: Paul Graham worked on a project with Robert and Trevor after Y Combinator.\n    \u001b[0m\u001b[1;3;38;2;237;90;200m[documents] A: Paul Graham worked on projects with his colleagues Robert and Trevor before Y Combinator.\n    \u001b[0m\u001b[1;3;38;2;90;149;237m[documents] A: Paul Graham worked on writing essays and working on Y Combinator during his time at Y Combinator.\n    \u001b[0m\n\n\n    Question: What did Paul Graham work on after Y Combinator?\n    Response: Paul Graham worked on a project with Robert and Trevor after Y Combinator.\n\n    Context Relevance Score: 0.0\n    Factual Accuracy Score: 1.0\n    Response Completeness Score: 0.5", "mimetype": "text/plain", "start_char_idx": 11615, "end_char_idx": 15572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12ce2c8c-66b0-41d1-88c6-e33210581848": {"__data__": {"id_": "12ce2c8c-66b0-41d1-88c6-e33210581848", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a451b0e3-04d3-4f66-afc7-6680ea984f2d", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "93a608f419ac7b48570eef62d47ab14263344b2e2bea2e63a3c45621860aa64f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1778b502-7e0a-4491-bbe0-28ea5af9b3cd", "node_type": "1", "metadata": {}, "hash": "99e0eefe2b515b3a5e90183386f2b3c92ee41f74e9cd6a4e734406ab8a536e75", "class_name": "RelatedNodeInfo"}}, "text": "Question: What did Paul Graham work on before Y Combinator?\n    Response: Paul Graham worked on projects with his colleagues Robert and Trevor before Y Combinator.\n\n    Context Relevance Score: 0.0\n    Factual Accuracy Score: 1.0\n    Response Completeness Score: 0.5\n\n\n    Question: What did Paul Graham work on during Y Combinator?\n    Response: Paul Graham worked on writing essays and working on Y Combinator during his time at Y Combinator.\n\n    Context Relevance Score: 0.0\n    Factual Accuracy Score: 0.5\n    Response Completeness Score: 0.5\n\n\n    Question: How was Paul Grahams life different before, during, and after YC?\n    Sub Query Completeness Score: 1.0\n\nHere's an example of the dashboard visualizing the scores of the sub-questions in the form of a bar chart:\n\n![image.png](https://uptrain-assets.s3.ap-south-1.amazonaws.com/images/llamaindex/image.png)\n\n# 3. Re-ranking\n\nRe-ranking is the process of reordering the nodes based on their relevance to the query. There are multiple classes of re-ranking algorithms offered by Llamaindex. We have used LLMRerank for this example.\n\nThe re-ranker allows you to enter the number of top n nodes that will be returned after re-ranking. If this value remains the same as the original number of nodes, the re-ranker will only re-rank the nodes and not change the number of nodes. Otherwise, it will re-rank the nodes and return the top n nodes.\n\nWe will perform different evaluations based on the number of nodes returned after re-ranking.\n\n## 3a. Re-ranking (With same number of nodes)\n\nIf the number of nodes returned after re-ranking is the same as the original number of nodes, the following evaluation will be performed:\n\n- **[Context Reranking](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking)**: Checks if the order of re-ranked nodes is more relevant to the query than the original order.\n\n```python\ncallback_handler = UpTrainCallbackHandler(\n    key_type=\"openai\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    project_name_prefix=\"llama\",\n)\nSettings.callback_manager = CallbackManager([callback_handler])\n\nrerank_postprocessor = SentenceTransformerRerank(\n    top_n=3,  # number of nodes after reranking\n    keep_retrieval_score=True,\n)\n\nindex = VectorStoreIndex.from_documents(\n    documents=documents,\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=3,  # number of nodes before reranking\n    node_postprocessors=[rerank_postprocessor],\n)\n\nresponse = query_engine.query(\n    \"What did Sam Altman do in this essay?\",\n)\n```\n\n    Question: What did Sam Altman do in this essay?\n    Context Reranking Score: 0.0\n\n\n    Question: What did Sam Altman do in this essay?\n    Response: Sam Altman was asked to become the president of Y Combinator after the original founders decided to step back and reorganize the company for long-term sustainability.\n\n    Context Relevance Score: 1.0\n    Factual Accuracy Score: 1.0\n    Response Completeness Score: 0.5\n\n# 3b. Re-ranking (With different number of nodes)\n\nIf the number of nodes returned after re-ranking is the lesser as the original number of nodes, the following evaluation will be performed:\n\n- **[Context Conciseness](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness)**: Examines whether the reduced number of nodes still provides all the required information.\n\n```python\ncallback_handler = UpTrainCallbackHandler(\n    key_type=\"openai\",\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    project_name_prefix=\"llama\",\n)\nSettings.callback_manager = CallbackManager([callback_handler])\n\nrerank_postprocessor = SentenceTransformerRerank(\n    top_n=2,  # Number of nodes after re-ranking\n    keep_retrieval_score=True,\n)\n\nindex = VectorStoreIndex.from_documents(\n    documents=documents,\n)\nquery_engine = index.as_query_engine(\n    similarity_top_k=5,  # Number of nodes before re-ranking\n    node_postprocessors=[rerank_postprocessor],\n)\n\n# Use your advanced RAG\nresponse = query_engine.query(\n    \"What did Sam Altman do in this essay?\",\n)\n```\n\n    Question: What did Sam Altman do in this essay?\n    Context Conciseness Score: 0.0\n\n\n    Question: What did Sam Altman do in this essay?\n    Response: Sam Altman offered unsolicited advice to the author during a visit to California for interviews.", "mimetype": "text/plain", "start_char_idx": 15579, "end_char_idx": 19868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1778b502-7e0a-4491-bbe0-28ea5af9b3cd": {"__data__": {"id_": "1778b502-7e0a-4491-bbe0-28ea5af9b3cd", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12ce2c8c-66b0-41d1-88c6-e33210581848", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "bd3f029c269908a7929840cf9ddf6f734f4c55f983efb1a494f73ae4c1f9cc55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6700c1d6-15f8-400e-8fd9-0fefa8f2ca28", "node_type": "1", "metadata": {}, "hash": "d9695cfdaf527b7e5a510a2c1c03756a534405184fbffcdfb097500d4ad400b5", "class_name": "RelatedNodeInfo"}}, "text": "Context Relevance Score: 1.0\n    Factual Accuracy Score: 1.0\n    Response Completeness Score: 0.5\n\n# UpTrain's Managed Service Dashboard and Insights\n\nTo use the UpTrain's managed service via the UpTrain callback handler, the only change required is to set the `key_type` and `api_key` parameters. The rest of the code remains the same.\n\n```python\ncallback_handler = UpTrainCallbackHandler(\n    key_type=\"uptrain\",\n    api_key=\"up-******************************\",\n    project_name_prefix=\"llama\",\n)\n```\n\nHere's a short GIF showcasing the dashboard and the insights that you can get from the UpTrain managed service:\n\n![output.gif](https://uptrain-assets.s3.ap-south-1.amazonaws.com/images/llamaindex/output.gif)\n\n# 2. Using UpTrain's EvalLlamaIndex <a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/UpTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Install UpTrain and LlamaIndex\n\n```bash\npip install uptrain llama_index\n```\n\n## Import required libraries\n\n```python\nimport httpx\nimport os\nimport openai\nimport pandas as pd\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom uptrain import Evals, EvalLlamaIndex, Settings as UpTrainSettings\n```\n\n## Create the dataset folder for the query engine\n\nYou can use any documents that you have to do this. For this tutorial, we will use data on New York City extracted from wikipedia. We will only add one document to the folder, but you can add as many as you want.\n\n```python\nurl = \"https://uptrain-assets.s3.ap-south-1.amazonaws.com/data/nyc_text.txt\"\nif not os.path.exists(\"nyc_wikipedia\"):\n    os.makedirs(\"nyc_wikipedia\")\ndataset_path = os.path.join(\"./nyc_wikipedia\", \"nyc_text.txt\")\n\nif not os.path.exists(dataset_path):\n    r = httpx.get(url)\n    with open(dataset_path, \"wb\") as f:\n        f.write(r.content)\n```\n\n## Make the list of queries\n\nBefore we can generate responses, we need to create a list of queries. Since the query engine is trained on New York City, we will create a list of queries related to New York City.\n\n```python\ndata = [\n    {\"question\": \"What is the population of New York City?\"},\n    {\"question\": \"What is the area of New York City?\"},\n    {\"question\": \"What is the largest borough in New York City?\"},\n    {\"question\": \"What is the average temperature in New York City?\"},\n    {\"question\": \"What is the main airport in New York City?\"},\n    {\"question\": \"What is the famous landmark in New York City?\"},\n    {\"question\": \"What is the official language of New York City?\"},\n    {\"question\": \"What is the currency used in New York City?\"},\n    {\"question\": \"What is the time zone of New York City?\"},\n    {\"question\": \"What is the famous sports team in New York City?\"},\n]\n```\n\n**This notebook uses the OpenAI API to generate text for prompts as well as to create the Vector Store Index. So, set openai.api_key to your OpenAI API key.**\n\n```python\nopenai.api_key = \"sk-************************\"  # your OpenAI API key\n```\n\n## Create a query engine using LlamaIndex\n\nLet's create a vector store index using LLamaIndex and then use that as a query engine to retrieve relevant sections from the documentation.\n\n```python\nSettings.chunk_size = 512\n\ndocuments = SimpleDirectoryReader(\"./nyc_wikipedia/\").load_data()\n\nvector_index = VectorStoreIndex.from_documents(\n    documents,\n)\n\nquery_engine = vector_index.as_query_engine()\n```\n\n## Setup\n\nUpTrain provides you with:\n1. Dashboards with advanced drill-down and filtering options\n1. Insights and common topics among failing cases\n1. Observability and real-time monitoring of production data\n1. Regression testing via seamless integration with your CI/CD pipelines\n\nYou can choose between the following two alternatives for evaluating using UpTrain:\n\n# Alternative 1: Evaluate using UpTrain's Open-Source Software (OSS)\n\nYou can use the open-source evaluation service to evaluate your model. In this case, you will need to provide an OpenAI API key. You can get yours [here](https://platform.openai.com/account/api-keys).", "mimetype": "text/plain", "start_char_idx": 19875, "end_char_idx": 23996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6700c1d6-15f8-400e-8fd9-0fefa8f2ca28": {"__data__": {"id_": "6700c1d6-15f8-400e-8fd9-0fefa8f2ca28", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1778b502-7e0a-4491-bbe0-28ea5af9b3cd", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "dd11e3500f11a4c7d86a9fcf9d6e3b58de3e185499b4e3c89833501ade7d8212", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bddee492-c1fe-4af1-891e-67b228788bf4", "node_type": "1", "metadata": {}, "hash": "3adef8ac1b88611d60bd832ffd06d0aa25121409fd5b9207693d23d053d2d0e1", "class_name": "RelatedNodeInfo"}}, "text": "You can get yours [here](https://platform.openai.com/account/api-keys).\n\nIn order to view your evaluations in the UpTrain dashboard, you will need to set it up by running the following commands in your terminal:\n\n```bash\ngit clone https://github.com/uptrain-ai/uptrain\ncd uptrain\nbash run_uptrain.sh\n```\n\nThis will start the UpTrain dashboard on your local machine. You can access it at `http://localhost:3000/dashboard`.\n\n**Note:** The `project_name` will be the project name under which the evaluations performed will be shown in the UpTrain dashboard.\n\n```python\nsettings = UpTrainSettings(\n    openai_api_key=openai.api_key,\n)\n```\n\n## Create the EvalLlamaIndex object\n\nNow that we have created the query engine, we can use it to create an EvalLlamaIndex object. This object will be used to generate responses for the queries.\n\n```python\nllamaindex_object = EvalLlamaIndex(\n    settings=settings, query_engine=query_engine\n)\n```\n\n## Run the evaluation\n\nNow that we have the list of queries, we can use the EvalLlamaIndex object to generate responses for the queries and then perform evaluations on the responses. You can find an extensive list of the evaluations offered by UpTrain [here](https://docs.uptrain.ai/key-components/evals). We have chosen two that we found to be the most relevant for this tutorial:\n\n1. **Context Relevance**: This evaluation checks whether the retrieved context is relevant to the query. This is important because the retrieved context is used to generate the response. If the retrieved context is not relevant to the query, then the response will not be relevant to the query either.\n\n2. **Response Conciseness**: This evaluation checks whether the response is concise. This is important because the response should be concise and should not contain any unnecessary information.\n\n```python\nresults = llamaindex_object.evaluate(\n    project_name=\"uptrain-llama-index\",\n    evaluation_name=\"nyc_wikipedia\",  # adding project and evaluation names allow you to track the results in the UpTrain dashboard\n    data=data,\n    checks=[Evals.CONTEXT_RELEVANCE, Evals.RESPONSE_CONCISENESS],\n)\n```\n\n```python\npd.DataFrame(results)\n```\n\n# Alternative 2: Evaluate using UpTrain's Managed Service and Dashboards\n\nAlternatively, you can use UpTrain's managed service to evaluate your model. You can create a free UpTrain account [here](https://uptrain.ai/) and get free trial credits. If you want more trial credits, [book a call with the maintainers of UpTrain here](https://calendly.com/uptrain-sourabh/30min).\n\nThe benefits of using the managed service are:\n1. No need to set up the UpTrain dashboard on your local machine.\n1. Access to many LLMs without needing their API keys.\n\nOnce you perform the evaluations, you can view them in the UpTrain dashboard at `https://dashboard.uptrain.ai/dashboard`\n\n**Note:** The `project_name` will be the project name under which the evaluations performed will be shown in the UpTrain dashboard.\n\n```python\nUPTRAIN_API_KEY = \"up-**********************\"  # your UpTrain API key\n\n# We use `uptrain_access_token` parameter instead of 'openai_api_key' in settings in this case\nsettings = UpTrainSettings(\n    uptrain_access_token=UPTRAIN_API_KEY,\n)\n```\n\n## Create the EvalLlamaIndex object\n\nNow that we have created the query engine, we can use it to create an EvalLlamaIndex object. This object will be used to generate responses for the queries.\n\n```python\nllamaindex_object = EvalLlamaIndex(\n    settings=settings, query_engine=query_engine\n)\n```\n\n## Run the evaluation\n\nNow that we have the list of queries, we can use the EvalLlamaIndex object to generate responses for the queries and then perform evaluations on the responses. You can find an extensive list of the evaluations offered by UpTrain [here](https://docs.uptrain.ai/key-components/evals). We have chosen two that we found to be the most relevant for this tutorial:\n\n1. **Context Relevance**: This evaluation checks whether the retrieved context is relevant to the query. This is important because the retrieved context is used to generate the response. If the retrieved context is not relevant to the query, then the response will not be relevant to the query either.\n\n2. **Response Conciseness**: This evaluation checks whether the response is concise. This is important because the response should be concise and should not contain any unnecessary information.", "mimetype": "text/plain", "start_char_idx": 23925, "end_char_idx": 28311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bddee492-c1fe-4af1-891e-67b228788bf4": {"__data__": {"id_": "bddee492-c1fe-4af1-891e-67b228788bf4", "embedding": null, "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0", "node_type": "4", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "5c2549dd23508b26a5b3e5192d8a5a984eedf9d20eafaf990b00869bbf22e332", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6700c1d6-15f8-400e-8fd9-0fefa8f2ca28", "node_type": "1", "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}, "hash": "ca826804c6820eb8d533d45f0cffe90d7d85d29ef2117e18c10884fd6caf6434", "class_name": "RelatedNodeInfo"}}, "text": "This is important because the response should be concise and should not contain any unnecessary information.\n\n```python\nresults = llamaindex_object.evaluate(\n    project_name=\"uptrain-llama-index\",\n    evaluation_name=\"nyc_wikipedia\",  # adding project and evaluation names allow you to track the results in the UpTrain dashboard\n    data=data,\n    checks=[Evals.CONTEXT_RELEVANCE, Evals.RESPONSE_CONCISENESS],\n)\n```\n\n```python\npd.DataFrame(results)\n```\n\n### Dashboards:\n\nHistogram of score vs number of cases with that score\n\n![nyc_dashboard.png](https://uptrain-assets.s3.ap-south-1.amazonaws.com/images/llamaindex/nyc_dashboard.png)\n\nYou can filter failure cases and generate common topics among them. This can help identify the core issue and help fix it\n\n![nyc_insights.png](https://uptrain-assets.s3.ap-south-1.amazonaws.com/images/llamaindex/nyc_insights.png)\n\n## Learn More\n\n1. [Colab Notebook on UpTrainCallbackHandler](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/callbacks/UpTrainCallback.ipynb)\n1. [Colab Notebook on UpTrain Integration with LlamaIndex](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/UpTrain.ipynb)\n1. [UpTrain Github Repository](https://github.com/uptrain-ai/uptrain)\n1. [UpTrain Documentation](https://docs.uptrain.ai/)", "mimetype": "text/plain", "start_char_idx": 28203, "end_char_idx": 29556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "762e4b18-2a15-4e06-bab1-26aaccd04d16": {"__data__": {"id_": "762e4b18-2a15-4e06-bab1-26aaccd04d16", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4abe42c-d54d-4d45-9c97-78c827dd66e8", "node_type": "1", "metadata": {}, "hash": "4f07dc2d1670c13382b543f065619a72491e562e7d7b217b54c14184a4307a24", "class_name": "RelatedNodeInfo"}}, "text": "# Using Vector Stores\n\nLlamaIndex offers multiple integration points with vector stores / vector databases:\n\n1. LlamaIndex can use a vector store itself as an index. Like any other index, this index can store documents and be used to answer queries.\n2. LlamaIndex can load data from vector stores, similar to any other data connector. This data can then be used within LlamaIndex data structures.\n\n## Using a Vector Store as an Index\n\nLlamaIndex also supports different vector stores\nas the storage backend for `VectorStoreIndex`.\n\n- Alibaba Cloud OpenSearch (`AlibabaCloudOpenSearchStore`). [QuickStart](https://help.aliyun.com/zh/open-search/vector-search-edition).\n- Amazon Neptune - Neptune Analytics (`NeptuneAnalyticsVectorStore`). [Working with vector similarity in Neptune Analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/vector-similarity.html).\n- Apache Cassandra\u00ae and Astra DB through CQL (`CassandraVectorStore`). [Installation](https://cassandra.apache.org/doc/stable/cassandra/getting_started/installing.html) [Quickstart](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html)\n- Astra DB (`AstraDBVectorStore`). [Quickstart](https://docs.datastax.com/en/astra/home/astra.html).\n- AWS Document DB (`AWSDocDbVectorStore`). [Quickstart](https://docs.aws.amazon.com/documentdb/latest/developerguide/get-started-guide.html).\n- Azure AI Search (`AzureAISearchVectorStore`). [Quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector)\n- Chroma (`ChromaVectorStore`) [Installation](https://docs.trychroma.com/getting-started)\n- ClickHouse (`ClickHouseVectorStore`) [Installation](https://clickhouse.com/docs/en/install)\n- Couchbase (`CouchbaseVectorStore`) [Installation](https://www.couchbase.com/products/capella/)\n- DashVector (`DashVectorStore`). [Installation](https://help.aliyun.com/document_detail/2510230.html).\n- DeepLake (`DeepLakeVectorStore`) [Installation](https://docs.deeplake.ai/en/latest/Installation.html)\n- DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`). [Installation/Python Client](https://github.com/docarray/docarray#installation).\n- Elasticsearch (`ElasticsearchStore`) [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html)\n- Epsilla (`EpsillaVectorStore`) [Installation/Quickstart](https://epsilla-inc.gitbook.io/epsilladb/quick-start)\n- Faiss (`FaissVectorStore`). [Installation](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md).\n- txtai (`TxtaiVectorStore`). [Installation](https://neuml.github.io/txtai/install/).\n- Jaguar (`JaguarVectorStore`). [Installation](http://www.jaguardb.com/docsetup.html).\n- Lantern (`LanternVectorStore`). [Quickstart](https://docs.lantern.dev/get-started/overview).\n- Milvus (`MilvusVectorStore`). [Installation](https://milvus.io/docs)\n- MongoDB Atlas (`MongoDBAtlasVectorSearch`). [Installation/Quickstart](https://www.mongodb.com/atlas/database).\n- MyScale (`MyScaleVectorStore`). [Quickstart](https://docs.myscale.com/en/quickstart/). [Installation/Python Client](https://docs.myscale.com/en/python-client/).\n- Neo4j (`Neo4jVectorIndex`). [Installation](https://neo4j.com/docs/operations-manual/current/installation/).\n- Pinecone (`PineconeVectorStore`). [Installation/Quickstart](https://docs.pinecone.io/docs/quickstart).\n- Qdrant (`QdrantVectorStore`) [Installation](https://qdrant.tech/documentation/install/) [Python Client](https://qdrant.tech/documentation/install/#python-client)\n- LanceDB (`LanceDBVectorStore`) [Installation/Quickstart](https://lancedb.github.io/lancedb/basic/)\n- Redis (`RedisVectorStore`). [Installation](https://redis.io/docs/latest/operate/oss_and_stack/install/install-stack/).\n- Relyt (`RelytVectorStore`). [Quickstart](https://docs.relyt.cn/docs/vector-engine/).\n- Supabase (`SupabaseVectorStore`). [Quickstart](https://supabase.github.io/vecs/api/).\n- TiDB (`TiDBVectorStore`). [Quickstart](../../examples/vector_stores/TiDBVector.ipynb).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4abe42c-d54d-4d45-9c97-78c827dd66e8": {"__data__": {"id_": "f4abe42c-d54d-4d45-9c97-78c827dd66e8", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "762e4b18-2a15-4e06-bab1-26aaccd04d16", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "db45c090ecb5c41a31d935234cdb10171a5217c56e090abea3af3ebc2faa805a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbe143fa-1682-477b-be13-dc548e826e27", "node_type": "1", "metadata": {}, "hash": "8eacc0d5c30fd06da7ede8260c52eb144a98e6483e870dac5492ce48d0e0b14c", "class_name": "RelatedNodeInfo"}}, "text": "- TiDB (`TiDBVectorStore`). [Quickstart](../../examples/vector_stores/TiDBVector.ipynb). [Installation](https://tidb.cloud/ai). [Python Client](https://github.com/pingcap/tidb-vector-python).\n- TimeScale (`TimescaleVectorStore`). [Installation](https://github.com/timescale/python-vector).\n- Upstash (`UpstashVectorStore`). [Quickstart](https://upstash.com/docs/vector/overall/getstarted)\n- Vertex AI Vector Search (`VertexAIVectorStore`). [Quickstart](https://cloud.google.com/vertex-ai/docs/vector-search/quickstart)\n- Weaviate (`WeaviateVectorStore`). [Installation](https://weaviate.io/developers/weaviate/installation). [Python Client](https://weaviate.io/developers/weaviate/client-libraries/python).\n- Zep (`ZepVectorStore`). [Installation](https://docs.getzep.com/deployment/quickstart/). [Python Client](https://docs.getzep.com/sdk/).\n- Zilliz (`MilvusVectorStore`). [Quickstart](https://zilliz.com/doc/quick_start)\n\nA detailed API reference is [found here](../../api_reference/storage/vector_store/index.md).\n\nSimilar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection\nof documents. We use the vector store within the index to store embeddings for the input text chunks.\n\nOnce constructed, the index can be used for querying.\n\n**Default Vector Store Index Construction/Querying**\n\nBy default, `VectorStoreIndex` uses an in-memory `SimpleVectorStore`\nthat's initialized as part of the default storage context.\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n```\n\n**Custom Vector Store Index Construction/Querying**\n\nWe can query over a custom vector store as follows:\n\n```python\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n)\nfrom llama_index.vector_stores.deeplake import DeepLakeVectorStore\n\n# construct vector store and customize storage context\nstorage_context = StorageContext.from_defaults(\n    vector_store=DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\n)\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n\n# Query index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n```\n\nBelow we show more examples of how to construct various vector stores we support.", "mimetype": "text/plain", "start_char_idx": 3906, "end_char_idx": 6631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbe143fa-1682-477b-be13-dc548e826e27": {"__data__": {"id_": "bbe143fa-1682-477b-be13-dc548e826e27", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4abe42c-d54d-4d45-9c97-78c827dd66e8", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "394fe715b9e3e6b5503643740fe2f20c4285b7cd09d8c219c41ceb9e88292fb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa459331-8fdc-4afe-8d6d-6f9ad154e68c", "node_type": "1", "metadata": {}, "hash": "ac65a0aa5a05b3a2133b9d7fcfb9643ab4462d4a8919e26db767797316388686", "class_name": "RelatedNodeInfo"}}, "text": "```\n\nBelow we show more examples of how to construct various vector stores we support.\n\n**Alibaba Cloud OpenSearch**\n\n```python\nfrom llama_index.vector_stores.alibabacloud_opensearch import (\n    AlibabaCloudOpenSearchStore,\n    AlibabaCloudOpenSearchConfig,\n)\n\nconfig = AlibabaCloudOpenSearchConfig(\n    endpoint=\"***\",\n    instance_id=\"***\",\n    username=\"your_username\",\n    password=\"your_password\",\n    table_name=\"llama\",\n)\n\nvector_store = AlibabaCloudOpenSearchStore(config)\n```\n\n**Amazon Neptune - Neptune Analytics**\n\n```python\nfrom llama_index.vector_stores.neptune import NeptuneAnalyticsVectorStore\n\ngraph_identifier = \"\"\nembed_dim = 1536\n\nneptune_vector_store = NeptuneAnalyticsVectorStore(\n    graph_identifier=graph_identifier, embedding_dimension=1536\n)\n```\n\n**Apache Cassandra\u00ae**\n\n```python\nfrom llama_index.vector_stores.cassandra import CassandraVectorStore\nimport cassio\n\n# To use an Astra DB cloud instance through CQL:\ncassio.init(database_id=\"1234abcd-...\", token=\"AstraCS:...\")\n\n# For a Cassandra cluster:\nfrom cassandra.cluster import Cluster\n\ncluster = Cluster([\"127.0.0.1\"])\ncassio.init(session=cluster.connect(), keyspace=\"my_keyspace\")\n\n# After the above `cassio.init(...)`, create a vector store:\nvector_store = CassandraVectorStore(\n    table=\"cass_v_table\", embedding_dimension=1536\n)\n```\n\n**Astra DB**\n\n```python\nfrom llama_index.vector_stores.astra_db import AstraDBVectorStore\n\nastra_db_store = AstraDBVectorStore(\n    token=\"AstraCS:xY3b...\",  # Your Astra DB token\n    api_endpoint=\"https://012...abc-us-east1.apps.astra.datastax.com\",  # Your Astra DB API endpoint\n    collection_name=\"astra_v_table\",  # Table name of your choice\n    embedding_dimension=1536,  # Embedding dimension of the embeddings model used\n)\n```\n\n**Azure Cognitive Search**\n\n```python\nfrom azure.core.credentials import AzureKeyCredential\nfrom llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore\n\nsearch_service_api_key = \"YOUR-AZURE-SEARCH-SERVICE-ADMIN-KEY\"\nsearch_service_endpoint = \"YOUR-AZURE-SEARCH-SERVICE-ENDPOINT\"\nsearch_service_api_version = \"2023-11-01\"\ncredential = AzureKeyCredential(search_service_api_key)\n\n# Index name to use\nindex_name = \"llamaindex-vector-demo\"\n\nclient = SearchIndexClient(\n    endpoint=search_service_endpoint,\n    credential=credential,\n)\n\nvector_store = AzureAISearchVectorStore(\n    search_or_index_client=client,\n    index_name=index_name,\n    embedding_dimensionality=1536,\n)\n```\n\n**Chroma**\n\n```python\nimport chromadb\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\n\n# Creating a Chroma client\n# EphemeralClient operates purely in-memory, PersistentClient will also save to disk\nchroma_client = chromadb.EphemeralClient()\nchroma_collection = chroma_client.create_collection(\"quickstart\")\n\n# construct vector store\nvector_store = ChromaVectorStore(\n    chroma_collection=chroma_collection,\n)\n```\n\n**ClickHouse**\n\n```python\nimport clickhouse_connect\nfrom llama_index.vector_stores import ClickHouseVectorStore\n\n# Creating a ClickHouse client\nclient = clickhouse_connect.get_client(\n    host=\"YOUR_CLUSTER_HOST\",\n    port=8123,\n    username=\"YOUR_USERNAME\",\n    password=\"YOUR_CLUSTER_PASSWORD\",\n)\n\n# construct vector store\nvector_store = ClickHouseVectorStore(clickhouse_client=client)\n```\n\n**Couchbase**\n\n```python\nfrom datetime import timedelta\n\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.options import ClusterOptions\n\n# Create a Couchbase Cluster object\nauth = PasswordAuthenticator(\"DATABASE_USERNAME\", \"DATABASE_PASSWORD\")\noptions = ClusterOptions(auth)\ncluster = Cluster(\"CLUSTER_CONNECTION_STRING\", options)\n\n# Wait until the cluster is ready for use.", "mimetype": "text/plain", "start_char_idx": 6545, "end_char_idx": 10249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa459331-8fdc-4afe-8d6d-6f9ad154e68c": {"__data__": {"id_": "aa459331-8fdc-4afe-8d6d-6f9ad154e68c", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbe143fa-1682-477b-be13-dc548e826e27", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "21cfecbaf540bbcbc2a19bed3211c55396f82d3fb04fd2e2ea31d25c1aeed735", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "821b4839-9952-4d71-9a18-f49a07ba3a8f", "node_type": "1", "metadata": {}, "hash": "0be6a94107272363dd39ffd41ec5fd1e8e93358d9d4856a55f0ef298032988c6", "class_name": "RelatedNodeInfo"}}, "text": "cluster.wait_until_ready(timedelta(seconds=5))\n\n# Create the Vector Store\nvector_store = CouchbaseVectorStore(\n    cluster=cluster,\n    bucket_name=\"BUCKET_NAME\",\n    scope_name=\"SCOPE_NAME\",\n    collection_name=\"COLLECTION_NAME\",\n    index_name=\"SEARCH_INDEX_NAME\",\n)\n```\n\n**DashVector**\n\n```python\nimport dashvector\nfrom llama_index.vector_stores.dashvector import DashVectorStore\n\n# init dashvector client\nclient = dashvector.Client(\n    api_key=\"your-dashvector-api-key\",\n    endpoint=\"your-dashvector-cluster-endpoint\",\n)\n\n# creating a DashVector collection\nclient.create(\"quickstart\", dimension=1536)\ncollection = client.get(\"quickstart\")\n\n# construct vector store\nvector_store = DashVectorStore(collection)\n```\n\n**DeepLake**\n\n```python\nimport os\nimport getpath\nfrom llama_index.vector_stores.deeplake import DeepLakeVectorStore\n\nos.environ[\"OPENAI_API_KEY\"] = getpath.getpath(\"OPENAI_API_KEY: \")\nos.environ[\"ACTIVELOOP_TOKEN\"] = getpath.getpath(\"ACTIVELOOP_TOKEN: \")\ndataset_path = \"hub://adilkhan/paul_graham_essay\"\n\n# construct vector store\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n```\n\n**DocArray**\n\n```python\nfrom llama_index.vector_stores.docarray import (\n    DocArrayHnswVectorStore,\n    DocArrayInMemoryVectorStore,\n)\n\n# construct vector store\nvector_store = DocArrayHnswVectorStore(work_dir=\"hnsw_index\")\n\n# alternatively, construct the in-memory vector store\nvector_store = DocArrayInMemoryVectorStore()\n```\n\n**Elasticsearch**\n\nFirst, you can start Elasticsearch either locally or on [Elastic cloud](https://cloud.elastic.co/registration?utm_source=llama-index&utm_content=documentation).\n\nTo start Elasticsearch locally with docker, run the following command:\n\n```bash\ndocker run -p 9200:9200 \\\n  -e \"discovery.type=single-node\" \\\n  -e \"xpack.security.enabled=false\" \\\n  -e \"xpack.security.http.ssl.enabled=false\" \\\n  -e \"xpack.license.self_generated.type=trial\" \\\n  docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n```\n\nThen connect and use Elasticsearch as a vector database with LlamaIndex\n\n```python\nfrom llama_index.vector_stores.elasticsearch import ElasticsearchStore\n\nvector_store = ElasticsearchStore(\n    index_name=\"llm-project\",\n    es_url=\"http://localhost:9200\",\n    # Cloud connection options:\n    # es_cloud_id=\"<cloud_id>\",\n    # es_user=\"elastic\",\n    # es_password=\"<password>\",\n)\n```\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.\n\n**Epsilla**\n\n```python\nfrom pyepsilla import vectordb\nfrom llama_index.vector_stores.epsilla import EpsillaVectorStore\n\n# Creating an Epsilla client\nepsilla_client = vectordb.Client()\n\n# Construct vector store\nvector_store = EpsillaVectorStore(client=epsilla_client)\n```\n\n**Note**: `EpsillaVectorStore` depends on the `pyepsilla` library and a running Epsilla vector database.\nUse `pip/pip3 install pyepsilla` if not installed yet.\nA running Epsilla vector database could be found through docker image.\nFor complete instructions, see the following documentation:\nhttps://epsilla-inc.gitbook.io/epsilladb/quick-start\n\n**Faiss**\n\n```python\nimport faiss\nfrom llama_index.vector_stores.faiss import FaissVectorStore\n\n# create faiss index\nd = 1536\nfaiss_index = faiss.IndexFlatL2(d)\n\n# construct vector store\nvector_store = FaissVectorStore(faiss_index)\n\n...\n\n# NOTE: since faiss index is in-memory, we need to explicitly call\n#       vector_store.persist() or storage_context.persist() to save it to disk.\n#       persist() takes in optional arg persist_path. If none give, will use default paths.", "mimetype": "text/plain", "start_char_idx": 10250, "end_char_idx": 13848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "821b4839-9952-4d71-9a18-f49a07ba3a8f": {"__data__": {"id_": "821b4839-9952-4d71-9a18-f49a07ba3a8f", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa459331-8fdc-4afe-8d6d-6f9ad154e68c", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "b613e82427bdb716cccf024a669e00727d589975f449aa374a543dd41bdc3f46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b024718d-2f71-4a2e-86e5-3e9747c17c70", "node_type": "1", "metadata": {}, "hash": "44aee1014c9409891fcb697a36a384b87954739afbea11068830592884ace14a", "class_name": "RelatedNodeInfo"}}, "text": "#       persist() takes in optional arg persist_path. If none give, will use default paths.\nstorage_context.persist()\n```\n\n**txtai**\n\n```python\nimport txtai\nfrom llama_index.vector_stores.txtai import TxtaiVectorStore\n\n# create txtai index\ntxtai_index = txtai.ann.ANNFactory.create(\n    {\"backend\": \"numpy\", \"dimension\": 512}\n)\n\n# construct vector store\nvector_store = TxtaiVectorStore(txtai_index)\n```\n\n**Jaguar**\n\n```python\nfrom llama_index.core.schema import TextNode\nfrom llama_index.core.vector_stores import VectorStoreQuery\nfrom jaguardb_http_client.JaguarHttpClient import JaguarHttpClient\nfrom llama_index.vector_stores.jaguar import JaguarVectorStore\n\n\n# construct vector store client\nurl = \"http://127.0.0.1:8080/fwww/\"\npod = \"vdb\"\nstore = \"llamaindex_rag_store\"\nvector_index = \"v\"\nvector_type = \"cosine_fraction_float\"\nvector_dimension = 3\n\n# require JAGUAR_API_KEY environment variable or file $HOME/.jagrc to hold the\n# jaguar API key to connect to jaguar store server\nvector_store = JaguarVectorStore(\n    pod, store, vector_index, vector_type, vector_dimension, url\n)\n\n# login to jaguar server for security authentication\nvector_store.login()\n\n# create a vector store on the back-end server\nmetadata_fields = \"author char(32), category char(16)\"\ntext_size = 1024\nvector_store.create(metadata_fields, text_size)\n\n# store some text\nnode = TextNode(\n    text=\"Return of King Lear\",\n    metadata={\"author\": \"William\", \"category\": \"Tragedy\"},\n    embedding=[0.9, 0.1, 0.4],\n)\nvector_store.add(nodes=[node], use_node_metadata=True)\n\n# make a query\nqembedding = [0.4, 0.2, 0.8]\nvsquery = VectorStoreQuery(query_embedding=qembedding, similarity_top_k=1)\nquery_result = vector_store.query(vsquery)\n\n# make a query with metadata filter (where condition)\nqembedding = [0.6, 0.1, 0.4]\nvsquery = VectorStoreQuery(query_embedding=qembedding, similarity_top_k=3)\nwhere = \"author='Eve' or (author='Adam' and category='History')\"\nquery_result = vector_store.query(vsquery, where=where)\n\n# make a query ignoring old data (with time cutoff)\nqembedding = [0.3, 0.3, 0.8]\nvsquery = VectorStoreQuery(query_embedding=qembedding, similarity_top_k=3)\nargs = \"day_cutoff=180\"  # only search recent 180 days data\nquery_result = vector_store.query(vsquery, args=args)\n\n# check if a vector is anomalous\ntext = (\"Gone With The Wind\",)\nembed_of_text = [0.7, 0.1, 0.2]\nnode = TextNode(text=text, embedding=embed_of_text)\ntrue_or_false = vector_store.is_anomalous(node)\n\n# llama_index RAG application\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import StorageContext\nfrom llama_index.core import VectorStoreIndex\n\nquestion = \"What did the author do growing up?\"\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nembed_model = OpenAIEmbedding()\nembed_of_question = [0.7, 0.1, 0.2]\n\ndb_documents = vector_store.load_documents(embed_of_question, 10)\nindex = VectorStoreIndex.from_documents(\n    db_documents,\n    embed_model=embed_model,\n    storage_context=storage_context,\n)\n\nquery_engine = index.as_query_engine()\nprint(f\"Question: {question}\")\nresponse = query_engine.query(question)\nprint(f\"Answer: {str(response)}\")\n\n# logout to clean up resources\nvector_store.logout()\n```\n\n**Note**: Client(requires jaguardb-http-client) <--> Http Gateway <--> JaguarDB Server\nClient side needs to run: \"pip install -U jaguardb-http-client\"\n\n**Milvus**\n\n- Milvus Index offers the ability to store both Documents and their embeddings.\n\n```python\nimport pymilvus\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\n\n# construct vector store\nvector_store = MilvusVectorStore(\n    uri=\"https://localhost:19530\", overwrite=\"True\"\n)\n```\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.\nUse `pip install pymilvus` if not already installed.", "mimetype": "text/plain", "start_char_idx": 13757, "end_char_idx": 17549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b024718d-2f71-4a2e-86e5-3e9747c17c70": {"__data__": {"id_": "b024718d-2f71-4a2e-86e5-3e9747c17c70", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "821b4839-9952-4d71-9a18-f49a07ba3a8f", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "626fb1406334048dbb529f5ce7be0862e5941fba72213d0cf2b9226eb00ee080", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9caa6f7d-2d68-4650-a0ed-9c6697363e25", "node_type": "1", "metadata": {}, "hash": "433e7830d7f7dd416e43926c23c5e4143e5853c6af0ef1bef444558516b5821f", "class_name": "RelatedNodeInfo"}}, "text": "Use `pip install pymilvus` if not already installed.\nIf you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.\n\n**MongoDBAtlas**\n\n```python\n# Provide URI to constructor, or use environment variable\nimport pymongo\nfrom llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core import StorageContext\nfrom llama_index.core import SimpleDirectoryReader\n\n# mongo_uri = os.environ[\"MONGO_URI\"]\nmongo_uri = (\n    \"mongodb+srv://<username>:<password>@<host>?retryWrites=true&w=majority\"\n)\nmongodb_client = pymongo.MongoClient(mongo_uri)\n\n# construct store\nstore = MongoDBAtlasVectorSearch(mongodb_client)\nstorage_context = StorageContext.from_defaults(vector_store=store)\nuber_docs = SimpleDirectoryReader(\n    input_files=[\"../data/10k/uber_2021.pdf\"]\n).load_data()\n\n# construct index\nindex = VectorStoreIndex.from_documents(\n    uber_docs, storage_context=storage_context\n)\n```\n\n**MyScale**\n\n```python\nimport clickhouse_connect\nfrom llama_index.vector_stores.myscale import MyScaleVectorStore\n\n# Creating a MyScale client\nclient = clickhouse_connect.get_client(\n    host=\"YOUR_CLUSTER_HOST\",\n    port=8443,\n    username=\"YOUR_USERNAME\",\n    password=\"YOUR_CLUSTER_PASSWORD\",\n)\n\n\n# construct vector store\nvector_store = MyScaleVectorStore(myscale_client=client)\n```\n\n**Neo4j**\n\n- Neo4j stores texts, metadata, and embeddings and can be customized to return graph data in the form of metadata.\n\n```python\nfrom llama_index.vector_stores.neo4jvector import Neo4jVectorStore\n\n# construct vector store\nneo4j_vector = Neo4jVectorStore(\n    username=\"neo4j\",\n    password=\"pleaseletmein\",\n    url=\"bolt://localhost:7687\",\n    embed_dim=1536,\n)\n```\n\n**Pinecone**\n\n```python\nimport pinecone\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\n\n# Creating a Pinecone index\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\n# construct vector store\nvector_store = PineconeVectorStore(pinecone_index=index)\n```\n\n**Qdrant**\n\n```python\nimport qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\n# Creating a Qdrant vector store\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\", api_key=\"<qdrant-api-key>\", https=True\n)\ncollection_name = \"paul_graham\"\n\n# construct vector store\nvector_store = QdrantVectorStore(\n    client=client,\n    collection_name=collection_name,\n)\n```\n\n**Redis**\n\nFirst, start Redis-Stack (or get url from Redis provider)\n\n```bash\ndocker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n```\n\nThen connect and use Redis as a vector database with LlamaIndex\n\n```python\nfrom llama_index.vector_stores.redis import RedisVectorStore\n\nvector_store = RedisVectorStore(\n    index_name=\"llm-project\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True,\n)\n```\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.", "mimetype": "text/plain", "start_char_idx": 17497, "end_char_idx": 20736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9caa6f7d-2d68-4650-a0ed-9c6697363e25": {"__data__": {"id_": "9caa6f7d-2d68-4650-a0ed-9c6697363e25", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b024718d-2f71-4a2e-86e5-3e9747c17c70", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "0bfc71d1ab60b38ba54334709ef9967f3683fe74d834321df9028988d0572c31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "974be74c-dd52-4dcb-9a6d-31370839ae67", "node_type": "1", "metadata": {}, "hash": "a39f7b1d98c7a1ddca994cb8bef3968392b106ba4e4a0737842bae6eff960f3b", "class_name": "RelatedNodeInfo"}}, "text": "**SingleStore**\n\n```python\nfrom llama_index.vector_stores.singlestoredb import SingleStoreVectorStore\nimport os\n\n# can set the singlestore db url in env\n# or pass it in as an argument to the SingleStoreVectorStore constructor\nos.environ[\"SINGLESTOREDB_URL\"] = \"PLACEHOLDER URL\"\nvector_store = SingleStoreVectorStore(\n    table_name=\"embeddings\",\n    content_field=\"content\",\n    metadata_field=\"metadata\",\n    vector_field=\"vector\",\n    timeout=30,\n)\n```\n\n**TiDB**\n\n```python\nfrom llama_index.vector_stores.tidbvector import TiDBVectorStore\n\ntidbvec = TiDBVectorStore(\n    # connection url format\n    # - mysql+pymysql://root@34.212.137.91:4000/test\n    connection_string=\"PLACEHOLDER URL\",\n    table_name=\"llama_index_vectorstore\",\n    distance_strategy=\"cosine\",\n    vector_dimension=1536,\n)\n```\n\n**Timescale**\n\n```python\nfrom llama_index.vector_stores.timescalevector import TimescaleVectorStore\n\nvector_store = TimescaleVectorStore.from_params(\n    service_url=\"YOUR TIMESCALE SERVICE URL\",\n    table_name=\"paul_graham_essay\",\n)\n```\n\n**Upstash**\n\n```python\nfrom llama_index.vector_stores.upstash import UpstashVectorStore\n\nvector_store = UpstashVectorStore(url=\"YOUR_URL\", token=\"YOUR_TOKEN\")\n```\n\n**Vertex AI Vector Search**\n\n```python\nfrom llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n\nvector_store = VertexAIVectorStore(\n    project_id=\"[your-google-cloud-project-id]\",\n    region=\"[your-google-cloud-region]\",\n    index_id=\"[your-index-resource-name]\",\n    endpoint_id=\"[your-index-endpoint-name]\",\n)\n```\n\n**Weaviate**\n\n```python\nimport weaviate\nfrom llama_index.vector_stores.weaviate import WeaviateVectorStore\n\n# creating a Weaviate client\nresource_owner_config = weaviate.AuthClientPassword(\n    username=\"<username>\",\n    password=\"<password>\",\n)\nclient = weaviate.Client(\n    \"https://<cluster-id>.semi.network/\",\n    auth_client_secret=resource_owner_config,\n)\n\n# construct vector store\nvector_store = WeaviateVectorStore(weaviate_client=client)\n```\n\n**Zep**\n\nZep stores texts, metadata, and embeddings. All are returned in search results.\n\n```python\nfrom llama_index.vector_stores.zep import ZepVectorStore\n\nvector_store = ZepVectorStore(\n    api_url=\"<api_url>\",\n    api_key=\"<api_key>\",\n    collection_name=\"<unique_collection_name>\",  # Can either be an existing collection or a new one\n    embedding_dimensions=1536,  # Optional, required if creating a new collection\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n\n# Query index using both a text query and metadata filters\nfilters = MetadataFilters(\n    filters=[ExactMatchFilter(key=\"theme\", value=\"Mafia\")]\n)\nretriever = index.as_retriever(filters=filters)\nresult = retriever.retrieve(\"What is inception about?\")\n```\n\n**Zilliz**\n\n- Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.\n\n```python\nimport pymilvus\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\n\n\n# construct vector store\nvector_store = MilvusVectorStore(\n    uri=\"foo.vectordb.zillizcloud.com\",\n    token=\"your_token_here\",\n    overwrite=\"True\",\n)\n```\n\n[Example notebooks can be found here](https://github.com/jerryjliu/llama_index/tree/main/docs/docs/examples/vector_stores).\n\n## Loading Data from Vector Stores using Data Connector\n\nLlamaIndex supports loading data from a huge number of sources. See [Data Connectors](../../module_guides/loading/connector/modules.md) for more details and API documentation.\n\nChroma stores both documents and vectors. This is an example of how to use Chroma:\n\n```python\nfrom llama_index.readers.chroma import ChromaReader\nfrom llama_index.core import SummaryIndex\n\n# The chroma reader loads data from a persisted Chroma collection.\n# This requires a collection name and a persist directory.", "mimetype": "text/plain", "start_char_idx": 20738, "end_char_idx": 24605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "974be74c-dd52-4dcb-9a6d-31370839ae67": {"__data__": {"id_": "974be74c-dd52-4dcb-9a6d-31370839ae67", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9caa6f7d-2d68-4650-a0ed-9c6697363e25", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "a8a782f0a5552092f04ffd24782991502d3f2f8fc894cb73d5c4a666173f8bae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "749aab62-5a39-4801-8622-7ba1658e44ef", "node_type": "1", "metadata": {}, "hash": "36899ae47ea65724e3468ff449307e0d0192755e4caf00f331f46aa92c1fc071", "class_name": "RelatedNodeInfo"}}, "text": "# This requires a collection name and a persist directory.\nreader = ChromaReader(\n    collection_name=\"chroma_collection\",\n    persist_directory=\"examples/data_connectors/chroma_collection\",\n)\n\nquery_vector = [n1, n2, n3, ...]\n\ndocuments = reader.load_data(\n    collection_name=\"demo\", query_vector=query_vector, limit=5\n)\nindex = SummaryIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n```\n\nQdrant also stores both documents and vectors. This is an example of how to use Qdrant:\n\n```python\nfrom llama_index.readers.qdrant import QdrantReader\n\nreader = QdrantReader(host=\"localhost\")\n\n# the query_vector is an embedding representation of your query_vector\n# Example query_vector\n# query_vector = [0.3, 0.3, 0.3, 0.3, ...]\n\nquery_vector = [n1, n2, n3, ...]\n\n# NOTE: Required args are collection_name, query_vector.\n# See the Python client: https;//github.com/qdrant/qdrant_client\n# for more details\n\ndocuments = reader.load_data(\n    collection_name=\"demo\", query_vector=query_vector, limit=5\n)\n```\n\nNOTE: Since Weaviate can store a hybrid of document and vector objects, the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query. See below for usage.\n\n```python\n# option 1: specify class_name and properties\n\n# 1) load data using class_name and properties\ndocuments = reader.load_data(\n    class_name=\"<class_name>\",\n    properties=[\"property1\", \"property2\", \"...\"],\n    separate_documents=True,\n)\n\n# 2) example GraphQL query\nquery = \"\"\"\n{\n    Get {\n        <class_name> {\n            <property1>\n            <property2>\n        }\n    }\n}\n\"\"\"\n\ndocuments = reader.load_data(graphql_query=query, separate_documents=True)\n```\n\nNOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere. Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load_data call.\n\nFor instance, this is an example usage of the Pinecone data loader `PineconeReader`:\n\n```python\nfrom llama_index.readers.pinecone import PineconeReader\n\nreader = PineconeReader(api_key=api_key, environment=\"us-west1-gcp\")\n\nid_to_text_map = {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\n\nquery_vector = [n1, n2, n3, ...]\n\ndocuments = reader.load_data(\n    index_name=\"quickstart\",\n    id_to_text_map=id_to_text_map,\n    top_k=3,\n    vector=query_vector,\n    separate_documents=True,\n)\n```\n\n[Example notebooks can be found here](https://github.com/jerryjliu/llama_index/tree/main/docs/docs/examples/data_connectors).", "mimetype": "text/plain", "start_char_idx": 24547, "end_char_idx": 27246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "749aab62-5a39-4801-8622-7ba1658e44ef": {"__data__": {"id_": "749aab62-5a39-4801-8622-7ba1658e44ef", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "9e1f46b8beb38839c61669bee163b17069abd3c451f8990732301e5bb12de8f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "974be74c-dd52-4dcb-9a6d-31370839ae67", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "ad716cb77254dd6bd8db99071e9dcb309479a68f1c0a8ab13a8f2e088a344f14", "class_name": "RelatedNodeInfo"}}, "text": "## Vector Store Examples\n\n- [Alibaba Cloud OpenSearch](../../examples/vector_stores/AlibabaCloudOpenSearchIndexDemo.ipynb)\n- [Amazon Neptune - Neptune Analytics](../../examples/vector_stores/AmazonNeptuneVectorDemo.ipynb)\n- [Astra DB](../../examples/vector_stores/AstraDBIndexDemo.ipynb)\n- [Async Index Creation](../../examples/vector_stores/AsyncIndexCreationDemo.ipynb)\n- [Azure AI Search](../../examples/vector_stores/AzureAISearchIndexDemo.ipynb)\n- [Azure Cosmos DB](../../examples/vector_stores/AzureCosmosDBMongoDBvCoreDemo.ipynb)\n- [Caasandra](../../examples/vector_stores/CassandraIndexDemo.ipynb)\n- [Chromadb](../../examples/vector_stores/ChromaIndexDemo.ipynb)\n- [Couchbase](../../examples/vector_stores/CouchbaseVectorStoreDemo.ipynb)\n- [Dash](../../examples/vector_stores/DashvectorIndexDemo.ipynb)\n- [Deeplake](../../examples/vector_stores/DeepLakeIndexDemo.ipynb)\n- [DocArray HNSW](../../examples/vector_stores/DocArrayHnswIndexDemo.ipynb)\n- [DocArray in-Memory](../../examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb)\n- [Espilla](../../examples/vector_stores/EpsillaIndexDemo.ipynb)\n- [LanceDB](../../examples/vector_stores/LanceDBIndexDemo.ipynb)\n- [Lantern](../../examples/vector_stores/LanternIndexDemo.ipynb)\n- [Metal](../../examples/vector_stores/MetalIndexDemo.ipynb)\n- [Milvus](../../examples/vector_stores/MilvusIndexDemo.ipynb)\n- [Milvus Hybrid Search](../../examples/vector_stores/MilvusHybridIndexDemo.ipynb)\n- [MyScale](../../examples/vector_stores/MyScaleIndexDemo.ipynb)\n- [ElsaticSearch](../../examples/vector_stores/ElasticsearchIndexDemo.ipynb)\n- [FAISS](../../examples/vector_stores/FaissIndexDemo.ipynb)\n- [MongoDB Atlas](../../examples/vector_stores/MongoDBAtlasVectorSearch.ipynb)\n- [Neo4j](../../examples/vector_stores/Neo4jVectorDemo.ipynb)\n- [OpenSearch](../../examples/vector_stores/OpensearchDemo.ipynb)\n- [Pinecone](../../examples/vector_stores/PineconeIndexDemo.ipynb)\n- [Pinecone Hybrid Search](../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb)\n- [PGvectoRS](../../examples/vector_stores/PGVectoRsDemo.ipynb)\n- [Postgres](../../examples/vector_stores/postgres.ipynb)\n- [Redis](../../examples/vector_stores/RedisIndexDemo.ipynb)\n- [Qdrant](../../examples/vector_stores/QdrantIndexDemo.ipynb)\n- [Qdrant Hybrid Search](../../examples/vector_stores/qdrant_hybrid.ipynb)\n- [Rockset](../../examples/vector_stores/RocksetIndexDemo.ipynb)\n- [Simple](../../examples/vector_stores/SimpleIndexDemo.ipynb)\n- [Supabase](../../examples/vector_stores/SupabaseVectorIndexDemo.ipynb)\n- [Tair](../../examples/vector_stores/TairIndexDemo.ipynb)\n- [Tencent](../../examples/vector_stores/TencentVectorDBIndexDemo.ipynb)\n- [Timesacle](../../examples/vector_stores/Timescalevector.ipynb)\n- [Upstash](../../examples/vector_stores/UpstashVectorDemo.ipynb)\n- [Weaviate](../../examples/vector_stores/WeaviateIndexDemo.ipynb)\n- [Weaviate Hybrid Search](../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb)\n- [Zep](../../examples/vector_stores/ZepIndexDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 27248, "end_char_idx": 30251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d35da44-b194-4e02-9d90-d84a6afde63e": {"__data__": {"id_": "2d35da44-b194-4e02-9d90-d84a6afde63e", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "916ff78cec40c2c67492af2e717ed2276683ec41", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "50618fe01925287a76b9ebbac39cbdc0cde768d43acc3811a6b5483d66f536f3", "class_name": "RelatedNodeInfo"}}, "text": "# Llama Packs \ud83e\udd99\ud83d\udce6\n\n## Concept\n\nLlama Packs are a community-driven hub of **prepackaged modules/templates** you can use to kickstart your LLM app.\n\nThis directly tackles a big pain point in building LLM apps; every use case requires cobbling together custom components and a lot of tuning/dev time. Our goal is to accelerate that through a community led effort.\n\nThey can be used in two ways:\n\n- On one hand, they are **prepackaged modules** that can be initialized with parameters and run out of the box to achieve a given use case (whether that\u2019s a full RAG pipeline, application template, or more). You can also import submodules (e.g. LLMs, query engines) to use directly.\n- On the other hand, LlamaPacks are **templates** that you can inspect, modify, and use.\n\n**All packs are found on [LlamaHub](https://llamahub.ai/).** Go to the dropdown menu and select \"LlamaPacks\" to filter by packs.\n\n**Please check the README of each pack for details on how to use**. [Example pack here](https://llamahub.ai/l/llama_packs-voyage_query_engine).\n\nSee our [launch blog post](https://blog.llamaindex.ai/introducing-llama-packs-e14f453b913a) for more details.\n\n## Usage Pattern\n\nYou can use Llama Packs through either the CLI or Python.\n\nCLI:\n\n```bash\nllamaindex-cli download-llamapack <pack_name> --download-dir <pack_directory>\n```\n\nPython:\n\n```python\nfrom llama_index.core.llama_pack import download_llama_pack\n\n# download and install dependencies\npack_cls = download_llama_pack(\"<pack_name>\", \"<pack_directory>\")\n```\n\nYou can use the pack in different ways, either to inspect modules, run it e2e, or customize the templates.\n\n```python\n# every pack is initialized with different args\npack = pack_cls(*args, **kwargs)\n\n# get modules\nmodules = pack.get_modules()\ndisplay(modules)\n\n# run (every pack will have different args)\noutput = pack.run(*args, **kwargs)\n```\n\nImportantly, you can/should also go into `pack_directory` to inspect the source files/customize it. That's part of the point!\n\n## Module Guides\n\nSome example module guides are given below. Remember, go on [LlamaHub](https://llamahub.ai) to access the full range of packs.\n\n- [LlamaPacks Example](../../examples/llama_hub/llama_packs_example.ipynb)\n- [Resume LlamaPack](../../examples/llama_hub/llama_pack_resume.ipynb)\n- [Ollama LlamaPack](../../examples/llama_hub/llama_pack_ollama.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfcd6113-628a-412b-8e39-164aa737814f": {"__data__": {"id_": "cfcd6113-628a-412b-8e39-164aa737814f", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d18e0fa6c6ef302a627263bbeaa9304cf00cf783", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "a6109c118523b2e4dc3f50106ade0a813b4364cbfc82a8d1ab0bc8c5f33ecd79", "class_name": "RelatedNodeInfo"}}, "text": "# Examples\n\nIn the navigation to the left, you will find many example notebooks, displaying the usage of various llama-index components and use-cases.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "075c2f7b-2ecd-40fb-9026-c9e034f2d1db": {"__data__": {"id_": "075c2f7b-2ecd-40fb-9026-c9e034f2d1db", "embedding": null, "metadata": {"filename": "concepts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc10f69934e71f412af3a179f180659770cce0d0", "node_type": "4", "metadata": {"filename": "concepts.md", "author": "LlamaIndex"}, "hash": "5932d4f6c8ca24ea12333105d14f66254ba3d9732f2d39ec80feb2c2f590bdb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47463a30-136c-4f56-906d-2d95f521adfb", "node_type": "1", "metadata": {}, "hash": "b721f30f26a8d864e4bf46cbea8a9420b8f74189f316f7fafce235f97110a23a", "class_name": "RelatedNodeInfo"}}, "text": "# High-Level Concepts\n\nThis is a quick guide to the high-level concepts you'll encounter frequently when building LLM applications.\n\n## Use cases\n\nThere are endless use cases for data-backed LLM applications but they can be roughly grouped into four categories:\n\n[**Structured Data Extraction**](../use_cases/extraction/)\nPydantic extractors allow you to specify a precise data structure to extract from your data and use LLMs to fill in the missing pieces in a type-safe way. This is useful for extracting structured data from unstructured sources like PDFs, websites, and more, and is key to automating workflows.\n\n[**Query Engines**](../module_guides/deploying/query_engine/index.md):\nA query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n\n[**Chat Engines**](../module_guides/deploying/chat_engines/index.md):\nA chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n\n[**Agents**](../module_guides/deploying/agents/index.md):\nAn agent is an automated decision-maker powered by an LLM that interacts with the world via a set of [tools](../module_guides/deploying/agents/tools.md). Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.\n\n## Retrieval Augmented Generation (RAG)\n\n!!! tip\n    If you haven't, [install LlamaIndex](./installation.md) and complete the [starter tutorial](./starter_example.md) before you read this. It will help ground these steps in your experience.\n\nLLMs are trained on enormous bodies of data but they aren't trained on **your** data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation. Query engines, chat engines and agents often use RAG to complete their tasks.\n\nIn RAG, your data is loaded and prepared for queries or \"indexed\". User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n\nEven if what you're building is a chatbot or an agent, you'll want to know RAG techniques for getting data into your application.\n\n![](../_static/getting_started/basic_rag.png)\n\n### Stages within RAG\n\nThere are five key stages within RAG, which in turn will be a part of most larger applications you build. These are:\n\n- **Loading**: this refers to getting your data from where it lives -- whether it's text files, PDFs, another website, a database, or an API -- into your pipeline. [LlamaHub](https://llamahub.ai/) provides hundreds of connectors to choose from.\n\n- **Indexing**: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating `vector embeddings`, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n\n- **Storing**: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n\n- **Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n\n- **Evaluation**: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n\n![](../_static/getting_started/stages.png)\n\n### Important concepts within RAG\n\nThere are also some terms you'll encounter that refer to steps within each of these stages.\n\n#### Loading stage\n\n[**Nodes and Documents**](../module_guides/loading/documents_and_nodes/index.md): A `Document` is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source `Document`. Nodes have metadata that relate them to the document they are in and to other nodes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47463a30-136c-4f56-906d-2d95f521adfb": {"__data__": {"id_": "47463a30-136c-4f56-906d-2d95f521adfb", "embedding": null, "metadata": {"filename": "concepts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc10f69934e71f412af3a179f180659770cce0d0", "node_type": "4", "metadata": {"filename": "concepts.md", "author": "LlamaIndex"}, "hash": "5932d4f6c8ca24ea12333105d14f66254ba3d9732f2d39ec80feb2c2f590bdb5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "075c2f7b-2ecd-40fb-9026-c9e034f2d1db", "node_type": "1", "metadata": {"filename": "concepts.md", "author": "LlamaIndex"}, "hash": "f23c86a404c1b7cb2db4daaf092fe6fb8e4d56ee8ea61f40fb1bc5904b852ecf", "class_name": "RelatedNodeInfo"}}, "text": "Nodes have metadata that relate them to the document they are in and to other nodes.\n\n[**Connectors**](../module_guides/loading/connector/index.md):\nA data connector (often called a `Reader`) ingests data from different data sources and data formats into `Documents` and `Nodes`.\n\n#### Indexing Stage\n\n[**Indexes**](../module_guides/indexing/index.md):\nOnce you've ingested your data, LlamaIndex will help you index the data into a structure that's easy to retrieve. This usually involves generating `vector embeddings` which are stored in a specialized database called a `vector store`. Indexes can also store a variety of metadata about your data.\n\n[**Embeddings**](../module_guides/models/embeddings.md): LLMs generate numerical representations of data called `embeddings`. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n\n#### Querying Stage\n\n[**Retrievers**](../module_guides/querying/retriever/index.md):\nA retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it's done.\n\n[**Routers**](../module_guides/querying/router/index.md):\nA router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the `RouterRetriever`\u00a0class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate's metadata and the query.\n\n[**Node Postprocessors**](../module_guides/querying/node_postprocessors/index.md):\nA node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n\n[**Response Synthesizers**](../module_guides/querying/response_synthesizers/index.md):\nA response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n\n!!! tip\n    * Tell me how to [customize things](./customization.md)\n    * Continue learning with our [understanding LlamaIndex](../understanding/index.md) guide\n    * Ready to dig deep? Check out the [component guides](../module_guides/index.md)", "mimetype": "text/plain", "start_char_idx": 4384, "end_char_idx": 6695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c227ed5f-8fec-4610-835f-184a4c091c6a": {"__data__": {"id_": "c227ed5f-8fec-4610-835f-184a4c091c6a", "embedding": null, "metadata": {"filename": "create_llama.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96e9abaceb5401652823a676eeea7ddd55e2f8dd", "node_type": "4", "metadata": {"filename": "create_llama.md", "author": "LlamaIndex"}, "hash": "44e6f2aae7ba26d09226c165db157fe57b939a41817cb15bae3e41c8f3741cb2", "class_name": "RelatedNodeInfo"}}, "text": "# `create-llama`\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 22, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae9acf8-04d0-48e5-a543-b6ab5a16f6de": {"__data__": {"id_": "2ae9acf8-04d0-48e5-a543-b6ab5a16f6de", "embedding": null, "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58672402edf2a60067ad81e382e144ca2a4384e8", "node_type": "4", "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "hash": "1aa4fb393ee4982f68007ac963e708bd94b52d72e272db9e924b7133fc7f3bce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ff7170d-66ee-44b6-9589-2d0edbe0ef55", "node_type": "1", "metadata": {}, "hash": "675ee5e01b69ef02b6f66a27e2c66ca647716557180ac0c8614afb47bc27e891", "class_name": "RelatedNodeInfo"}}, "text": "# Frequently Asked Questions (FAQ)\n\n!!! tip\n    If you haven't already, [install LlamaIndex](installation.md) and complete the [starter tutorial](starter_example.md). If you run into terms you don't recognize, check out the [high-level concepts](concepts.md).\n\nIn this section, we start with the code you wrote for the [starter example](starter_example.md) and show you the most common ways you might want to customize it for your use case:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\n---\n\n## **\"I want to parse my documents into smaller chunks\"**\n\n```python\n# Global settings\nfrom llama_index.core import Settings\n\nSettings.chunk_size = 512\n\n# Local settings\nfrom llama_index.core.node_parser import SentenceSplitter\n\nindex = VectorStoreIndex.from_documents(\n    documents, transformations=[SentenceSplitter(chunk_size=512)]\n)\n```\n\n---\n\n## **\"I want to use a different vector store\"**\n\nFirst, you can install the vector store you want to use. For example, to use Chroma as the vector store, you can install it using pip:\n\n```bash\npip install llama-index-vector-stores-chroma\n```\n\nTo learn more about all integrations available, check out [LlamaHub](https://llamahub.ai).\n\nThen, you can use it in your code:\n\n```python\nimport chromadb\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nfrom llama_index.core import StorageContext\n\nchroma_client = chromadb.PersistentClient()\nchroma_collection = chroma_client.create_collection(\"quickstart\")\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n```\n\n`StorageContext` defines the storage backend for where the documents, embeddings, and indexes are stored. You can learn more about [storage](../module_guides/storing/index.md) and [how to customize it](../module_guides/storing/customization.md).\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\n---\n\n## **\"I want to retrieve more context when I query\"**\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine(similarity_top_k=5)\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\n`as_query_engine` builds a default `retriever` and `query engine` on top of the index. You can configure the retriever and query engine by passing in keyword arguments. Here, we configure the retriever to return the top 5 most similar documents (instead of the default of 2). You can learn more about [retrievers](../module_guides/querying/retriever/retrievers.md) and [query engines](../module_guides/querying/retriever/index.md).\n\n---\n\n## **\"I want to use a different LLM\"**\n\n```python\n# Global settings\nfrom llama_index.core import Settings\nfrom llama_index.llms.ollama import Ollama\n\nSettings.llm = Ollama(model=\"mistral\", request_timeout=60.0)\n\n# Local settings\nindex.as_query_engine(llm=Ollama(model=\"mistral\", request_timeout=60.0))\n```\n\nYou can learn more about [customizing LLMs](../module_guides/models/llms.md).\n\n---\n\n## **\"I want to use a different response mode\"**\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine(response_mode=\"tree_summarize\")\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\nYou can learn more about [query engines](../module_guides/querying/index.md) and [response modes](../module_guides/deploying/query_engine/response_modes.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ff7170d-66ee-44b6-9589-2d0edbe0ef55": {"__data__": {"id_": "6ff7170d-66ee-44b6-9589-2d0edbe0ef55", "embedding": null, "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58672402edf2a60067ad81e382e144ca2a4384e8", "node_type": "4", "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "hash": "1aa4fb393ee4982f68007ac963e708bd94b52d72e272db9e924b7133fc7f3bce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae9acf8-04d0-48e5-a543-b6ab5a16f6de", "node_type": "1", "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "hash": "6f8f9c56a36555408b08a29f3763b5e6b24d83accfc706e8cbc50c6dddc6541c", "class_name": "RelatedNodeInfo"}}, "text": "---\n\n## **\"I want to stream the response back\"**\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine(streaming=True)\nresponse = query_engine.query(\"What did the author do growing up?\")\nresponse.print_response_stream()\n```\n\nYou can learn more about [streaming responses](../module_guides/deploying/query_engine/streaming.md).\n\n---\n\n## **\"I want a chatbot instead of Q&A\"**\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_chat_engine()\nresponse = query_engine.chat(\"What did the author do growing up?\")\nprint(response)\n\nresponse = query_engine.chat(\"Oh interesting, tell me more.\")\nprint(response)\n```\n\nLearn more about the [chat engine](../module_guides/deploying/chat_engines/usage_pattern.md).\n\n---\n\n## Next Steps\n\n- Want a thorough walkthrough of (almost) everything you can configure? Get started with [Understanding LlamaIndex](../understanding/index.md).\n- Want more in-depth understanding of specific modules? Check out the [component guides](../module_guides/index.md).", "mimetype": "text/plain", "start_char_idx": 4243, "end_char_idx": 5552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1035c692-0298-480f-85b1-992d1d6e11f1": {"__data__": {"id_": "1035c692-0298-480f-85b1-992d1d6e11f1", "embedding": null, "metadata": {"filename": "discover_llamaindex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf80744237deefb64f56086b326ce0580b43033c", "node_type": "4", "metadata": {"filename": "discover_llamaindex.md", "author": "LlamaIndex"}, "hash": "e63991f048c3bcb58ea7639977cd3c16e052ff3c7aa933aff1c62c9e31f900d5", "class_name": "RelatedNodeInfo"}}, "text": "# Discover LlamaIndex Video Series\n\nIf you like learning from videos, now's a good time to check out our \"Discover LlamaIndex\" series. If not, we recommend heading on to our [Understanding LlamaIndex](../understanding/index.md) tutorial.\n\n## Bottoms-Up Development (Llama Docs Bot)\n\nThis is a sub-series within Discover LlamaIndex that shows you how to build a document chatbot from scratch.\n\nWe show you how to do this in a \"bottoms-up\" fashion - start by using the LLMs, and data objects as independent modules. Then gradually add higher-level abstractions like indexing, and advanced retrievers/rerankers.\n\n[Full Repo](https://github.com/run-llama/llama_docs_bot)\n[[Part 1] LLMs and Prompts](https://www.youtube.com/watch?v=p0jcvGiBKSA)\n[[Part 2] Documents and Metadata](https://www.youtube.com/watch?v=nGNoacku0YY)\n[[Part 3] Evaluation](https://www.youtube.com/watch?v=LQy8iHOJE2A)\n[[Part 4] Embeddings](https://www.youtube.com/watch?v=2c64G-iDJKQ)\n[[Part 5] Retrievers and Postprocessors](https://www.youtube.com/watch?v=mIyZ_9gqakE)\n\n## SubQuestionQueryEngine + 10K Analysis\n\nThis video covers the `SubQuestionQueryEngine` and how it can be applied to financial documents to help decompose complex queries into multiple sub-questions.\n\n[Youtube](https://www.youtube.com/watch?v=GT_Lsj3xj1o)\n\n[Notebook](../examples/usecases/10k_sub_question.ipynb)\n\n## Discord Document Management\n\nThis video covers managing documents from a source that is constantly updating (i.e. Discord) and how you can avoid document duplication and save embedding tokens.\n\n[Youtube](https://www.youtube.com/watch?v=j6dJcODLd_c)\n\n[Notebook and Supplementary Material](https://github.com/jerryjliu/llama_index/tree/main/docs/docs/examples/discover_llamaindex/document_management/)\n\n[Reference Docs](../module_guides/indexing/document_management.md)\n\n## Joint Text to SQL and Semantic Search\n\nThis video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface.\n\n[Youtube](https://www.youtube.com/watch?v=ZIvcVJGtCrY)\n\n[Notebook](../examples/query_engine/SQLAutoVectorQueryEngine.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85afe527-52eb-42ab-b52d-318713e0cb30": {"__data__": {"id_": "85afe527-52eb-42ab-b52d-318713e0cb30", "embedding": null, "metadata": {"filename": "installation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a47db45817a3ffd872c3924d6d0506c02130060", "node_type": "4", "metadata": {"filename": "installation.md", "author": "LlamaIndex"}, "hash": "1b9bf7906cf95b6e5c7b784d648fc9a07054a592ca8d33cd68653fb758ecdd8b", "class_name": "RelatedNodeInfo"}}, "text": "# Installation and Setup\n\nThe LlamaIndex ecosystem is structured using a collection of namespaced packages.\n\nWhat this means for users is that LlamaIndex comes with a core starter bundle, and additional integrations can be installed as needed.\n\nA complete list of packages and available integrations is available on [LlamaHub](https://llamahub.ai/).\n\n## Quickstart Installation from Pip\n\nTo get started quickly, you can install with:\n\n```\npip install llama-index\n```\n\nThis is a starter bundle of packages, containing\n\n- `llama-index-core`\n- `llama-index-legacy  # temporarily included`\n- `llama-index-llms-openai`\n- `llama-index-embeddings-openai`\n- `llama-index-program-openai`\n- `llama-index-question-gen-openai`\n- `llama-index-agent-openai`\n- `llama-index-readers-file`\n- `llama-index-multi-modal-llms-openai`\n\n**NOTE:** LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, ...). Use the environment variable \"LLAMA_INDEX_CACHE_DIR\" to control where these files are saved.\n\n### Important: OpenAI Environment Setup\n\nBy default, we use the OpenAI `gpt-3.5-turbo` model for text generation and `text-embedding-ada-002` for retrieval and embeddings. In order to use this, you must have an OPENAI_API_KEY set up as an environment variable.\nYou can obtain an API key by logging into your OpenAI account and [and creating a new API key](https://platform.openai.com/account/api-keys).\n\n!!! tip\n    You can also [use one of many other available LLMs](../module_guides/models/llms/usage_custom.md). You may need additional environment keys + tokens setup depending on the LLM provider.\n\n[Check out our OpenAI Starter Example](starter_example.md)\n\n## Custom Installation from Pip\n\nIf you aren't using OpenAI, or want a more selective installation, you can install individual packages as needed.\n\nFor example, for a local setup with Ollama and HuggingFace embeddings, the installation might look like:\n\n```\npip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n```\n\n[Check out our Starter Example with Local Models](starter_example_local.md)\n\nA full guide to using and configuring LLMs is available [here](../module_guides/models/llms.md).\n\nA full guide to using and configuring embedding models is available [here](../module_guides/models/embeddings.md).\n\n## Installation from Source\n\nGit clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do the following:\n\n- [Install poetry](https://python-poetry.org/docs/#installation) - this will help you manage package dependencies\n- `poetry shell` - this command creates a virtual environment, which keeps installed packages contained to this project\n- `poetry install` - this will install the core starter package requirements\n- (Optional) `poetry install --with dev, docs` - this will install all dependencies needed for most local development\n\nFrom there, you can install integrations as needed with `pip`, For example:\n\n```\npip install -e llama-index-integrations/llms/llama-index-llms-ollama\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c950f0c-b34b-41bc-bbce-054a3ff0c6da": {"__data__": {"id_": "7c950f0c-b34b-41bc-bbce-054a3ff0c6da", "embedding": null, "metadata": {"filename": "reading.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c18c27023b15f1ba0a55f687ddb894da962ad0c6", "node_type": "4", "metadata": {"filename": "reading.md", "author": "LlamaIndex"}, "hash": "01942ec0f304af0e34be64dd0f26e82212e16a1708987c7564584e90da476589", "class_name": "RelatedNodeInfo"}}, "text": "# How to read these docs\n\nWelcome to the LlamaIndex documentation! We've tried hard to make these docs approachable regardless of your experience level with LlamaIndex and with LLMs and generative AI in general.\n\n## Before you start\n\nLlamaIndex is a Python library, so you should have Python [installed](https://www.python.org/downloads/) and a basic working understanding of how to write it. If you prefer JavaScript, we recommend trying out our [TypeScript package](https://ts.llamaindex.ai/).\n\nMany of our examples are formatted as Notebooks, by which we mean Jupyter-style notebooks. You don't have to have Jupyter installed; you can try out most of our examples on a hosted service like [Google Colab](https://colab.research.google.com/).\n\n## Structure of these docs\n\nOur docs are structured so you should be able to roughly progress simply by moving across the links at the top of the page from left to right, or just hitting the \"next\" link at the bottom of each page.\n\n1. **Getting started:** The section you're in right now. We can get you going from knowing nothing about LlamaIndex and LLMs. [Install the library](installation.md), write your first demo in [five lines of code](starter_example.md), learn more about the [high level concepts](concepts.md) of LLM applications, and then see how you can [customize the five-line example](customization.md) to meet your needs.\n\n2. **Learn:** Once you've completed the Getting Started section, this is the next place to go. In a series of bite-sized tutorials, we'll walk you through every stage of building a production LlamaIndex application and help you level up on the concepts of the library and LLMs in general as you go.\n\n3. **Use cases:** If you're a dev trying to figure out whether LlamaIndex will work for your use case, we have an overview of the types of things you can build.\n\n4. **Examples:** We have rich notebook examples for nearly every feature under the sun. Explore these to find and learn something new about LlamaIndex.\n\n5. **Component guides:** Arranged in the same order of building an LLM application as our Learn section, these are comprehensive, lower-level guides to the individual components of LlamaIndex and how to use them.\n\n6. **Advanced Topics:** Already got a working LlamaIndex application and looking to further refine it? Our advanced section will walk you through the [first things you should try optimizing](../optimizing/basic_strategies/basic_strategies.md) like your embedding model and chunk size through progressively more complex and subtle customizations all the way to [fine tuning](../optimizing/fine-tuning/fine-tuning.md) your model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f89b8060-14c9-4920-b261-3564b01b807d": {"__data__": {"id_": "f89b8060-14c9-4920-b261-3564b01b807d", "embedding": null, "metadata": {"filename": "starter_example.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05be9f4d914e8dd038fb31eee61f1c41fd15afa8", "node_type": "4", "metadata": {"filename": "starter_example.md", "author": "LlamaIndex"}, "hash": "24324786a3d5cfc732617ee52efb138d0bf784f54a7ffeb4cc960e91248cc2a6", "class_name": "RelatedNodeInfo"}}, "text": "# Starter Tutorial (OpenAI)\n\nThis is our famous \"5 lines of code\" starter example using OpenAI.\n\n!!! tip\n    Make sure you've followed the [installation](installation.md) steps first.\n\n!!! tip\n    Want to use local models?\n    If you want to do our starter tutorial using only local models, [check out this tutorial instead](starter_example_local.md).\n\n## Download data\n\nThis example uses the text of Paul Graham's essay, [\"What I Worked On\"](http://paulgraham.com/worked.html). This and many other examples can be found in the `examples` folder of our repo.\n\nThe easiest way to get it is to [download it via this link](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt) and save it in a folder called `data`.\n\n## Set your OpenAI API key\n\nLlamaIndex uses OpenAI's `gpt-3.5-turbo` by default. Make sure your API key is available to your code by setting it as an environment variable. In MacOS and Linux, this is the command:\n\n```\nexport OPENAI_API_KEY=XXXXX\n```\n\nand on Windows it is\n\n```\nset OPENAI_API_KEY=XXXXX\n```\n\n## Load data and build an index\n\nIn the same folder where you created the `data` folder, create a file called `starter.py` file with the following:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nThis builds an index over the documents in the `data` folder (which in this case just consists of the essay text, but could contain many documents).\n\nYour directory structure should look like this:\n\n<pre>\n\u251c\u2500\u2500 starter.py\n\u2514\u2500\u2500 data\n \u00a0\u00a0 \u2514\u2500\u2500 paul_graham_essay.txt\n</pre>\n\n## Query your data\n\nAdd the following lines to `starter.py`\n\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\nThis creates an engine for Q&A over your index and asks a simple question. You should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\n\n## Viewing Queries and Events Using Logging\n\nWant to see what's happening under the hood? Let's add some logging. Add these lines to the top of `starter.py`:\n\n```python\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n```\n\nYou can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.\n\n## Storing your index\n\nBy default, the data you just loaded is stored in memory as a series of vector embeddings. You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:\n\n```python\nindex.storage_context.persist()\n```\n\nBy default, this will save the data to the directory `storage`, but you can change that by passing a `persist_dir` parameter.\n\nOf course, you don't get the benefits of persisting unless you load the data. So let's modify `starter.py` to generate and store the index if it doesn't exist, but load it if it does:\n\n```python\nimport os.path\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n    load_index_from_storage,\n)\n\n# check if storage already exists\nPERSIST_DIR = \"./storage\"\nif not os.path.exists(PERSIST_DIR):\n    # load the documents and create the index\n    documents = SimpleDirectoryReader(\"data\").load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    # store it for later\n    index.storage_context.persist(persist_dir=PERSIST_DIR)\nelse:\n    # load the existing index\n    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n    index = load_index_from_storage(storage_context)\n\n# Either way we can now query the index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\nNow you can efficiently query to your heart's content! But this is just the beginning of what you can do with LlamaIndex.\n\n!!! tip\n    - learn more about the [high-level concepts](./concepts.md).\n    - tell me how to [customize things](./customization.md).\n    - curious about a specific module? check out the [component guides](../module_guides/index.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5832e624-06c2-450a-aa8d-b35a4ae589a8": {"__data__": {"id_": "5832e624-06c2-450a-aa8d-b35a4ae589a8", "embedding": null, "metadata": {"filename": "starter_example_local.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb99e8ea9919822e6be72f75bb353e047c2fc65d", "node_type": "4", "metadata": {"filename": "starter_example_local.md", "author": "LlamaIndex"}, "hash": "ada87b55e4ffcdb2a71f8808081f6ce4a62079aa9ed99477c616dd88cd3f31d5", "class_name": "RelatedNodeInfo"}}, "text": "# Starter Tutorial (Local Models)\n\n!!! tip\n    Make sure you've followed the [custom installation](installation.md) steps first.\n\nThis is our famous \"5 lines of code\" starter example with local LLM and embedding models. We will use [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) as our embedding model and `Llama3` served through `Ollama`.\n\n## Download data\n\nThis example uses the text of Paul Graham's essay, [\"What I Worked On\"](http://paulgraham.com/worked.html). This and many other examples can be found in the `examples` folder of our repo.\n\nThe easiest way to get it is to [download it via this link](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt) and save it in a folder called `data`.\n\n## Setup\n\nOllama is a tool to help you get set up with LLMs locally (currently supported on OSX and Linux. You can install Ollama on Windows through WSL 2).\n\nFollow the [README](https://github.com/jmorganca/ollama) to learn how to install it.\n\nTo download the Llama3 model just do `ollama pull llama3`.\n\n**NOTE**: You will need a machine with at least 32GB of RAM.\n\nTo import `llama_index.llms.ollama`, you should run `pip install llama-index-llms-ollama`.\n\nTo import `llama_index.embeddings.huggingface`, you should run `pip install llama-index-embeddings-huggingface`.\n\nMore integrations are all listed on [https://llamahub.ai](https://llamahub.ai).\n\n## Load data and build an index\n\nIn the same folder where you created the `data` folder, create a file called `starter.py` file with the following:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.ollama import Ollama\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\n\n# bge-base embedding model\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n\n# ollama\nSettings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\n```\n\nThis builds an index over the documents in the `data` folder (which in this case just consists of the essay text, but could contain many documents).\n\nYour directory structure should look like this:\n\n<pre>\n\u251c\u2500\u2500 starter.py\n\u2514\u2500\u2500 data\n \u00a0\u00a0 \u2514\u2500\u2500 paul_graham_essay.txt\n</pre>\n\nWe use the `BAAI/bge-base-en-v1.5` model through our [`HuggingFaceEmbedding`](../api_reference/embeddings/huggingface.md#llama_index.embeddings.huggingface.HuggingFaceEmbedding) class and our `Ollama` LLM wrapper to load in the Llama3 model. Learn more in the [Local Embedding Models](../module_guides/models/embeddings.md#local-embedding-models) page.\n\n## Query your data\n\nAdd the following lines to `starter.py`\n\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\nThis creates an engine for Q&A over your index and asks a simple question. You should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\n\nYou can view logs, persist/load the index similar to our [starter example](starter_example.md).\n\n!!! tip\n    - learn more about the [high-level concepts](./concepts.md).\n    - tell me how to [customize things](./customization.md).\n    - curious about a specific module? check out the [component guides](../module_guides/index.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7698bcfb-b51c-4088-acfe-f5f7094415c9": {"__data__": {"id_": "7698bcfb-b51c-4088-acfe-f5f7094415c9", "embedding": null, "metadata": {"filename": "starter_projects.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "096a588190ce7458a2ee39bba893a36a105cddef", "node_type": "4", "metadata": {"filename": "starter_projects.md", "author": "LlamaIndex"}, "hash": "6d65d709dc58ea2099f098456b1cb32786c56d5028234050c37598faa5e811f5", "class_name": "RelatedNodeInfo"}}, "text": "# Starter Projects\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 24, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85566be3-83f8-4b51-be47-7f0390262b33": {"__data__": {"id_": "85566be3-83f8-4b51-be47-7f0390262b33", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e806aa9811b226443b89d58d38d935b5870c416c", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "110763f85a5dd5248c53b300e4a8fc67e9b21a293cedbddd99d8a4a185a68705", "class_name": "RelatedNodeInfo"}}, "text": "# Starter Tools\n\nWe have created a variety of open-source tools to help you bootstrap your generative AI projects.\n\n## create-llama: Full-stack web application generator\n\nThe `create-llama` tool is a CLI tool that helps you create a full-stack web application with your choice of frontend and backend that indexes your documents and allows you to chat with them. Running it is as simple as running:\n\n```shell\nnpx create-llama@latest\n```\n\nFor full documentation, check out the [create-llama README on npm](https://www.npmjs.com/package/create-llama).\n\n## SEC Insights: advanced query techniques\n\nIndexing and querying financial filings is a very common use-case for generative AI. To help you get started, we have created and open-sourced a full-stack application that lets you select filings from public companies across multiple years and summarize and compare them. It uses advanced querying and retrieval techniques to achieve high quality results.\n\nYou can use the app yourself at [SECinsights.ai](https://www.secinsights.ai/) or check out the code on [GitHub](https://github.com/run-llama/sec-insights).\n\n![SEC Insights](secinsights.png)\n\n## Chat LlamaIndex: Full-stack chat application\n\nChat LlamaIndex is another full-stack, open-source application that has a variety of interaction modes including streaming chat and multi-modal querying over images. It's a great way to see advanced chat application techniques. You can use it at [chat.llamaindex.ai](https://chat.llamaindex.ai/) or check out the code on [GitHub](https://github.com/run-llama/chat-llamaindex).\n\n![Chat LlamaIndex](chatllamaindex.png)\n\n## LlamaBot: Slack and Discord apps\n\nLlamaBot is another open-source application, this time for building a Slack bot that listens to messages within your organization and answers questions about what's going on. You can check out the [full tutorial and code on GitHub]https://github.com/run-llama/llamabot). If you prefer Discord, there is a [Discord version contributed by the community](https://twitter.com/clusteredbytes/status/1754220009885163957).\n\n![LlamaBot](llamabot.png)\n\n## RAG CLI: quick command-line chat with any document\n\nWe provide a command-line tool that quickly lets you chat with documents. Learn more in the [RAG CLI documentation](rag_cli.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "296105a5-d1ef-49a5-bc74-4257ce966caa": {"__data__": {"id_": "296105a5-d1ef-49a5-bc74-4257ce966caa", "embedding": null, "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e8fec25473a3a8acbf31c22c0724e70656b79eb", "node_type": "4", "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "hash": "d5895b1145b8b49d715392c3e42bb6fe5359b15c53e8a66dd32e0ab37f3ecfbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f966ec6f-bad4-48fb-bdfb-1044c2ad8c91", "node_type": "1", "metadata": {}, "hash": "f90456ca1b87b0bda1cb7906d9f85a467514b0bb4d924b15d3fc743446bdec67", "class_name": "RelatedNodeInfo"}}, "text": "# RAG CLI\n\nOne common use case is chatting with an LLM about files you have saved locally on your computer.\n\nWe have written a CLI tool to help you do just that! You can point the rag CLI tool to a set of files you've saved locally, and it will ingest those files into a local vector database that is then used for a Chat Q&A repl within your terminal.\n\nBy default, this tool uses OpenAI for the embeddings & LLM as well as a local Chroma Vector DB instance. **Warning**: this means that, by default, the local data you ingest with this tool _will_ be sent to OpenAI's API.\n\nHowever, you do have the ability to customize the models and databases used in this tool. This includes the possibility of running all model execution locally! See the **Customization** section below.\n\n## Setup\n\nTo set-up the CLI tool, make sure you've installed the library:\n\n`$ pip install -U llama-index`\n\nYou will also need to install [Chroma](../../examples/vector_stores/ChromaIndexDemo.ipynb):\n\n`$ pip install -U chromadb`\n\nAfter that, you can start using the tool:\n\n```shell\n$ llamaindex-cli rag -h\nusage: llamaindex-cli rag [-h] [-q QUESTION] [-f FILES] [-c] [-v] [--clear] [--create-llama]\n\noptions:\n  -h, --help            show this help message and exit\n  -q QUESTION, --question QUESTION\n                        The question you want to ask.\n  -f FILES, --files FILES\n                        The name of the file or directory you want to ask a question about,such as \"file.pdf\".\n  -c, --chat            If flag is present, opens a chat REPL.\n  -v, --verbose         Whether to print out verbose information during execution.\n  --clear               Clears out all currently embedded data.\n  --create-llama        Create a LlamaIndex application based on the selected files.\n```\n\n## Usage\n\nHere are some high level steps to get you started:\n\n1. **Set the `OPENAI_API_KEY` environment variable:** By default, this tool uses OpenAI's API. As such, you'll need to ensure the OpenAI API Key is set under the `OPENAI_API_KEY` environment variable whenever you use the tool.\n   ```shell\n   $ export OPENAI_API_KEY=<api_key>\n   ```\n1. **Ingest some files:** Now, you need to point the tool at some local files that it can ingest into the local vector database. For this example, we'll ingest the LlamaIndex `README.md` file:\n   ```shell\n   $ llamaindex-cli rag --files \"./README.md\"\n   ```\n   You can also specify a file glob pattern such as:\n   ```shell\n   $ llamaindex-cli rag --files \"./docs/**/*.rst\"\n   ```\n1. **Ask a Question**: You can now start asking questions about any of the documents you'd ingested in the prior step:\n   ```shell\n   $ llamaindex-cli rag --question \"What is LlamaIndex?\"\n   LlamaIndex is a data framework that helps in ingesting, structuring, and accessing private or domain-specific data for LLM-based applications. It provides tools such as data connectors to ingest data from various sources, data indexes to structure the data, and engines for natural language access to the data. LlamaIndex follows a Retrieval-Augmented Generation (RAG) approach, where it retrieves information from data sources, adds it to the question as context, and then asks the LLM to generate an answer based on the enriched prompt. This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, up-to-date, and trustworthy solution for data augmentation. LlamaIndex is designed for both beginner and advanced users, with a high-level API for easy usage and lower-level APIs for customization and extension.\n   ```\n1. **Open a Chat REPL**: You can even open a chat interface within your terminal! Just run `$ llamaindex-cli rag --chat` and start asking questions about the files you've ingested.\n\n### Create a LlamaIndex chat application\n\nYou can also create a full-stack chat application with a FastAPI backend and NextJS frontend based on the files that you have selected.\n\nTo bootstrap the application, make sure you have NodeJS and npx installed on your machine. If not, please refer to the [LlamaIndex.TS](https://ts.llamaindex.ai/getting_started/installation) documentation for instructions.\n\nOnce you have everything set up, creating a new application is easy.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f966ec6f-bad4-48fb-bdfb-1044c2ad8c91": {"__data__": {"id_": "f966ec6f-bad4-48fb-bdfb-1044c2ad8c91", "embedding": null, "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e8fec25473a3a8acbf31c22c0724e70656b79eb", "node_type": "4", "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "hash": "d5895b1145b8b49d715392c3e42bb6fe5359b15c53e8a66dd32e0ab37f3ecfbe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "296105a5-d1ef-49a5-bc74-4257ce966caa", "node_type": "1", "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "hash": "b2a24d9a246fa6bababcc99000aec6990a057cfcc94a6b5fcb7a919e10a4a356", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdcec2eb-6d10-48ab-9710-cce8e567f72d", "node_type": "1", "metadata": {}, "hash": "7a9c75228e1ec19b9f38ff2ec58e0122b361df2152f144fc6a2cf9afb369018e", "class_name": "RelatedNodeInfo"}}, "text": "Once you have everything set up, creating a new application is easy. Simply run the following command:\n\n`$ llamaindex-cli rag --create-llama`\n\nIt will call our `create-llama` tool, so you will need to provide several pieces of information to create the app. You can find more information about the `create-llama` on [npmjs - create-llama](https://www.npmjs.com/package/create-llama#example)\n\n```shell\n\u276f llamaindex-cli rag --create-llama\n\nCalling create-llama using data from /tmp/rag-data/...\n\n\u2714 What is your project named? \u2026 my-app\n\u2714 Which model would you like to use? \u203a gpt-3.5-turbo\n\u2714 Please provide your OpenAI API key (leave blank to skip): \u2026\n? How would you like to proceed? \u203a - Use arrow-keys. Return to submit.\n   Just generate code (~1 sec)\n   Generate code and install dependencies (~2 min)\n\u276f  Generate code, install dependencies, and run the app (~2 min)\n...\n```\n\nIf you choose the option `Generate code, install dependencies, and run the app (~2 min)`, all dependencies will be installed and the app will run automatically. You can then access the application by going to this address: http://localhost:3000.\n\n### Supported File Types\n\nInternally, the `rag` CLI tool uses the [SimpleDirectoryReader](../../module_guides/loading/simpledirectoryreader.md) to parse the raw files in your local filesystem into strings.\n\nThis module has custom readers for a wide variety of file types. Some of those may require that you `pip install` another module that is needed for parsing that particular file type.\n\nIf a file type is encountered with a file extension that the `SimpleDirectoryReader` does not have a custom reader for, it will just read the file as a plain text file.\n\nSee the next section for information on how to add your own custom file readers + customize other aspects of the CLI tool!\n\n## Customization\n\nThe `rag` CLI tool is highly customizable! The tool is powered by combining the [`IngestionPipeline`](../../module_guides/loading/ingestion_pipeline/index.md) & [`QueryPipeline`](../../module_guides/querying/pipeline/index.md) modules within the [`RagCLI`](https://github.com/run-llama/llama_index/blob/main/llama_index/command_line/rag.py) module.\n\nTo create your own custom rag CLI tool, you can simply create a script that instantiates the `RagCLI` class with a `IngestionPipeline` & `QueryPipeline` that you've configured yourself. From there, you can simply run `rag_cli_instance.cli()` in your script to run the same ingestion and Q&A commands against your own choice of embedding models, LLMs, vector DBs, etc.\n\nHere's some high-level code to show the general setup:\n\n```python\n#!/path/to/your/virtualenv/bin/python\nimport os\nfrom llama_index.core.ingestion import IngestionPipeline, IngestionCache\nfrom llama_index.core.query_pipeline import QueryPipeline\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.cli.rag import RagCLI", "mimetype": "text/plain", "start_char_idx": 4121, "end_char_idx": 7014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdcec2eb-6d10-48ab-9710-cce8e567f72d": {"__data__": {"id_": "bdcec2eb-6d10-48ab-9710-cce8e567f72d", "embedding": null, "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e8fec25473a3a8acbf31c22c0724e70656b79eb", "node_type": "4", "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "hash": "d5895b1145b8b49d715392c3e42bb6fe5359b15c53e8a66dd32e0ab37f3ecfbe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f966ec6f-bad4-48fb-bdfb-1044c2ad8c91", "node_type": "1", "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}, "hash": "e137d210389283e9b29be15fbdd845f197887e52eba577d1ff55ed171429b375", "class_name": "RelatedNodeInfo"}}, "text": "# optional, set any API keys your script may need (perhaps using python-dotenv library instead)\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n\ndocstore = SimpleDocumentStore()\n\nvec_store = ...  # your vector store instance\nllm = ...  # your LLM instance - optional, will default to OpenAI gpt-3.5-turbo\n\ncustom_ingestion_pipeline = IngestionPipeline(\n    transformations=[...],\n    vector_store=vec_store,\n    docstore=docstore,\n    cache=IngestionCache(),\n)\n\n# Setting up the custom QueryPipeline is optional!\n# You can still customize the vector store, LLM, and ingestion transformations without\n# having to customize the QueryPipeline\ncustom_query_pipeline = QueryPipeline()\ncustom_query_pipeline.add_modules(...)\ncustom_query_pipeline.add_link(...)\n\n# you can optionally specify your own custom readers to support additional file types.\nfile_extractor = {\".html\": ...}\n\nrag_cli_instance = RagCLI(\n    ingestion_pipeline=custom_ingestion_pipeline,\n    llm=llm,  # optional\n    query_pipeline=custom_query_pipeline,  # optional\n    file_extractor=file_extractor,  # optional\n)\n\nif __name__ == \"__main__\":\n    rag_cli_instance.cli()\n```\n\nFrom there, you're just a few steps away from being able to use your custom CLI script:\n\n1. Make sure to replace the python path at the top to the one your virtual environment is using _(run `$ which python` while your virtual environment is activated)_\n\n1. Let's say you saved your file at `/path/to/your/script/my_rag_cli.py`. From there, you can simply modify your shell's configuration file _(like `.bashrc` or `.zshrc`)_ with a line like `$ export PATH=\"/path/to/your/script:$PATH\"`.\n1. After that do `$ chmod +x my_rag_cli.py` to give executable permissions to the file.\n1. That's it! You can now just open a new terminal session and run `$ my_rag_cli.py -h`. You can now run the script with the same parameters but using your custom code configurations!\n   - Note: you can remove the `.py` file extension from your `my_rag_cli.py` file if you just want to run the command as `$ my_rag_cli --chat`", "mimetype": "text/plain", "start_char_idx": 7017, "end_char_idx": 9053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3ba5492-f10e-4161-9f52-419f51fed945": {"__data__": {"id_": "d3ba5492-f10e-4161-9f52-419f51fed945", "embedding": null, "metadata": {"filename": "v0_10_0_migration.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc96944a4a6324fae65221bbb5ae98f94eab2d29", "node_type": "4", "metadata": {"filename": "v0_10_0_migration.md", "author": "LlamaIndex"}, "hash": "632dd03e8275fbbaa7d13ede58ee9c884c49685b0fcbe4f56467bd6f98d9cf33", "class_name": "RelatedNodeInfo"}}, "text": "# Updating to v0.10.0\n\nWith the introduction of LlamaIndex v0.10.0, there were several changes\n\n- integrations have separate `pip install`s (See the [full registry](https://llamahub.ai/))\n- many imports changed\n- the `ServiceContext` was deprecated\n\nThankfully, we've tried to make these changes as easy as possible!\n\n## Migrating Imports\n\n### Option 1: Use temporary legacy imports\n\nSince this is such a large change, we have also provided a `legacy` import package so that existing code can migrate to v0.10.0 with minimal impact.\n\nUsing find+replace, you can update your imports from:\n\n```python\nfrom llama_index import VectorStoreIndex\nfrom llama_index.llms import Ollama\n\n...\n```\n\nto:\n\n```python\nfrom llama_index.legacy import VectorStoreIndex\nfrom llama_index.legacy.llms import Ollama\n\n...\n```\n\n### Option 2: Full migration\n\nTo help assist with migrating, `pip install llama-index` and `pip install llama-index-core` both come with a command-line tool to update existing code and notebooks.\n\n**NOTE:** The CLI tool updates files in place. Please ensure you have your data backed up to undo any changes as needed.\n\nAfter installing v0.10.0, you can upgrade your existing imports automatically:\n\n```\nllamaindex-cli upgrade-file <file_path>\n# OR\nllamaindex-cli upgrade <folder_path>\n```\n\nFor notebooks, new `pip install` statements are inserted and imports are updated.\n\nFor `.py` and `.md` files, import statements are also updated, and new requirements are printed to the terminal.\n\n## Deprecated ServiceContext\n\nIn addition to import changes, the existing `ServiceContext` has been deprecated. While it will be supported for a limited time, the preferred way of setting up the same options will be either globally in the `Settings` object or locally in the APIs that use certain modules.\n\nFor example, before you might have had:\n\n```\nfrom llama_index import ServiceContext, set_global_service_context\n\nservice_context = ServiceContext.from_defaults(\n  llm=llm, embed_model=embed_model, chunk_size=512\n)\nset_global_service_context(service_context)\n```\n\nWhich now looks like:\n\n```\nfrom llama_index.core import Settings\n\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.chunk_size = 512\n```\n\nYou can see the `ServiceContext` -> `Settings` migration guide for [more details](../module_guides/supporting_modules/service_context_migration.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c168c344-3d07-4216-879f-559c38f1dc68": {"__data__": {"id_": "c168c344-3d07-4216-879f-559c38f1dc68", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea415d37e1d324bb37943c19ed2e911f8b6575a0", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "43275637822364d2e4efe55c7acd8a70bab3dced16ff5136b6b83aaef04ebb89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e5d43b6-bd31-4fcd-9989-59c90b07994f", "node_type": "1", "metadata": {}, "hash": "8b4ef539273cdaa9d74ac30c9d9e977b846899cd063ed8ded57c0589509e2dd2", "class_name": "RelatedNodeInfo"}}, "text": "<script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n\n# Welcome to LlamaIndex \ud83e\udd99 !\n\nLlamaIndex is a framework for building context-augmented generative AI applications with [LLMs](https://en.wikipedia.org/wiki/Large_language_model).\n\n<div class=\"grid cards\" markdown>\n\n-  <span style=\"font-size: 200%\">[Introduction](#introduction)</span>\n\n    What is context augmentation? How does LlamaIndex help?\n\n-   <span style=\"font-size: 200%\">[Use cases](#use-cases)</span>\n\n    What kind of apps can you build with LlamaIndex? Who should use it?\n\n-   <span style=\"font-size: 200%\">[Getting started](#getting-started)</span>\n\n    Get started in Python or TypeScript in just 5 lines of code!\n\n-   <span style=\"font-size: 200%\">[LlamaCloud](#llamacloud)</span>\n\n    Managed services for LlamaIndex including [LlamaParse](https://docs.cloud.llamaindex.ai/llamaparse/getting_started), the world's best document parser.\n\n-   <span style=\"font-size: 200%\">[Community](#community)</span>\n\n    Get help and meet collaborators on Discord, Twitter, LinkedIn, and learn how to contribute to the project.\n\n-   <span style=\"font-size: 200%\">[Related projects](#related-projects)</span>\n\n    Check out our library of connectors, readers, and other integrations at [LlamaHub](https://llamahub.ai) as well as demos and starter apps like [create-llama](https://www.npmjs.com/package/create-llama).\n\n</div>\n\n## Introduction\n\n### What is context augmentation?\n\nLLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on **your** data. Your data may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\n\nContext augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\n\nThe most popular example of context-augmentation is [Retrieval-Augmented Generation or RAG](./getting_started/concepts.md), which combines context with LLMs at inference time.\n\n### LlamaIndex is the Data Framework for Context-Augmented LLM Apps\n\nLlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, semi-autonomous agents, and more. It just makes using them easier. We provide tools like:\n\n- **Data connectors** ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\n- **Data indexes** structure your data in intermediate representations that are easy and performant for LLMs to consume.\n- **Engines** provide natural language access to your data. For example:\n    - Query engines are powerful interfaces for question-answering (e.g. a RAG pipeline).\n    - Chat engines are conversational interfaces for multi-message, \"back and forth\" interactions with your data.\n- **Agents** are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\n- **Observability/Evaluation** integrations that enable you to rigorously experiment, evaluate, and monitor your app in a virtuous cycle.\n\n## Use cases\n\nSome popular use cases for LlamaIndex and context augmentation in general include:\n\n- [Question-Answering](./use_cases/q_and_a/index.md) (Retrieval-Augmented Generation aka RAG)\n- [Chatbots](./use_cases/chatbots.md)\n- [Document Understanding and Data Extraction](./use_cases/extraction.md)\n- [Autonomous Agents](./use_cases/agents.md) that can perform research and take actions\n- [Multi-modal applications](./use_cases/multimodal.md) that combine text, images, and other data types\n- [Fine-tuning](./use_cases/fine_tuning.md) models on data to improve performance\n\nCheck out our [use cases](./use_cases/index.md) documentation for more examples and links to tutorials.\n\n### \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 Who is LlamaIndex for?\n\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e5d43b6-bd31-4fcd-9989-59c90b07994f": {"__data__": {"id_": "3e5d43b6-bd31-4fcd-9989-59c90b07994f", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea415d37e1d324bb37943c19ed2e911f8b6575a0", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "43275637822364d2e4efe55c7acd8a70bab3dced16ff5136b6b83aaef04ebb89", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c168c344-3d07-4216-879f-559c38f1dc68", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d0c9d26c7609ad7003b4ffcb5ed5b8188539c84d80d88bcddb2452a7016f50f0", "class_name": "RelatedNodeInfo"}}, "text": "LlamaIndex provides tools for beginners, advanced users, and everyone in between.\n\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\n\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module\u2014data connectors, indices, retrievers, query engines, reranking modules\u2014to fit their needs.\n\n## Getting Started\n\nLlamaIndex is available in Python (these docs) and [Typescript](https://ts.llamaindex.ai/). If you're not sure where to start, we recommend reading [how to read these docs](./getting_started/reading.md) which will point you to the right place based on your experience level.\n\n### 30 second quickstart\n\nSet an environment variable called `OPENAI_API_KEY` with an [OpenAI API key](https://platform.openai.com/api-keys). Install the Python library:\n\n```bash\npip install llama-index\n```\n\nPut some documents in a folder called `data`, then ask questions about them with our famous 5-line starter:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Some question about the data should go here\")\nprint(response)\n```\n\nIf any part of this trips you up, don't worry! Check out our more comprehensive starter tutorials using [remote APIs like OpenAI](./getting_started/starter_example.md) or [any model that runs on your laptop](./getting_started/starter_example_local.md).\n\n## LlamaCloud\n\nIf you're an enterprise developer, check out [**LlamaCloud**](https://llamaindex.ai/enterprise). It is an end-to-end managed service for data parsing, ingestion, indexing, and retrieval, allowing you to get production-quality data for your production LLM application. It's available both hosted on our servers or as a self-hosted solution.\n\n### LlamaParse\n\nLlamaParse is our state-of-the-art document parsing solution. It's available as part of LlamaCloud and also available as a self-serve API. You can [sign up](https://cloud.llamaindex.ai/) and parse up to 1000 pages/day for free, or enter a credit card for unlimited parsing. [Learn more](https://llamaindex.ai/enterprise).\n\n## Community\n\nNeed help? Have a feature suggestion? Join the LlamaIndex community:\n\n- [Twitter](https://twitter.com/llama_index)\n- [Discord](https://discord.gg/dGcwcsnxhU)\n- [LinkedIn](https://www.linkedin.com/company/llamaindex/)\n\n### Getting the library\n\n- LlamaIndex Python\n    - [LlamaIndex Python Github](https://github.com/run-llama/llama_index)\n    - [Python Docs](https://docs.llamaindex.ai/) (what you're reading now)\n    - [LlamaIndex on PyPi](https://pypi.org/project/llama-index/)\n- LlamaIndex.TS (Typescript/Javascript package):\n    - [LlamaIndex.TS Github](https://github.com/run-llama/LlamaIndexTS)\n    - [TypeScript Docs](https://ts.llamaindex.ai/)\n    - [LlamaIndex.TS on npm](https://www.npmjs.com/package/llamaindex)\n\n### Contributing\n\nWe are open-source and always welcome contributions to the project! Check out our [contributing guide](./CONTRIBUTING.md) for full details on how to extend the core library or add an integration to a third party like an LLM, a vector store, an agent tool and more.\n\n## Related projects\n\nThere's more to the LlamaIndex universe! Check out some of our other projects:\n\n- [LlamaHub](https://llamahub.ai) | A large (and growing!) collection of custom data connectors\n- [SEC Insights](https://secinsights.ai) | A LlamaIndex-powered application for financial research\n- [create-llama](https://www.npmjs.com/package/create-llama) | A CLI tool to quickly scaffold LlamaIndex projects", "mimetype": "text/plain", "start_char_idx": 4081, "end_char_idx": 7789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afa98542-ed95-4d19-899f-02ba8b72d152": {"__data__": {"id_": "afa98542-ed95-4d19-899f-02ba8b72d152", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10936e3d4d5965b1d10371f938d4ec5412780f5f", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "fc0895932e86ce010c775b3935c2f8eaf0daa5ce556fcddcd8ad008d7a080953", "class_name": "RelatedNodeInfo"}}, "text": "# LlamaCloud\n\nLlamaCloud is a managed platform for data parsing and ingestion, allowing you to get production-quality data for your production LLM application.\n\nLlamaCloud integrates seamlessly with the open-source library, providing parsing, indexes, and retrievers to build advanced RAG, agents, and more.\n\nLlamaCloud consists of the following main components:\n\n- **LlamaParse**: our self-serve document parsing API\n- **Ingestion and Retrieval API**: Connect to 10+ data sources and sinks. Easily setup a data pipeline that can handle large volumes of data and incremental updates. Get back an endpoint with state-of-the-art indexing/retrieval to solve your complex document needs.\n- **Evaluations and Observability**: Run and track evaluations on your data and model, both in the cloud and locally.\n\n## Why LlamaCloud?\n\nBuilding production LLM applications (e.g. advanced RAG) over your data is hard, and one of the biggest issues is data quality.\n\nAs an enterprise developer, you want LLM applications with 1) high response quality and low hallucinations, 2) an easy way to improve and tune your applications, and 3) an easy way to scale to more data.\n\nLlamaCloud allows you to spend less time wrangling with your data and more time building the business logic of your LLM application.\n\n## Resources\n\n- If you're interested in LlamaCloud, [come talk to us](https://www.llamaindex.ai/contact).\n- [LlamaParse](./llama_parse.md) is available to **everyone** via self-serve (with free and premium tiers). [Sign up here](cloud.llamaindex.ai).\n- [LlamaCloud launch blog post](https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7ce5151-bfee-492e-93aa-1b706fe3d763": {"__data__": {"id_": "e7ce5151-bfee-492e-93aa-1b706fe3d763", "embedding": null, "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3df7f73a5d0b0323962df3503372039615edce86", "node_type": "4", "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}, "hash": "f475c4122408e66baaa41e16f1cee2aaac3da1ed50d71edfeb83febf51676c28", "class_name": "RelatedNodeInfo"}}, "text": "# LlamaParse\n\nLlamaParse is the world's first genAI-native document parsing platform - built with LLMs and for LLM use cases.\n\nYour LLM application performance is only as good as your data. The main goal of LlamaParse is to parse and clean your data, ensuring that it's good quality before passing to any downstream LLM use case such as advanced RAG.\n\nIt comes equipped with the following features:\n\n- State-of-the-art table extraction\n- Provide *natural language instructions* to parse the output in the exact format you want it.\n- JSON mode\n- Image extraction\n- Support for 10+ file types (.pdf, .pptx, .docx, .html, .xml, and more)\n- Foreign language support\n\nLlamaParse exists as a standalone API and also as part of the LlamaCloud platform. The API is self-serve and available to everyone. The LlamaCloud platform is in private preview ([come talk to us if interested](https://www.llamaindex.ai/contact)).\n\n## Pricing\n\nYou get 1k free pages a day. If you sign up for the paid plan, you get 7k free pages a week, and then $0.003 for each page.\n\n## Resources\n\n- [Signup here](https://cloud.llamaindex.ai/)\n- [Launch blog post](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b22d224-b093-4a7b-a833-c189f1c95769": {"__data__": {"id_": "1b22d224-b093-4a7b-a833-c189f1c95769", "embedding": null, "metadata": {"filename": "agent_runner.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a5bcc880bf595840eb43742d69c694c7caa9ea6", "node_type": "4", "metadata": {"filename": "agent_runner.md", "author": "LlamaIndex"}, "hash": "fac1a8435f5926d88b5420649fa7043397999d692e859adb1f7fadcf0af87a22", "class_name": "RelatedNodeInfo"}}, "text": "# Lower-Level Agent API\n\nWe offer a lower-level agent API that offers a host of capabilities beyond simply executing a user query end-to-end.\n\nThese capabilities let you step through and control the agent in a much more granular fashion. The end goal is that you can create **reliable** agentic software systems over your data.\n\nWe took inspiration from the [Agent Protocol](https://agentprotocol.ai/), the [OpenAI Assistants API](https://platform.openai.com/docs/assistants/overview), and of course a host of [agent](https://arxiv.org/abs/2210.03629) [research](https://arxiv.org/abs/2305.18323) [papers](https://arxiv.org/abs/2312.04511).\n\n**NOTE**: This is still under development, so interfaces may change. In fact, we'd love to get your feedback on how to make this better.\n\n## High-Level Agent Architecture\n\nOur \"agents\" are composed of `AgentRunner` objects that interact with `AgentWorkers`:\n\n- `AgentRunner`s are orchestrators that store state (including conversational memory), create and maintain tasks, run steps through each task, and offer the user-facing, high-level interface for users to interact with.\n- `AgentWorker`s **control the step-wise execution of a Task**. Given an input step, an agent worker is responsible for generating the next step. They can be initialized with parameters and act upon state passed down from the Task/TaskStep objects, but do not inherently store state themselves. The outer `AgentRunner` is responsible for calling an `AgentWorker` and collecting/aggregating the results.\n\nSome auxiliary classes:\n\n- `Task`: high-level task, takes in a user query + passes along other info like memory\n- `TaskStep`: represents a single step. Feed this in as input to `AgentWorker`, get back a `TaskStepOutput`. Completing a `Task` can involve multiple `TaskStep`.\n- `TaskStepOutput`: Output from a given step execution. Outputs whether or not a task is done.\n\n![](../../../_static/agents/agent_step_execute.png)\n\n## Benefits\n\nHere are some key benefits to using this lower-level API:\n\n- Decouple task creation from execution - control when you want to execute a given task.\n- Get greater debuggability into the execution of each step.\n- Get greater visibility: view completed steps and next steps.\n- [Coming Soon] Steerability: directly control/modify intermediate steps by injecting human feedback\n- Abandon task: give up if a task has derailed throughout the course of execution, without affecting the core agent memory.\n- [Coming Soon] Undoing a step.\n- Easier Customization: it's easy to subclass/implement new agent algorithms (incl. ReAct, OpenAI, but also plan+solve, LLMCompiler) by implementing an `AgentWorker`.\n\n## Usage Pattern\n\nYou can either use an `OpenAIAgent` or `ReActAgent`, or create your own via the `AgentRunner` and `AgentWorker`:\n\n```python\nfrom llama_index.core.agent import AgentRunner\nfrom llama_index.agent.openai import OpenAIAgentWorker\n\n# construct OpenAIAgent from tools\nopenai_step_engine = OpenAIAgentWorker.from_tools(tools, llm=llm, verbose=True)\nagent = AgentRunner(openai_step_engine)\n\n# create task\ntask = agent.create_task(\"What is (121 * 3) + 42?\")\n\n# execute step\nstep_output = agent.run_step(task)\n\n# if step_output is done, finalize response\nif step_output.is_last:\n    response = agent.finalize_response(task.task_id)\n\n# list tasks\ntask.list_tasks()\n\n# get completed steps\ntask.get_completed_steps(task.task_id)\n\nprint(str(response))\n```\n\n**NOTE**: The older legacy implementations of `OpenAIAgent` and `ReActAgent` (which did not allow for step-wise execution) are still available via:\n\n```python\nfrom llama_index.core.agent import OldOpenAIAgent, OldReActAgent\n```\n\n## Additional Module Guides\n\nCheck out our lower-level [agent module guides](./modules.md#custom-agents) for more details!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "faa587c0-1bff-4ac2-90a7-5ffe096eb1a3": {"__data__": {"id_": "faa587c0-1bff-4ac2-90a7-5ffe096eb1a3", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97d01c63bebf308fa89525d6bc23a465ff5fe10b", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "667b1951b44d60ea7888eb902cac08a253c3cb8838b912b313e061723463f7b0", "class_name": "RelatedNodeInfo"}}, "text": "# Agents\n\n## Concept\n\nData Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are capable of the following:\n\n- Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n- Calling any external service API in a structured fashion, and processing the response + storing it for later.\n\nIn that sense, agents are a step beyond our [query engines](../query_engine/index.md) in that they can not only \"read\" from a static source of data, but can dynamically ingest and modify data from a variety of different tools.\n\nBuilding a data agent requires the following core components:\n\n- A reasoning loop\n- Tool abstractions\n\nA data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.\n\n### Reasoning Loop\n\nThe reasoning loop depends on the type of agent. We have support for the following agents:\n\n- Function Calling Agents (integrates with any function calling LLM)\n- ReAct agent (works across any chat/text completion endpoint).\n- \"Advanced Agents\": [LLMCompiler](https://llamahub.ai/l/llama-packs/llama-index-packs-agents-llm-compiler?from=), [Chain-of-Abstraction](https://llamahub.ai/l/llama-packs/llama-index-packs-agents-coa?from=), [Language Agent Tree Search](https://llamahub.ai/l/llama-packs/llama-index-packs-agents-lats?from=), and more.\n\n### Tool Abstractions\n\nYou can learn more about our Tool abstractions in our [Tools section](tools/index.md).\n\n### Blog Post\n\nFor full details, please check out our detailed [blog post](https://medium.com/llamaindex-blog/data-agents-eed797d7972f).\n\n### Lower-level API: Step-Wise Execution\n\nBy default, our agents expose `query` and `chat` functions that will execute a user-query end-to-end.\n\nWe also offer a **lower-level API** allowing you to perform step-wise execution of an agent. This gives you much more control in being able to create tasks, and analyze + act upon the input/output of each step within a task.\n\nCheck out [our guide](agent_runner.md).\n\n## Usage Pattern\n\nData agents can be used in the following manner (the example uses the OpenAI Function API)\n\n```python\nfrom llama_index.agent.openai import OpenAIAgent\nfrom llama_index.llms.openai import OpenAI\n\n# import and define tools\n...\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n```\n\nSee our [usage pattern guide](usage_pattern.md) for more details.\n\n## Modules\n\nLearn more about our different agent types and use cases in our [module guides](./modules.md).\n\nWe also have a [lower-level api guide](./agent_runner.md) for agent runenrs and workers.\n\nAlso take a look at our [tools section](tools/index.md)!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87f906b3-263d-4145-b0fd-7b06da6db63e": {"__data__": {"id_": "87f906b3-263d-4145-b0fd-7b06da6db63e", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc2242219ed84c2611720b75a4dc898f8cf9137", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "a99a91428397c00a1fd87fd9864bdece18da263e4dbad583a44117b38e13adf7", "class_name": "RelatedNodeInfo"}}, "text": "# Module Guides\n\nThese guide provide an overview of how to use our agent classes.\n\nFor more detailed guides on how to use specific tools, check out our [tools module guides](tools/index.md).\n\n## Agent with OpenAI Models\n\n- [OpenAI Agent](../../../examples/agent/openai_agent.ipynb)\n- [OpenAI Agent with Query Engine Tools](../../../examples/agent/openai_agent_with_query_engine.ipynb)\n- [Retrieval Augmented Agent](../../../examples/agent/openai_agent_retrieval.ipynb)\n- [OpenAI Agent Cookbook](../../../examples/agent/openai_agent_query_cookbook.ipynb)\n- [Query Planning](../../../examples/agent/openai_agent_query_plan.ipynb)\n- [Context Retrieval Agent](../../../examples/agent/openai_agent_context_retrieval.ipynb)\n- [Recursive Retriever Agents](../../../examples/query_engine/recursive_retriever_agents.ipynb)\n- [Multi Document Agents](../../../examples/agent/multi_document_agents.ipynb)\n- [Agent Builder](../../../examples/agent/agent_builder.ipynb)\n- [Parallel Function Calling](../../../examples/agent/openai_agent_parallel_function_calling.ipynb)\n- [Agent with Planning](../../../examples/agent/structured_planner.ipynb)\n\n## [Beta] OpenAI Assistant Agent\n\n- [OpenAI Assistant](../../../examples/agent/openai_assistant_agent.ipynb)\n- [OpenAI Assistant Retrieval Benchmark](../../../examples/agent/openai_retrieval_benchmark.ipynb)\n- [Assistant Query Cookbook](../../../examples/agent/openai_assistant_query_cookbook.ipynb)\n\n## Other Function Calling Agents\n\n- [Mistral Agent](../../../examples/agent/mistral_agent.ipynb)\n\n\n## ReAct Agent\n\n- [ReAct Agent](../../../examples/agent/react_agent.ipynb)\n- [ReAct Agent with Query Engine Tools](../../../examples/agent/react_agent_with_query_engine.ipynb)\n\n## Additional Agents (available on LlamaHub)\n\n- [LLMCompiler Agent](https://llamahub.ai/l/llama-packs/llama-index-packs-agents-llm-compiler?from=) ([Cookbook](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-agents-llm-compiler/examples/llm_compiler.ipynb))\n- [Chain-of-Abstraction Agent](https://llamahub.ai/l/llama-packs/llama-index-packs-agents-coa?from=) ([Cookbook](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb))\n- [Language Agent Tree Search Agent](https://llamahub.ai/l/llama-packs/llama-index-packs-agents-lats?from=) ([Cookbook](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/lats_agent.ipynb))\n- [Instrospective Agent](https://llamahub.ai/l/agent/llama-index-agent-introspective?from=agent) ([Cookbook](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/introspective_agent_toxicity_reduction.ipynb))\n\n## Custom Agents\n\n- [Custom Agent](../../../examples/agent/custom_agent.ipynb)\n- [Query Pipeline Agent](../../../examples/agent/agent_runner/query_pipeline_agent.ipynb)\n\n## Lower-Level Agent API\n\n- [Agent Runner](../../../examples/agent/agent_runner/agent_runner.ipynb)\n- [Agent Runner RAG](../../../examples/agent/agent_runner/agent_runner_rag.ipynb)\n- [Agent with Planning](../../../examples/agent/structured_planner.ipynb)\n- [Controllable Agent Runner](../../../examples/agent/agent_runner/agent_runner_rag_controllable.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d68c4121-62b3-45af-b381-024dddb2a546": {"__data__": {"id_": "d68c4121-62b3-45af-b381-024dddb2a546", "embedding": null, "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "830b99379ad5a29329aa2d7c01205eb59cb03b07", "node_type": "4", "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "hash": "432a6fe9e6f19a1b9f20ef057c3760efacb369caf8d2b70c78cc7bea06dc0dc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c2efcbb-f5b8-43fe-b397-0b9dbcb5431e", "node_type": "1", "metadata": {}, "hash": "e906b7daee473fc9f5d37405a8740289fc08c0d987c432aa811c6cf0b6ca4a57", "class_name": "RelatedNodeInfo"}}, "text": "# Tools\n\n## Concept\n\nHaving proper tool abstractions is at the core of building [data agents](../index.md). Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a **Tool** as well as a **ToolSpec** containing a series of functions under the hood.\n\nWhen using an agent or LLM with function calling, the tool selected (and the arguments written for that tool) rely strongly on the **tool name** and **description** of the tools purpose and arguments. Spending time tuning these parameters can result in larges changes in how the LLM calls these tools.\n\nA Tool implements a very generic interface - simply define `__call__` and also return some basic metadata (name, description, function schema).\n\nWe offer a few different types of Tools:\n\n- `FunctionTool`: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema.\n- `QueryEngineTool`: A tool that wraps an existing [query engine](../../query_engine/index.md). Note: since our agent abstractions inherit from `BaseQueryEngine`, these tools can also wrap other agents.\n- Community contributed `ToolSpecs` that define one or more tools around a single service (like Gmail)\n- Utiltiy tools for wrapping other tools to handle returning large amounts of data from a tool\n\n## FunctionTool\n\nA function tool is a simple wrapper around any existing function (both sync and async are supported!).\n\n```python\nfrom llama_index.core.tools import FunctionTool\n\n\ndef get_weather(location: str) -> str:\n    \"\"\"Usfeful for getting the weather for a given location.\"\"\"\n    ...\n\n\ntool = FunctionTool.from_defaults(\n    get_weather,\n    # async_fn=aget_weather,  # optional!\n)\n\nagent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n```\n\nFor a better function definition, you can also leverage pydantic for the function arguments.\n\n```python\nfrom pydantic import Field\n\n\ndef get_weather(\n    location: str = Field(\n        description=\"A city name and state, formatted like '<name>, <state>'\"\n    ),\n) -> str:\n    \"\"\"Usfeful for getting the weather for a given location.\"\"\"\n    ...\n\n\ntool = FunctionTool.from_defaults(get_weather)\n```\n\nBy default, the tool name will be the function name, and the docstring will be the tool description. But you can also override this.\n\n```python\ntool = FunctionTool.from_defaults(get_weather, name=\"...\", description=\"...\")\n```\n\n## QueryEngineTool\n\nAny query engine can be turned into a tool, using `QueryEngineTool`:\n\n```python\nfrom llama_index.core.tools import QueryEngineTool\n\ntool = QueryEngineTool.from_defaults(\n    query_engine, name=\"...\", description=\"...\"\n)\n```\n\n## Tool Specs\n\nWe also offer a rich set of Tools and Tool Specs through [LlamaHub](https://llamahub.ai/) \ud83e\udd99.\n\nYou can think of tool specs like bundles of tools meant to be used together. Usually these cover useful tools across a single interface/service, like Gmail.\n\nTo use with an agent, you can install the specific tool spec integration:\n\n```bash\npip install llama-index-tools-google\n```\n\nAnd then use it:\n\n```python\nfrom llama_index.agent.openai import OpenAIAgent\nfrom llama_index.tools.google import GmailToolSpec\n\ntool_spec = GmailToolSpec()\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)\n```\n\nSee [LlamaHub](https://llamahub.ai) for a full list of community contributed tool specs, or check out [our guide](./llamahub_tools_guide.md) for a full overview of the Tools/Tool Specs in LlamaHub!\n\n## Utility Tools\n\nOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).\n\nTo tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.\n\nLet\u2019s walk through our two main utility tools below.\n\n### OnDemandLoaderTool\n\nThis tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c2efcbb-f5b8-43fe-b397-0b9dbcb5431e": {"__data__": {"id_": "6c2efcbb-f5b8-43fe-b397-0b9dbcb5431e", "embedding": null, "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "830b99379ad5a29329aa2d7c01205eb59cb03b07", "node_type": "4", "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "hash": "432a6fe9e6f19a1b9f20ef057c3760efacb369caf8d2b70c78cc7bea06dc0dc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d68c4121-62b3-45af-b381-024dddb2a546", "node_type": "1", "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "hash": "5ee6a6c7dee82d5238f576226766d1bad740977aa03bcea086a945aaec1bd401", "class_name": "RelatedNodeInfo"}}, "text": "The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con-demand\u201d. All three of these steps happen in a single tool call.\n\nOftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.\n\nA usage example is given below:\n\n```python\nfrom llama_index.readers.wikipedia import WikipediaReader\nfrom llama_index.core.tools.ondemand_loader_tool import OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n    reader,\n    name=\"Wikipedia Tool\",\n    description=\"A tool for loading data and querying articles from Wikipedia\",\n)\n```\n\n### LoadAndSearchToolSpec\n\nThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a `load` tool and then a `search` tool.\n\nThe `load` Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The `search` Tool execution would take in a query string as input and call the underlying index.\n\nThis is helpful for any API endpoint that will by default return large volumes of data - for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.\n\nExample usage is shown below:\n\n```python\nfrom llama_index.tools.wikipedia import WikipediaToolSpec\nfrom llama_index.core.tools.tool_spec.load_and_search import (\n    LoadAndSearchToolSpec,\n)\n\nwiki_spec = WikipediaToolSpec()\n# Get the search wikipedia tool\ntool = wiki_spec.to_tool_list()[1]\n\n# Create the Agent with load/search tools\nagent = OpenAIAgent.from_tools(\n    LoadAndSearchToolSpec.from_defaults(tool).to_tool_list(), verbose=True\n)\n```\n\n### Return Direct\n\nYou'll notice the option `return_direct` in the tool class constructor. If this is set to `True`, the response from an agent is returned directly, without being interpreted and rewritten by the agent. This can be helpful for decreasing runtime, or designing/specifying tools that will end the agent reasoning loop.\n\nFor example, say you specify a tool:\n\n```python\ntool = QueryEngineTool.from_defaults(\n    query_engine,\n    name=\"<name>\",\n    description=\"<description>\",\n    return_direct=True,\n)\n\nagent = OpenAIAgent.from_tools([tool])\n\nresponse = agent.chat(\"<question that invokes tool>\")\n```\n\nIn the above example, the query engine tool would be invoked, and the response from that tool would be directly returned as the response, and the execution loop would end.\n\nIf `return_direct=False` was used, then the agent would rewrite the response using the context of the chat history, or even make another tool call.\n\nWe have also provided an [example notebook](../../../examples/agent/return_direct_agent.ipynb) of using `return_direct`.\n\n## Debugging Tools\n\nOften, it can be useful to debug what exactly the tool definition is that is being sent to APIs.\n\nYou can get a good peek at this by using the underlying function to get the current tool schema, which is levereged in APIs like OpenAI and Anthropic.\n\n```python\nschema = tool.metadata.get_parameters_dict()\nprint(schema)\n```", "mimetype": "text/plain", "start_char_idx": 4369, "end_char_idx": 7813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5043496-0a82-472c-8a40-d5078c3b8f93": {"__data__": {"id_": "f5043496-0a82-472c-8a40-d5078c3b8f93", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f783826a679fe51abb4c0d66fdc46c9c020e279", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "bf1f6f2280595336cc4df8bf9c35040ac5610868006b47d03559f1a28b3a9aa7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d8ab80f-bb36-40b8-8ae0-8e8d774a27f7", "node_type": "1", "metadata": {}, "hash": "8c958c3825e165fcd7fc21028b217fc18cb37ecd9b8f14940d7d00617836c0fe", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern\n\n## Getting Started\n\nAn agent is initialized from a set of Tools. Here's an example of instantiating a ReAct\nagent from a set of Tools.\n\n```python\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.agent import ReActAgent\n\n\n# define sample Tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n# initialize ReAct agent\nagent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\n```\n\nAn agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.\n\nExample usage:\n\n```python\nagent.chat(\"What is 2123 * 215123\")\n```\n\nTo automatically pick the best agent depending on the LLM, you can use the `from_llm` method to generate an agent.\n\n```python\nfrom llama_index.core.agent import AgentRunner\n\nagent = AgentRunner.from_llm([multiply_tool], llm=llm, verbose=True)\n```\n\n## Defining Tools\n\n### Query Engine Tools\n\nIt is easy to wrap query engines as tools for an agent as well. Simply do the following:\n\n```python\nfrom llama_index.core.agent import ReActAgent\nfrom llama_index.core.tools import QueryEngineTool\n\n# NOTE: lyft_index and uber_index are both SimpleVectorIndex instances\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=\"Provides information about Lyft financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n        return_direct=False,\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=\"Provides information about Uber financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n        return_direct=False,\n    ),\n]\n\n# initialize ReAct agent\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n```\n\n### Use other agents as Tools\n\nA nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools\nthrough our `QueryEngineTool`.\n\n```python\nfrom llama_index.core.tools import QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sql_agent,\n        metadata=ToolMetadata(\n            name=\"sql_agent\", description=\"Agent that can execute SQL queries.\"\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=gmail_agent,\n        metadata=ToolMetadata(\n            name=\"gmail_agent\",\n            description=\"Tool that can send emails on Gmail.\",\n        ),\n    ),\n]\n\nouter_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n```\n\n## Agent With Planning\n\nBreaking down an initial task into easier-to-digest sub-tasks is a powerful pattern.\n\nLlamaIndex provides an agent planning module that does just this:\n\n```python\nfrom llama_index.agent.openai import OpenAIAgentWorker\nfrom llama_index.core.agent import (\n    StructuredPlannerAgent,\n    FunctionCallingAgentWorker,\n)\n\nworker = FunctionCallingAgentWorker.from_tools(tools, llm=llm)\nagent = StructuredPlannerAgent(worker)\n```\n\nIn general, this agent may take longer to respond compared to the basic `AgentRunner` class, but the outputs will often be more complete. Another tradeoff to consider is that planning often requires a very capable LLM (for context, `gpt-3.5-turbo` is sometimes flakey for planning, while `gpt-4-turbo` does much better.)\n\nSee more in the [complete guide](../../../examples/agent/structured_planner.ipynb)\n\n## Lower-Level API\n\nThe OpenAIAgent and ReActAgent are simple wrappers on top of an `AgentRunner` interacting with an `AgentWorker`.\n\n_All_ agents can be defined this manner.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d8ab80f-bb36-40b8-8ae0-8e8d774a27f7": {"__data__": {"id_": "1d8ab80f-bb36-40b8-8ae0-8e8d774a27f7", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f783826a679fe51abb4c0d66fdc46c9c020e279", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "bf1f6f2280595336cc4df8bf9c35040ac5610868006b47d03559f1a28b3a9aa7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5043496-0a82-472c-8a40-d5078c3b8f93", "node_type": "1", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "bf479c206dd87308e2815bcd14fad6947bb40d84b05df4efd1af33cede101b50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca2dfc41-c648-406d-9a97-de4a51b4a9bb", "node_type": "1", "metadata": {}, "hash": "6ff4af0ac1bb5b3bb4e6aa13fb8337688f34bff69c3da64075090549999fc9c4", "class_name": "RelatedNodeInfo"}}, "text": "_All_ agents can be defined this manner. For example for the OpenAIAgent:\n\n```python\nfrom llama_index.core.agent import AgentRunner\nfrom llama_index.agent.openai import OpenAIAgentWorker\n\n# construct OpenAIAgent from tools\nopenai_step_engine = OpenAIAgentWorker.from_tools(tools, llm=llm, verbose=True)\nagent = AgentRunner(openai_step_engine)\n```\n\nThis is also the preferred format for custom agents.\n\nCheck out the [lower-level agent guide](agent_runner.md) for more details.\n\n## Customizing your Agent\n\nIf you wish to define a custom agent, the easiest way to do so is to just define a stateful function and wrap it with a `FnAgentWorker`.\n\nThe `state` variable passed in and out of the function can contain anything you want it to, whether it's tools or arbitrary variables. It also contains task and output objects.\n\n```python\n## This is an example showing a trivial function that multiplies an input number by 2 each time.\n## Pass this into an agent\ndef multiply_agent_fn(state: dict) -> Tuple[Dict[str, Any], bool]:\n    \"\"\"Mock agent input function.\"\"\"\n    if \"max_count\" not in state:\n        raise ValueError(\"max_count must be specified.\")\n\n    # __output__ is a special key indicating the final output of the agent\n    # __task__ is a special key representing the Task object passed by the agent to the function.\n    # `task.input` is the input string passed\n    if \"__output__\" not in state:\n        state[\"__output__\"] = int(state[\"__task__\"].input)\n        state[\"count\"] = 0\n    else:\n        state[\"__output__\"] = state[\"__output__\"] * 2\n        state[\"count\"] += 1\n\n    is_done = state[\"count\"] >= state[\"max_count\"]\n\n    # the output of this function should be a tuple of the state variable and is_done\n    return state, is_done\n\n\nfrom llama_index.core.agent import FnAgentWorker\n\nagent = FnAgentWorker(\n    fn=multiply_agent_fn, initial_state={\"max_count\": 5}\n).as_agent()\nagent.query(\"5\")\n```\n\nCheck out our [Custom Agent Notebook Guide](../../../examples/agent/custom_agent.ipynb) for more details.\n\n\n## Advanced Concepts (for `OpenAIAgent`, in beta)\n\nYou can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and\nbeing able to perform query planning over an existing set of Tools.\n\nThese are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support\nfor our more general `ReActAgent` is something we're actively investigating.\n\nNOTE: these are largely still in beta. The abstractions may change and become more general over time.\n\n### Function Retrieval Agents\n\nIf the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.\n\nWe first build an `ObjectIndex` over an existing set of Tools.\n\n```python\n# define an \"object\" index over these tools\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.objects import ObjectIndex\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n)\n```\n\nWe then define our `OpenAIAgent`:\n\n```python\nfrom llama_index.agent.openai import OpenAIAgent\n\nagent = OpenAIAgent.from_tools(\n    tool_retriever=obj_index.as_retriever(similarity_top_k=2), verbose=True\n)\n```\n\nYou can find more details on the object index in the [full guide](../../../examples/objects/object_index.ipynb).\n\n### Context Retrieval Agents\n\nOur context-augmented OpenAI Agent will always perform retrieval before calling any tools.\n\nThis helps to provide additional context that can help the agent better pick Tools, versus\njust trying to make a decision without any context.\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.agent.openai_legacy import ContextRetrieverOpenAIAgent", "mimetype": "text/plain", "start_char_idx": 4024, "end_char_idx": 7897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca2dfc41-c648-406d-9a97-de4a51b4a9bb": {"__data__": {"id_": "ca2dfc41-c648-406d-9a97-de4a51b4a9bb", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f783826a679fe51abb4c0d66fdc46c9c020e279", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "bf1f6f2280595336cc4df8bf9c35040ac5610868006b47d03559f1a28b3a9aa7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d8ab80f-bb36-40b8-8ae0-8e8d774a27f7", "node_type": "1", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "711f6ab27c5b693c16613c48e70cb1dbbe4d3a00130646ea9f199cce1f88822a", "class_name": "RelatedNodeInfo"}}, "text": "# toy index - stores a list of Abbreviations\ntexts = [\n    \"Abbreviation: X = Revenue\",\n    \"Abbreviation: YZ = Risk Factors\",\n    \"Abbreviation: Z = Costs\",\n]\ndocs = [Document(text=t) for t in texts]\ncontext_index = VectorStoreIndex.from_documents(docs)\n\n# add context agent\ncontext_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n    query_engine_tools,\n    context_index.as_retriever(similarity_top_k=1),\n    verbose=True,\n)\nresponse = context_agent.chat(\"What is the YZ of March 2022?\")\n```\n\n### Query Planning\n\nOpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent\nwith a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query\nplan over a set of subtools.\n\n```python\n# define query plan tool\nfrom llama_index.core.tools import QueryPlanTool\nfrom llama_index.core import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n    service_context=service_context\n)\nquery_plan_tool = QueryPlanTool.from_defaults(\n    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\n    response_synthesizer=response_synthesizer,\n)\n\n# initialize agent\nagent = OpenAIAgent.from_tools(\n    [query_plan_tool],\n    max_function_calls=10,\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n    verbose=True,\n)\n\n# should output a query plan to call march, june, and september tools\nresponse = agent.query(\n    \"Analyze Uber revenue growth in March, June, and September\"\n)\n```", "mimetype": "text/plain", "start_char_idx": 7900, "end_char_idx": 9425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71c9be82-2fc3-4242-a4ad-a384caa76eca": {"__data__": {"id_": "71c9be82-2fc3-4242-a4ad-a384caa76eca", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90a29a791ea806e4f8165ea892ad3cab04964e80", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e18c57493eb83ce81d675fc08e603d99b63e8d1bf9cbe1ac722812b7734e2f39", "class_name": "RelatedNodeInfo"}}, "text": "# Chat Engine\n\n## Concept\n\nChat engine is a high-level interface for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\nThink ChatGPT, but augmented with your knowledge base.\n\nConceptually, it is a **stateful** analogy of a [Query Engine](../query_engine/index.md).\nBy keeping track of the conversation history, it can answer questions with past context in mind.\n\n!!! tip\n    If you want to ask standalone question over your data (i.e. without keeping track of conversation history), use [Query Engine](../query_engine/index.md) instead.\n\n## Usage Pattern\n\nGet started with:\n\n```python\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"Tell me a joke.\")\n```\n\nTo stream response:\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\nfor token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nMore details in the complete [usage pattern guide](./usage_pattern.md).\n\n## Modules\n\nIn our [modules section](./modules.md), you can find corresponding tutorials to see the available chat engines in action.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adb51f20-8ed7-441e-9a2e-c11fe2c94ded": {"__data__": {"id_": "adb51f20-8ed7-441e-9a2e-c11fe2c94ded", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d514bf223b78bb6aec5f04d07eb8539aa351a2b", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "abb35352a3e01584a9b0d6d7061858b5997bfe42edd2782d27aecbff11a75920", "class_name": "RelatedNodeInfo"}}, "text": "# Module Guides\n\nWe provide a few simple implementations to start, with more sophisticated modes coming soon!\n\nMore specifically, the `SimpleChatEngine` does not make use of a knowledge base,\nwhereas all others make use of a query engine over knowledge base.\n\n- [ReAct Chat Engine](../../../examples/chat_engine/chat_engine_react.ipynb)\n- [OpenAI Chat Engine](../../../examples/chat_engine/chat_engine_openai.ipynb)\n- [Condense Question Chat Engine](../../../examples/chat_engine/chat_engine_condense_question.ipynb)\n- [Context Chat Engine](../../../examples/chat_engine/chat_engine_context.ipynb)\n- [Context Plus Condense Chat Engine](../../../examples/chat_engine/chat_engine_condense_plus_context.ipynb)\n- [Simple Chat Engine](../../../examples/chat_engine/chat_engine_repl.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c6a06c3-5afb-46f0-99cf-abdf8c23675e": {"__data__": {"id_": "7c6a06c3-5afb-46f0-99cf-abdf8c23675e", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "654c85513f7e2ebf22f882ab7584dbad7e414e4d", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "915093b1ad95908ea550948b0ad1d4c0d4631c2c75c52bdbd6cbdcf57765c731", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern\n\n## Get Started\n\nBuild a chat engine from index:\n\n```python\nchat_engine = index.as_chat_engine()\n```\n\n!!! tip\n    To learn how to build an index, see [Indexing](../../indexing/index_guide.md)\n\nHave a conversation with your data:\n\n```python\nresponse = chat_engine.chat(\"Tell me a joke.\")\n```\n\nReset chat history to start a new conversation:\n\n```python\nchat_engine.reset()\n```\n\nEnter an interactive chat REPL:\n\n```python\nchat_engine.chat_repl()\n```\n\n## Configuring a Chat Engine\n\nConfiguring a chat engine is very similar to configuring a query engine.\n\n### High-Level API\n\nYou can directly build and configure a chat engine from an index in 1 line of code:\n\n```python\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n```\n\n> Note: you can access different chat engines by specifying the `chat_mode` as a kwarg. `condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`, `context` corresponds to a `ContextChatEngine`.\n\n> Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\n\n#### Available Chat Modes\n\n- `best` - Turn the query engine into a tool, for use with a `ReAct` data agent or an `OpenAI` data agent, depending on what your LLM supports. `OpenAI` data agents require `gpt-3.5-turbo` or `gpt-4` as they use the function calling API from OpenAI.\n- `condense_question` - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine.\n- `context` - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n- `condense_plus_context` - A combination of `condense_question` and `context`. Look at the chat history and re-write the user message to be a retrieval query for the index. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.\n- `simple` - A simple chat with the LLM directly, no query engine involved.\n- `react` - Same as `best`, but forces a `ReAct` data agent.\n- `openai` - Same as `best`, but forces an `OpenAI` data agent.\n\n### Low-Level Composition API\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`.\n\n> Note: You may need to look at API references or example notebooks.\n\nHere's an example where we configure the following:\n\n- configure the condense question prompt,\n- initialize the conversation with some existing history,\n- print verbose debug message.\n\n```python\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.llms import ChatMessage, MessageRole\nfrom llama_index.core.chat_engine import CondenseQuestionChatEngine\n\ncustom_prompt = PromptTemplate(\n    \"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.\n\n<Chat History>\n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\"\n)\n\n# list of `ChatMessage` objects\ncustom_chat_history = [\n    ChatMessage(\n        role=MessageRole.USER,\n        content=\"Hello assistant, we are having a insightful discussion about Paul Graham today.\",\n    ),\n    ChatMessage(role=MessageRole.ASSISTANT, content=\"Okay, sounds good.\"),\n]\n\nquery_engine = index.as_query_engine()\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine,\n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True,\n)\n```\n\n### Streaming\n\nTo enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.\n\n!!! warning\nThis somewhat inconsistent with query engine (where you pass in a `streaming=True` flag). We are working on making the behavior more consistent!\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\nfor token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nSee an [end-to-end tutorial](../../../examples/customization/streaming/chat_engine_condense_question_stream_response.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c14e1b4-3b02-43c8-8318-59b091d5891d": {"__data__": {"id_": "8c14e1b4-3b02-43c8-8318-59b091d5891d", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "baad5c9fc1e138f10e9032dd52d6701b6afafc23", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "235364560f0c77785b5602776810724849b6744646c3ea62657084ea44aa671a", "class_name": "RelatedNodeInfo"}}, "text": "# Query Engine\n\n## Concept\n\nQuery engine is a generic interface that allows you to ask question over your data.\n\nA query engine takes in a natural language query, and returns a rich response.\nIt is most often (but not always) built on one or many [indexes](../../indexing/index.md) via [retrievers](../../querying/retriever/index.md).\nYou can compose multiple query engines to achieve more advanced capability.\n\n!!! tip\n    If you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at [Chat Engine](../chat_engines/index.md)\n\n## Usage Pattern\n\nGet started with:\n\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.\")\n```\n\nTo stream response:\n\n```python\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Who is Paul Graham.\")\nstreaming_response.print_response_stream()\n```\n\nSee the full [usage pattern](./usage_pattern.md) for more details.\n\n## Modules\n\nFind all the modules in the [modules guide](./modules.md).\n\n## Supporting Modules\n\nThere are also [supporting modules](./supporting_modules.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e40b6f0b-66c6-4b35-a706-97d76da18af9": {"__data__": {"id_": "e40b6f0b-66c6-4b35-a706-97d76da18af9", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1780c4afaa8a6255c1434ec43e9acc22bfc7074", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "07eb963a17950f5eca955f83a1f5fe4c61267c340faf4c398ed67afac7d0d939", "class_name": "RelatedNodeInfo"}}, "text": "# Module Guides\n\n## Basic\n\nFirst, check out our [module guide on Indexes](../../indexing/modules.md) for in-depth guides for each index (vector index, summary index, knowledge graph index). Each index corresponds to a default query engine for that index.\n\nThen check out the rest of the sections below.\n\n- [Custom Query Engine](../../../examples/query_engine/custom_query_engine.ipynb)\n- [Retriever Query Engine](../../../examples/query_engine/CustomRetrievers.ipynb)\n\n## Structured & Semi-Structured Data\n\n- [Text-to-SQL](../../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb)\n- [JSON Query Engine](../../../examples/query_engine/json_query_engine.ipynb)\n- [Pandas Query Engine](../../../examples/query_engine/pandas_query_engine.ipynb)\n- [JSONalyze Query Engine](../../../examples/query_engine/JSONalyze_query_engine.ipynb)\n- [Knowledge Graph Query Engine](../../../examples/query_engine/knowledge_graph_query_engine.ipynb)\n- [KG RAG Retriever](../../../examples/query_engine/knowledge_graph_rag_query_engine.ipynb)\n- [Multi-Docment Auto Retrieval](../../../examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.ipynb)\n\n## Advanced\n\n- [Router Query Engine](../../../examples/query_engine/RouterQueryEngine.ipynb)\n- [Retriever Router Query Engine](../../../examples/query_engine/RetrieverRouterQueryEngine.ipynb)\n- [Joint QA Summary Engine](../../../examples/query_engine/JointQASummary.ipynb)\n- [Sub-Question Query Engine](../../../examples/query_engine/sub_question_query_engine.ipynb)\n- [MultiStep Query Engine](../../../examples/query_transformations/SimpleIndexDemo-multistep.ipynb)\n- [SQL Router](../../../examples/query_engine/SQLRouterQueryEngine.ipynb)\n- [SQL Auto-Vector](../../../examples/query_engine/SQLAutoVectorQueryEngine.ipynb)\n- [SQL Join Query Engien](../../../examples/query_engine/SQLJoinQueryEngine.ipynb)\n- [PGVector SQL Query Engien](../../../examples/query_engine/pgvector_sql_query_engine.ipynb)\n- [DuckDB Query Engine](../../../examples/index_structs/struct_indices/duckdb_sql_query.ipynb)\n- [Retry Query Engine](../../../examples/evaluation/RetryQuery.ipynb)\n- [Citation Query Engine](../../../examples/query_engine/citation_query_engine.ipynb)\n- [Recursive Table Retriever](../../../examples/query_engine/pdf_tables/recursive_retriever.ipynb)\n- [Tesla 10q Example](../../../examples/query_engine/sec_tables/tesla_10q_table.ipynb)\n- [Recursive Agents](../../../examples/query_engine/recursive_retriever_agents.ipynb)\n- [Ensemble Query Engine](../../../examples/query_engine/ensemble_query_engine.ipynb)\n\n### Advanced: Towards Multi Document Querying/Analysis\n\nThis specific subsection showcases modules that help with querying multiple documents.\n\n- [Sub-Question Query Engine](../../../examples/query_engine/sub_question_query_engine.ipynb)\n- [Recursive Agents](../../../examples/query_engine/recursive_retriever_agents.ipynb)\n- [Multi Document Agents](../../../examples/agent/multi_document_agents.ipynb)\n- [Improved Multi Document Agents](../../../examples/agent/multi_document_agents-v1.ipynb)\n\n## Experimental\n\n- [FLARE Query Engine](../../../examples/query_engine/flare_query_engine.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f78eba9b-4c52-4562-b58e-c3e5c3077c71": {"__data__": {"id_": "f78eba9b-4c52-4562-b58e-c3e5c3077c71", "embedding": null, "metadata": {"filename": "response_modes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b37a463d35c14412c1cc3c083db5849fe3546f17", "node_type": "4", "metadata": {"filename": "response_modes.md", "author": "LlamaIndex"}, "hash": "028e2b85933de9cfcdd3d4bf446e3ec511ecb9e7a95b9b8464199d22e081e10b", "class_name": "RelatedNodeInfo"}}, "text": "# Response Modes\n\nRight now, we support the following options:\n\n- `refine`: **_create and refine_** an answer by sequentially going through each retrieved text chunk.\n  This makes a separate LLM call per Node/retrieved chunk.\n\n  **Details:** the first chunk is used in a query using the\n  `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used\n  in another query with the `refine_template` prompt. And so on until all chunks have been parsed.\n\n  If a chunk is too large to fit within the window (considering the prompt size), it is split using a `TokenTextSplitter`\n  (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks\n  of the original chunks collection (and thus queried with the `refine_template` as well).\n\n  Good for more detailed answers.\n\n- `compact` (default): similar to `refine` but **_compact_** (concatenate) the chunks beforehand, resulting in less LLM calls.\n\n  **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window\n  (considering the maximum prompt size between `text_qa_template` and `refine_template`).\n  If the text is too long to fit in one prompt, it is split in as many parts as needed\n  (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).\n\n  Each text part is considered a \"chunk\" and is sent to the `refine` synthesizer.\n\n  In short, it is like `refine`, but with less LLM calls.\n\n- `tree_summarize`: Query the LLM using the `summary_template` prompt as many times as needed so that all concatenated chunks\n  have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call\n  and so on, until there's only one chunk left, and thus only one final answer.\n\n  **Details:** concatenate the chunks as much as possible to fit within the context window using the `summary_template` prompt,\n  and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against\n  `summary_template` (there is no **_refine_** query !) and get as many answers.\n\n  If there is only one answer (because there was only one chunk), then it's the final answer.\n\n  If there are more than one answer, these themselves are considered as chunks and sent recursively\n  to the `tree_summarize` process (concatenated/splitted-to-fit/queried).\n\n  Good for summarization purposes.\n\n- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick\n  summarization purposes, but may lose detail due to truncation.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,\n  without actually sending them. Then can be inspected by checking `response.source_nodes`.\n- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n  chunk while accumulating the responses into an array. Returns a concatenated string of all\n  responses. Good for when you need to run the same query separately against each text\n  chunk.\n- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n  `compact`, and run the same query against each text chunk.\n\nSee [Response Synthesizer](../../querying/response_synthesizers/index.md) to learn more.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f1df1b-6799-424c-ac1d-01fa8a9c4be4": {"__data__": {"id_": "d7f1df1b-6799-424c-ac1d-01fa8a9c4be4", "embedding": null, "metadata": {"filename": "streaming.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1ea31222f5293c15f9309b5e8fdc7b60fceb400", "node_type": "4", "metadata": {"filename": "streaming.md", "author": "LlamaIndex"}, "hash": "5dfa20bb97087273ca078cd3280eacc26df89eb70f65ce61ac5768b9376fe53b", "class_name": "RelatedNodeInfo"}}, "text": "# Streaming\n\nLlamaIndex supports streaming the response as it's being generated.\nThis allows you to start printing or processing the beginning of the response before the full response is finished.\nThis can drastically reduce the perceived latency of queries.\n\n### Setup\n\nTo enable streaming, you need to use an LLM that supports streaming.\nRight now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).\n\n> Note: if streaming is not supported by the LLM you choose a `NotImplementedError` will be raised.\n\nTo configure query engine to use streaming using the high-level API, set `streaming=True` when building a query engine.\n\n```python\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\n```\n\nIf you are using the low-level API to compose the query engine,\npass `streaming=True` when constructing the `Response Synthesizer`:\n\n```python\nfrom llama_index.core import get_response_synthesizer\n\nsynth = get_response_synthesizer(streaming=True, ...)\nquery_engine = RetrieverQueryEngine(response_synthesizer=synth, ...)\n```\n\n### Streaming Response\n\nAfter properly configuring both the LLM and the query engine,\ncalling `query` now returns a `StreamingResponse` object.\n\n```python\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\",\n)\n```\n\nThe response is returned immediately when the LLM call _starts_, without having to wait for the full completion.\n\n> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.\n\nYou can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\n\n```python\nfor text in streaming_response.response_gen:\n    # do something with text as they arrive.\n    pass\n```\n\nAlternatively, if you just want to print the text as they arrive:\n\n```\nstreaming_response.print_response_stream()\n```\n\nSee an [end-to-end example](../../../examples/customization/streaming/SimpleIndexDemo-streaming.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ceb3a0f-5076-47bb-a58c-cb4ee72e3a6a": {"__data__": {"id_": "3ceb3a0f-5076-47bb-a58c-cb4ee72e3a6a", "embedding": null, "metadata": {"filename": "supporting_modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e33362e052dbd20d533a14930fd8fb6fd8f0a1d6", "node_type": "4", "metadata": {"filename": "supporting_modules.md", "author": "LlamaIndex"}, "hash": "d4d5ccd9a7aa0cdaa95316f5cbebfeda6cc5b23ce5eee9386fb98e0c186e931a", "class_name": "RelatedNodeInfo"}}, "text": "# Supporting Modules\n\n- [Query Transformations](../../../optimizing/advanced_retrieval/query_transformations.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2603283-c8ba-4411-b89d-b1ad5ab873a7": {"__data__": {"id_": "d2603283-c8ba-4411-b89d-b1ad5ab873a7", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26d2a4ea4bb24eebf07cad04779b578d822643cc", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "f5c4a2d55383d929516c9e8561220eb24605f968418e79f157fbf916943ce128", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern\n\n## Get Started\n\nBuild a query engine from index:\n\n```python\nquery_engine = index.as_query_engine()\n```\n\n!!! tip\n    To learn how to build an index, see [Indexing](../../indexing/index.md)\n\nAsk a question over your data\n\n```python\nresponse = query_engine.query(\"Who is Paul Graham?\")\n```\n\n## Configuring a Query Engine\n\n### High-Level API\n\nYou can directly build and configure a query engine from an index in 1 line of code:\n\n```python\nquery_engine = index.as_query_engine(\n    response_mode=\"tree_summarize\",\n    verbose=True,\n)\n```\n\n> Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\n\nSee [**Response Modes**](./response_modes.md) for a full list of response modes and what they do.\n\n### Low-Level Composition API\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`.\n\n> Note: You may need to look at API references or example notebooks.\n\n```python\nfrom llama_index.core import VectorStoreIndex, get_response_synthesizer\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"tree_summarize\",\n)\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\n# query\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\n### Streaming\n\nTo enable streaming, you simply need to pass in a `streaming=True` flag\n\n```python\nquery_engine = index.as_query_engine(\n    streaming=True,\n)\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\",\n)\nstreaming_response.print_response_stream()\n```\n\n- Read the full [streaming guide](streaming.md)\n- See an [end-to-end example](../../../examples/customization/streaming/SimpleIndexDemo-streaming.ipynb)\n\n## Defining a Custom Query Engine\n\nYou can also define a custom query engine. Simply subclass the `CustomQueryEngine` class, define any attributes you'd want to have (similar to defining a Pydantic class), and implement a `custom_query` function that returns either a `Response` object or a string.\n\n```python\nfrom llama_index.core.query_engine import CustomQueryEngine\nfrom llama_index.core.retrievers import BaseRetriever\nfrom llama_index.core import get_response_synthesizer\nfrom llama_index.core.response_synthesizers import BaseSynthesizer\n\n\nclass RAGQueryEngine(CustomQueryEngine):\n    \"\"\"RAG Query Engine.\"\"\"\n\n    retriever: BaseRetriever\n    response_synthesizer: BaseSynthesizer\n\n    def custom_query(self, query_str: str):\n        nodes = self.retriever.retrieve(query_str)\n        response_obj = self.response_synthesizer.synthesize(query_str, nodes)\n        return response_obj\n```\n\nSee the [Custom Query Engine guide](../../../examples/query_engine/custom_query_engine.ipynb) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "275b0e72-b556-4a60-9f1f-04b37ad2d886": {"__data__": {"id_": "275b0e72-b556-4a60-9f1f-04b37ad2d886", "embedding": null, "metadata": {"filename": "contributing_llamadatasets.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69070b34fa86205567406e2c0eaadbb58f27ff4f", "node_type": "4", "metadata": {"filename": "contributing_llamadatasets.md", "author": "LlamaIndex"}, "hash": "8c856786587e6a32b3969cabc5b6788790de58299f95240152cb65c025b5985d", "class_name": "RelatedNodeInfo"}}, "text": "# Contributing A `LabelledRagDataset`\n\nBuilding a more robust RAG system requires a diversified evaluation suite. That is\nwhy we launched `LlamaDatasets` in [llama-hub](https://llamahub.ai). In this page,\nwe discuss how you can contribute the first kind of `LlamaDataset` made available\nin llama-hub, that is, `LabelledRagDataset`.\n\nContributing a `LabelledRagDataset` involves two high level steps. Generally speaking,\nyou must create the `LabelledRagDataset`, save it as a json and submit both this\njson file and the source text files to our [llama-datasets repository](https://github.com/run-llama/llama_datasets). Additionally, you'll have to make\na pull request, to upload required metadata of the dataset to our [llama-hub repository](https://github.com/run-llama/llama-hub).\n\nTo help make the submission process a lot smoother, we've prepared a template\nnotebook that you can follow to create a `LabelledRagDataset` from scratch (or\nconvert a similarly structured question-answering dataset into one) and perform\nother required steps to make your submission. Please refer to the \"LlamaDataset Submission Template Notebook\" linked below.\n\n## Contributing Other llama-datasets\n\nThe general process for contributing any of our other llama-datasets such as the\n`LabelledEvaluatorDataset` is the same as for the `LabelledRagDataset` previously\ndescribed. Submission templates for these other datasets are coming soon!\n\n## Submission Example\n\nRead the full [submission example Notebook](../../examples/llama_dataset/ragdataset_submission_template.ipynb).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "923f5321-9efc-46a2-839c-a29566c04bfb": {"__data__": {"id_": "923f5321-9efc-46a2-839c-a29566c04bfb", "embedding": null, "metadata": {"filename": "evaluating_evaluators_with_llamadatasets.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48d75b05a3a9951a67b52b423e6527fb07921ebb", "node_type": "4", "metadata": {"filename": "evaluating_evaluators_with_llamadatasets.md", "author": "LlamaIndex"}, "hash": "f6a14106bda866963af3afa77e35004d26b9a67d1f53c2dd8bcdb15fce1bb06b", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluating Evaluators with `LabelledEvaluatorDataset`'s\n\nThe purpose of the llama-datasets is to provide builders the means to quickly benchmark\nLLM systems or tasks. In that spirit, the `LabelledEvaluatorDataset` exists to\nfacilitate the evaluation of evaluators in a seamless and effortless manner.\n\nThis dataset consists of examples that carries mainly the following attributes:\n`query`, `answer`, `ground_truth_answer`, `reference_score`, and `reference_feedback` along with some\nother supplementary attributes. The user flow for producing evaluations with this\ndataset consists of making predictions over the dataset with a provided LLM\nevaluator, and then computing metrics that measure goodness of evaluations by\ncomputationally comparing them to the corresponding references.\n\nBelow is a snippet of code that makes use of the `EvaluatorBenchmarkerPack` to\nconveniently handle the above mentioned process flow.\n\n```python\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.core.llama_pack import download_llama_pack\nfrom llama_index.core.evaluation import CorrectnessEvaluator\nfrom llama_index.llms.gemini import Gemini\n\n# download dataset\nevaluator_dataset, _ = download_llama_dataset(\n    \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n)\n\n# define evaluator\ngemini_pro_llm = Gemini(model=\"models/gemini-pro\", temperature=0)\nevaluator = CorrectnessEvaluator(llm=gemini_pro_llm)\n\n# download EvaluatorBenchmarkerPack and define the benchmarker\nEvaluatorBenchmarkerPack = download_llama_pack(\n    \"EvaluatorBenchmarkerPack\", \"./pack\"\n)\nevaluator_benchmarker = EvaluatorBenchmarkerPack(\n    evaluator=evaluators[\"gpt-3.5\"],\n    eval_dataset=evaluator_dataset,\n    show_progress=True,\n)\n\n# produce the benchmark result\nbenchmark_df = await evaluator_benchmarker.arun(\n    batch_size=5, sleep_time_in_seconds=0.5\n)\n```\n\n## The related `LabelledPairwiseEvaluatorDataset`\n\nA related llama-dataset is the `LabelledPairwiseEvaluatorDataset`, which again\nis meant to evaluate an evaluator, but this time where the evaluator is tasked on\ncomparing a pair of LLM responses to a given query and to determine the better one\namongst them. The usage flow described above is exactly the same as it is for the\n`LabelledEvaluatorDataset`, with the exception that the LLM evaluator must be\nequipped to perform the pairwise evaluation task \u2014 i.e., should be a `PairwiseComparisonEvaluator`.\n\n## More learning materials\n\nTo see these datasets in action, be sure to checkout the notebooks listed below\nthat benchmark LLM evaluators on slightly adapted versions of the MT-Bench dataset.\n\n- [MTBench Single Grading](../../examples/evaluation/mt_bench_single_grading.ipynb)\n- [MTBench Human Judge](../../examples/evaluation/mt_bench_human_judgement.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5541ebb-c5c0-479d-9c3b-06ee8b534a26": {"__data__": {"id_": "d5541ebb-c5c0-479d-9c3b-06ee8b534a26", "embedding": null, "metadata": {"filename": "evaluating_with_llamadatasets.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cca36f8bb178dbcf9ade3e622a08968f6562ba4e", "node_type": "4", "metadata": {"filename": "evaluating_with_llamadatasets.md", "author": "LlamaIndex"}, "hash": "b93674dc8ed0a9caa7a42e2b00967e2bc74d3d03df9695f04d051d749b636bb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7817930b-212b-49bd-908a-731e14618654", "node_type": "1", "metadata": {}, "hash": "46a7dce61ddc62d8578ab5e6a097a027d4230048b4fa3c5741d5163b2d174f89", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluating With `LabelledRagDataset`'s\n\nWe have already gone through the core abstractions within the Evaluation module that\nenable various kinds of evaluation methodologies of LLM-based applications or systems, including RAG systems. Of course, to evaluate the system one needs an\nevaluation method, the system itself, as well as evaluation datasets. It is\nconsidered best practice to test the LLM application on several distinct datasets\nemanating from different sources and domains. Doing so helps to ensure the overall\nrobustness (that is, the level in which the system will work in unseen, new cases) of\nthe system.\n\nTo this end, we've included the `LabelledRagDataset` abstraction in our library. Their core purpose is to facilitate the\nevaluations of systems on various datasets, by making these easy to create, easy\nto use, and widely available.\n\nThis dataset consists of examples, where an example\ncarries a `query`, a `reference_answer`, as well as `reference_contexts`. The main\nreason for using a `LabelledRagDataset` is to test a RAG system's performance\nby first predicting a response to the given `query` and then comparing that predicted\n(or generated) response to the `reference_answer`.\n\n```python\nfrom llama_index.core.llama_dataset import (\n    LabelledRagDataset,\n    CreatedBy,\n    CreatedByType,\n    LabelledRagDataExample,\n)\n\nexample1 = LabelledRagDataExample(\n    query=\"This is some user query.\",\n    query_by=CreatedBy(type=CreatedByType.HUMAN),\n    reference_answer=\"This is a reference answer. Otherwise known as ground-truth answer.\",\n    reference_contexts=[\n        \"This is a list\",\n        \"of contexts used to\",\n        \"generate the reference_answer\",\n    ],\n    reference_by=CreatedBy(type=CreatedByType.HUMAN),\n)\n\n# a sad dataset consisting of one measely example\nrag_dataset = LabelledRagDataset(examples=[example1])\n```\n\n## Building A `LabelledRagDataset`\n\nAs we just saw at the end of the previous section, we can build a `LabelledRagDataset`\nmanually by constructing `LabelledRagDataExample`'s one by one. However, this is\na bit tedious, and while human-annoted datasets are extremely valuable, datasets\nthat are generated by strong LLMs are also very useful.\n\nAs such, the `llama_dataset` module is equipped with the `RagDatasetGenerator` that\nis able to generate a `LabelledRagDataset` over a set of source `Document`'s.\n\n```python\nfrom llama_index.core.llama_dataset.generator import RagDatasetGenerator\nfrom llama_index.llms.openai import OpenAI\nimport nest_asyncio\n\nnest_asyncio.apply()\n\ndocuments = ...  # a set of documents loaded by using for example a Reader\n\nllm = OpenAI(model=\"gpt-4\")\n\ndataset_generator = RagDatasetGenerator.from_documents(\n    documents=documents,\n    llm=llm,\n    num_questions_per_chunk=10,  # set the number of questions per nodes\n)\n\nrag_dataset = dataset_generator.generate_dataset_from_nodes()\n```\n\n## Using A `LabelledRagDataset`\n\nAs mentioned before, we want to use a `LabelledRagDataset` to evaluate a RAG\nsystem, built on the same source `Document`'s, performance with it. Doing so would\nrequire performing two steps: (1) making predictions on the dataset (i.e. generating\nresponses to the query of each individual example), and (2) evaluating the predicted\nresponse by comparing it to the reference answer. In step (2) we also evaluate the\nRAG system's retrieved contexts and compare it to the reference contexts, to gain\nan assessment on the retrieval component of the RAG system.\n\nFor convenience, we have a `LlamaPack` called the `RagEvaluatorPack` that\nstreamlines this evaluation process!\n\n```python\nfrom llama_index.core.llama_pack import download_llama_pack\n\nRagEvaluatorPack = download_llama_pack(\"RagEvaluatorPack\", \"./pack\")\n\nrag_evaluator = RagEvaluatorPack(\n    query_engine=query_engine,  # built with the same source Documents as the rag_dataset\n    rag_dataset=rag_dataset,\n)\nbenchmark_df = await rag_evaluator.run()\n```\n\nThe above `benchmark_df` contains the mean scores for evaluation measures introduced\npreviously: `Correctness`, `Relevancy`, `Faithfulness` as well as `Context Similarity`\nthat measures the semantic similarity between the reference contexts as well as the\ncontexts retrieved by the RAG system to generated the predicted response.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7817930b-212b-49bd-908a-731e14618654": {"__data__": {"id_": "7817930b-212b-49bd-908a-731e14618654", "embedding": null, "metadata": {"filename": "evaluating_with_llamadatasets.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cca36f8bb178dbcf9ade3e622a08968f6562ba4e", "node_type": "4", "metadata": {"filename": "evaluating_with_llamadatasets.md", "author": "LlamaIndex"}, "hash": "b93674dc8ed0a9caa7a42e2b00967e2bc74d3d03df9695f04d051d749b636bb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5541ebb-c5c0-479d-9c3b-06ee8b534a26", "node_type": "1", "metadata": {"filename": "evaluating_with_llamadatasets.md", "author": "LlamaIndex"}, "hash": "fe35dff4e7b3d042a3604b44444b39363bbee46706ed548641401f050717ae18", "class_name": "RelatedNodeInfo"}}, "text": "## Where To Find `LabelledRagDataset`'s\n\nYou can find all of the `LabelledRagDataset`'s in [llamahub](https://llamahub.ai). You can browse each one of these and decide\nif you do decide that you'd like to use it to benchmark your RAG pipeline, then\nyou can download the dataset as well as the source `Document`'s conveniently thru\none of two ways: the `llamaindex-cli` or through Python code using the\n`download_llama_dataset` utility function.\n\n```bash\n# using cli\nllamaindex-cli download-llamadataset PaulGrahamEssayDataset --download-dir ./data\n```\n\n```python\n# using python\nfrom llama_index.core.llama_dataset import download_llama_dataset\n\n# a LabelledRagDataset and a list of source Document's\nrag_dataset, documents = download_llama_dataset(\n    \"PaulGrahamEssayDataset\", \"./data\"\n)\n```\n\n### Contributing A `LabelledRagDataset`\n\nYou can also contribute a `LabelledRagDataset` to [llamahub](https://llamahub.ai).\nContributing a `LabelledRagDataset` involves two high level steps. Generally speaking,\nyou must create the `LabelledRagDataset`, save it as a json and submit both this\njson file and the source text files to our [llama_datasets](https://github.com/run-llama/llama_datasets) Github repository. Additionally, you'll have to make\na pull request, to upload required metadata of the dataset to our [llama_hub](https://github.com/run-llama/llama-hub) Github repository.\n\nPlease refer to the \"LlamaDataset Submission Template Notebook\" linked below.\n\n## Now, Go And Build Robust LLM Applications\n\nThis page hopefully has served as a good starting point for you to create, download\nand use `LlamaDataset`'s for building robust and performant LLM Applications. To\nlearn more, we recommend reading the notebook guides provided below.\n\n## Resources\n\n- [Labelled RAG datasets](../../examples/llama_dataset/labelled-rag-datasets.ipynb)\n- [Downloading Llama datasets](../../examples/llama_dataset/downloading_llama_datasets.ipynb)", "mimetype": "text/plain", "start_char_idx": 4253, "end_char_idx": 6186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a86b4e56-4314-47da-98a8-d2631b6addc9": {"__data__": {"id_": "a86b4e56-4314-47da-98a8-d2631b6addc9", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9fda169d2e74ed48369492a5eda601273460ff7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "adb6ae8326add882ac5b73f29b05e19939fe872b8ec5080f7720a7977939e9bd", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluating\n\n## Concept\n\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\n\nLlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality.\n\n- **Response Evaluation**: Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines?\n- **Retrieval Evaluation**: Are the retrieved sources relevant to the query?\n\nThis section describes how the evaluation components within LlamaIndex work.\n\n### Response Evaluation\n\nEvaluation of generated results can be difficult, since unlike traditional machine learning the predicted result isn't a single number, and it can be hard to define quantitative metrics for this problem.\n\nLlamaIndex offers **LLM-based** evaluation modules to measure the quality of results. This uses a \"gold\" LLM (e.g. GPT-4) to decide whether the predicted answer is correct in a variety of ways.\n\nNote that many of these current evaluation modules\ndo _not_ require ground-truth labels. Evaluation can be done with some combination of the query, context, response,\nand combine these with LLM calls.\n\nThese evaluation modules are in the following forms:\n\n- **Correctness**: Whether the generated answer matches that of the reference answer given the query (requires labels).\n- **Semantic Similarity** Whether the predicted answer is semantically similar to the reference answer (requires labels).\n- **Faithfulness**: Evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there's hallucination).\n- **Context Relevancy**: Whether retrieved context is relevant to the query.\n- **Answer Relevancy**: Whether the generated answer is relevant to the query.\n- **Guideline Adherence**: Whether the predicted answer adheres to specific guidelines.\n\n#### Question Generation\n\nIn addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\n\n### Retrieval Evaluation\n\nWe also provide modules to help evaluate retrieval independently.\n\nThe concept of retrieval evaluation is not new; given a dataset of questions and ground-truth rankings, we can evaluate retrievers using ranking metrics like mean-reciprocal rank (MRR), hit-rate, precision, and more.\n\nThe core retrieval evaluation steps revolve around the following:\n\n- **Dataset generation**: Given an unstructured text corpus, synthetically generate (question, context) pairs.\n- **Retrieval Evaluation**: Given a retriever and a set of questions, evaluate retrieved results using ranking metrics.\n\n## Integrations\n\nWe also integrate with community evaluation tools.\n\n- [UpTrain](https://github.com/uptrain-ai/uptrain)\n- [Tonic Validate](../../community/integrations/tonicvalidate.md)(Includes Web UI for visualizing results)\n- [DeepEval](https://github.com/confident-ai/deepeval)\n- [Ragas](https://github.com/explodinggradients/ragas/blob/main/docs/howtos/integrations/llamaindex.ipynb)\n- [RAGChecker](https://github.com/amazon-science/RAGChecker)\n\n## Usage Pattern\n\nFor full usage details, see the usage pattern below.\n\n- [Query Eval Usage Pattern](usage_pattern.md)\n- [Retrieval Eval Usage Pattern](usage_pattern_retrieval.md)\n\n## Modules\n\nNotebooks with usage of these components can be found in the [module guides](./modules.md).\n\n## Evaluating with `LabelledRagDataset`'s\n\nFor details on how to perform evaluation of a RAG system with various evaluation\ndatasets, called `LabelledRagDataset`'s see below:\n\n- [Evaluating](evaluating_with_llamadatasets.md)\n- [Contributing](contributing_llamadatasets.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "502c2412-a41f-40d9-95ae-6b8b2cffd446": {"__data__": {"id_": "502c2412-a41f-40d9-95ae-6b8b2cffd446", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f881cab5d82a49f2ed10efd47d7828cfe9a8ab52", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "e41a4a5ad16e84409b9612e9f8c4d96b6ce6e84688a1787c2516b8f9102a98fa", "class_name": "RelatedNodeInfo"}}, "text": "# Modules\n\nNotebooks with usage of these components can be found below.\n\n## Response Evaluation\n\n- [Faithfulness](../../examples/evaluation/faithfulness_eval.ipynb)\n- [Relevancy](../../examples/evaluation/relevancy_eval.ipynb)\n- [Answer and Context Relevancy](../../examples/evaluation/answer_and_context_relevancy.ipynb)\n- [Deepeval Integration](../../examples/evaluation/Deepeval.ipynb)\n- [Guideline Eval](../../examples/evaluation/guideline_eval.ipynb)\n- [Correctness Eval](../../examples/evaluation/correctness_eval.ipynb)\n- [Semantic Eval](../../examples/evaluation/semantic_similarity_eval.ipynb)\n- [Question Generation](../../examples/evaluation/QuestionGeneration.ipynb)\n- [Batch Eval](../../examples/evaluation/batch_eval.ipynb)\n- [Multi-Modal RAG eval](../../examples/evaluation/multi_modal/multi_modal_rag_evaluation.ipynb)\n- [Uptrain Integration](../../examples/evaluation/UpTrain.ipynb)\n- [RAGChecker Integration](../../examples/evaluation/RAGChecker.ipynb)\n\n## Retrieval Evaluation\n\n- [Retriever Eval](../../examples/evaluation/retrieval/retriever_eval.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51f15e10-99a6-462d-91dd-416c3ea15636": {"__data__": {"id_": "51f15e10-99a6-462d-91dd-416c3ea15636", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8586922541acf4113bdcfe51f28d0ebad05f715", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "f8e6543d624723bee3022a71ce98d9cedd6406211208b9c76d7c1e3a93d9d522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4b4cd52-be2a-4dbc-b30a-1aef6cd2bfb2", "node_type": "1", "metadata": {}, "hash": "b4e1c9305634ff545b422cc60aca2651503d8622b030c05b8d0469b349750976", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern (Response Evaluation)\n\n## Using `BaseEvaluator`\n\nAll of the evaluation modules in LlamaIndex implement the `BaseEvaluator` class, with two main methods:\n\n1. The `evaluate` method takes in `query`, `contexts`, `response`, and additional keyword arguments.\n\n```\n    def evaluate(\n        self,\n        query: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        response: Optional[str] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n```\n\n2. The `evaluate_response` method provide an alternative interface that takes in a llamaindex `Response` object (which contains response string and source nodes) instead of separate `contexts` and `response`.\n\n```\ndef evaluate_response(\n    self,\n    query: Optional[str] = None,\n    response: Optional[Response] = None,\n    **kwargs: Any,\n) -> EvaluationResult:\n```\n\nIt's functionally the same as `evaluate`, just simpler to use when working with llamaindex objects directly.\n\n## Using `EvaluationResult`\n\nEach evaluator outputs a `EvaluationResult` when executed:\n\n```python\neval_result = evaluator.evaluate(query=..., contexts=..., response=...)\neval_result.passing  # binary pass/fail\neval_result.score  # numerical score\neval_result.feedback  # string feedback\n```\n\nDifferent evaluators may populate a subset of the result fields.\n\n## Evaluating Response Faithfulness (i.e. Hallucination)\n\nThe `FaithfulnessEvaluator` evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there's hallucination).\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.evaluation import FaithfulnessEvaluator\n\n# create llm\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\n\n# build index\n...\n\n# define evaluator\nevaluator = FaithfulnessEvaluator(llm=llm)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\n    \"What battles took place in New York City in the American Revolution?\"\n)\neval_result = evaluator.evaluate_response(response=response)\nprint(str(eval_result.passing))\n```\n\n![](../../_static/evaluation/eval_response_context.png)\n\nYou can also choose to evaluate each source context individually:\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.evaluation import FaithfulnessEvaluator\n\n# create llm\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\n\n# build index\n...\n\n# define evaluator\nevaluator = FaithfulnessEvaluator(llm=llm)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\n    \"What battles took place in New York City in the American Revolution?\"\n)\nresponse_str = response.response\nfor source_node in response.source_nodes:\n    eval_result = evaluator.evaluate(\n        response=response_str, contexts=[source_node.get_content()]\n    )\n    print(str(eval_result.passing))\n```\n\nYou'll get back a list of results, corresponding to each source node in `response.source_nodes`.\n\n## Evaluating Query + Response Relevancy\n\nThe `RelevancyEvaluator` evaluates if the retrieved context and the answer is relevant and consistent for the given query.\n\nNote that this evaluator requires the `query` to be passed in, in addition to the `Response` object.\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.evaluation import RelevancyEvaluator\n\n# create llm\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\n\n# build index\n...\n\n# define evaluator\nevaluator = RelevancyEvaluator(llm=llm)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nquery = \"What battles took place in New York City in the American Revolution?\"\nresponse = query_engine.query(query)\neval_result = evaluator.evaluate_response(query=query, response=response)\nprint(str(eval_result))\n```\n\n![](../../_static/evaluation/eval_query_response_context.png)\n\nSimilarly, you can also evaluate on a specific source node.\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.evaluation import RelevancyEvaluator\n\n# create llm\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\n\n# build index\n...\n\n# define evaluator\nevaluator = RelevancyEvaluator(llm=llm)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nquery = \"What battles took place in New York City in the American Revolution?\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4b4cd52-be2a-4dbc-b30a-1aef6cd2bfb2": {"__data__": {"id_": "a4b4cd52-be2a-4dbc-b30a-1aef6cd2bfb2", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8586922541acf4113bdcfe51f28d0ebad05f715", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "f8e6543d624723bee3022a71ce98d9cedd6406211208b9c76d7c1e3a93d9d522", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51f15e10-99a6-462d-91dd-416c3ea15636", "node_type": "1", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "b85700874c07e6dda3d207b70bee7689e08ebcee76894d5c911ce3bb19bd53e5", "class_name": "RelatedNodeInfo"}}, "text": "response = query_engine.query(query)\nresponse_str = response.response\nfor source_node in response.source_nodes:\n    eval_result = evaluator.evaluate(\n        query=query,\n        response=response_str,\n        contexts=[source_node.get_content()],\n    )\n    print(str(eval_result.passing))\n```\n\n![](../../_static/evaluation/eval_query_sources.png)\n\n## Question Generation\n\nLlamaIndex can also generate questions to answer using your data. Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.llama_dataset.generator import RagDatasetGenerator\n\n# create llm\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\n\n# build documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# define generator, generate questions\ndataset_generator = RagDatasetGenerator.from_documents(\n    documents=documents,\n    llm=llm,\n    num_questions_per_chunk=10,  # set the number of questions per nodes\n)\n\nrag_dataset = dataset_generator.generate_questions_from_nodes()\nquestions = [e.query for e in rag_dataset.examples]\n```\n\n## Batch Evaluation\n\nWe also provide a batch evaluation runner for running a set of evaluators across many questions.\n\n```python\nfrom llama_index.core.evaluation import BatchEvalRunner\n\nrunner = BatchEvalRunner(\n    {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n    workers=8,\n)\n\neval_results = await runner.aevaluate_queries(\n    vector_index.as_query_engine(), queries=questions\n)\n```\n\n## Integrations\n\nWe also integrate with community evaluation tools.\n\n- [UpTrain](https://github.com/uptrain-ai/uptrain)\n- [DeepEval](https://github.com/confident-ai/deepeval)\n- [Ragas](https://github.com/explodinggradients/ragas/blob/main/docs/howtos/integrations/llamaindex.ipynb)\n\n### DeepEval\n\n[DeepEval](https://github.com/confident-ai/deepeval) offers 6 evaluators (including 3 RAG evaluators, for both retriever and generator evaluation) powered by its proprietary evaluation metrics. To being, install `deepeval`:\n\n```\npip install -U deepeval\n```\n\nYou can then import and use evaluators from `deepeval`. Full example:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom deepeval.integrations.llama_index import DeepEvalAnswerRelevancyEvaluator\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nrag_application = index.as_query_engine()\n\n# An example input to your RAG application\nuser_input = \"What is LlamaIndex?\"\n\n# LlamaIndex returns a response object that contains\n# both the output string and retrieved nodes\nresponse_object = rag_application.query(user_input)\n\nevaluator = DeepEvalAnswerRelevancyEvaluator()\nevaluation_result = evaluator.evaluate_response(\n    query=user_input, response=response_object\n)\nprint(evaluation_result)\n```\n\nHere is how you can import all 6 evaluators from `deepeval`:\n\n```python\nfrom deepeval.integrations.llama_index import (\n    DeepEvalAnswerRelevancyEvaluator,\n    DeepEvalFaithfulnessEvaluator,\n    DeepEvalContextualRelevancyEvaluator,\n    DeepEvalSummarizationEvaluator,\n    DeepEvalBiasEvaluator,\n    DeepEvalToxicityEvaluator,\n)\n```\n\nTo learn more on how to use `deepeval`'s evaluation metrics with LlamaIndex and take advantage of its full LLM testing suite, visit the [docs.](https://docs.confident-ai.com/docs/integrations-llamaindex)", "mimetype": "text/plain", "start_char_idx": 4422, "end_char_idx": 7931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83a5d336-db48-47fc-a5e6-bd51d457928e": {"__data__": {"id_": "83a5d336-db48-47fc-a5e6-bd51d457928e", "embedding": null, "metadata": {"filename": "usage_pattern_retrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffae42c9393a37ada2fbf9ed7d462e36ecd838e8", "node_type": "4", "metadata": {"filename": "usage_pattern_retrieval.md", "author": "LlamaIndex"}, "hash": "63f20c14e03bfd645ed53ea6f57e6a43211f1ad0cd14736dd5b2e41b1d84ccbc", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern (Retrieval)\n\n## Using `RetrieverEvaluator`\n\nThis runs evaluation over a single query + ground-truth document set given a retriever.\n\nThe standard practice is to specify a set of valid metrics with `from_metrics`.\n\n```python\nfrom llama_index.core.evaluation import RetrieverEvaluator\n\n# define retriever somewhere (e.g. from index)\n# retriever = index.as_retriever(similarity_top_k=2)\nretriever = ...\n\nretriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [\"mrr\", \"hit_rate\"], retriever=retriever\n)\n\nretriever_evaluator.evaluate(\n    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n)\n```\n\n## Building an Evaluation Dataset\n\nYou can manually curate a retrieval evaluation dataset of questions + node id's. We also offer synthetic dataset generation over an existing text corpus with our `generate_question_context_pairs` function:\n\n```python\nfrom llama_index.core.evaluation import generate_question_context_pairs\n\nqa_dataset = generate_question_context_pairs(\n    nodes, llm=llm, num_questions_per_chunk=2\n)\n```\n\nThe returned result is a `EmbeddingQAFinetuneDataset` object (containing `queries`, `relevant_docs`, and `corpus`).\n\n### Plugging it into `RetrieverEvaluator`\n\nWe offer a convenience function to run a `RetrieverEvaluator` over a dataset in batch mode.\n\n```python\neval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n```\n\nThis should run much faster than you trying to call `.evaluate` on each query separately.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae7ff0f9-b680-4325-acf3-2a70ac99752a": {"__data__": {"id_": "ae7ff0f9-b680-4325-acf3-2a70ac99752a", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be87b382526ef0e021562a7d760349640eb423ab", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "af90ca15697d4fce741b3db8095b930a6078f9318b07a10e2917605571ee5f12", "class_name": "RelatedNodeInfo"}}, "text": "# Component Guides\n\nUse the navigation on the left to explore specific module guides!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7196dcb5-366b-4075-86cd-d4c9e3f528c1": {"__data__": {"id_": "7196dcb5-366b-4075-86cd-d4c9e3f528c1", "embedding": null, "metadata": {"filename": "document_management.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97b753800ec87dcb4bdb5b0e49cd4c95d7e8b038", "node_type": "4", "metadata": {"filename": "document_management.md", "author": "LlamaIndex"}, "hash": "1705de24891ff786edc513097811039d8a643d32166f8adfed6469f7928eb27c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ba3e436-d5db-4ebb-804c-1b87b9c8b4a1", "node_type": "1", "metadata": {}, "hash": "1cb8ebc8373d7c44c3401651560c12600e4d86a3d2cb6c22f30fbcd262a9ab1f", "class_name": "RelatedNodeInfo"}}, "text": "# Document Management\n\nMost LlamaIndex index structures allow for **insertion**, **deletion**, **update**, and **refresh** operations.\n\n## Insertion\n\nYou can \"insert\" a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index.\n\nThe underlying mechanism behind insertion depends on the index structure. For instance, for the summary index, a new Document is inserted as additional node(s) in the list.\nFor the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.\n\nAn example code snippet is given below:\n\n```python\nfrom llama_index.core import SummaryIndex, Document\n\nindex = SummaryIndex([])\ntext_chunks = [\"text_chunk_1\", \"text_chunk_2\", \"text_chunk_3\"]\n\ndoc_chunks = []\nfor i, text in enumerate(text_chunks):\n    doc = Document(text=text, id_=f\"doc_id_{i}\")\n    doc_chunks.append(doc)\n\n# insert\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)\n```\n\n## Deletion\n\nYou can \"delete\" a Document from most index data structures by specifying a document_id. (**NOTE**: the tree index currently does not support deletion). All nodes corresponding to the document will be deleted.\n\n```python\nindex.delete_ref_doc(\"doc_id_0\", delete_from_docstore=True)\n```\n\n`delete_from_docstore` will default to `False` in case you are sharing nodes between indexes using the same docstore. However, these nodes will not be used when querying when this is set to `False` as they will be deleted from the `index_struct` of the index, which keeps track of which nodes can be used for querying.\n\n## Update\n\nIf a Document is already present within an index, you can \"update\" a Document with the same doc `id_` (for instance, if the information in the Document has changed).\n\n```python\n# NOTE: the document has a `doc_id` specified\ndoc_chunks[0].text = \"Brand new document text\"\nindex.update_ref_doc(\n    doc_chunks[0],\n    update_kwargs={\"delete_kwargs\": {\"delete_from_docstore\": True}},\n)\n```\n\nHere, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\n\n## Refresh\n\nIf you set the doc `id_` of each document when loading your data, you can also automatically refresh the index.\n\nThe `refresh()` function will only update documents who have the same doc `id_`, but different text contents. Any documents not present in the index at all will also be inserted.\n\n`refresh()` also returns a boolean list, indicating which documents in the input have been refreshed in the index.\n\n```python\n# modify first document, with the same doc_id\ndoc_chunks[0] = Document(text=\"Super new document text\", id_=\"doc_id_0\")\n\n# add a new document\ndoc_chunks.append(\n    Document(\n        text=\"This isn't in the index yet, but it will be soon!\",\n        id_=\"doc_id_3\",\n    )\n)\n\n# refresh the index\nrefreshed_docs = index.refresh_ref_docs(\n    doc_chunks, update_kwargs={\"delete_kwargs\": {\"delete_from_docstore\": True}}\n)\n\n# refreshed_docs[0] and refreshed_docs[-1] should be true\n```\n\nAgain, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\n\nIf you `print()` the output of `refresh()`, you would see which input documents were refreshed:\n\n```python\nprint(refreshed_docs)\n# > [True, False, False, True]\n```\n\nThis is most useful when you are reading from a directory that is constantly updating with new information.\n\nTo automatically set the doc `id_` when using the `SimpleDirectoryReader`, you can set the `filename_as_id` flag. You can learn more about [customzing Documents](../loading/documents_and_nodes/usage_documents.md).\n\n## Document Tracking\n\nAny index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ba3e436-d5db-4ebb-804c-1b87b9c8b4a1": {"__data__": {"id_": "8ba3e436-d5db-4ebb-804c-1b87b9c8b4a1", "embedding": null, "metadata": {"filename": "document_management.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97b753800ec87dcb4bdb5b0e49cd4c95d7e8b038", "node_type": "4", "metadata": {"filename": "document_management.md", "author": "LlamaIndex"}, "hash": "1705de24891ff786edc513097811039d8a643d32166f8adfed6469f7928eb27c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7196dcb5-366b-4075-86cd-d4c9e3f528c1", "node_type": "1", "metadata": {"filename": "document_management.md", "author": "LlamaIndex"}, "hash": "a323137b1bfc7ddd0734f4244a337d121c66bd83501e59dd9c264105238a48e7", "class_name": "RelatedNodeInfo"}}, "text": "all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.\n\n```python\nprint(index.ref_doc_info)\n\"\"\"\n> {'doc_id_1': RefDocInfo(node_ids=['071a66a8-3c47-49ad-84fa-7010c6277479'], metadata={}),\n   'doc_id_2': RefDocInfo(node_ids=['9563e84b-f934-41c3-acfd-22e88492c869'], metadata={}),\n   'doc_id_0': RefDocInfo(node_ids=['b53e6c2f-16f7-4024-af4c-42890e945f36'], metadata={}),\n   'doc_id_3': RefDocInfo(node_ids=['6bedb29f-15db-4c7c-9885-7490e10aa33f'], metadata={})}\n\"\"\"\n```\n\nEach entry in the output shows the ingested doc `id_`s as keys, and their associated `node_ids` of the nodes they were split into.\n\nLastly, the original `metadata` dictionary of each input document is also tracked. You can read more about the `metadata` attribute in [Customizing Documents](../loading/documents_and_nodes/usage_documents.md).", "mimetype": "text/plain", "start_char_idx": 3723, "end_char_idx": 4605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88b1db63-99cf-451a-bdd4-82497d2adb58": {"__data__": {"id_": "88b1db63-99cf-451a-bdd4-82497d2adb58", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e48eb7420437477cadae15b0255a6661888fd83", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "fce6e1a7288a4f86b5924041a3e45ea9098f718ca68a2372e94f1c4e3b11975d", "class_name": "RelatedNodeInfo"}}, "text": "# Indexing\n\n## Concept\n\nAn `Index` is a data structure that allows us to quickly retrieve relevant context for a user query.\nFor LlamaIndex, it's the core foundation for retrieval-augmented generation (RAG) use-cases.\n\nAt a high-level, `Indexes` are built from [Documents](../loading/documents_and_nodes/index.md).\nThey are used to build [Query Engines](../deploying/query_engine/index.md) and [Chat Engines](../deploying/chat_engines/index.md)\nwhich enables question & answer and chat over your data.\n\nUnder the hood, `Indexes` store data in `Node` objects (which represent chunks of the original documents), and expose a [Retriever](../querying/retriever/index.md) interface that supports additional configuration and automation.\n\nThe most common index by far is the `VectorStoreIndex`; the best place to start is the [VectorStoreIndex usage guide](vector_store_index.md).\n\nFor other indexes, check out our guide to [how each index works](index_guide.md) to help you decide which one matches your use-case.\n\n## Other Index resources\n\nSee the [modules guide](./modules.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a10a7420-9c69-4454-aa62-1eac19023ab2": {"__data__": {"id_": "a10a7420-9c69-4454-aa62-1eac19023ab2", "embedding": null, "metadata": {"filename": "index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20c7bc0cd799f988a578e8ddf58de75afe4604ee", "node_type": "4", "metadata": {"filename": "index_guide.md", "author": "LlamaIndex"}, "hash": "c9344ee88e91e6937942869c4164ba3016bf0cdb1d3e5f4f607840a28f2a8fa9", "class_name": "RelatedNodeInfo"}}, "text": "# How Each Index Works\n\nThis guide describes how each index works with diagrams.\n\nSome terminology:\n\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to\n  [specify different response modes](../deploying/query_engine/response_modes.md).\n\n## Summary Index (formerly List Index)\n\nThe summary index simply stores Nodes as a sequential chain.\n\n![](../../_static/indices/list.png)\n\n### Querying\n\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\nour Response Synthesis module.\n\n![](../../_static/indices/list_query.png)\n\nThe summary index does offer numerous ways of querying a summary index, from an embedding-based query which\nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\n\n![](../../_static/indices/list_filter_query.png)\n\n## Vector Store Index\n\nThe vector store index stores each Node and a corresponding embedding in a [Vector Store](../../community/integrations/vector_stores.md#using-a-vector-store-as-an-index).\n\n![](../../_static/indices/vector_store.png)\n\n### Querying\n\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\nthose into our Response Synthesis module.\n\n![](../../_static/indices/vector_store_query.png)\n\n## Tree Index\n\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\n\n![](../../_static/indices/tree.png)\n\n### Querying\n\nQuerying a tree index involves traversing from root nodes down\nto leaf nodes. By default, (`child_branch_factor=1`), a query\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\nchooses two child nodes per level.\n\n![](../../_static/indices/tree_query.png)\n\n## Keyword Table Index\n\nThe keyword table index extracts keywords from each Node and builds a mapping from\neach keyword to the corresponding Nodes of that keyword.\n\n![](../../_static/indices/keyword.png)\n\n### Querying\n\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our\nResponse Synthesis module.\n\n![](../../_static/indices/keyword_query.png)\n\n## Property Graph Index\n\nThe Property Graph Index works by first building a knowledge graph containing labelled nodes and relations. The construction of this graph is extremely customizable, ranging from letting the LLM extract whatever it wants, to extracting using a strict schema, to even implementing your own extraction modules.\n\nOptionally, nodes can also be embedded for retrieval later.\n\nYou can also skip creation, and connect to an existing knowledge graph using an integration like Neo4j.\n\n### Querying\n\nQuerying a Property Graph Index is also highly flexible. Retrieval works by using several sub-retrievers and combining results. By default, keyword + synoymn expanasion is used, as well as vector retrieval (if your graph was embedded), to retrieve relevant triples.\n\nYou can also chose to include the source text in addition to the retrieved triples (unavailble for graphs created outside of LlamaIndex).\n\nSee more in the [full guide for Property Graphs](./lpg_index_guide.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "109027ef-a4c6-427b-985a-d6eb2873aff7": {"__data__": {"id_": "109027ef-a4c6-427b-985a-d6eb2873aff7", "embedding": null, "metadata": {"filename": "llama_cloud_index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "966b2643797e3d9a354326d911abf292c21f2eb5", "node_type": "4", "metadata": {"filename": "llama_cloud_index.md", "author": "LlamaIndex"}, "hash": "90f8abb75be9fea8c1409cbe3b6b1520c1211a33401458ad72c8c7aa33ba099d", "class_name": "RelatedNodeInfo"}}, "text": "# LlamaCloudIndex + LlamaCloudRetriever\n\nLlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.\n\nCurrently, LlamaCloud supports\n\n- Managed Ingestion API, handling parsing and document management\n- Managed Retrieval API, configuring optimal retrieval for your RAG system\n\n## Access\n\nWe are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you\u2019re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come [talk to us.](https://www.llamaindex.ai/contact)\n\nIf you have access to LlamaCloud, you can visit [LlamaCloud](https://cloud.llamaindex.ai) to sign in and get an API key.\n\n## Setup\n\nFirst, make sure you have the latest LlamaIndex version installed.\n\n**NOTE:** If you are upgrading from v0.9.X, we recommend following our [migration guide](../../getting_started/v0_10_0_migration.md), as well as uninstalling your previous version first.\n\n```\npip uninstall llama-index  # run this if upgrading from v0.9.x or older\npip install -U llama-index --upgrade --no-cache-dir --force-reinstall\n```\n\nThe `llama-index-indices-managed-llama-cloud` package is included with the above install, but you can also install directly\n\n```\npip install -U llama-index-indices-managed-llama-cloud\n```\n\n## Usage\n\nYou can create an index on LlamaCloud using the following code:\n\n```python\nimport os\n\nos.environ[\n    \"LLAMA_CLOUD_API_KEY\"\n] = \"llx-...\"  # can provide API-key in env or in the constructor later on\n\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n\n# create a new index\nindex = LlamaCloudIndex.from_documents(\n    documents,\n    \"my_first_index\",\n    project_name=\"default\",\n    api_key=\"llx-...\",\n    verbose=True,\n)\n\n# connect to an existing index\nindex = LlamaCloudIndex(\"my_first_index\", project_name=\"default\")\n```\n\nYou can also configure a retriever for managed retrieval:\n\n```python\n# from the existing index\nindex.as_retriever()\n\n# from scratch\nfrom llama_index.indices.managed.llama_cloud import LlamaCloudRetriever\n\nretriever = LlamaCloudRetriever(\"my_first_index\", project_name=\"default\")\n```\n\nAnd of course, you can use other index shortcuts to get use out of your new managed index:\n\n```python\nquery_engine = index.as_query_engine(llm=llm)\n\nchat_engine = index.as_chat_engine(llm=llm)\n```\n\n## Retriever Settings\n\nA full list of retriever settings/kwargs is below:\n\n- `dense_similarity_top_k`: Optional[int] -- If greater than 0, retrieve `k` nodes using dense retrieval\n- `sparse_similarity_top_k`: Optional[int] -- If greater than 0, retrieve `k` nodes using sparse retrieval\n- `enable_reranking`: Optional[bool] -- Whether to enable reranking or not. Sacrifices some speed for accuracy\n- `rerank_top_n`: Optional[int] -- The number of nodes to return after reranking initial retrieval results\n- `alpha` Optional[float] -- The weighting between dense and sparse retrieval. 1 = Full dense retrieval, 0 = Full sparse retrieval.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01a5c1a0-9148-4228-97b0-15885528aa9f": {"__data__": {"id_": "01a5c1a0-9148-4228-97b0-15885528aa9f", "embedding": null, "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885", "node_type": "4", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "c181f908ef56a12681f17837e576cd1dde6ae53095232aaaa0491692d61527c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99644ce3-417d-41a3-8512-fe058705dbfc", "node_type": "1", "metadata": {}, "hash": "bdeaf2020831961ffebe0e4da431dcf8377c4881c92faa09a45ee4f293bff2ba", "class_name": "RelatedNodeInfo"}}, "text": "# Using a Property Graph Index\n\nA property graph is a knowledge collection of labeled nodes (i.e. entity categories, text labels, etc.) with properties (i.e. metadata), linked together by relationships into structured paths.\n\nIn LlamaIndex, the `PropertyGraphIndex` provides key orchestration around\n\n- constructing a graph\n- querying a graph\n\n## Usage\n\nBasic usage can be found by simply importing the class and using it:\n\n```python\nfrom llama_index.core import PropertyGraphIndex\n\n# create\nindex = PropertyGraphIndex.from_documents(\n    documents,\n)\n\n# use\nretriever = index.as_retriever(\n    include_text=True,  # include source chunk with matching paths\n    similarity_top_k=2,  # top k for vector kg node retrieval\n)\nnodes = retriever.retrieve(\"Test\")\n\nquery_engine = index.as_query_engine(\n    include_text=True,  # include source chunk with matching paths\n    similarity_top_k=2,  # top k for vector kg node retrieval\n)\nresponse = query_engine.query(\"Test\")\n\n# save and load\nindex.storage_context.persist(persist_dir=\"./storage\")\n\nfrom llama_index.core import StorageContext, load_index_from_storage\n\nindex = load_index_from_storage(\n    StorageContext.from_defaults(persist_dir=\"./storage\")\n)\n\n# loading from existing graph store (and optional vector store)\n# load from existing graph/vector store\nindex = PropertyGraphIndex.from_existing(\n    property_graph_store=graph_store, vector_store=vector_store, ...\n)\n```\n\n### Construction\n\nProperty graph construction in LlamaIndex works by performing a series of `kg_extractors` on each chunk, and attaching entities and relations as metadata to each llama-index node. You can use as many as you like here, and they will all get applied.\n\nIf you've used transformations or metadata extractors with the [ingestion pipeline](../loading/ingestion_pipeline/index.md), then this will be very familiar (and these `kg_extractors` are compatible with the ingestion pipeline)!\n\nExtractors are set using the appropriate kwarg:\n\n```python\nindex = PropertyGraphIndex.from_documents(\n    documents,\n    kg_extractors=[extractor1, extractor2, ...],\n)\n\n# insert additional documents / nodes\nindex.insert(document)\nindex.insert_nodes(nodes)\n```\n\nIf not provided, the defaults are `SimpleLLMPathExtractor` and `ImplicitPathExtractor`.\n\nAll `kg_extractors` are detailed below.\n\n#### (default) `SimpleLLMPathExtractor`\n\nExtract short statements using an LLM to prompt and parse single-hop paths in the format (`entity1`, `relation`, `entity2`)\n\n```python\nfrom llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n\nkg_extractor = SimpleLLMPathExtractor(\n    llm=llm,\n    max_paths_per_chunk=10,\n    num_workers=4,\n    show_progress=False,\n)\n```\n\nIf you want, you can also customize the prompt and the function used to parse the paths.\n\nHere's a simple (but naive) example:\n\n```python\nprompt = (\n    \"Some text is provided below. Given the text, extract up to \"\n    \"{max_paths_per_chunk} \"\n    \"knowledge triples in the form of `subject,predicate,object` on each line. Avoid stopwords.\\n\"\n)\n\n\ndef parse_fn(response_str: str) -> List[Tuple[str, str, str]]:\n    lines = response_str.split(\"\\n\")\n    triples = [line.split(\",\") for line in lines]\n    return triples\n\n\nkg_extractor = SimpleLLMPathExtractor(\n    llm=llm,\n    extract_prompt=prompt,\n    parse_fn=parse_fn,\n)\n```\n\n#### (default) `ImplicitPathExtractor`\n\nExtract paths using the `node.relationships` attribute on each llama-index node object.\n\nThis extractor does not need an LLM or embedding model to run, since it's merely parsing properties that already exist on llama-index node objects.\n\n```python\nfrom llama_index.core.indices.property_graph import ImplicitPathExtractor\n\nkg_extractor = ImplicitPathExtractor()\n```\n\n### `DynamicLLMPathExtractor`\n\nWill extract paths (including entity types!) according to optional list of allowed entity types and relation types. If none are provided, then the LLM will assign types as it sees fit. If they are provided, it will help guide the LLM, but will not enforce exactly those types.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99644ce3-417d-41a3-8512-fe058705dbfc": {"__data__": {"id_": "99644ce3-417d-41a3-8512-fe058705dbfc", "embedding": null, "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885", "node_type": "4", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "c181f908ef56a12681f17837e576cd1dde6ae53095232aaaa0491692d61527c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01a5c1a0-9148-4228-97b0-15885528aa9f", "node_type": "1", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "23b4fce508f2a1f711add0f4d8e062dd2341823de2a26138dc241f01eb96206e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d555cd51-f1f7-4fc0-b79e-b5f1b4fabc0e", "node_type": "1", "metadata": {}, "hash": "f9c0bdd24548a0d704b65ecb33c3f3100fff0221a0a77217bc26d0492fef64d7", "class_name": "RelatedNodeInfo"}}, "text": "If they are provided, it will help guide the LLM, but will not enforce exactly those types.\n\n```python\nfrom llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n\nkg_extractor = DynamicLLMPathExtractor(\n    llm=llm,\n    max_triplets_per_chunk=20,\n    num_workers=4,\n    allowed_entity_types=[\"POLITICIAN\", \"POLITICAL_PARTY\"],\n    allowed_relation_types=[\"PRESIDENT_OF\", \"MEMBER_OF\"],\n)\n```\n\n#### `SchemaLLMPathExtractor`\n\nExtract paths following a strict schema of allowed entities, relationships, and which entities can be connected to which relationships.\n\nUsing pydantic, structured outputs from LLMs, and some clever validation, we can dynamically specify a schema and verify the extractions per-path.\n\n```python\nfrom typing import Literal\nfrom llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n\n# recommended uppercase, underscore separated\nentities = Literal[\"PERSON\", \"PLACE\", \"THING\"]\nrelations = Literal[\"PART_OF\", \"HAS\", \"IS_A\"]\nschema = {\n    \"PERSON\": [\"PART_OF\", \"HAS\", \"IS_A\"],\n    \"PLACE\": [\"PART_OF\", \"HAS\"],\n    \"THING\": [\"IS_A\"],\n}\n\nkg_extractor = SchemaLLMPathExtractor(\n    llm=llm,\n    possible_entities=entities,\n    possible_relations=relations,\n    kg_validation_schema=schema,\n    strict=True,  # if false, will allow triples outside of the schema\n    num_workers=4,\n    max_paths_per_chunk=10,\n    show_progres=False,\n)\n```\n\nThis extractor is extremely customizable, and has options to customize\n- various aspects of the schema (as seen above)\n- the `extract_prompt`\n- `strict=False` vs. `strict=True`, to allow triples outside of the schema or not\n- passing in your own custom `kg_schema_cls` if you are a pydantic pro and wanted to create you own pydantic class with custom validation.\n\n### Retrieval and Querying\n\nLabeled property graphs can be queried in several ways to retrieve nodes and paths. And in LlamaIndex, we can combine several node retrieval methods at once!\n\n```python\n# create a retriever\nretriever = index.as_retriever(sub_retrievers=[retriever1, retriever2, ...])\n\n# create a query engine\nquery_engine = index.as_query_engine(\n    sub_retrievers=[retriever1, retriever2, ...]\n)\n```\n\nIf no sub-retrievers are provided, the defaults are\n`LLMSynonymRetriever` and `VectorContextRetriever` (if embeddings are enabled).\n\nAll retrievers currently include:\n- `LLMSynonymRetriever` - retrieve based on LLM generated keywords/synonyms\n- `VectorContextRetriever` - retrieve based on embedded graph nodes\n- `TextToCypherRetriever` - ask the LLM to generate cypher based on the schema of the property graph\n- `CypherTemplateRetriever` - use a cypher template with params inferred by the LLM\n- `CustomPGRetriever` - easy to subclass and implement custom retrieval logic\n\nGenerally, you would define one or more of these sub-retrievers and pass them to the `PGRetriever`:\n\n```python\nfrom llama_index.core.indices.property_graph import (\n    PGRetriever,\n    VectorContextRetriever,\n    LLMSynonymRetriever,\n)\n\nsub_retrievers = [\n    VectorContextRetriever(index.property_graph_store, ...),\n    LLMSynonymRetriever(index.property_graph_store, ...),\n]\n\nretriever = PGRetriever(sub_retrievers=sub_retrievers)\n\nnodes = retriever.retrieve(\"<query>\")\n```\n\nRead on below for more details on all retrievers.\n\n#### (default) `LLMSynonymRetriever`\n\nThe `LLMSynonymRetriever` takes the query, and tries to generate keywords and synonyms to retrieve nodes (and therefore the paths connected to those nodes).\n\nExplicitly declaring the retriever allows you to customize several options. Here are the defaults:\n\n```python\nfrom llama_index.core.indices.property_graph import LLMSynonymRetriever\n\nprompt = (\n    \"Given some initial query, generate synonyms or related keywords up to {max_keywords} in total, \"\n    \"considering possible cases of capitalization, pluralization, common expressions, etc.\\n\"\n    \"Provide all synonyms/keywords separated by '^' symbols: 'keyword1^keyword2^...'\\n\"\n    \"Note, result should be in one-line, separated by '^' symbols.\"\n    \"----\\n\"\n    \"QUERY: {query_str}\\n\"\n    \"----\\n\"\n    \"KEYWORDS: \"\n)", "mimetype": "text/plain", "start_char_idx": 3946, "end_char_idx": 8029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d555cd51-f1f7-4fc0-b79e-b5f1b4fabc0e": {"__data__": {"id_": "d555cd51-f1f7-4fc0-b79e-b5f1b4fabc0e", "embedding": null, "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885", "node_type": "4", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "c181f908ef56a12681f17837e576cd1dde6ae53095232aaaa0491692d61527c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99644ce3-417d-41a3-8512-fe058705dbfc", "node_type": "1", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "97d9388c33e13906792d947b318d7e22261200f8b0d5309134bff343be71e52b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1c0ae13-4b34-4b35-8aa6-cd9007b59c22", "node_type": "1", "metadata": {}, "hash": "4142d400da918530f3ec1cf4cbad075270b94fc6a87835c917112ea4e58575b8", "class_name": "RelatedNodeInfo"}}, "text": "\"----\\n\"\n    \"QUERY: {query_str}\\n\"\n    \"----\\n\"\n    \"KEYWORDS: \"\n)\n\n\ndef parse_fn(self, output: str) -> list[str]:\n    matches = output.strip().split(\"^\")\n\n    # capitalize to normalize with ingestion\n    return [x.strip().capitalize() for x in matches if x.strip()]\n\n\nsynonym_retriever = LLMSynonymRetriever(\n    index.property_graph_store,\n    llm=llm,\n    # include source chunk text with retrieved paths\n    include_text=False,\n    synonym_prompt=prompt,\n    output_parsing_fn=parse_fn,\n    max_keywords=10,\n    # the depth of relations to follow after node retrieval\n    path_depth=1,\n)\n\nretriever = index.as_retriever(sub_retrievers=[synonym_retriever])\n```\n\n#### (default, if supported) `VectorContextRetriever`\n\nThe `VectorContextRetriever` retrieves nodes based on their vector similarity, and then fetches the paths connected to those nodes.\n\nIf your graph store supports vectors, then you only need to manage that graph store for storage. Otherwise, you will need to provide a vector store in addition to the graph store (by default, uses the in-memory `SimpleVectorStore`).\n\n```python\nfrom llama_index.core.indices.property_graph import VectorContextRetriever\n\nvector_retriever = VectorContextRetriever(\n    index.property_graph_store,\n    # only needed when the graph store doesn't support vector queries\n    # vector_store=index.vector_store,\n    embed_model=embed_model,\n    # include source chunk text with retrieved paths\n    include_text=False,\n    # the number of nodes to fetch\n    similarity_top_k=2,\n    # the depth of relations to follow after node retrieval\n    path_depth=1,\n    # can provide any other kwargs for the VectorStoreQuery class\n    ...,\n)\n\nretriever = index.as_retriever(sub_retrievers=[vector_retriever])\n```\n\n#### `TextToCypherRetriever`\n\nThe `TextToCypherRetriever` uses a graph store schema, your query, and a prompt template for text-to-cypher in order to generate and execute a cypher query.\n\n**NOTE:** Since the `SimplePropertyGraphStore` is not actually a graph database, it does not support cypher queries.\n\nYou can inspect the schema by using `index.property_graph_store.get_schema_str()`.\n\n```python\nfrom llama_index.core.indices.property_graph import TextToCypherRetriever\n\nDEFAULT_RESPONSE_TEMPLATE = (\n    \"Generated Cypher query:\\n{query}\\n\\n\" \"Cypher Response:\\n{response}\"\n)\nDEFAULT_ALLOWED_FIELDS = [\"text\", \"label\", \"type\"]\n\nDEFAULT_TEXT_TO_CYPHER_TEMPLATE = (\n    index.property_graph_store.text_to_cypher_template,\n)\n\n\ncypher_retriever = TextToCypherRetriever(\n    index.property_graph_store,\n    # customize the LLM, defaults to Settings.llm\n    llm=llm,\n    # customize the text-to-cypher template.\n    # Requires `schema` and `question` template args\n    text_to_cypher_template=DEFAULT_TEXT_TO_CYPHER_TEMPLATE,\n    # customize how the cypher result is inserted into\n    # a text node. Requires `query` and `response` template args\n    response_template=DEFAULT_RESPONSE_TEMPLATE,\n    # an optional callable that can clean/verify generated cypher\n    cypher_validator=None,\n    # allowed fields in the resulting\n    allowed_output_field=DEFAULT_ALLOWED_FIELDS,\n)\n```\n\n**NOTE:** Executing arbitrary cypher has its risks. Ensure you take the needed measures (read-only roles, sandboxed env, etc.) to ensure safe usage in a production environment.\n\n#### `CypherTemplateRetriever`\n\nThis is a more constrained version of the `TextToCypherRetriever`. Rather than letting the LLM have free-range of generating any cypher statement, we can instead provide a cypher template and have the LLM fill in the blanks.\n\nTo illustrate how this works, here is a small example:\n\n```python\n# NOTE: current v1 is needed\nfrom pydantic.v1 import BaseModel, Field\nfrom llama_index.core.indices.property_graph import CypherTemplateRetriever\n\n# write a query with template params\ncypher_query = \"\"\"\nMATCH (c:Chunk)-[:MENTIONS]->(o)\nWHERE o.name IN $names\nRETURN c.text, o.name, o.label;\n\"\"\"", "mimetype": "text/plain", "start_char_idx": 7962, "end_char_idx": 11889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1c0ae13-4b34-4b35-8aa6-cd9007b59c22": {"__data__": {"id_": "a1c0ae13-4b34-4b35-8aa6-cd9007b59c22", "embedding": null, "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885", "node_type": "4", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "c181f908ef56a12681f17837e576cd1dde6ae53095232aaaa0491692d61527c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d555cd51-f1f7-4fc0-b79e-b5f1b4fabc0e", "node_type": "1", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "94fb6e9ac1ad5d3d566fe920bb17d1a5f056e8c07f7a7fc40107ec198165bb4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "256e9a35-9af5-4466-87b7-18b9f7a13201", "node_type": "1", "metadata": {}, "hash": "1a00a60aa2bc701481adee01acf3a3c873fb7ae5f805974b52e83be958912f4b", "class_name": "RelatedNodeInfo"}}, "text": "# create a pydantic class to represent the params for our query\n# the class fields are directly used as params for running the cypher query\nclass TemplateParams(BaseModel):\n    \"\"\"Template params for a cypher query.\"\"\"\n\n    names: list[str] = Field(\n        description=\"A list of entity names or keywords to use for lookup in a knowledge graph.\"\n    )\n\n\ntemplate_retriever = CypherTemplateRetriever(\n    index.property_graph_store, TemplateParams, cypher_query\n)\n```\n\n## Storage\n\nCurrently, supported graph stores for property graphs include:\n\n|                     | In-Memory | Native Embedding Support | Async | Server or disk based? |\n|---------------------|-----------|--------------------------|-------|-----------------------|\n| SimplePropertyGraphStore | \u2705         | \u274c                        | \u274c     | Disk                  |\n| Neo4jPropertyGraphStore  | \u274c         | \u2705                        | \u274c     | Server                |\n| NebulaPropertyGraphStore | \u274c         | \u274c                        | \u274c     | Server                |\n| TiDBPropertyGraphStore   | \u274c         | \u2705                        | \u274c     | Server                |\n\n### Saving to/from disk\n\nThe default property graph store, `SimplePropertyGraphStore`, stores everything in memory and persists and loads from disk.\n\nHere's an example of saving/loading an index with the default graph store:\n\n```python\nfrom llama_index.core import StorageContext, load_index_from_storage\nfrom llama_index.core.indices import PropertyGraphIndex\n\n# create\nindex = PropertyGraphIndex.from_documents(documents)\n\n# save\nindex.storage_context.persist(\"./storage\")\n\n# load\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)\n```\n\n### Saving and Loading with Integrations\n\nIntegrations typically save automatically. Some graph stores will support vectors, others might not. You can always combine a graph store with an external vector db as well.\n\nThis example shows how you might save/load a property graph index using Neo4j and Qdrant.\n\n**Note:** If qdrant wasn't passed in, neo4j would store and use the embeddings on its own. This example illustrates the flexibility beyond that.\n\n`pip install llama-index-graph-stores-neo4j llama-index-vector-stores-qdrant`\n\n```python\nfrom llama_index.core import StorageContext, load_index_from_storage\nfrom llama_index.core.indices import PropertyGraphIndex\nfrom llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient, AsyncQdrantClient\n\nvector_store = QdrantVectorStore(\n    \"graph_collection\",\n    client=QdrantClient(...),\n    aclient=AsyncQdrantClient(...),\n)\n\ngraph_store = Neo4jPropertyGraphStore(\n    username=\"neo4j\",\n    password=\"<password>\",\n    url=\"bolt://localhost:7687\",\n)\n\n# creates an index\nindex = PropertyGraphIndex.from_documents(\n    documents,\n    property_graph_store=graph_store,\n    # optional, neo4j also supports vectors directly\n    vector_store=vector_store,\n    embed_kg_nodes=True,\n)\n\n# load from existing graph/vector store\nindex = PropertyGraphIndex.from_existing(\n    property_graph_store=graph_store,\n    # optional, neo4j also supports vectors directly\n    vector_store=vector_store,\n    embed_kg_nodes=True,\n)\n```\n\n### Using the Property Graph Store Directly\n\nThe base storage class for property graphs is the `PropertyGraphStore`. These property graph stores are constructured using different types of `LabeledNode` objects, and connected using `Relation` objects.\n\nWe can create these ourselves, and also insert ourselves!\n\n```python\nfrom llama_index.core.graph_stores import (\n    SimplePropertyGraphStore,\n    EntityNode,\n    Relation,\n)\nfrom llama_index.core.schema import TextNode\n\ngraph_store = SimplePropertyGraphStore()\n\nentities = [\n    EntityNode(name=\"llama\", label=\"ANIMAL\", properties={\"key\": \"val\"}),\n    EntityNode(name=\"index\", label=\"THING\", properties={\"key\": \"val\"}),\n]\n\nrelations = [\n    Relation(\n        label=\"HAS\",\n        source_id=entities[0].id,\n        target_id=entities[1].id,\n        properties={},\n    )\n]\n\ngraph_store.upsert_nodes(entities)\ngraph_store.upsert_relations(relations)\n\n# optionally, we can also insert text chunks\nsource_chunk = TextNode(id_=\"source\", text=\"My llama has an index.\")", "mimetype": "text/plain", "start_char_idx": 11892, "end_char_idx": 16222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "256e9a35-9af5-4466-87b7-18b9f7a13201": {"__data__": {"id_": "256e9a35-9af5-4466-87b7-18b9f7a13201", "embedding": null, "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885", "node_type": "4", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "c181f908ef56a12681f17837e576cd1dde6ae53095232aaaa0491692d61527c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1c0ae13-4b34-4b35-8aa6-cd9007b59c22", "node_type": "1", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "937eb1cea863950544a61db371c7d7e3c134ef164a0bdb0b08df2d3a7af37d34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42cfa1ee-ce02-450e-bdd0-453468e0c07d", "node_type": "1", "metadata": {}, "hash": "46447a6121391da750d2e1ee9240f3f5b20d3169258826dbf6d5916a3d21286f", "class_name": "RelatedNodeInfo"}}, "text": "# create relation for each of our entities\nsource_relations = [\n    Relation(\n        label=\"HAS_SOURCE\",\n        source_id=entities[0].id,\n        target_id=\"source\",\n    ),\n    Relation(\n        label=\"HAS_SOURCE\",\n        source_id=entities[1].id,\n        target_id=\"source\",\n    ),\n]\ngraph_store.upsert_llama_nodes([source_chunk])\ngraph_store.upsert_relations(source_relations)\n```\n\nOther helpful methods on the graph store include:\n- `graph_store.get(ids=[])` - gets nodes based on ids\n- `graph_store.get(properties={\"key\": \"val\"})` - gets nodes based on matching properties\n- `graph_store.get_rel_map([entity_node], depth=2)` - gets triples up to a certain depth\n- `graph_store.get_llama_nodes(['id1'])` - gets the original text nodes\n- `graph_store.delete(ids=['id1'])` - delete based on ids\n- `graph_store.delete(properties={\"key\": \"val\"})` - delete based on properties\n- `graph_store.structured_query(\"<cypher query>\")` - runs a cypher query (assuming the graph store supports it)\n\nIn addition `a` versions exist for all of these for async support (i.e. `aget`, `adelete`, etc.).\n\n## Advanced Customization\n\nAs with all components in LlamaIndex, you can sub-class modules and customize things to work exactly as you need, or try out new ideas and research new modules!\n\n### Sub-Classing Extractors\n\nGraph extractors in LlamaIndex subclass the `TransformComponent` class. If you've worked with the ingestion pipeline before, this will be familiar since it is the same class.\n\nThe requirement for extractors is that the insert graph data into the metadata of the node, which will then be processed later on by the index.\n\nHere is a small example of sub-classing to create a custom extractor:\n\n```python\nfrom llama_index.core.graph_store.types import (\n    EntityNode,\n    Relation,\n    KG_NODES_KEY,\n    KG_RELATIONS_KEY,\n)\nfrom llama_index.core.schema import BaseNode, TransformComponent\n\n\nclass MyGraphExtractor(TransformComponent):\n    # the init is optional\n    # def __init__(self, ...):\n    #     ...\n\n    def __call__(\n        self, llama_nodes: list[BaseNode], **kwargs\n    ) -> list[BaseNode]:\n        for llama_node in llama_nodes:\n            # be sure to not overwrite existing entities/relations\n\n            existing_nodes = llama_node.metadata.pop(KG_NODES_KEY, [])\n            existing_relations = llama_node.metadata.pop(KG_RELATIONS_KEY, [])\n\n            existing_nodes.append(\n                EntityNode(\n                    name=\"llama\", label=\"ANIMAL\", properties={\"key\": \"val\"}\n                )\n            )\n            existing_nodes.append(\n                EntityNode(\n                    name=\"index\", label=\"THING\", properties={\"key\": \"val\"}\n                )\n            )\n\n            existing_relations.append(\n                Relation(\n                    label=\"HAS\",\n                    source_id=\"llama\",\n                    target_id=\"index\",\n                    properties={},\n                )\n            )\n\n            # add back to the metadata\n\n            llama_node.metadata[KG_NODES_KEY] = existing_nodes\n            llama_node.metadata[KG_RELATIONS_KEY] = existing_relations\n\n        return llama_nodes\n\n    # optional async method\n    # async def acall(self, llama_nodes: list[BaseNode], **kwargs) -> list[BaseNode]:\n    #    ...\n```\n\n### Sub-Classing Retrievers\n\nThe retriever is a bit more complicated than the extractors, and has it's own special class to help make sub-classing easier.\n\nThe return type of the retrieval is extremely flexible. It could be\n- a string\n- a `TextNode`\n- a `NodeWithScore`\n- a list of one of the above\n\nHere is a small example of sub-classing to create a custom retriever:\n\n```python\nfrom llama_index.core.indices.property_graph import (\n    CustomPGRetriever,\n    CUSTOM_RETRIEVE_TYPE,\n)\n\n\nclass MyCustomRetriever(CustomPGRetriever):\n    def init(self, my_option_1: bool = False, **kwargs) -> None:\n        \"\"\"Uses any kwargs passed in from class constructor.\"\"\"\n        self.my_option_1 = my_option_1\n        # optionally do something with self.graph_store\n\n    def custom_retrieve(self, query_str: str) -> CUSTOM_RETRIEVE_TYPE:\n        # some some operation with self.graph_store\n        return \"result\"\n\n    # optional async method\n    # async def acustom_retrieve(self, query_str: str) -> str:\n    #     ...", "mimetype": "text/plain", "start_char_idx": 16224, "end_char_idx": 20522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42cfa1ee-ce02-450e-bdd0-453468e0c07d": {"__data__": {"id_": "42cfa1ee-ce02-450e-bdd0-453468e0c07d", "embedding": null, "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885", "node_type": "4", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "c181f908ef56a12681f17837e576cd1dde6ae53095232aaaa0491692d61527c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "256e9a35-9af5-4466-87b7-18b9f7a13201", "node_type": "1", "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}, "hash": "26f37c9948f7d54aa9642412a11455dcedc0c964d81688a5462be81906abf9c3", "class_name": "RelatedNodeInfo"}}, "text": "custom_retriever = MyCustomRetriever(graph_store, my_option_1=True)\n\nretriever = index.as_retriever(sub_retrievers=[custom_retriever])\n```\n\nFor more complicated customization and use-cases, it is recommended to check out the source code and directly sub-class `BasePGRetriever`.\n\n# Examples\n\nBelow, you can find some example notebooks showcasing the `PropertyGraphIndex`\n\n- [Basic Usage](../../examples/property_graph/property_graph_basic.ipynb)\n- [Using Neo4j](../../examples/property_graph/property_graph_neo4j.ipynb)\n- [Using Nebula](../../examples/property_graph/property_graph_nebula.ipynb)\n- [Advanced Usage with Neo4j and local models](../../examples/property_graph/property_graph_advanced.ipynb)\n- [Using a Property Graph Store](../../examples/property_graph/graph_store.ipynb)\n- [Creating a Custom Graph Retriever](../../examples/property_graph/property_graph_custom_retriever.ipynb)\n- [Comparing KG Extractors](../../examples/property_graph/Dynamic_KG_Extraction.ipynb)", "mimetype": "text/plain", "start_char_idx": 20525, "end_char_idx": 21504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1e2dcfa-a758-4dd2-97d8-3830c9bcdbcb": {"__data__": {"id_": "d1e2dcfa-a758-4dd2-97d8-3830c9bcdbcb", "embedding": null, "metadata": {"filename": "metadata_extraction.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "003fdbf2cfd080fcda93238c19ad70a942de95db", "node_type": "4", "metadata": {"filename": "metadata_extraction.md", "author": "LlamaIndex"}, "hash": "4aa619a078d0f06e015c3fff090d1b63e7f278297e25ddca68886f5f7427bec7", "class_name": "RelatedNodeInfo"}}, "text": "# Metadata Extraction\n\n## Introduction\n\nIn many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text.\n\nTo combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.\n\nWe show this in an [example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/docs/examples/metadata_extraction/MetadataExtractionSEC.ipynb) and demonstrate its effectiveness in processing long documents.\n\n## Usage\n\nFirst, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.\n\nWe then feed this to the node parser, which will add the additional metadata to each node.\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.extractors import (\n    SummaryExtractor,\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n    KeywordExtractor,\n)\nfrom llama_index.extractors.entity import EntityExtractor\n\ntransformations = [\n    SentenceSplitter(),\n    TitleExtractor(nodes=5),\n    QuestionsAnsweredExtractor(questions=3),\n    SummaryExtractor(summaries=[\"prev\", \"self\"]),\n    KeywordExtractor(keywords=10),\n    EntityExtractor(prediction_threshold=0.5),\n]\n```\n\nThen, we can run our transformations on input documents or nodes:\n\n```python\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(transformations=transformations)\n\nnodes = pipeline.run(documents=documents)\n```\n\nHere is an sample of extracted metadata:\n\n```\n{'page_label': '2',\n 'file_name': '10k-132.pdf',\n 'document_title': 'Uber Technologies, Inc. 2019 Annual Report: Revolutionizing Mobility and Logistics Across 69 Countries and 111 Million MAPCs with $65 Billion in Gross Bookings',\n 'questions_this_excerpt_can_answer': '\\n\\n1. How many countries does Uber Technologies, Inc. operate in?\\n2. What is the total number of MAPCs served by Uber Technologies, Inc.?\\n3. How much gross bookings did Uber Technologies, Inc. generate in 2019?',\n 'prev_section_summary': \"\\n\\nThe 2019 Annual Report provides an overview of the key topics and entities that have been important to the organization over the past year. These include financial performance, operational highlights, customer satisfaction, employee engagement, and sustainability initiatives. It also provides an overview of the organization's strategic objectives and goals for the upcoming year.\",\n 'section_summary': '\\nThis section discusses a global tech platform that serves multiple multi-trillion dollar markets with products leveraging core technology and infrastructure. It enables consumers and drivers to tap a button and get a ride or work. The platform has revolutionized personal mobility with ridesharing and is now leveraging its platform to redefine the massive meal delivery and logistics industries. The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.',\n 'excerpt_keywords': '\\nRidesharing, Mobility, Meal Delivery, Logistics, Network, Technology, Operational Excellence, Product Expertise, Point A, Point B'}\n```\n\n## Custom Extractors\n\nIf the provided extractors do not fit your needs, you can also define a custom extractor like so:\n\n```python\nfrom llama_index.core.extractors import BaseExtractor\n\n\nclass CustomExtractor(BaseExtractor):\n    async def aextract(self, nodes) -> List[Dict]:\n        metadata_list = [\n            {\n                \"custom\": node.metadata[\"document_title\"]\n                + \"\\n\"\n                + node.metadata[\"excerpt_keywords\"]\n            }\n            for node in nodes\n        ]\n        return metadata_list\n```\n\n`extractor.extract()` will automatically call `aextract()` under the hood, to provide both sync and async entrypoints.\n\nIn a more advanced example, it can also make use of an `llm` to extract features from the node content and the existing metadata. Refer to the [source code of the provided metadata extractors](https://github.com/jerryjliu/llama_index/blob/main/llama_index/node_parser/extractors/metadata_extractors.py) for more details.\n\n## Modules\n\nBelow you will find guides and tutorials for various metadata extractors.\n\n- [SEC Documents Metadata Extraction](../../examples/metadata_extraction/MetadataExtractionSEC.ipynb)\n- [LLM Survey Extraction](../../examples/metadata_extraction/MetadataExtraction_LLMSurvey.ipynb)\n- [Entity Extraction](../../examples/metadata_extraction/EntityExtractionClimate.ipynb)\n- [Marvin Metadata Extraction](../../examples/metadata_extraction/MarvinMetadataExtractorDemo.ipynb)\n- [Pydantic Metadata Extraction](../../examples/metadata_extraction/PydanticExtractor.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb9aa5f6-ebab-49c9-95b9-36f24a564c2f": {"__data__": {"id_": "bb9aa5f6-ebab-49c9-95b9-36f24a564c2f", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9f07e858ff7f7ed3f676c006fc3295319ecd6d2", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "25fcfe22e285f7eb4f0b1c0d1e93a08d3bfd3f539c1e4979086a6150786fd682", "class_name": "RelatedNodeInfo"}}, "text": "# Module Guides\n\n- [Vector Store Index](./vector_store_index.md)\n- [Summary Index](./index_guide.md)\n- [Tree Index](./index_guide.md)\n- [Keyword Table Index](./index_guide.md)\n- [Knowledge Graph Index](../../examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb)\n- [Knowledge Graph Query Engine](../../examples/query_engine/knowledge_graph_query_engine.ipynb)\n- [Knoweldge Graph RAG Query Engine](../../examples/query_engine/knowledge_graph_rag_query_engine.ipynb)\n- [REBEL + Knowledge Graph Index](https://colab.research.google.com/drive/1G6pcR0pXvSkdMQlAK_P-IrYgo-_staxd?usp=sharing)\n- [REBEL + Wikipedia Filtering](../../examples/index_structs/knowledge_graph/knowledge_graph2.ipynb)\n- [SQL Query Engine](../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb)\n- [DuckDB Query Engine](../../examples/index_structs/struct_indices/duckdb_sql_query.ipynb)\n- [Document Summary Index](../../examples/index_structs/doc_summary/DocSummary.ipynb)\n- [Object Index](../../examples/objects/object_index.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cac9ece9-1f14-4866-986b-31a55cf7bb5a": {"__data__": {"id_": "cac9ece9-1f14-4866-986b-31a55cf7bb5a", "embedding": null, "metadata": {"filename": "vector_store_index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0d2076e49dc2ca51f0382a9ad9bbc12d1b196cc", "node_type": "4", "metadata": {"filename": "vector_store_index.md", "author": "LlamaIndex"}, "hash": "b65ab65c4c27e977bd39ff25c570a34698567d0d140d39c03770950fbb40ce31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12468260-8cef-435f-907a-04367701d5ea", "node_type": "1", "metadata": {}, "hash": "3e6b5cdad6cf010872eb75f6b4c8abbec21d0da61fbd3ac3b4503a022650f47d", "class_name": "RelatedNodeInfo"}}, "text": "# Using VectorStoreIndex\n\nVector Stores are a key component of retrieval-augmented generation (RAG) and so you will end up using them in nearly every application you make using LlamaIndex, either directly or indirectly.\n\nVector stores accept a list of [`Node` objects](../loading/documents_and_nodes/index.md) and build an index from them\n\n## Loading data into the index\n\n### Basic usage\n\nThe simplest way to use a Vector Store is to load a set of documents and build an index from them using `from_documents`:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\n    \"../../examples/data/paul_graham\"\n).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n!!! tip\n    If you are using `from_documents` on the command line, it can be convenient to pass `show_progress=True` to display a progress bar during index construction.\n\nWhen you use `from_documents`, your Documents are split into chunks and parsed into [`Node` objects](../loading/documents_and_nodes/index.md), lightweight abstractions over text strings that keep track of metadata and relationships.\n\nFor more on how to load documents, see [Understanding Loading](../loading/index.md).\n\nBy default, VectorStoreIndex stores everything in memory. See [Using Vector Stores](#using-vector-stores) below for more on how to use persistent vector stores.\n\n!!! tip\n    By default, the `VectorStoreIndex` will generate and insert vectors in batches of 2048 nodes. If you are memory constrained (or have a surplus of memory), you can modify this by passing `insert_batch_size=2048` with your desired batch size.\n\n    This is especially helpful when you are inserting into a remotely hosted vector database.\n\n### Using the ingestion pipeline to create nodes\n\nIf you want more control over how your documents are indexed, we recommend using the ingestion pipeline. This allows you to customize the chunking, metadata, and embedding of the nodes.\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.extractors import TitleExtractor\nfrom llama_index.core.ingestion import IngestionPipeline, IngestionCache\n\n# create the pipeline with transformations\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\n\n# run the pipeline\nnodes = pipeline.run(documents=[Document.example()])\n```\n\n!!! tip\n    You can learn more about [how to use the ingestion pipeline](../loading/ingestion_pipeline/index.md).\n\n### Creating and managing nodes directly\n\nIf you want total control over your index you can [create and define nodes manually](../loading/documents_and_nodes/usage_nodes.md) and pass them directly to the index constructor:\n\n```python\nfrom llama_index.core.schema import TextNode\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnodes = [node1, node2]\nindex = VectorStoreIndex(nodes)\n```\n\n#### Handling Document Updates\n\nWhen managing your index directly, you will want to deal with data sources that change over time. `Index` classes have **insertion**, **deletion**, **update**, and **refresh** operations and you can learn more about them below:\n\n- [Metadata Extraction](metadata_extraction.md)\n- [Document Management](document_management.md)\n\n## Storing the vector index\n\nLlamaIndex supports [dozens of vector stores](../storing/vector_stores.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12468260-8cef-435f-907a-04367701d5ea": {"__data__": {"id_": "12468260-8cef-435f-907a-04367701d5ea", "embedding": null, "metadata": {"filename": "vector_store_index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0d2076e49dc2ca51f0382a9ad9bbc12d1b196cc", "node_type": "4", "metadata": {"filename": "vector_store_index.md", "author": "LlamaIndex"}, "hash": "b65ab65c4c27e977bd39ff25c570a34698567d0d140d39c03770950fbb40ce31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cac9ece9-1f14-4866-986b-31a55cf7bb5a", "node_type": "1", "metadata": {"filename": "vector_store_index.md", "author": "LlamaIndex"}, "hash": "2617c938c689fca41c866ef86c4edd19a1639b2653fa7ddb4628a4e6ea66051e", "class_name": "RelatedNodeInfo"}}, "text": "You can specify which one to use by passing in a `StorageContext`, on which in turn you specify the `vector_store` argument, as in this example using Pinecone:\n\n```python\nimport pinecone\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n)\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\n\n# init pinecone\npinecone.init(api_key=\"<api_key>\", environment=\"<environment>\")\npinecone.create_index(\n    \"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\n)\n\n# construct vector store and customize storage context\nstorage_context = StorageContext.from_defaults(\n    vector_store=PineconeVectorStore(pinecone.Index(\"quickstart\"))\n)\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\n    \"../../examples/data/paul_graham\"\n).load_data()\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n```\n\nFor more examples of how to use VectorStoreIndex, see our [vector store index usage examples notebook](./vector_store_guide.ipynb).\n\nFor examples of how to use VectorStoreIndex with specific vector stores, check out [vector stores](../storing/vector_stores.md) under the Storing section.\n\n## Composable Retrieval\n\nThe `VectorStoreIndex` (and any other index/retriever) is capable of retrieving generic objects, including\n\n- references to nodes\n- query engines\n- retrievers\n- query pipelines\n\nIf these objects are retrieved, they will be automatically ran using the provided query.\n\nFor example:\n\n```python\nfrom llama_index.core.schema import IndexNode\n\nquery_engine = other_index.as_query_engine\nobj = IndexNode(\n    text=\"A query engine describing X, Y, and Z.\",\n    obj=query_engine,\n    index_id=\"my_query_engine\",\n)\n\nindex = VectorStoreIndex(nodes=nodes, objects=[obj])\nretriever = index.as_retreiver(verbose=True)\n```\n\nIf the index node containing the query engine is retrieved, the query engine will be ran and the resulting response returned as a node.\n\nFor more details, checkout [the guide](../../examples/retrievers/composable_retrievers.ipynb)", "mimetype": "text/plain", "start_char_idx": 3620, "end_char_idx": 5693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d598c8a-2401-4a87-81fa-d1b6d168e09f": {"__data__": {"id_": "8d598c8a-2401-4a87-81fa-d1b6d168e09f", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db1628c3f26ab1e24771a7bb9a9fded18ba0db60", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "29d4d0418ee3ce8350ed19b93e4942cfe8577f7167f5be1cc4439006a7ebf039", "class_name": "RelatedNodeInfo"}}, "text": "# Data Connectors (LlamaHub)\n\n## Concept\n\nA data connector (aka `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n!!! tip\n    Once you've ingested your data, you can build an [Index](../../indexing/index.md) on top, ask questions using a [Query Engine](../../deploying/query_engine/index.md), and have a conversation using a [Chat Engine](../../deploying/chat_engines/index.md).\n\n## LlamaHub\n\nOur data connectors are offered through [LlamaHub](https://llamahub.ai/) \ud83e\udd99.\nLlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.\n\n![](../../../_static/data_connectors/llamahub.png)\n\n## Usage Pattern\n\nGet started with:\n\n```python\nfrom llama_index.core import download_loader\n\nfrom llama_index.readers.google import GoogleDocsReader\n\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=[...])\n```\n\nSee the full [usage pattern guide](./usage_pattern.md) for more details.\n\n## Modules\n\nSome sample data connectors:\n\n- local file directory (`SimpleDirectoryReader`). Can support parsing a wide range of file types: `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n- [Notion](https://developers.notion.com/) (`NotionPageReader`)\n- [Google Docs](https://developers.google.com/docs/api) (`GoogleDocsReader`)\n- [Slack](https://api.slack.com/) (`SlackReader`)\n- [Discord](https://discord.com/developers/docs/intro) (`DiscordReader`)\n- [Apify Actors](https://llamahub.ai/l/apify-actor) (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n\nSee the [modules guide](./modules.md) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41bb98cb-73cd-4afe-9dd7-5cba47e857dd": {"__data__": {"id_": "41bb98cb-73cd-4afe-9dd7-5cba47e857dd", "embedding": null, "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb93eb0420a35bd32139dc025311c9f02731162a", "node_type": "4", "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}, "hash": "573c8059fe8b0b17b464a9f6b5229483879ff32ec67524b02ea9dd6a19fee306", "class_name": "RelatedNodeInfo"}}, "text": "# LlamaParse\n\nLlamaParse is a service created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n\nLlamaParse directly integrates with [LlamaIndex](https://github.com/run-llama/llama_index).\n\nYou can sign up and use LlamaParse for free! Dozens of document types are supported including PDFs, Word Files, PowerPoint, Excel spreadsheets and many more.\n\nFor information on how to get started, check out the [LlamaParse documentation](https://docs.cloud.llamaindex.ai/llamaparse/getting_started).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bbc4f80-32ef-4f92-b8f3-b625a7bcfff9": {"__data__": {"id_": "8bbc4f80-32ef-4f92-b8f3-b625a7bcfff9", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4e2b156a173ab2c193789762a34d7c4f2c1045b", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "7546d70662261df0997f56eb62b4200097ff502dc86527e2c111546d5f964028", "class_name": "RelatedNodeInfo"}}, "text": "# Module Guides\n\n- [Simple Directory Reader](../../../examples/data_connectors/simple_directory_reader.ipynb)\n- [Psychic Reader](../../../examples/data_connectors/PsychicDemo.ipynb)\n- [Deeplake Reader](../../../examples/data_connectors/DeepLakeReader.ipynb)\n- [Qdrant Reader](../../../examples/data_connectors/QdrantDemo.ipynb)\n- [Discord Reader](../../../examples/data_connectors/DiscordDemo.ipynb)\n- [MongoDB Reader](../../../examples/data_connectors/MongoDemo.ipynb)\n- [Chroma Reader](../../../examples/data_connectors/ChromaDemo.ipynb)\n- [MyScale Reader](../../../examples/data_connectors/MyScaleReaderDemo.ipynb)\n- [FAISS Reader](../../../examples/data_connectors/FaissDemo.ipynb)\n- [Obsidian Reader](../../../examples/data_connectors/ObsidianReaderDemo.ipynb)\n- [Slack Reader](../../../examples/data_connectors/SlackDemo.ipynb)\n- [Webpage Reader](../../../examples/data_connectors/WebPageDemo.ipynb)\n- [Pinecone Reader](../../../examples/data_connectors/PineconeDemo.ipynb)\n- [Pathway Reader](../../../examples/data_connectors/PathwayReaderDemo.ipynb)\n- [MBox Reader](../../../examples/data_connectors/MboxReaderDemo.ipynb)\n- [Milvus Reader](../../../examples/data_connectors/MilvusReaderDemo.ipynb)\n- [Notion Reader](../../../examples/data_connectors/NotionDemo.ipynb)\n- [Github Reader](../../../examples/data_connectors/GithubRepositoryReaderDemo.ipynb)\n- [Google Docs Reader](../../../examples/data_connectors/GoogleDocsDemo.ipynb)\n- [Database Reader](../../../examples/data_connectors/DatabaseReaderDemo.ipynb)\n- [Twitter Reader](../../../examples/data_connectors/TwitterDemo.ipynb)\n- [Weaviate Reader](../../../examples/data_connectors/WeaviateDemo.ipynb)\n- [Make Reader](../../../examples/data_connectors/MakeDemo.ipynb)\n- [Deplot Reader](../../../examples/data_connectors/deplot/DeplotReader.ipynb)\n\n```\n\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddefba23-0586-490c-912a-5ec6c11cb97e": {"__data__": {"id_": "ddefba23-0586-490c-912a-5ec6c11cb97e", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "febe1a6dc3ae70f1ccc84d9ab73b9b0d4d41f4d1", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "bbb49e1d927a40d329dde4eb7bcea65d798d9e19060674c44d9c0aa7c4a1fadc", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern\n\n## Get Started\n\nEach data loader contains a \"Usage\" section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which\ndownloads the loader file into a module that you can use within your application.\n\nExample usage:\n\n```python\nfrom llama_index.core import VectorStoreIndex, download_loader\n\nfrom llama_index.readers.google import GoogleDocsReader\n\ngdoc_ids = [\"1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\"]\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=gdoc_ids)\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nquery_engine.query(\"Where did the author go to school?\")\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55db4779-fe1f-4228-86ca-40db50c7cdc7": {"__data__": {"id_": "55db4779-fe1f-4228-86ca-40db50c7cdc7", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a2cde29bc150fa880d1870c64523de9e3959eb", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "a3db45dff3809b4c46ca6c85e12b9402411b00b61c0708e84446b085fd14155d", "class_name": "RelatedNodeInfo"}}, "text": "# Documents / Nodes\n\n## Concept\n\nDocument and Node objects are core abstractions within LlamaIndex.\n\nA **Document** is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below.\n\n- `metadata` - a dictionary of annotations that can be appended to the text.\n- `relationships` - a dictionary containing relationships to other Documents/Nodes.\n\n_Note_: We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities.\n\nA **Node** represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \"file_name\" filed in the Document is propagated to every Node).\n\n## Usage Pattern\n\nHere are some simple snippets to get started with Documents and Nodes.\n\n#### Documents\n\n```python\nfrom llama_index.core import Document, VectorStoreIndex\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n#### Nodes\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# load documents\n...\n\n# parse nodes\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n\n# build index\nindex = VectorStoreIndex(nodes)\n```\n\n### Document/Node Usage\n\nTake a look at our in-depth guides for more details on how to use Documents/Nodes.\n\n- [Using Documents](usage_documents.md)\n- [Using Nodes](usage_nodes.md)\n- [Ingestion Pipeline](../ingestion_pipeline/transformations.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d49d386-97a3-497b-b97d-6930b747b955": {"__data__": {"id_": "8d49d386-97a3-497b-b97d-6930b747b955", "embedding": null, "metadata": {"filename": "usage_documents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5aea5486d84d09f270c793a59536240261f8810a", "node_type": "4", "metadata": {"filename": "usage_documents.md", "author": "LlamaIndex"}, "hash": "3dc754e1e884b4f9c31aaff054373c41ba10ca2be80c00b6d8393db107adb3d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e213b74-6511-4a04-8d2b-ffbeda15e6bf", "node_type": "1", "metadata": {}, "hash": "4c758b048141577f277adee232ec119aff5df35c2cbf51e1db5b403b0135fd35", "class_name": "RelatedNodeInfo"}}, "text": "# Defining and Customizing Documents\n\n## Defining Documents\n\nDocuments can either be created automatically via data loaders, or constructed manually.\n\nBy default, all of our [data loaders](../connector/index.md) (including those offered on LlamaHub) return `Document` objects through the `load_data` function.\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n```\n\nYou can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.\n\n```python\nfrom llama_index.core import Document\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n```\n\nTo speed up prototyping and development, you can also quickly create a document using some default text:\n\n```python\ndocument = Document.example()\n```\n\n## Customizing Documents\n\nThis section covers various ways to customize `Document` objects. Since the `Document` object is a subclass of our `TextNode` object, all these settings and details apply to the `TextNode` object class as well.\n\n### Metadata\n\nDocuments also offer the chance to include useful metadata. Using the `metadata` dictionary on each document, additional information can be included to help inform responses and track down sources for query responses. This information can be anything, such as filenames or categories. If you are integrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).\n\nAny information set in the `metadata` dictionary of each document will show up in the `metadata` of each source node created from the document. Additionally, this information is included in the nodes, enabling the index to utilize it on queries and responses. By default, the metadata is injected into the text for both embedding and LLM model calls.\n\nThere are a few ways to set up this dictionary:\n\n1. In the document constructor:\n\n```python\ndocument = Document(\n    text=\"text\",\n    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n)\n```\n\n2. After the document is created:\n\n```python\ndocument.metadata = {\"filename\": \"<doc_file_name>\"}\n```\n\n3. Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook. This will automatically run the hook on each document to set the `metadata` field:\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\n\nfilename_fn = lambda filename: {\"file_name\": filename}\n\n# automatically sets the metadata of each document according to filename_fn\ndocuments = SimpleDirectoryReader(\n    \"./data\", file_metadata=filename_fn\n).load_data()\n```\n\n### Customizing the id\n\nAs detailed in the section [Document Management](../../indexing/document_management.md), the `doc_id` is used to enable efficient refreshing of documents in the index. When using the `SimpleDirectoryReader`, you can automatically set the doc `doc_id` to be the full path to each document:\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\nprint([x.doc_id for x in documents])\n```\n\nYou can also set the `doc_id` of any `Document` directly!\n\n```python\ndocument.doc_id = \"My new document id!\"\n```\n\nNote: the ID can also be set through the `node_id` or `id_` property on a Document object, similar to a `TextNode` object.\n\n### Advanced - Metadata Customization\n\nA key detail mentioned above is that by default, any metadata you set is included in the embeddings generation and LLM.\n\n#### Customizing LLM Metadata Text\n\nTypically, a document might have many metadata keys, but you might not want all of them visible to the LLM during response synthesis. In the above examples, we may not want the LLM to read the `file_name` of our document. However, the `file_name` might include information that will help generate better embeddings. A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.\n\nWe can exclude it like so:\n\n```python\ndocument.excluded_llm_metadata_keys = [\"file_name\"]\n```\n\nThen, we can test what the LLM will actually end up reading using the `get_content()` function and specifying `MetadataMode.LLM`:\n\n```python\nfrom llama_index.core.schema import MetadataMode\n\nprint(document.get_content(metadata_mode=MetadataMode.LLM))\n```\n\n#### Customizing Embedding Metadata Text\n\nSimilar to customing the metadata visible to the LLM, we can also customize the metadata visible to embeddings. In this case, you can specifically exclude metadata visible to the embedding model, in case you DON'T want particular text to bias the embeddings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e213b74-6511-4a04-8d2b-ffbeda15e6bf": {"__data__": {"id_": "3e213b74-6511-4a04-8d2b-ffbeda15e6bf", "embedding": null, "metadata": {"filename": "usage_documents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5aea5486d84d09f270c793a59536240261f8810a", "node_type": "4", "metadata": {"filename": "usage_documents.md", "author": "LlamaIndex"}, "hash": "3dc754e1e884b4f9c31aaff054373c41ba10ca2be80c00b6d8393db107adb3d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d49d386-97a3-497b-b97d-6930b747b955", "node_type": "1", "metadata": {"filename": "usage_documents.md", "author": "LlamaIndex"}, "hash": "0ae1b92258cef1c2bbf76a2146b0bc08bc36b3035357bdc5b41afaf4fa3c366f", "class_name": "RelatedNodeInfo"}}, "text": "In this case, you can specifically exclude metadata visible to the embedding model, in case you DON'T want particular text to bias the embeddings.\n\n```python\ndocument.excluded_embed_metadata_keys = [\"file_name\"]\n```\n\nThen, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `MetadataMode.EMBED`:\n\n```python\nfrom llama_index.core.schema import MetadataMode\n\nprint(document.get_content(metadata_mode=MetadataMode.EMBED))\n```\n\n#### Customizing Metadata Format\n\nAs you know by now, metadata is injected into the actual text of each document/node when sent to the LLM or embedding model. By default, the format of this metadata is controlled by three attributes:\n\n1. `Document.metadata_seperator` -> default = `\"\\n\"`\n\nWhen concatenating all key/value fields of your metadata, this field controls the separator between each key/value pair.\n\n2. `Document.metadata_template` -> default = `\"{key}: {value}\"`\n\nThis attribute controls how each key/value pair in your metadata is formatted. The two variables `key` and `value` string keys are required.\n\n3. `Document.text_template` -> default = `{metadata_str}\\n\\n{content}`\n\nOnce your metadata is converted into a string using `metadata_seperator` and `metadata_template`, this templates controls what that metadata looks like when joined with the text content of your document/node. The `metadata` and `content` string keys are required.\n\n### Summary\n\nKnowing all this, let's create a short example using all this power:\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.core.schema import MetadataMode\n\ndocument = Document(\n    text=\"This is a super-customized document\",\n    metadata={\n        \"file_name\": \"super_secret_document.txt\",\n        \"category\": \"finance\",\n        \"author\": \"LlamaIndex\",\n    },\n    excluded_llm_metadata_keys=[\"file_name\"],\n    metadata_seperator=\"::\",\n    metadata_template=\"{key}=>{value}\",\n    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n)\n\nprint(\n    \"The LLM sees this: \\n\",\n    document.get_content(metadata_mode=MetadataMode.LLM),\n)\nprint(\n    \"The Embedding model sees this: \\n\",\n    document.get_content(metadata_mode=MetadataMode.EMBED),\n)\n```\n\n### Advanced - Automatic Metadata Extraction\n\nWe have [initial examples](./usage_metadata_extractor.md) of using LLMs themselves to perform metadata extraction.", "mimetype": "text/plain", "start_char_idx": 4541, "end_char_idx": 6932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9630c7e9-f254-418a-929d-e34957752420": {"__data__": {"id_": "9630c7e9-f254-418a-929d-e34957752420", "embedding": null, "metadata": {"filename": "usage_metadata_extractor.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6651b33b6853f1c4f0987cb8490f884551e448c", "node_type": "4", "metadata": {"filename": "usage_metadata_extractor.md", "author": "LlamaIndex"}, "hash": "cb6973ea1090579201addabba2b4dfd5b423a0d6afe8d23660fe21a9ed04bf72", "class_name": "RelatedNodeInfo"}}, "text": "# Metadata Extraction Usage Pattern\n\nYou can use LLMs to automate metadata extraction with our `Metadata Extractor` modules.\n\nOur metadata extractor modules include the following \"feature extractors\":\n\n- `SummaryExtractor` - automatically extracts a summary over a set of Nodes\n- `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer\n- `TitleExtractor` - extracts a title over the context of each Node\n- `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node\n\nThen you can chain the `Metadata Extractor`s with our node parser:\n\n```python\nfrom llama_index.core.extractors import (\n    TitleExtractor,\n    QuestionsAnsweredExtractor,\n)\nfrom llama_index.core.node_parser import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(\n    separator=\" \", chunk_size=512, chunk_overlap=128\n)\ntitle_extractor = TitleExtractor(nodes=5)\nqa_extractor = QuestionsAnsweredExtractor(questions=3)\n\n# assume documents are defined -> extract nodes\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[text_splitter, title_extractor, qa_extractor]\n)\n\nnodes = pipeline.run(\n    documents=documents,\n    in_place=True,\n    show_progress=True,\n)\n```\n\nor insert into an index:\n\n```python\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(\n    documents, transformations=[text_splitter, title_extractor, qa_extractor]\n)\n```\n\n## Resources\n\n- [SEC Documents Metadata Extraction](../../../examples/metadata_extraction/MetadataExtractionSEC.ipynb)\n- [LLM Survey Extraction](../../../examples/metadata_extraction/MetadataExtraction_LLMSurvey.ipynb)\n- [Entity Extraction](../../../examples/metadata_extraction/EntityExtractionClimate.ipynb)\n- [Marvin Metadata Extraction](../../../examples/metadata_extraction/MarvinMetadataExtractorDemo.ipynb)\n- [Pydantic Metadata Extraction](../../../examples/metadata_extraction/PydanticExtractor.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f571887-a1a9-418a-b82a-26bd8f9a6198": {"__data__": {"id_": "0f571887-a1a9-418a-b82a-26bd8f9a6198", "embedding": null, "metadata": {"filename": "usage_nodes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9d2857886388f91254d810017f749b7a953526b", "node_type": "4", "metadata": {"filename": "usage_nodes.md", "author": "LlamaIndex"}, "hash": "e0ee1664c6d7ac023966056543e5e61a46c9c4723c504f9b1bdf99d205b643e6", "class_name": "RelatedNodeInfo"}}, "text": "# Defining and Customizing Nodes\n\nNodes represent \"chunks\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information\nwith other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\n\nparser = SentenceSplitter()\n\nnodes = parser.get_nodes_from_documents(documents)\n```\n\nYou can also choose to construct Node objects manually and skip the first section. For instance,\n\n```python\nfrom llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n# set relationships\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(\n    node_id=node2.node_id\n)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(\n    node_id=node1.node_id\n)\nnodes = [node1, node2]\n```\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\n```python\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(\n    node_id=node1.node_id, metadata={\"key\": \"val\"}\n)\n```\n\n### Customizing the ID\n\nEach node has an `node_id` property that is automatically generated if not manually specified. This ID can be used for\na variety of purposes; this includes being able to update nodes in storage, being able to define relationships\nbetween nodes (through `IndexNode`), and more.\n\nYou can also get and set the `node_id` of any `TextNode` directly.\n\n```python\nprint(node.node_id)\nnode.node_id = \"My new node_id!\"\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abc8f1f6-6097-4a59-9262-8c0251dd4b54": {"__data__": {"id_": "abc8f1f6-6097-4a59-9262-8c0251dd4b54", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c59e954477b96d4c8ef57891fc813cf4a770611", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "24f594538820f3f9c81579d66dbd9fa7b27d368387aa5236697c4fb296310b30", "class_name": "RelatedNodeInfo"}}, "text": "# Loading Data\n\nThe key to data ingestion in LlamaIndex is loading and transformations. Once you have loaded Documents, you can process them via transformations and output Nodes.\n\nOnce you have [learned about the basics of loading data](../../understanding/loading/loading.md) in our Understanding section, you can read on to learn more about:\n\n### Loading\n\n- [SimpleDirectoryReader](simpledirectoryreader.md), our built-in loader for loading all sorts of file types from a local directory\n- [LlamaParse](connector/llama_parse.md), LlamaIndex's official tool for PDF parsing, available as a managed API.\n- [LlamaHub](connector/index.md), our registry of hundreds of data loading libraries to ingest data from any source\n\n### Transformations\n\nThis includes common operations like splitting text.\n\n- [Node Parser Usage Pattern](node_parsers/index.md), showing you how to use our node parsers\n- [Node Parser Modules](node_parsers/modules.md), showing our text splitters (sentence, token, HTML, JSON) and other parser modules.\n\n### Putting it all Together\n\n- [The ingestion pipeline](ingestion_pipeline/index.md) which allows you to set up a repeatable, cache-optimized process for loading data.\n\n### Abstractions\n\n- [Document and Node objects](documents_and_nodes/index.md) and how to customize them for more advanced use cases", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de3fa269-76eb-4504-81aa-01c4d11e8623": {"__data__": {"id_": "de3fa269-76eb-4504-81aa-01c4d11e8623", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7b4bc4956dc73a6179c2c38c289a7c578bd82f4", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c98380877b864964fb07bfba1652c41dfb6286d3279fe1274b728fa1e6916790", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "312097a2-60ff-4c06-a683-def9dde59b9b", "node_type": "1", "metadata": {}, "hash": "fff9eb133e085b538a5734898d637e09df8f1676cfd89a6920b9f54c2d834365", "class_name": "RelatedNodeInfo"}}, "text": "# Ingestion Pipeline\n\nAn `IngestionPipeline` uses a concept of `Transformations` that are applied to input data. These `Transformations` are applied to your input data, and the resulting nodes are either returned or inserted into a vector database (if given). Each node+transformation pair is cached, so that subsequent runs (if the cache is persisted) with the same node+transformation combination can use the cached result and save you time.\n\nTo see an interactive example of `IngestionPipeline` being put in use, check out the [RAG CLI](../../../getting_started/starter_tools/rag_cli.md).\n\n## Usage Pattern\n\nThe simplest usage is to instantiate an `IngestionPipeline` like so:\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.extractors import TitleExtractor\nfrom llama_index.core.ingestion import IngestionPipeline, IngestionCache\n\n# create the pipeline with transformations\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\n\n# run the pipeline\nnodes = pipeline.run(documents=[Document.example()])\n```\n\nNote that in a real-world scenario, you would get your documents from `SimpleDirectoryReader` or another reader from Llama Hub.\n\n## Connecting to Vector Databases\n\nWhen running an ingestion pipeline, you can also chose to automatically insert the resulting nodes into a remote vector store.\n\nThen, you can construct an index from that vector store later on.\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.extractors import TitleExtractor\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nimport qdrant_client\n\nclient = qdrant_client.QdrantClient(location=\":memory:\")\nvector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    vector_store=vector_store,\n)\n\n# Ingest directly into a vector db\npipeline.run(documents=[Document.example()])\n\n# Create your index\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_vector_store(vector_store)\n```\n\n## Calculating embeddings in a pipeline\n\nNote that in the above example, embeddings are calculated as part of the pipeline. If you are connecting your pipeline to a vector store, embeddings must be a stage of your pipeline or your later instantiation of the index will fail.\n\nYou can omit embeddings from your pipeline if you are not connecting to a vector store, i.e. just producing a list of nodes.\n\n## Caching\n\nIn an `IngestionPipeline`, each node + transformation combination is hashed and cached. This saves time on subsequent runs that use the same data.\n\nThe following sections describe some basic usage around caching.\n\n### Local Cache Management\n\nOnce you have a pipeline, you may want to store and load the cache.\n\n```python\n# save\npipeline.persist(\"./pipeline_storage\")\n\n# load and restore state\nnew_pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n    ],\n)\nnew_pipeline.load(\"./pipeline_storage\")\n\n# will run instantly due to the cache\nnodes = pipeline.run(documents=[Document.example()])\n```\n\nIf the cache becomes too large, you can clear it\n\n```python\n# delete all context of the cache\ncache.clear()\n```\n\n### Remote Cache Management\n\nWe support multiple remote storage backends for caches\n\n- `RedisCache`\n- `MongoDBCache`\n- `FirestoreCache`\n\nHere as an example using the `RedisCache`:\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.extractors import TitleExtractor\nfrom llama_index.core.ingestion import IngestionPipeline, IngestionCache\nfrom llama_index.storage.kvstore.redis import RedisKVStore as RedisCache", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "312097a2-60ff-4c06-a683-def9dde59b9b": {"__data__": {"id_": "312097a2-60ff-4c06-a683-def9dde59b9b", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7b4bc4956dc73a6179c2c38c289a7c578bd82f4", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c98380877b864964fb07bfba1652c41dfb6286d3279fe1274b728fa1e6916790", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de3fa269-76eb-4504-81aa-01c4d11e8623", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e893188b68ef11466cf277c4356e478592c47e749e7e09a860a97c8fc11e591c", "class_name": "RelatedNodeInfo"}}, "text": "ingest_cache = IngestionCache(\n    cache=RedisCache.from_host_and_port(host=\"127.0.0.1\", port=6379),\n    collection=\"my_test_cache\",\n)\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    cache=ingest_cache,\n)\n\n# Ingest directly into a vector db\nnodes = pipeline.run(documents=[Document.example()])\n```\n\nHere, no persist step is needed, since everything is cached as you go in the specified remote collection.\n\n## Async Support\n\nThe `IngestionPipeline` also has support for async operation\n\n```python\nnodes = await pipeline.arun(documents=documents)\n```\n\n## Document Management\n\nAttaching a `docstore` to the ingestion pipeline will enable document management.\n\nUsing the `document.doc_id` or `node.ref_doc_id` as a grounding point, the ingestion pipeline will actively look for duplicate documents.\n\nIt works by:\n\n- Storing a map of `doc_id` -> `document_hash`\n- If a vector store is attached:\n  - If a duplicate `doc_id` is detected, and the hash has changed, the document will be re-processed and upserted\n  - If a duplicate `doc_id` is detected and the hash is unchanged, the node is skipped\n- If only a vector store is not attached:\n  - Checks all existing hashes for each node\n  - If a duplicate is found, the node is skipped\n  - Otherwise, the node is processed\n\n**NOTE:** If we do not attach a vector store, we can only check for and remove duplicate inputs.\n\n```python\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\n\npipeline = IngestionPipeline(\n    transformations=[...], docstore=SimpleDocumentStore()\n)\n```\n\nA full walkthrough is found in our [demo notebook](../../../examples/ingestion/document_management_pipeline.ipynb).\n\nAlso check out another guide using [Redis as our entire ingestion stack](../../../examples/ingestion/redis_ingestion_pipeline.ipynb).\n\n## Parallel Processing\n\nThe `run` method of `IngestionPipeline` can be executed with parallel processes.\nIt does so by making use of `multiprocessing.Pool` distributing batches of nodes\nto across processors.\n\nTo execute with parallel processing, set `num_workers` to the number of processes\nyou'd like use:\n\n```python\nfrom llama_index.core.ingestion import IngestionPipeline\n\npipeline = IngestionPipeline(\n    transformations=[...],\n)\npipeline.run(documents=[...], num_workers=4)\n```\n\n## Modules\n\n- [Transformations Guide](transformations.md)\n- [Advanced Ingestion Pipeline](../../../examples/ingestion/advanced_ingestion_pipeline.ipynb)\n- [Async Ingestion Pipeline](../../../examples/ingestion/async_ingestion_pipeline.ipynb)\n- [Document Management Pipeline](../../../examples/ingestion/document_management_pipeline.ipynb)\n- [Redis Ingestion Pipeline](../../../examples/ingestion/redis_ingestion_pipeline.ipynb)\n- [Google Drive Ingestion Pipeline](../../../examples/ingestion/ingestion_gdrive.ipynb)\n- [Parallel Execution Pipeline](../../../examples/ingestion/parallel_execution_ingestion_pipeline.ipynb)s", "mimetype": "text/plain", "start_char_idx": 4291, "end_char_idx": 7368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b13daa-4c47-4309-bb1b-025cfda1a80d": {"__data__": {"id_": "b4b13daa-4c47-4309-bb1b-025cfda1a80d", "embedding": null, "metadata": {"filename": "transformations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "945556c52ede7e60e504797cf3f6fd8ca72ea488", "node_type": "4", "metadata": {"filename": "transformations.md", "author": "LlamaIndex"}, "hash": "a707ce017bd50ddd9186079b6f012758f22068796456ed6338c5e6ff330b1925", "class_name": "RelatedNodeInfo"}}, "text": "# Transformations\n\nA transformation is something that takes a list of nodes as an input, and returns a list of nodes. Each component that implements the `Transformation` base class has both a synchronous `__call__()` definition and an async `acall()` definition.\n\nCurrently, the following components are `Transformation` objects:\n\n- [`TextSplitter`](../../../module_guides/loading/node_parsers/modules.md#text-splitters)\n- [`NodeParser`](../node_parsers/modules.md)\n- [`MetadataExtractor`](../documents_and_nodes/usage_metadata_extractor.md)\n- `Embeddings`model (check our [list of supported embeddings](../../models/embeddings.md#list-of-supported-embeddings))\n\n## Usage Pattern\n\nWhile transformations are best used with with an [`IngestionPipeline`](./index.md), they can also be used directly.\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.extractors import TitleExtractor\n\nnode_parser = SentenceSplitter(chunk_size=512)\nextractor = TitleExtractor()\n\n# use transforms directly\nnodes = node_parser(documents)\n\n# or use a transformation in async\nnodes = await extractor.acall(nodes)\n```\n\n## Combining with An Index\n\nTransformations can be passed into an index or overall global settings, and will be used when calling `from_documents()` or `insert()` on an index.\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.extractors import (\n    TitleExtractor,\n    QuestionsAnsweredExtractor,\n)\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import TokenTextSplitter\n\ntransformations = [\n    TokenTextSplitter(chunk_size=512, chunk_overlap=128),\n    TitleExtractor(nodes=5),\n    QuestionsAnsweredExtractor(questions=3),\n]\n\n# global\nfrom llama_index.core import Settings\n\nSettings.transformations = [text_splitter, title_extractor, qa_extractor]\n\n# per-index\nindex = VectorStoreIndex.from_documents(\n    documents, transformations=transformations\n)\n```\n\n## Custom Transformations\n\nYou can implement any transformation yourself by implementing the base class.\n\nThe following custom transformation will remove any special characters or punctutaion in text.\n\n```python\nimport re\nfrom llama_index.core import Document\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.schema import TransformComponent\n\n\nclass TextCleaner(TransformComponent):\n    def __call__(self, nodes, **kwargs):\n        for node in nodes:\n            node.text = re.sub(r\"[^0-9A-Za-z ]\", \"\", node.text)\n        return nodes\n```\n\nThese can then be used directly or in any `IngestionPipeline`.\n\n```python\n# use in a pipeline\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TextCleaner(),\n        OpenAIEmbedding(),\n    ],\n)\n\nnodes = pipeline.run(documents=[Document.example()])\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e0e9601-4c6e-4b9b-943d-8792acada127": {"__data__": {"id_": "9e0e9601-4c6e-4b9b-943d-8792acada127", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdacaa57234ac784192ea6e96219f5a450e733e6", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d379460acbfd3c081c7c4276babbf5f6a44bd3c2a7d07be9f05279e1d8a4ae5b", "class_name": "RelatedNodeInfo"}}, "text": "# Node Parser Usage Pattern\n\nNode parsers are a simple abstraction that take a list of documents, and chunk them into `Node` objects, such that each node is a specific chunk of the parent document. When a document is broken into nodes, all of it's attributes are inherited to the children nodes (i.e. `metadata`, text and metadata templates, etc.). You can read more about `Node` and `Document` properties [here](../documents_and_nodes/index.md).\n\n## Getting Started\n\n### Standalone Usage\n\nNode parsers can be used on their own:\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.core.node_parser import SentenceSplitter\n\nnode_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n\nnodes = node_parser.get_nodes_from_documents(\n    [Document(text=\"long text\")], show_progress=False\n)\n```\n\n### Transformation Usage\n\nNode parsers can be included in any set of transformations with an ingestion pipeline.\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import TokenTextSplitter\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\npipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n\nnodes = pipeline.run(documents=documents)\n```\n\n### Index Usage\n\nOr set inside a `transformations` or global settings to be used automatically when an index is constructed using `.from_documents()`:\n\n```python\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.node_parser import SentenceSplitter\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# global\nfrom llama_index.core import Settings\n\nSettings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n\n# per-index\nindex = VectorStoreIndex.from_documents(\n    documents,\n    transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\n)\n```\n\n## Modules\n\nSee the full [modules guide](./modules.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b929670a-dd74-4230-a697-36b7945b3a77": {"__data__": {"id_": "b929670a-dd74-4230-a697-36b7945b3a77", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16a2e41dfd88d8ce6c6c34540bcba1a965173514", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "63d66f691d146d36abbb17de7c1bf7451dc62002eed92dae5b583970c26a2e1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cbb4201-a375-4ba0-98ba-57c95fcc1e85", "node_type": "1", "metadata": {}, "hash": "1d0f34df44da550cd37db073e336bb420e24406dc6d80590c832d5e88b412295", "class_name": "RelatedNodeInfo"}}, "text": "# Node Parser Modules\n\n## File-Based Node Parsers\n\nThere are several file-based node parsers, that will create nodes based on the type of content that is being parsed (JSON, Markdown, etc.)\n\nThe simplest flow is to combine the `FlatFileReader` with the `SimpleFileNodeParser` to automatically use the best node parser for each type of content. Then, you may want to chain the file-based node parser with a text-based node parser to account for the actual length of the text.\n\n### SimpleFileNodeParser\n\n```python\nfrom llama_index.core.node_parser import SimpleFileNodeParser\nfrom llama_index.readers.file import FlatReader\nfrom pathlib import Path\n\nmd_docs = FlatReader().load_data(Path(\"./test.md\"))\n\nparser = SimpleFileNodeParser()\nmd_nodes = parser.get_nodes_from_documents(md_docs)\n```\n\n### HTMLNodeParser\n\nThis node parser uses `beautifulsoup` to parse raw HTML.\n\nBy default, it will parse a select subset of HTML tags, but you can override this.\n\nThe default tags are: `[\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]`\n\n```python\nfrom llama_index.core.node_parser import HTMLNodeParser\n\nparser = HTMLNodeParser(tags=[\"p\", \"h1\"])  # optional list of tags\nnodes = parser.get_nodes_from_documents(html_docs)\n```\n\n### JSONNodeParser\n\nThe `JSONNodeParser` parses raw JSON.\n\n```python\nfrom llama_index.core.node_parser import JSONNodeParser\n\nparser = JSONNodeParser()\n\nnodes = parser.get_nodes_from_documents(json_docs)\n```\n\n### MarkdownNodeParser\n\nThe `MarkdownNodeParser` parses raw markdown text.\n\n```python\nfrom llama_index.core.node_parser import MarkdownNodeParser\n\nparser = MarkdownNodeParser()\n\nnodes = parser.get_nodes_from_documents(markdown_docs)\n```\n\n## Text-Splitters\n\n### CodeSplitter\n\nSplits raw code-text based on the language it is written in.\n\nCheck the full list of [supported languages here](https://github.com/grantjenks/py-tree-sitter-languages#license).\n\n```python\nfrom llama_index.core.node_parser import CodeSplitter\n\nsplitter = CodeSplitter(\n    language=\"python\",\n    chunk_lines=40,  # lines per chunk\n    chunk_lines_overlap=15,  # lines overlap between chunks\n    max_chars=1500,  # max chars per chunk\n)\nnodes = splitter.get_nodes_from_documents(documents)\n```\n\n### LangchainNodeParser\n\nYou can also wrap any existing text splitter from langchain with a node parser.\n\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom llama_index.core.node_parser import LangchainNodeParser\n\nparser = LangchainNodeParser(RecursiveCharacterTextSplitter())\nnodes = parser.get_nodes_from_documents(documents)\n```\n\n### SentenceSplitter\n\nThe `SentenceSplitter` attempts to split text while respecting the boundaries of sentences.\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(\n    chunk_size=1024,\n    chunk_overlap=20,\n)\nnodes = splitter.get_nodes_from_documents(documents)\n```\n\n### SentenceWindowNodeParser\n\nThe `SentenceWindowNodeParser` is similar to other node parsers, except that it splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata. Note that this metadata will not be visible to the LLM or embedding model.\n\nThis is most useful for generating embeddings that have a very specific scope. Then, combined with a `MetadataReplacementNodePostProcessor`, you can replace the sentence with it's surrounding context before sending the node to the LLM.\n\nAn example of setting up the parser with default settings is below. In practice, you would usually only want to adjust the window size of sentences.\n\n```python\nimport nltk\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\n\nnode_parser = SentenceWindowNodeParser.from_defaults(\n    # how many sentences on either side to capture\n    window_size=3,\n    # the metadata key that holds the window of surrounding sentences\n    window_metadata_key=\"window\",\n    # the metadata key that holds the original sentence\n    original_text_metadata_key=\"original_sentence\",\n)\n```\n\nA full example can be found [here in combination with the `MetadataReplacementNodePostProcessor`](../../../examples/node_postprocessor/MetadataReplacementDemo.ipynb).\n\n### SemanticSplitterNodeParser\n\n\"Semantic chunking\" is a new concept proposed Greg Kamradt in his video tutorial on 5 levels of embedding chunking: [https://youtu.be/8OJC21T2SL4?t=1933](https://youtu.be/8OJC21T2SL4?t=1933).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cbb4201-a375-4ba0-98ba-57c95fcc1e85": {"__data__": {"id_": "3cbb4201-a375-4ba0-98ba-57c95fcc1e85", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16a2e41dfd88d8ce6c6c34540bcba1a965173514", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "63d66f691d146d36abbb17de7c1bf7451dc62002eed92dae5b583970c26a2e1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b929670a-dd74-4230-a697-36b7945b3a77", "node_type": "1", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "2cf1a2b10c0b6392492d537470206baed0ae629c81f26cc031ce866774c66cdb", "class_name": "RelatedNodeInfo"}}, "text": "Instead of chunking text with a **fixed** chunk size, the semantic splitter adaptively picks the breakpoint in-between sentences using embedding similarity. This ensures that a \"chunk\" contains sentences that are semantically related to each other.\n\nWe adapted it into a LlamaIndex module.\n\nCheck out our notebook below!\n\nCaveats:\n\n- The regex primarily works for English sentences\n- You may have to tune the breakpoint percentile threshold.\n\n```python\nfrom llama_index.core.node_parser import SemanticSplitterNodeParser\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nsplitter = SemanticSplitterNodeParser(\n    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n)\n```\n\nA full example can be found in our [guide on using the `SemanticSplitterNodeParser`](../../../examples/node_parsers/semantic_chunking.ipynb).\n\n### TokenTextSplitter\n\nThe `TokenTextSplitter` attempts to split to a consistent chunk size according to raw token counts.\n\n```python\nfrom llama_index.core.node_parser import TokenTextSplitter\n\nsplitter = TokenTextSplitter(\n    chunk_size=1024,\n    chunk_overlap=20,\n    separator=\" \",\n)\nnodes = splitter.get_nodes_from_documents(documents)\n```\n\n## Relation-Based Node Parsers\n\n### HierarchicalNodeParser\n\nThis node parser will chunk nodes into hierarchical nodes. This means a single input will be chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node.\n\nWhen combined with the `AutoMergingRetriever`, this enables us to automatically replace retrieved nodes with their parents when a majority of children are retrieved. This process provides the LLM with more complete context for response synthesis.\n\n```python\nfrom llama_index.core.node_parser import HierarchicalNodeParser\n\nnode_parser = HierarchicalNodeParser.from_defaults(\n    chunk_sizes=[2048, 512, 128]\n)\n```\n\nA full example can be found [here in combination with the `AutoMergingRetriever`](../../../examples/retrievers/auto_merging_retriever.ipynb).", "mimetype": "text/plain", "start_char_idx": 4449, "end_char_idx": 6493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d81c3e60-6c45-4fd7-839d-ba7b6e0e4076": {"__data__": {"id_": "d81c3e60-6c45-4fd7-839d-ba7b6e0e4076", "embedding": null, "metadata": {"filename": "simpledirectoryreader.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dce108780a52d09a89f0720320a5dc820fa9887b", "node_type": "4", "metadata": {"filename": "simpledirectoryreader.md", "author": "LlamaIndex"}, "hash": "d991c149f9accfec316ba4bf332fb9e06f8b21ece510e92485d6032c688a5856", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfd62baa-6d4a-4f20-b1aa-58c2a3375864", "node_type": "1", "metadata": {}, "hash": "52dc58906a55a895e7fd3089bcfd68cc9802c4a184163938dc431b987b609796", "class_name": "RelatedNodeInfo"}}, "text": "# SimpleDirectoryReader\n\n`SimpleDirectoryReader` is the simplest way to load data from local files into LlamaIndex. For production use cases it's more likely that you'll want to use one of the many Readers available on [LlamaHub](https://llamahub.ai/), but `SimpleDirectoryReader` is a great way to get started.\n\n## Supported file types\n\nBy default `SimpleDirectoryReader` will try to read any files it finds, treating them all as text. In addition to plain text, it explicitly supports the following file types, which are automatically detected based on file extension:\n\n- .csv - comma-separated values\n- .docx - Microsoft Word\n- .epub - EPUB ebook format\n- .hwp - Hangul Word Processor\n- .ipynb - Jupyter Notebook\n- .jpeg, .jpg - JPEG image\n- .mbox - MBOX email archive\n- .md - Markdown\n- .mp3, .mp4 - audio and video\n- .pdf - Portable Document Format\n- .png - Portable Network Graphics\n- .ppt, .pptm, .pptx - Microsoft PowerPoint\n\nOne file type you may be expecting to find here is JSON; for that we recommend you use our [JSON Loader](https://llamahub.ai/l/readers/llama-index-readers-json).\n\n## Usage\n\nThe most basic usage is to pass an `input_dir` and it will load all supported files in that directory:\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\n\nreader = SimpleDirectoryReader(input_dir=\"path/to/directory\")\ndocuments = reader.load_data()\n```\n\nDocuments can also be loaded with parallel processing if loading many files from\na directory. Note that there are differences when using `multiprocessing` with\nWindows and Linux/MacOS machines, which is explained throughout the `multiprocessing` docs\n(e.g. see [here](https://docs.python.org/3/library/multiprocessing.html?highlight=process#the-spawn-and-forkserver-start-methods)).\nUltimately, Windows users may see less or no performance gains whereas Linux/MacOS\nusers would see these gains when loading the exact same set of files.\n\n```python\n...\ndocuments = reader.load_data(num_workers=4)\n```\n\n### Reading from subdirectories\n\nBy default, `SimpleDirectoryReader` will only read files in the top level of the directory. To read from subdirectories, set `recursive=True`:\n\n```python\nSimpleDirectoryReader(input_dir=\"path/to/directory\", recursive=True)\n```\n\n### Iterating over files as they load\n\nYou can also use the `iter_data()` method to iterate over and process files as they load\n\n```python\nreader = SimpleDirectoryReader(input_dir=\"path/to/directory\", recursive=True)\nall_docs = []\nfor docs in reader.iter_data():\n    # <do something with the documents per file>\n    all_docs.extend(docs)\n```\n\n### Restricting the files loaded\n\nInstead of all files you can pass a list of file paths:\n\n```python\nSimpleDirectoryReader(input_files=[\"path/to/file1\", \"path/to/file2\"])\n```\n\nor you can pass a list of file paths to **exclude** using `exclude`:\n\n```python\nSimpleDirectoryReader(\n    input_dir=\"path/to/directory\", exclude=[\"path/to/file1\", \"path/to/file2\"]\n)\n```\n\nYou can also set `required_exts` to a list of file extensions to only load files with those extensions:\n\n```python\nSimpleDirectoryReader(\n    input_dir=\"path/to/directory\", required_exts=[\".pdf\", \".docx\"]\n)\n```\n\nAnd you can set a maximum number of files to be loaded with `num_files_limit`:\n\n```python\nSimpleDirectoryReader(input_dir=\"path/to/directory\", num_files_limit=100)\n```\n\n### Specifying file encoding\n\n`SimpleDirectoryReader` expects files to be `utf-8` encoded but you can override this using the `encoding` parameter:\n\n```python\nSimpleDirectoryReader(input_dir=\"path/to/directory\", encoding=\"latin-1\")\n```\n\n### Extracting metadata\n\nYou can specify a function that will read each file and extract metadata that gets attached to the resulting `Document` object for each file by passing the function as `file_metadata`:\n\n```python\ndef get_meta(file_path):\n    return {\"foo\": \"bar\", \"file_path\": file_path}", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfd62baa-6d4a-4f20-b1aa-58c2a3375864": {"__data__": {"id_": "cfd62baa-6d4a-4f20-b1aa-58c2a3375864", "embedding": null, "metadata": {"filename": "simpledirectoryreader.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dce108780a52d09a89f0720320a5dc820fa9887b", "node_type": "4", "metadata": {"filename": "simpledirectoryreader.md", "author": "LlamaIndex"}, "hash": "d991c149f9accfec316ba4bf332fb9e06f8b21ece510e92485d6032c688a5856", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d81c3e60-6c45-4fd7-839d-ba7b6e0e4076", "node_type": "1", "metadata": {"filename": "simpledirectoryreader.md", "author": "LlamaIndex"}, "hash": "ff5d359289e63bf54717dbc9c1eb001639f049937b709930f2580392e488dc45", "class_name": "RelatedNodeInfo"}}, "text": "SimpleDirectoryReader(input_dir=\"path/to/directory\", file_metadata=get_meta)\n```\n\nThe function should take a single argument, the file path, and return a dictionary of metadata.\n\n### Extending to other file types\n\nYou can extend `SimpleDirectoryReader` to read other file types by passing a dictionary of file extensions to instances of `BaseReader` as `file_extractor`. A BaseReader should read the file and return a list of Documents. For example, to add custom support for `.myfile` files :\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core.readers.base import BaseReader\nfrom llama_index.core import Document\n\n\nclass MyFileReader(BaseReader):\n    def load_data(self, file, extra_info=None):\n        with open(file, \"r\") as f:\n            text = f.read()\n        # load_data returns a list of Document objects\n        return [Document(text=text + \"Foobar\", extra_info=extra_info or {})]\n\n\nreader = SimpleDirectoryReader(\n    input_dir=\"./data\", file_extractor={\".myfile\": MyFileReader()}\n)\n\ndocuments = reader.load_data()\nprint(documents)\n```\n\nNote that this mapping will override the default file extractors for the file types you specify, so you'll need to add them back in if you want to support them.\n\n### Support for External FileSystems\n\nAs with other modules, the `SimpleDirectoryReader` takes an optional `fs` parameter that can be used to traverse remote filesystems.\n\nThis can be any filesystem object that is implemented by the [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) protocol.\nThe `fsspec` protocol has open-source implementations for a variety of remote filesystems including [AWS S3](https://github.com/fsspec/s3fs), [Azure Blob & DataLake](https://github.com/fsspec/adlfs), [Google Drive](https://github.com/fsspec/gdrivefs), [SFTP](https://github.com/fsspec/sshfs), and [many others](https://github.com/fsspec/).\n\nHere's an example that connects to S3:\n\n```python\nfrom s3fs import S3FileSystem\n\ns3_fs = S3FileSystem(key=\"...\", secret=\"...\")\nbucket_name = \"my-document-bucket\"\n\nreader = SimpleDirectoryReader(\n    input_dir=bucket_name,\n    fs=s3_fs,\n    recursive=True,  # recursively searches all subdirectories\n)\n\ndocuments = reader.load_data()\nprint(documents)\n```\n\nA full example notebook can be found [here](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/data_connectors/simple_directory_reader_remote_fs.ipynb).", "mimetype": "text/plain", "start_char_idx": 3855, "end_char_idx": 6270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c202918d-9b5f-4f43-80ea-e1abeb5c9609": {"__data__": {"id_": "c202918d-9b5f-4f43-80ea-e1abeb5c9609", "embedding": null, "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3eaef138dde935320ea0ea0d9062eed782ee2d91", "node_type": "4", "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "hash": "28fdadd3a6e26d8aca0025a2f2f449449ae0e34024f5bf3d5eebe593425c45f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef3eb03d-7622-4fc2-8248-b2dffea3a2cb", "node_type": "1", "metadata": {}, "hash": "12e3d507fec4930484e7c6f521372dc45ee5c59e02fd2d4988d7bcbf8bd7193a", "class_name": "RelatedNodeInfo"}}, "text": "# Embeddings\n\n## Concept\n\nEmbeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!\n\nAt a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.\n\nWhen calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.\n\nThere are many embedding models to pick from. By default, LlamaIndex uses `text-embedding-ada-002` from OpenAI. We also support any embedding model offered by Langchain [here](https://python.langchain.com/docs/modules/data_connection/text_embedding/), as well as providing an easy to extend base class for implementing your own embeddings.\n\n## Usage Pattern\n\nMost commonly in LlamaIndex, embedding models will be specified in the `Settings` object, and then used in a vector index. The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on. You can also specify embedding models per-index.\n\nIf you don't already have your embeddings installed:\n\n```\npip install llama-index-embeddings-openai\n```\n\nThen:\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core import Settings\n\n# global\nSettings.embed_model = OpenAIEmbedding()\n\n# per-index\nindex = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n```\n\nTo save costs, you may want to use a local model.\n\n```\npip install llama-index-embeddings-huggingface\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n```\n\nThis will use a well-performing and fast default from Hugging Face.\n\nYou can find more usage details and available customization options below.\n\n## Getting Started\n\nThe most common usage for an embedding model will be setting it in the global `Settings` object, and then using it to construct an index and query. The input documents will be broken into nodes, and the embedding model will generate an embedding for each node.\n\nBy default, LlamaIndex will use `text-embedding-ada-002`, which is what the example below manually sets up for you.\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\n# global default\nSettings.embed_model = OpenAIEmbedding()\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nThen, at query time, the embedding model will be used again to embed the query text.\n\n```python\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query string\")\n```\n\n## Customization\n\n### Batch Size\n\nBy default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small.\n\n```python\n# set the batch size to 42\nembed_model = OpenAIEmbedding(embed_batch_size=42)\n```\n\n### Local Embedding Models\n\nThe easiest way to use a local model is:\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n```\n\n### HuggingFace Optimum ONNX Embeddings\n\nLlamaIndex also supports creating and using ONNX embeddings using the Optimum library from HuggingFace. Simple create and save the ONNX embeddings, and use them.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef3eb03d-7622-4fc2-8248-b2dffea3a2cb": {"__data__": {"id_": "ef3eb03d-7622-4fc2-8248-b2dffea3a2cb", "embedding": null, "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3eaef138dde935320ea0ea0d9062eed782ee2d91", "node_type": "4", "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "hash": "28fdadd3a6e26d8aca0025a2f2f449449ae0e34024f5bf3d5eebe593425c45f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c202918d-9b5f-4f43-80ea-e1abeb5c9609", "node_type": "1", "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "hash": "5a54749400a564ea9769ae836bbb1a9bf26650cec603eb36aa49a5e93fe0c6ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e45c70e1-2d02-4c14-9d03-814dbd76623f", "node_type": "1", "metadata": {}, "hash": "8509d526317994cbdcf90f8222954bab9d26e0f80a5f9c1f36bea1d0dedc6ded", "class_name": "RelatedNodeInfo"}}, "text": "Simple create and save the ONNX embeddings, and use them.\n\nSome prerequisites:\n\n```\npip install transformers optimum[exporters]\npip install llama-index-embeddings-huggingface-optimum\n```\n\nCreation with specifying the model and output path:\n\n```python\nfrom llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n\nOptimumEmbedding.create_and_save_optimum_model(\n    \"BAAI/bge-small-en-v1.5\", \"./bge_onnx\"\n)\n```\n\nAnd then usage:\n\n```python\nSettings.embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")\n```\n\n### LangChain Integrations\n\nWe also support any embeddings offered by Langchain [here](https://python.langchain.com/docs/modules/data_connection/text_embedding/).\n\nThe example below loads a model from Hugging Face, using Langchain's embedding class.\n\n```\npip install llama-index-embeddings-langchain\n```\n\n```python\nfrom langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\nfrom llama_index.core import Settings\n\nSettings.embed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en\")\n```\n\n### Custom Embedding Model\n\nIf you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!\n\nThe example below uses Instructor Embeddings ([install/setup details here](https://huggingface.co/hkunlp/instructor-large)), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \"instructions\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.\n\n```python\nfrom typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\nfrom llama_index.core.embeddings import BaseEmbedding", "mimetype": "text/plain", "start_char_idx": 3966, "end_char_idx": 5656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e45c70e1-2d02-4c14-9d03-814dbd76623f": {"__data__": {"id_": "e45c70e1-2d02-4c14-9d03-814dbd76623f", "embedding": null, "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3eaef138dde935320ea0ea0d9062eed782ee2d91", "node_type": "4", "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "hash": "28fdadd3a6e26d8aca0025a2f2f449449ae0e34024f5bf3d5eebe593425c45f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef3eb03d-7622-4fc2-8248-b2dffea3a2cb", "node_type": "1", "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}, "hash": "ff1b891f727814a7e0c63dfe0988037cc6554e5cb5316627ddbf4ee6964f2a98", "class_name": "RelatedNodeInfo"}}, "text": "```python\nfrom typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\nfrom llama_index.core.embeddings import BaseEmbedding\n\n\nclass InstructorEmbeddings(BaseEmbedding):\n    def __init__(\n        self,\n        instructor_model_name: str = \"hkunlp/instructor-large\",\n        instruction: str = \"Represent the Computer Science documentation or question:\",\n        **kwargs: Any,\n    ) -> None:\n        self._model = INSTRUCTOR(instructor_model_name)\n        self._instruction = instruction\n        super().__init__(**kwargs)\n\n        def _get_query_embedding(self, query: str) -> List[float]:\n            embeddings = self._model.encode([[self._instruction, query]])\n            return embeddings[0]\n\n        def _get_text_embedding(self, text: str) -> List[float]:\n            embeddings = self._model.encode([[self._instruction, text]])\n            return embeddings[0]\n\n        def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n            embeddings = self._model.encode(\n                [[self._instruction, text] for text in texts]\n            )\n            return embeddings\n\n        async def _get_query_embedding(self, query: str) -> List[float]:\n            return self._get_query_embedding(query)\n\n        async def _get_text_embedding(self, text: str) -> List[float]:\n            return self._get_text_embedding(text)\n```\n\n## Standalone Usage\n\nYou can also use embeddings as a standalone module for your project, existing application, or general testing and exploration.\n\n```python\nembeddings = embed_model.get_text_embedding(\n    \"It is raining cats and dogs here!\"\n)\n```\n\n## List of supported embeddings\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers.\n\n- [Azure OpenAI](../../examples/customization/llms/AzureOpenAI.ipynb)\n- [CalrifAI](../../examples/embeddings/clarifai.ipynb)\n- [Cohere](../../examples/embeddings/cohereai.ipynb)\n- [Custom](../../examples/embeddings/custom_embeddings.ipynb)\n- [Dashscope](../../examples/embeddings/dashscope_embeddings.ipynb)\n- [ElasticSearch](../../examples/embeddings/elasticsearch.ipynb)\n- [FastEmbed](../../examples/embeddings/fastembed.ipynb)\n- [Google Palm](../../examples/embeddings/google_palm.ipynb)\n- [Gradient](../../examples/embeddings/gradient.ipynb)\n- [Anyscale](../../examples/embeddings/Anyscale.ipynb)\n- [Huggingface](../../examples/embeddings/huggingface.ipynb)\n- [JinaAI](../../examples/embeddings/jinaai_embeddings.ipynb)\n- [Langchain](../../examples/embeddings/Langchain.ipynb)\n- [LLM Rails](../../examples/embeddings/llm_rails.ipynb)\n- [MistralAI](../../examples/embeddings/mistralai.ipynb)\n- [OpenAI](../../examples/embeddings/OpenAI.ipynb)\n- [Sagemaker](../../examples/embeddings/sagemaker_embedding_endpoint.ipynb)\n- [Text Embedding Inference](../../examples/embeddings/text_embedding_inference.ipynb)\n- [TogetherAI](../../examples/embeddings/together.ipynb)\n- [Upstage](../../examples/embeddings/upstage.ipynb)\n- [VoyageAI](../../examples/embeddings/voyageai.ipynb)\n- [Nomic](../../examples/embeddings/nomic.ipynb)\n- [Fireworks AI](../../examples/embeddings/fireworks.ipynb)", "mimetype": "text/plain", "start_char_idx": 5521, "end_char_idx": 8628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9b3f4dd-d922-439c-a8fc-be1c6639103c": {"__data__": {"id_": "f9b3f4dd-d922-439c-a8fc-be1c6639103c", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b1cd9fc723729fb62e156324143f99c75482674", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "64c7e332d370833e4917dc82c19684000b703599ad2d44d6696d2269a8b8361e", "class_name": "RelatedNodeInfo"}}, "text": "# Models\n\nThere are a few primary ways you interact with models in LlamaIndex:\n\n- [LLMs](./llms.md)\n- [Embeddings](./embeddings.md)\n\nExperimental\n\n- [MultiModal](./multi_modal.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14cfdac9-4e3e-4918-8ff0-27c4a9fb256d": {"__data__": {"id_": "14cfdac9-4e3e-4918-8ff0-27c4a9fb256d", "embedding": null, "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987", "node_type": "4", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "eedc898af03ba40aca899bb6879dc43d8729e3a6871ac8571df5042e889d23b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9edfdc8b-14a2-4ef4-b70d-6d3e3ad8d220", "node_type": "1", "metadata": {}, "hash": "21d06c5d3e7893f40deae397023468ebe84fd7bf13e42adb0061eb0ab0bb0526", "class_name": "RelatedNodeInfo"}}, "text": "# Using LLMs\n\n## Concept\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it's from OpenAI, Hugging Face, or LangChain, so that you\ndon't have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n\n- Support for **text completion** and **chat** endpoints (details below)\n- Support for **streaming** and **non-streaming** endpoints\n- Support for **synchronous** and **asynchronous** endpoints\n\n## Usage Pattern\n\nThe following code snippet shows how you can get started using LLMs.\n\nIf you don't already have it, install your LLM:\n\n```\npip install llama-index-llms-openai\n```\n\nThen:\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# non-streaming\nresp = OpenAI().complete(\"Paul Graham is \")\nprint(resp)\n```\n\nFind more details on [standalone usage](./llms/usage_standalone.md) or [custom usage](./llms/usage_custom.md).\n\n## A Note on Tokenization\n\nBy default, LlamaIndex uses a global tokenizer for all token counting. This defaults to `cl100k` from tiktoken, which is the tokenizer to match the default LLM `gpt-3.5-turbo`.\n\nIf you change the LLM, you may need to update this tokenizer to ensure accurate token counts, chunking, and prompting.\n\nThe single requirement for a tokenizer is that it is a callable function, that takes a string, and returns a list.\n\nYou can set a global tokenizer like so:\n\n```python\nfrom llama_index.core import Settings\n\n# tiktoken\nimport tiktoken\n\nSettings.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n\n# huggingface\nfrom transformers import AutoTokenizer\n\nSettings.tokenizer = AutoTokenizer.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-beta\"\n)\n```\n\n## LLM Compatibility Tracking\n\nWhile LLMs are powerful, not every LLM is easy to set up. Furthermore, even with proper setup, some LLMs have trouble performing tasks that require strict instruction following.\n\nLlamaIndex offers integrations with nearly every LLM, but it can be often unclear if the LLM will work well out of the box, or if further customization is needed.\n\nThe tables below attempt to validate the **initial** experience with various LlamaIndex features for various LLMs. These notebooks serve as a best attempt to gauge performance, as well as how much effort and tweaking is needed to get things to function properly.\n\nGenerally, paid APIs such as OpenAI or Anthropic are viewed as more reliable. However, local open-source models have been gaining popularity due to their customizability and approach to transparency.\n\n**Contributing:** Anyone is welcome to contribute new LLMs to the documentation. Simply copy an existing notebook, setup and test your LLM, and open a PR with your results.\n\nIf you have ways to improve the setup for existing notebooks, contributions to change this are welcome!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9edfdc8b-14a2-4ef4-b70d-6d3e3ad8d220": {"__data__": {"id_": "9edfdc8b-14a2-4ef4-b70d-6d3e3ad8d220", "embedding": null, "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987", "node_type": "4", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "eedc898af03ba40aca899bb6879dc43d8729e3a6871ac8571df5042e889d23b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14cfdac9-4e3e-4918-8ff0-27c4a9fb256d", "node_type": "1", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "79da4afbb382d030169eab850b6fa2f438b919b1ce0d48dde5d3c11b749d397c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "896cc3dc-06ad-4ca1-a99c-6311ec30e707", "node_type": "1", "metadata": {}, "hash": "4aaedced53a3dd394a0a6f1e46b8e24921f002036e469692a4c68876ef328bd5", "class_name": "RelatedNodeInfo"}}, "text": "If you have ways to improve the setup for existing notebooks, contributions to change this are welcome!\n\n**Legend**\n\n- \u2705 = should work fine\n- \u26a0\ufe0f = sometimes unreliable, may need prompt engineering to improve\n- \ud83d\uded1 = usually unreliable, would need prompt engineering/fine-tuning to improve\n\n### Paid LLM APIs\n\n| Model Name                                                                                                               | Basic Query Engines | Router Query Engine | Sub Question Query Engine | Text2SQL | Pydantic Programs | Data Agents | <div style=\"width:290px\">Notes</div>    |\n| ------------------------------------------------------------------------------------------------------------------------ | ------------------- | ------------------- | ------------------------- | -------- | ----------------- | ----------- | --------------------------------------- |\n| [gpt-3.5-turbo](https://colab.research.google.com/drive/1vvdcf7VYNQA67NOxBHCyQvgb2Pu7iY_5?usp=sharing) (openai)          | \u2705                  | \u2705                  | \u2705                        | \u2705       | \u2705                | \u2705          |                                         |\n| [gpt-3.5-turbo-instruct](https://colab.research.google.com/drive/1Ne-VmMNYGOKUeECvkjurdKqMDpfqJQHE?usp=sharing) (openai) | \u2705                  | \u2705                  | \u2705                        | \u2705       | \u2705                | \u26a0\ufe0f          | Tool usage in data-agents seems flakey. |\n| [gpt-4](https://colab.research.google.com/drive/1QUNyCVt8q5G32XHNztGw4YJ2EmEkeUe8?usp=sharing) (openai)                  | \u2705                  | \u2705                  | \u2705                        | \u2705       | \u2705                | \u2705          |                                         |\n| [claude-3 opus](https://colab.research.google.com/drive/1xeFgAmSLpY_9w7bcGPvIcE8UuFSI3xjF?usp=sharing)                   | \u2705                  | \u26a0\ufe0f                  | \u2705                        | \u2705       | \u2705                | \u2705          |                                         |\n| [claude-3 sonnet](https://colab.research.google.com/drive/1xeFgAmSLpY_9w7bcGPvIcE8UuFSI3xjF?usp=sharing)                 | \u2705                  | \u2705                  | \u2705                        | \u2705       | \u2705                | \u26a0\ufe0f          | Prone to hallucinating tool inputs.     |\n| [claude-2](https://colab.research.google.com/drive/1IuHRN67MYOaLx2_AgJ9gWVtlK7bIvS1f?usp=sharing) (anthropic)            | \u2705                  | \u2705                  | \u2705                        | \u2705       | \u2705                | \u26a0\ufe0f          | Prone to hallucinating tool inputs.     |\n| [claude-instant-1.2](https://colab.research.google.com/drive/1ahq-2kXwCVCA_3xyC5UMWHyfAcjoG8Gp?usp=sharing) (anthropic)  | \u2705                  | \u2705                  | \u2705                        | \u2705       | \u2705                | \u26a0\ufe0f          | Prone to hallucinating tool inputs.     |\n\n### Open Source LLMs\n\nSince open source LLMs require large amounts of resources, the quantization is reported. Quantization is just a method for reducing the size of an LLM by shrinking the accuracy of calculations within the model. Research has shown that up to 4Bit quantization can be achieved for large LLMs without impacting performance too severely.", "mimetype": "text/plain", "start_char_idx": 3237, "end_char_idx": 6419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "896cc3dc-06ad-4ca1-a99c-6311ec30e707": {"__data__": {"id_": "896cc3dc-06ad-4ca1-a99c-6311ec30e707", "embedding": null, "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987", "node_type": "4", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "eedc898af03ba40aca899bb6879dc43d8729e3a6871ac8571df5042e889d23b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9edfdc8b-14a2-4ef4-b70d-6d3e3ad8d220", "node_type": "1", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "1d48f47872aded279b37542a55a8fca7307efd7046790d58966fd06836071bed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9baea55c-b96f-4b84-89a3-82ec48aaa859", "node_type": "1", "metadata": {}, "hash": "07604a78a59a902662feb4e0c9858ea7d7bbd6c19662076d2c259d3f7dc1546b", "class_name": "RelatedNodeInfo"}}, "text": "Research has shown that up to 4Bit quantization can be achieved for large LLMs without impacting performance too severely.\n\n| Model Name                                                                                                                           | Basic Query Engines | Router Query Engine | SubQuestion Query Engine | Text2SQL | Pydantic Programs | Data Agents | <div style=\"width:290px\">Notes</div>                                                                                                                                                |\n| ------------------------------------------------------------------------------------------------------------------------------------ | ------------------- | ------------------- | ------------------------ | -------- | ----------------- | ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [llama2-chat-7b 4bit](https://colab.research.google.com/drive/1ByiIaBqCwbH9QXJOQWqOfUdsq4LEFq-g?usp=sharing) (huggingface)           | \u2705                  | \ud83d\uded1                  | \ud83d\uded1                       | \ud83d\uded1       | \ud83d\uded1                | \u26a0\ufe0f          | Llama2 seems to be quite chatty, which makes parsing structured outputs difficult. Fine-tuning and prompt engineering likely required for better performance on structured outputs. |\n| [llama2-13b-chat](https://colab.research.google.com/drive/1dpIv3iYQCV4OBB8z2ZRS7y4wUfsfNlO3?usp=sharing) (replicate)                 | \u2705                  | \u2705                  | \ud83d\uded1                       | \u2705       | \ud83d\uded1                | \ud83d\uded1          | Our ReAct prompt expects structured outputs, which llama-13b struggles at                                                                                                           |\n| [llama2-70b-chat](https://colab.research.google.com/drive/11h_Av5RG3tGjuOrZ-VKifd9UzcRPeN1J?usp=sharing) (replicate)                 | \u2705                  | \u2705                  | \u2705                       | \u2705       | \ud83d\uded1                | \u26a0\ufe0f          | There are still some issues with parsing structured outputs, especially with pydantic programs.                                                                                     |\n| [Mistral-7B-instruct-v0.1 4bit](https://colab.research.google.com/drive/1-f5v48TnX5rGdaMdWTr8XsjTGrWZ6Q7Y?usp=sharing) (huggingface) | \u2705                  | \ud83d\uded1                  | \ud83d\uded1                       | \u26a0\ufe0f       | \u26a0\ufe0f                | \u26a0\ufe0f          | Mistral seems slightly more reliable for structured outputs compared to Llama2. Likely with some prompt engineering, it may do better.                                              |\n| [zephyr-7b-alpha](https://colab.research.google.com/drive/1asitB49g9LMGrlODgY2J-g_xRExRM_ud?usp=sharing) (huggingface)               | \u2705                  | \u2705                  | \u2705                       | \u2705       | \u2705                | \u26a0\ufe0f          | Overall, `zyphyr-7b-alpha` is appears to be more reliable than other open-source models of this size. Although it still hallucinates a bit, especially as an agent.                 |\n| [zephyr-7b-beta](https://colab.research.google.com/drive/1C55IGyJNDe14DsHkAIIpIjn76NvK5pc1?usp=sharing) (huggingface)                | \u2705                  | \u2705                  | \u2705                       | \u2705       | \ud83d\uded1                | \u2705          | Compared to `zyphyr-7b-alpha`, `zyphyr-7b-beta` appears to perform well as an agent however it fails for Pydantic Programs                                                          |\n| [stablelm-zephyr-3b](https://colab.research.google.com/drive/1X_hEUkV62wHmMty3tNLIfJtp4IC6QNYN?usp=sharing) (huggingface)            | \u2705                  | \u26a0\ufe0f                  | \u2705                       | \ud83d\uded1       | \u2705                | \ud83d\uded1          | stablelm-zephyr-3b does surprisingly well, especially for structured outputs (surpassing much larger models). It struggles a bit with text-to-SQL and tool use.", "mimetype": "text/plain", "start_char_idx": 6297, "end_char_idx": 10278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9baea55c-b96f-4b84-89a3-82ec48aaa859": {"__data__": {"id_": "9baea55c-b96f-4b84-89a3-82ec48aaa859", "embedding": null, "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987", "node_type": "4", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "eedc898af03ba40aca899bb6879dc43d8729e3a6871ac8571df5042e889d23b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "896cc3dc-06ad-4ca1-a99c-6311ec30e707", "node_type": "1", "metadata": {"filename": "llms.md", "author": "LlamaIndex"}, "hash": "5ba52d8f09e8a36a73167ddd0475599ce5ad9d04aea874c2c3ae292ae03b291f", "class_name": "RelatedNodeInfo"}}, "text": "It struggles a bit with text-to-SQL and tool use.                     |\n| [starling-lm-7b-alpha](https://colab.research.google.com/drive/1z2tZMr4M9wBFU6YX8fvAZ7WLTa3tWKEm?usp=sharing) (huggingface)          | \u2705                  | \ud83d\uded1                  | \u2705                       | \u26a0\ufe0f       | \u2705                | \u2705          | starling-lm-7b-alpha does surprisingly well on agent tasks. It struggles a bit with routing, and is inconsistent with text-to-SQL.                                                  |\n| [phi-3-mini-4k-instruct](https://github.com/run-llama/llama_index/tree/main/docs/docs/examples/benchmarks/phi-3-mini-4k-instruct.ipynb) (microsoft)          | \u2705                  | \u26a0\ufe0f                  | \u2705                       | \u2705      | \u2705               | \u26a0\ufe0f          | phi-3-mini-4k-instruct does well on basic RAG, text-to-SQL, Pydantic Programs and Query planning tasks. It struggles with routing, and Agentic tasks.                                                  |\n\n## Modules\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\nSee the full [list of modules](./llms/modules.md).\n\n## Further reading\n\n- [Embeddings](./embeddings.md)\n- [Prompts](./prompts/index.md)\n- [Local LLMs](./llms/local.md)\n- [Running Llama2 Locally](https://replicate.com/blog/run-llama-locally)", "mimetype": "text/plain", "start_char_idx": 10229, "end_char_idx": 11524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2102222-0cd7-4c3b-9434-4cded109a41e": {"__data__": {"id_": "d2102222-0cd7-4c3b-9434-4cded109a41e", "embedding": null, "metadata": {"filename": "local.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59eb08939c32fd4818c84ce9d7e7590dc52305d2", "node_type": "4", "metadata": {"filename": "local.md", "author": "LlamaIndex"}, "hash": "c572ab492f6864add769be9bc6f84d977a843b4333b4d4f22ee3badd92c9ac0a", "class_name": "RelatedNodeInfo"}}, "text": "# Using local models\n\nRelevant Resources:\n\n- [Using LlamaIndex with Local Models](https://colab.research.google.com/drive/16QMQePkONNlDpgiltOi7oRQgmB8dU5fl?usp=sharing)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc3d7afd-ca11-404d-a784-2dab32b61dcd": {"__data__": {"id_": "dc3d7afd-ca11-404d-a784-2dab32b61dcd", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a95eac841f49acc114f9ae658bbfcb61f581af5", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "c85629e3f9f414ed9fdef0a2280f6171d98f5ffea9bbe9bf185a3ae9a80d38df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c33f1e3-3346-4155-8fda-b07b7f09346b", "node_type": "1", "metadata": {}, "hash": "4a2aa873e337b6e88cc1d0b784a5cdc8d07cdd1496dbaf74182fe2db7bb492e1", "class_name": "RelatedNodeInfo"}}, "text": "# Available LLM integrations\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c33f1e3-3346-4155-8fda-b07b7f09346b": {"__data__": {"id_": "3c33f1e3-3346-4155-8fda-b07b7f09346b", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a95eac841f49acc114f9ae658bbfcb61f581af5", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "c85629e3f9f414ed9fdef0a2280f6171d98f5ffea9bbe9bf185a3ae9a80d38df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc3d7afd-ca11-404d-a784-2dab32b61dcd", "node_type": "1", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "3f161c08cdd8cce9e3a6190ef28ba786b3e03aeafe446a3a507e59ef95f2ac07", "class_name": "RelatedNodeInfo"}}, "text": "# Available LLM integrations\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.\n\n- [AI21](../../../examples/llm/ai21.ipynb)\n- [Anthropic](../../../examples/llm/anthropic.ipynb)\n- [AnyScale](../../../examples/llm/anyscale.ipynb)\n- [Azure OpenAI](../../../examples/llm/azure_openai.ipynb)\n- [Bedrock](../../../examples/llm/bedrock.ipynb)\n- [Clarifai](../../../examples/llm/clarifai.ipynb)\n- [Cohere](../../../examples/llm/cohere.ipynb)\n- [Dashscope](../../../examples/llm/dashscope.ipynb)\n- [Dashscope Multi-Modal](../../../examples/multi_modal/dashscope_multi_modal.ipynb)\n- [EverlyAI](../../../examples/llm/everlyai.ipynb)\n- [Fireworks](../../../examples/llm/fireworks.ipynb)\n- [Friendli](../../../examples/llm/friendli.ipynb)\n- [Gradient](../../../examples/llm/gradient_base_model.ipynb)\n- [Gradient Model Adapter](../../../examples/llm/gradient_model_adapter.ipynb)\n- [Groq](../../../examples/llm/groq.ipynb)\n- [HuggingFace Camel-7B](../../../examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb)\n- [HuggingFace StableLM](../../../examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb)\n- [HuggingFace Llama2](../../../examples/vector_stores/SimpleIndexDemoLlama-Local.ipynb)\n- [Konko](../../../examples/llm/konko.ipynb)\n- [LangChain](../../../examples/llm/langchain.ipynb)\n- [LiteLLM](../../../examples/llm/litellm.ipynb)\n- [Llama API](../../../examples/llm/llama_api.ipynb)\n- [Llama CPP](../../../examples/llm/llama_2_llama_cpp.ipynb)\n- [LocalAI](../../../examples/llm/localai.ipynb)\n- [MariTalk](../../../examples/llm/maritalk.ipynb)\n- [MistralAI](../../../examples/llm/mistralai.ipynb)\n- [Modelscope](../../../examples/llm/modelscope.ipynb)\n- [MonsterAPI](../../../examples/llm/monsterapi.ipynb)\n- [MyMagic](../../../examples/llm/mymagic.ipynb)\n- [NeutrinoAI](../../../examples/llm/neutrino.ipynb)\n- [Nvidia TensorRT-LLM](../../../examples/llm/nvidia_tensorrt.ipynb)\n- [Nvidia Triton](../../../examples/llm/nvidia_triton.ipynb)\n- [Ollama](../../../examples/llm/ollama.ipynb)\n- [OpenAI](../../../examples/llm/openai.ipynb)\n- [OpenLLM](../../../examples/llm/openllm.ipynb)\n- [OpenRouter](../../../examples/llm/openrouter.ipynb)\n- [PaLM](../../../examples/llm/palm.ipynb)\n- [Perplexity](../../../examples/llm/perplexity.ipynb)\n- [PremAI](../../../examples/llm/premai.ipynb)\n- [Portkey](../../../examples/llm/portkey.ipynb)\n- [Predibase](../../../examples/llm/predibase.ipynb)\n- [Replicate Llama2](../../../examples/llm/llama_2.ipynb)\n- [Replicate Vicuna](../../../examples/llm/vicuna.ipynb)\n- [Replicate Vector Index Llama2](../../../examples/vector_stores/SimpleIndexDemoLlama2.ipynb)\n- [RunGPT](../../../examples/llm/rungpt.ipynb)\n- [SageMaker](../../../examples/llm/sagemaker_endpoint_llm.ipynb)\n- [Solar](../../../examples/llm/solar.ipynb)\n- [Together.ai](../../../examples/llm/together.ipynb)\n- [Unify AI](../../../examples/llm/unify.ipynb)\n- [Upstage](../../../examples/llm/upstage.ipynb)\n- [Vertex](../../../examples/llm/vertex.ipynb)\n- [vLLM](../../../examples/llm/vllm.ipynb)\n- [Xorbits Inference](../../../examples/llm/xinference_local_deployment.ipynb)\n- [Yi](../../../examples/llm/yi.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfcc0107-f2e4-45a7-bf3a-7e1c2bff1b30": {"__data__": {"id_": "dfcc0107-f2e4-45a7-bf3a-7e1c2bff1b30", "embedding": null, "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b", "node_type": "4", "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "hash": "f0bab16bf2e77cba8f512a0626545562856d287f9fef98e315b39dc5e2fe988a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59af9214-fe9f-44c1-b909-4693911612b3", "node_type": "1", "metadata": {}, "hash": "9a8f2ab4633e15f2ceaf593178346093dd5633fffd9d88a3880925af6457cefb", "class_name": "RelatedNodeInfo"}}, "text": "# Customizing LLMs within LlamaIndex Abstractions\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n\nBy default, we use OpenAI's `gpt-3.5-turbo` model. But you may choose to customize\nthe underlying LLM being used.\n\nBelow we show a few examples of LLM customization. This includes\n\n- changing the underlying LLM\n- changing the number of output tokens (for OpenAI, Cohere, or AI21)\n- having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n\n## Example: Changing the underlying LLM\n\nAn example snippet of customizing the LLM being used is shown below.\nIn this example, we use `gpt-4` instead of `gpt-3.5-turbo`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-instruct`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`.\n\nNote that\nyou may also plug in any LLM shown on Langchain's\n[LLM](https://python.langchain.com/docs/integrations/llms/) page.\n\n```python\nfrom llama_index.core import KeywordTableIndex, SimpleDirectoryReader\nfrom llama_index.llms.openai import OpenAI\n\n# alternatively\n# from langchain.llms import ...\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\n\n# define LLM\nllm = OpenAI(temperature=0.1, model=\"gpt-4\")\n\n# build index\nindex = KeywordTableIndex.from_documents(documents, llm=llm)\n\n# get response from query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\n    \"What did the author do after his time at Y Combinator?\"\n)\n```\n\n## Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\n\nThe number of output tokens is usually set to some low number by default (for instance,\nwith OpenAI the default is 256).\n\nFor OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter\n(or maxTokens for AI21). We will handle text chunking/calculations under the hood.\n\n```python\nfrom llama_index.core import KeywordTableIndex, SimpleDirectoryReader\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\n\n# define global LLM\nSettings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=512)\n```\n\n## Example: Explicitly configure `context_window` and `num_output`\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `Settings` since the information is not available by default.\n\n```python\nfrom llama_index.core import KeywordTableIndex, SimpleDirectoryReader\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\n\n\n# set context window\nSettings.context_window = 4096\n# set number of output tokens\nSettings.num_output = 256\n\n# define LLM\nSettings.llm = OpenAI(\n    temperature=0,\n    model=\"gpt-3.5-turbo\",\n    max_tokens=num_output,\n)\n```\n\n## Example: Using a HuggingFace LLM\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a [local embeddings model](../embeddings.md).\n\nMany open-source models from HuggingFace require either some preamble before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.\n\nBelow, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found [here](https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b).\n\n```python\nfrom llama_index.core import PromptTemplate\n\n\n# Transform a string into input zephyr-specific input\ndef completion_to_prompt(completion):\n    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59af9214-fe9f-44c1-b909-4693911612b3": {"__data__": {"id_": "59af9214-fe9f-44c1-b909-4693911612b3", "embedding": null, "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b", "node_type": "4", "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "hash": "f0bab16bf2e77cba8f512a0626545562856d287f9fef98e315b39dc5e2fe988a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfcc0107-f2e4-45a7-bf3a-7e1c2bff1b30", "node_type": "1", "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "hash": "60cf76ad3a9c77ab65930572f1546cc3e1386a72ccee30d2d07ac53d32bf1e0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "511386b0-0f77-488b-af76-53e45eb182fd", "node_type": "1", "metadata": {}, "hash": "6660313797fa8738f9d5612f72b619af3034236494d3341087e31e8417f268fd", "class_name": "RelatedNodeInfo"}}, "text": "# Transform a list of chat messages into zephyr-specific input\ndef messages_to_prompt(messages):\n    prompt = \"\"\n    for message in messages:\n        if message.role == \"system\":\n            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n        elif message.role == \"user\":\n            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n        elif message.role == \"assistant\":\n            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n\n    # ensure we start with a system prompt, insert blank if needed\n    if not prompt.startswith(\"<|system|>\\n\"):\n        prompt = \"<|system|>\\n</s>\\n\" + prompt\n\n    # add final assistant prompt\n    prompt = prompt + \"<|assistant|>\\n\"\n\n    return prompt\n\n\nimport torch\nfrom llama_index.llms.huggingface import HuggingFaceLLM\nfrom llama_index.core import Settings\n\nSettings.llm = HuggingFaceLLM(\n    model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n    tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n    context_window=3900,\n    max_new_tokens=256,\n    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    messages_to_prompt=messages_to_prompt,\n    completion_to_prompt=completion_to_prompt,\n    device_map=\"auto\",\n)\n```\n\nSome models will raise errors if all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`. Below is an example of configuring the predictor to remove this before passing the inputs to the model:\n\n```python\nHuggingFaceLLM(\n    # ...\n    tokenizer_outputs_to_remove=[\"token_type_ids\"]\n)\n```\n\nA full API reference can be found [here](../../../api_reference/llms/huggingface.md).\n\nSeveral example notebooks are also listed below:\n\n- [StableLM](../../../examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb)\n- [Camel](../../../examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb)\n\n## Example: Using a Custom LLM Model - Advanced\n\nTo use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface)\nYou will be responsible for passing the text to the model and returning the newly generated tokens.\n\nThis implementation could be some local model, or even a wrapper around your own API.\n\nNote that for a completely private experience, also setup a [local embeddings model](../embeddings.md).\n\nHere is a small boilerplate example:\n\n```python\nfrom typing import Optional, List, Mapping, Any\n\nfrom llama_index.core import SimpleDirectoryReader, SummaryIndex\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.llms import (\n    CustomLLM,\n    CompletionResponse,\n    CompletionResponseGen,\n    LLMMetadata,\n)\nfrom llama_index.core.llms.callbacks import llm_completion_callback\nfrom llama_index.core import Settings\n\n\nclass OurLLM(CustomLLM):\n    context_window: int = 3900\n    num_output: int = 256\n    model_name: str = \"custom\"\n    dummy_response: str = \"My response\"\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=self.context_window,\n            num_output=self.num_output,\n            model_name=self.model_name,\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        return CompletionResponse(text=self.dummy_response)\n\n    @llm_completion_callback()\n    def stream_complete(\n        self, prompt: str, **kwargs: Any\n    ) -> CompletionResponseGen:\n        response = \"\"\n        for token in self.dummy_response:\n            response += token\n            yield CompletionResponse(text=response, delta=token)\n\n\n# define our LLM\nSettings.llm = OurLLM()\n\n# define embed model\nSettings.embed_model = \"local:BAAI/bge-base-en-v1.5\"", "mimetype": "text/plain", "start_char_idx": 3923, "end_char_idx": 7648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "511386b0-0f77-488b-af76-53e45eb182fd": {"__data__": {"id_": "511386b0-0f77-488b-af76-53e45eb182fd", "embedding": null, "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b", "node_type": "4", "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "hash": "f0bab16bf2e77cba8f512a0626545562856d287f9fef98e315b39dc5e2fe988a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59af9214-fe9f-44c1-b909-4693911612b3", "node_type": "1", "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}, "hash": "042bd7e01d81b286753c599478cebcbdc54a7b24e68ae6d294d81090a4e546f8", "class_name": "RelatedNodeInfo"}}, "text": "# Load the your data\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\nindex = SummaryIndex.from_documents(documents)\n\n# Query and print response\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\nprint(response)\n```\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it's capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\nA list of all default internal prompts is available [here](https://github.com/run-llama/llama_index/blob/main/llama_index/prompts/default_prompts.py), and chat-specific prompts are listed [here](https://github.com/run-llama/llama_index/blob/main/llama_index/prompts/chat_prompts.py). You can also implement [your own custom prompts](../prompts/index.md).", "mimetype": "text/plain", "start_char_idx": 7651, "end_char_idx": 8936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d68da8e-8a9c-425f-a5fd-8d111fe93921": {"__data__": {"id_": "5d68da8e-8a9c-425f-a5fd-8d111fe93921", "embedding": null, "metadata": {"filename": "usage_standalone.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a2118da9168600fea22ccefe226ebe714f2e6c2", "node_type": "4", "metadata": {"filename": "usage_standalone.md", "author": "LlamaIndex"}, "hash": "d091f37973b7a7623ca33717629a3fdca58ab2d175d9cde8da6a830925e48389", "class_name": "RelatedNodeInfo"}}, "text": "# Using LLMs as standalone modules\n\nYou can use our LLM modules on their own.\n\n## Text Completion Example\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# non-streaming\ncompletion = OpenAI().complete(\"Paul Graham is \")\nprint(completion)\n\n# using streaming endpoint\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI()\ncompletions = llm.stream_complete(\"Paul Graham is \")\nfor completion in completions:\n    print(completion.delta, end=\"\")\n```\n\n## Chat Example\n\n```python\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.llms.openai import OpenAI\n\nmessages = [\n    ChatMessage(\n        role=\"system\", content=\"You are a pirate with a colorful personality\"\n    ),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n```\n\nCheck out our [modules section](modules.md) for usage guides for each LLM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88f8da1f-8bdb-4d3e-bcbc-70d3c0ffccc8": {"__data__": {"id_": "88f8da1f-8bdb-4d3e-bcbc-70d3c0ffccc8", "embedding": null, "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc2edf5349b16d42d5547db8da4d674ea3f15254", "node_type": "4", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "d0629c7f3007e0b50d2549f6c7652fd35f03f325c7914c6ef009c4e9a66bd6a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2059480-fe0e-41b6-8653-0cb34fa5f4c5", "node_type": "1", "metadata": {}, "hash": "a07393d5e07a904a2d911c1571b5cf28150e290496634f0d35eb7edb7d1ca1e6", "class_name": "RelatedNodeInfo"}}, "text": "# [Beta] Multi-modal models\n\n## Concept\n\nLarge language models (LLMs) are text-in, text-out. Large Multi-modal Models (LMMs) generalize this beyond the text modalities. For instance, models such as GPT-4V allow you to jointly input both images and text, and output text.\n\nWe've included a base `MultiModalLLM` abstraction to allow for text+image models. **NOTE**: This naming is subject to change!\n\n## Usage Pattern\n\n1. The following code snippet shows how you can get started using LMMs e.g. with GPT-4V.\n\n```python\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index.core.multi_modal_llms.generic_utils import load_image_urls\nfrom llama_index.core import SimpleDirectoryReader\n\n# load image documents from urls\nimage_documents = load_image_urls(image_urls)\n\n# load image documents from local directory\nimage_documents = SimpleDirectoryReader(local_directory).load_data()\n\n# non-streaming\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_KEY, max_new_tokens=300\n)\nresponse = openai_mm_llm.complete(\n    prompt=\"what is in the image?\", image_documents=image_documents\n)\n```\n\n2. The following code snippet shows how you can build MultiModal Vector Stores/Index.\n\n```python\nfrom llama_index.core.indices import MultiModalVectorStoreIndex\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import SimpleDirectoryReader, StorageContext\n\nimport qdrant_client\nfrom llama_index.core import SimpleDirectoryReader\n\n# Create a local Qdrant vector store\nclient = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n\n# if you only need image_store for image retrieval,\n# you can remove text_sotre\ntext_store = QdrantVectorStore(\n    client=client, collection_name=\"text_collection\"\n)\nimage_store = QdrantVectorStore(\n    client=client, collection_name=\"image_collection\"\n)\n\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n# Load text and image documents from local folder\ndocuments = SimpleDirectoryReader(\"./data_folder/\").load_data()\n# Create the MultiModal index\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)\n```\n\n3. The following code snippet shows how you can use MultiModal Retriever and Query Engine.\n\n```python\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.query_engine import SimpleMultiModalQueryEngine\n\nretriever_engine = index.as_retriever(\n    similarity_top_k=3, image_similarity_top_k=3\n)\n\n# retrieve more information from the GPT4V response\nretrieval_results = retriever_engine.retrieve(response)\n\n# if you only need image retrieval without text retrieval\n# you can use `text_to_image_retrieve`\n# retrieval_results = retriever_engine.text_to_image_retrieve(response)\n\nqa_tmpl_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_tmpl = PromptTemplate(qa_tmpl_str)\n\nquery_engine = index.as_query_engine(\n    multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n)\n\nquery_str = \"Tell me more about the Porsche\"\nresponse = query_engine.query(query_str)\n```\n\n**Legend**\n\n- \u2705 = should work fine\n- \u26a0\ufe0f = sometimes unreliable, may need more tuning to improve\n- \ud83d\uded1 = not available at the moment.\n\n### End to End Multi-Modal Work Flow\n\nThe tables below attempt to show the **initial** steps with various LlamaIndex features for building your own Multi-Modal RAGs (Retrieval Augmented Generation). You can combine different modules/steps together for composing your own Multi-Modal RAG orchestration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2059480-fe0e-41b6-8653-0cb34fa5f4c5": {"__data__": {"id_": "e2059480-fe0e-41b6-8653-0cb34fa5f4c5", "embedding": null, "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc2edf5349b16d42d5547db8da4d674ea3f15254", "node_type": "4", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "d0629c7f3007e0b50d2549f6c7652fd35f03f325c7914c6ef009c4e9a66bd6a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88f8da1f-8bdb-4d3e-bcbc-70d3c0ffccc8", "node_type": "1", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "779cdfdd57a7d321e392254b6ef9a257094b24df058c872f50ce30f647896c5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de8feeec-0063-4926-8d98-6720c89974b2", "node_type": "1", "metadata": {}, "hash": "ffff5e53b9031a9a12227dba8b298063e83892583c830c38f92c4b5739472c48", "class_name": "RelatedNodeInfo"}}, "text": "You can combine different modules/steps together for composing your own Multi-Modal RAG orchestration.\n\n| Query Type | Data Sources<br>for MultiModal<br>Vector Store/Index | MultiModal<br>Embedding                | Retriever                                        | Query<br>Engine        | Output<br>Data<br>Type                   |\n| ---------- | ---------------------------------------------------- | -------------------------------------- | ------------------------------------------------ | ---------------------- | ---------------------------------------- |\n| Text \u2705    | Text \u2705                                              | Text \u2705                                | Top-k retrieval \u2705<br>Simple Fusion retrieval \u2705 | Simple Query Engine \u2705 | Retrieved Text \u2705<br>Generated Text \u2705   |\n| Image \u2705   | Image \u2705                                             | Image \u2705<br>Image to Text Embedding \u2705 | Top-k retrieval \u2705<br>Simple Fusion retrieval \u2705 | Simple Query Engine \u2705 | Retrieved Image \u2705<br>Generated Image \ud83d\uded1 |\n| Audio \ud83d\uded1   | Audio \ud83d\uded1                                             | Audio \ud83d\uded1                               | \ud83d\uded1                                               | \ud83d\uded1                     | Audio \ud83d\uded1                                 |\n| Video \ud83d\uded1   | Video \ud83d\uded1                                             | Video \ud83d\uded1                               | \ud83d\uded1                                               | \ud83d\uded1                     | Video \ud83d\uded1                                 |\n\n### Multi-Modal LLM Models\n\nThese notebooks serve as examples how to leverage and integrate Multi-Modal LLM model, Multi-Modal embeddings, Multi-Modal vector stores, Retriever, Query engine for composing Multi-Modal Retrieval Augmented Generation (RAG) orchestration.\n\n| Multi-Modal<br>Vision Models                                                            | Single<br>Image<br>Reasoning | Multiple<br>Images<br>Reasoning | Image<br>Embeddings | Simple<br>Query<br>Engine | Pydantic<br>Structured<br>Output |\n| --------------------------------------------------------------------------------------- | ---------------------------- | ------------------------------- | ------------------- | ------------------------- | -------------------------------- |\n| [GPT4V](../../examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb)<br>(OpenAI API)   | \u2705                           | \u2705                              | \ud83d\uded1                  | \u2705                        | \u2705                               |\n| [GPT4V-Azure](../../examples/multi_modal/azure_openai_multi_modal.ipynb)<br>(Azure API) | \u2705                           | \u2705                              | \ud83d\uded1                  | \u2705                        | \u2705                               |\n| [Gemini](../../examples/multi_modal/gemini.ipynb)<br>(Google)                           | \u2705                           | \u2705                              | \ud83d\uded1                  | \u2705                        | \u2705                               |\n| [CLIP](../../examples/multi_modal/image_to_image_retrieval.ipynb)<br>(Local host)       | \ud83d\uded1                           | \ud83d\uded1                              | \u2705                  | \ud83d\uded1                        | \ud83d\uded1                               |\n| [LLaVa](../../examples/multi_modal/llava_multi_modal_tesla_10q.ipynb)<br>(replicate)    | \u2705                           | \ud83d\uded1                              | \ud83d\uded1                  | \u2705                        | \u26a0\ufe0f                               |\n| [Fuyu-8B](../../examples/multi_modal/replicate_multi_modal.ipynb)<br>(replicate)        | \u2705                           | \ud83d\uded1                              | \ud83d\uded1                  | \u2705                        | \u26a0\ufe0f                               |\n| [ImageBind<br>](https://imagebind.metademolab.com/)[To integrate]                       | \ud83d\uded1                           | \ud83d\uded1                              | \u2705                  | \ud83d\uded1                        | \ud83d\uded1                               |\n| [MiniGPT-4<br>](../../examples/multi_modal/replicate_multi_modal.ipynb)                 | \u2705                           | \ud83d\uded1                              | \ud83d\uded1                  | \u2705                        | \u26a0\ufe0f                               |\n| [CogVLM<br>](https://github.com/THUDM/CogVLM)                                           | \u2705                           | \ud83d\uded1                              | \ud83d\uded1                  | \u2705                        | \u26a0\ufe0f                               |\n| [Qwen-VL<br>](https://arxiv.org/abs/2308.12966)[To integrate]                           | \u2705                           | \ud83d\uded1                              | \ud83d\uded1                  | \u2705                        | \u26a0\ufe0f                               |\n\n### Multi Modal Vector Stores\n\nBelow table lists some vector stores supporting Multi-Modal use cases. Our LlamaIndex built-in `MultiModalVectorStoreIndex` supports building separate vector stores for image and text embedding vector stores. `MultiModalRetriever`, and `SimpleMultiModalQueryEngine` support text to text/image and image to image retrieval and simple ranking fusion functions for combining text and image retrieval results.", "mimetype": "text/plain", "start_char_idx": 3700, "end_char_idx": 8721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de8feeec-0063-4926-8d98-6720c89974b2": {"__data__": {"id_": "de8feeec-0063-4926-8d98-6720c89974b2", "embedding": null, "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc2edf5349b16d42d5547db8da4d674ea3f15254", "node_type": "4", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "d0629c7f3007e0b50d2549f6c7652fd35f03f325c7914c6ef009c4e9a66bd6a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2059480-fe0e-41b6-8653-0cb34fa5f4c5", "node_type": "1", "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}, "hash": "faf3bde253acabbf637770fd97a9ba42d3c3cde47896a921ee3f1798588995ea", "class_name": "RelatedNodeInfo"}}, "text": "| Multi-Modal<br>Vector Stores | Single<br>Vector<br>Store | Multiple<br>Vector<br>Stores | Text<br>Embedding | Image<br>Embedding |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------- | --------------------------- | --------------------------------------------------------- | ------------------------------------------------------- |\n| [LLamaIndex self-built<br>MultiModal Index](../../examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb) | \ud83d\uded1 | \u2705 | Can be arbitrary<br>text embedding<br>(Default is GPT3.5) | Can be arbitrary<br>Image embedding<br>(Default is CLIP) |\n| [Chroma](../../examples/multi_modal/ChromaMultiModalDemo.ipynb) | \u2705 | \ud83d\uded1 | CLIP \u2705 | CLIP \u2705 |\n| [Weaviate](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-bind)<br>[To integrate] | \u2705 | \ud83d\uded1 | CLIP \u2705<br>ImageBind \u2705 | CLIP \u2705<br>ImageBind \u2705 |\n\n## Multi-Modal LLM Modules\n\nWe support integrations with GPT4-V, Anthropic (Opus, Sonnet), Gemini (Google), CLIP (OpenAI), BLIP (Salesforce), and Replicate (LLaVA, Fuyu-8B, MiniGPT-4, CogVLM), and more.\n\n- [OpenAI](../../examples/multi_modal/openai_multi_modal.ipynb)\n- [Gemini](../../examples/multi_modal/gemini.ipynb)\n- [Anthropic](../../examples/multi_modal/anthropic_multi_modal.ipynb)\n- [Replicate](../../examples/multi_modal/replicate_multi_modal.ipynb)\n- [Pydantic Multi-Modal](../../examples/multi_modal/multi_modal_pydantic.ipynb)\n- [GPT-4v COT Experiments](../../examples/multi_modal/gpt4v_experiments_cot.ipynb)\n- [Llava Tesla 10q](../../examples/multi_modal/llava_multi_modal_tesla_10q.ipynb)\n\n## Multi-Modal Retrieval Augmented Generation\n\nWe support Multi-Modal Retrieval Augmented Generation with different Multi-Modal LLMs with Multi-Modal vector stores.\n\n- [GPT-4v Retrieval](../../examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb)\n- [Multi-Modal Retrieval](../../examples/multi_modal/multi_modal_retrieval.ipynb)\n- [Image-to-Image Retrieval](../../examples/multi_modal/image_to_image_retrieval.ipynb)\n- [Chroma Multi-Modal](../../examples/multi_modal/ChromaMultiModalDemo.ipynb)\n\n## Evaluation\n\nWe support basic evaluation for Multi-Modal LLM and Retrieval Augmented Generation.\n\n- [Multi-Modal RAG Eval](../../examples/evaluation/multi_modal/multi_modal_rag_evaluation.ipynb)", "mimetype": "text/plain", "start_char_idx": 8722, "end_char_idx": 11105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40eed48a-468c-45e6-8754-b488fbe02c9d": {"__data__": {"id_": "40eed48a-468c-45e6-8754-b488fbe02c9d", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0a9fc269f28231be4b85de9c8bb64f6dd0efd59", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c0095cf8ba9c534173627bdcf42218f1ca0c469434383aca718a56a145ac61ca", "class_name": "RelatedNodeInfo"}}, "text": "# Prompts\n\n## Concept\n\nPrompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion,\nperform traversal during querying, and to synthesize the final answer.\n\nLlamaIndex uses a set of [default prompt templates](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py) that work well out of the box.\n\nIn addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` [here](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/chat_prompts.py).\n\nUsers may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\n\n## Usage Pattern\n\nUsing prompts is simple.\n\n```python\nfrom llama_index.core import PromptTemplate\n\ntemplate = (\n    \"We have provided context information below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n)\nqa_template = PromptTemplate(template)\n\n# you can create text prompt (for completion API)\nprompt = qa_template.format(context_str=..., query_str=...)\n\n# or easily convert to message prompts (for chat API)\nmessages = qa_template.format_messages(context_str=..., query_str=...)\n```\n\nSee our [Usage Pattern Guide](./usage_pattern.md) for more details.\n\n## Example Guides\n\nSimple Customization Examples\n\n- [Completion prompts](../../../examples/customization/prompts/completion_prompts.ipynb)\n- [Chat prompts](../../../examples/customization/prompts/chat_prompts.ipynb)\n- [Prompt Mixin](../../../examples/prompts/prompt_mixin.ipynb)\n\nPrompt Engineering Guides\n\n- [Advanced Prompts](../../../examples/prompts/advanced_prompts.ipynb)\n- [RAG Prompts](../../../examples/prompts/prompts_rag.ipynb)\n\nExperimental\n\n- [Prompt Optimization](../../../examples/prompts/prompt_optimization.ipynb)\n- [Emotion Prompting](../../../examples/prompts/emotion_prompt.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b47abb54-6d58-4d05-9566-73353805deff": {"__data__": {"id_": "b47abb54-6d58-4d05-9566-73353805deff", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f9f0fda622658f6cda0735aa0c2948a60c5f9a5", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "8ed812f4ffd2160f1deb53bf4ab9dcfda212f1cc37687fa026c2ccc55db08416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dd800ac-82c6-4883-8ffb-1de7a0b4dbe5", "node_type": "1", "metadata": {}, "hash": "780658caf0e570ea621173e2277c3dcd7df30a7794f9398cf98f23d491086915", "class_name": "RelatedNodeInfo"}}, "text": "## Usage Pattern\n\n### Defining a custom prompt\n\nDefining a custom prompt is as simple as creating a format string\n\n```python\nfrom llama_index.core import PromptTemplate\n\ntemplate = (\n    \"We have provided context information below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n)\nqa_template = PromptTemplate(template)\n\n# you can create text prompt (for completion API)\nprompt = qa_template.format(context_str=..., query_str=...)\n\n# or easily convert to message prompts (for chat API)\nmessages = qa_template.format_messages(context_str=..., query_str=...)\n```\n\n> Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `PromptTemplate`). Now you can directly specify `PromptTemplate(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.\n\nYou can also define a template from chat messages\n\n```python\nfrom llama_index.core import ChatPromptTemplate\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\nmessage_templates = [\n    ChatMessage(content=\"You are an expert system.\", role=MessageRole.SYSTEM),\n    ChatMessage(\n        content=\"Generate a short story about {topic}\",\n        role=MessageRole.USER,\n    ),\n]\nchat_template = ChatPromptTemplate(message_templates=message_templates)\n\n# you can create message prompts (for chat API)\nmessages = chat_template.format_messages(topic=...)\n\n# or easily convert to text prompt (for completion API)\nprompt = chat_template.format(topic=...)\n```\n\n### Getting and Setting Custom Prompts\n\nSince LlamaIndex is a multi-step pipeline, it's important to identify the operation that you want to modify and pass in the custom prompt at the right place.\n\nFor instance, prompts are used in response synthesizer, retrievers, index construction, etc; some of these modules are nested in other modules (synthesizer is nested in query engine).\n\nSee [this guide](../../../examples/prompts/prompt_mixin.ipynb) for full details on accessing/customizing prompts.\n\n#### Commonly Used Prompts\n\nThe most commonly used prompts will be the `text_qa_template` and the `refine_template`.\n\n- `text_qa_template` - used to get an initial answer to a query using retrieved nodes\n- `refine_template` - used when the retrieved text does not fit into a single LLM call with `response_mode=\"compact\"` (the default), or when more than one node is retrieved using `response_mode=\"refine\"`. The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.\n\n#### Accessing Prompts\n\nYou can call `get_prompts` on many modules in LlamaIndex to get a flat list of prompts used within the module and nested submodules.\n\nFor instance, take a look at the following snippet.\n\n```python\nquery_engine = index.as_query_engine(response_mode=\"compact\")\nprompts_dict = query_engine.get_prompts()\nprint(list(prompts_dict.keys()))\n```\n\nYou might get back the following keys:\n\n```\n['response_synthesizer:text_qa_template', 'response_synthesizer:refine_template']\n```\n\nNote that prompts are prefixed by their sub-modules as \"namespaces\".\n\n#### Updating Prompts\n\nYou can customize prompts on any module that implements `get_prompts` with the `update_prompts` function. Just pass in argument values with the keys equal to the keys you see in the prompt dictionary\nobtained through `get_prompts`.\n\ne.g. regarding the example above, we might do the following\n\n```python\n# shakespeare!\nqa_prompt_tmpl_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query in the style of a Shakespeare play.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n\nquery_engine.update_prompts(\n    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n)\n```\n\n#### Modify prompts used in query engine\n\nFor query engines, you can also pass in custom prompts directly during query-time (i.e. for executing a query against an index and synthesizing the final response).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dd800ac-82c6-4883-8ffb-1de7a0b4dbe5": {"__data__": {"id_": "1dd800ac-82c6-4883-8ffb-1de7a0b4dbe5", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f9f0fda622658f6cda0735aa0c2948a60c5f9a5", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "8ed812f4ffd2160f1deb53bf4ab9dcfda212f1cc37687fa026c2ccc55db08416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b47abb54-6d58-4d05-9566-73353805deff", "node_type": "1", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "8fe9514d372961fb7ef0037c63ebb88e3ed927ae317faab935a7fabb15582a59", "class_name": "RelatedNodeInfo"}}, "text": "for executing a query against an index and synthesizing the final response).\n\nThere are also two equivalent ways to override the prompts:\n\n1. via the high-level API\n\n```python\nquery_engine = index.as_query_engine(\n    text_qa_template=custom_qa_prompt, refine_template=custom_refine_prompt\n)\n```\n\n2. via the low-level composition API\n\n```python\nretriever = index.as_retriever()\nsynth = get_response_synthesizer(\n    text_qa_template=custom_qa_prompt, refine_template=custom_refine_prompt\n)\nquery_engine = RetrieverQueryEngine(retriever, response_synthesizer)\n```\n\nThe two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity. You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.\n\nFor more details on which classes use which prompts, please visit\n[Query class references](../../../api_reference/response_synthesizers/index.md).\n\nCheck out the [reference documentation](../../../api_reference/prompts/index.md) for a full set of all prompts.\n\n#### Modify prompts used in index construction\n\nSome indices use different types of prompts during construction\n(**NOTE**: the most common ones, `VectorStoreIndex` and `SummaryIndex`, don't use any).\n\nFor instance, `TreeIndex` uses a summary prompt to hierarchically\nsummarize the nodes, and `KeywordTableIndex` uses a keyword extract prompt to extract keywords.\n\nThere are two equivalent ways to override the prompts:\n\n1. via the default nodes constructor\n\n```python\nindex = TreeIndex(nodes, summary_template=custom_prompt)\n```\n\n2. via the documents constructor.\n\n```python\nindex = TreeIndex.from_documents(docs, summary_template=custom_prompt)\n```\n\nFor more details on which index uses which prompts, please visit\n[Index class references](../../../api_reference/indices/index.md).\n\n### [Advanced] Advanced Prompt Capabilities\n\nIn this section we show some advanced prompt capabilities in LlamaIndex.\n\nRelated Guides:\n\n- [Advanced Prompts](../../../examples/prompts/advanced_prompts.ipynb)\n- [Prompt Engineering for RAG](../../../examples/prompts/prompts_rag.ipynb)\n\n#### Partial Formatting\n\nPartially format a prompt, filling in some variables while leaving others to be filled in later.\n\n```python\nfrom llama_index.core import PromptTemplate\n\nprompt_tmpl_str = \"{foo} {bar}\"\nprompt_tmpl = PromptTemplate(prompt_tmpl_str)\npartial_prompt_tmpl = prompt_tmpl.partial_format(foo=\"abc\")\n\nfmt_str = partial_prompt_tmpl.format(bar=\"def\")\n```\n\n#### Template Variable Mappings\n\nLlamaIndex prompt abstractions generally expect certain keys. E.g. our `text_qa_prompt` expects `context_str` for context and `query_str` for the user query.\n\nBut if you're trying to adapt a string template for use with LlamaIndex, it can be annoying to change out the template variables.\n\nInstead, define `template_var_mappings`:\n\n```python\ntemplate_var_mappings = {\"context_str\": \"my_context\", \"query_str\": \"my_query\"}\n\nprompt_tmpl = PromptTemplate(\n    qa_prompt_tmpl_str, template_var_mappings=template_var_mappings\n)\n```\n\n#### Function Mappings\n\nPass in functions as template variables instead of fixed values.\n\nThis is quite advanced and powerful; allows you to do dynamic few-shot prompting, etc.\n\nHere's an example of reformatting the `context_str`.\n\n```python\ndef format_context_fn(**kwargs):\n    # format context with bullet points\n    context_list = kwargs[\"context_str\"].split(\"\\n\\n\")\n    fmtted_context = \"\\n\\n\".join([f\"- {c}\" for c in context_list])\n    return fmtted_context\n\n\nprompt_tmpl = PromptTemplate(\n    qa_prompt_tmpl_str, function_mappings={\"context_str\": format_context_fn}\n)\n\nprompt_tmpl.format(context_str=\"context\", query_str=\"query\")\n```", "mimetype": "text/plain", "start_char_idx": 4330, "end_char_idx": 8028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77ae39e2-9b9b-42c2-afce-463f8733d32e": {"__data__": {"id_": "77ae39e2-9b9b-42c2-afce-463f8733d32e", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46f4b004e019779b647ea31563438eea98a10cb0", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "2ab2b7c3d15595f54870f7acb31ce408ffd8d66058c93e1ba63dfb8a147bebbc", "class_name": "RelatedNodeInfo"}}, "text": "# Callbacks\n\n## Concept\n\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library.\nUsing the callback manager, as many callbacks as needed can be added.\n\nIn addition to logging data related to events, you can also track the duration and number of occurrences\nof each event.\n\nFurthermore, a trace map of events is also recorded, and callbacks can use this data\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\nafter most operations.\n\n**Callback Event Types**\nWhile each callback may not leverage each event type, the following events are available to be tracked:\n\n- `CHUNKING` -> Logs for the before and after of text splitting.\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\n- `EMBEDDING` -> Logs for the number of texts embedded.\n- `LLM` -> Logs for the template and response of LLM calls.\n- `QUERY` -> Keeps track of the start and end of each query.\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\n- `TREE` -> Logs for the summary and level of summaries generated.\n- `SUB_QUESTION` -> Log for a generated sub question and answer.\n\nYou can implement your own callback to track and trace these events, or use an existing callback.\n\n## Modules\n\nCurrently supported callbacks are as follows:\n\n- [TokenCountingHandler](../../../examples/callbacks/TokenCountingHandler.ipynb) -> Flexible token counting for prompt, completion, and embedding token usage. See [the migration details](../callbacks/token_counting_migration.md)\n- [LlamaDebugHanlder](../../../examples/callbacks/LlamaDebugHandler.ipynb) -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\n- [WandbCallbackHandler](../../../examples/callbacks/WandbCallbackHandler.ipynb) -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at [Wandb](https://docs.wandb.ai/guides/prompts/quickstart)\n- [AimCallback](../../../examples/callbacks/AimCallback.ipynb) -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.\n- [OpenInferenceCallbackHandler](../../../examples/callbacks/OpenInferenceCallback.ipynb) -> Tracking of AI model inferences. Example usage can be found in the notebook below.\n- [OpenAIFineTuningHandler](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb) -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7422c027-9d12-4fc9-98da-dc047a5d19c6": {"__data__": {"id_": "7422c027-9d12-4fc9-98da-dc047a5d19c6", "embedding": null, "metadata": {"filename": "root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32ed2ba753bd1b4012ea41e7ff2024775759e300", "node_type": "4", "metadata": {"filename": "root.md", "author": "LlamaIndex"}, "hash": "6e8267c6fc5c89c8a09db44a0d1af68ba04f0bee8cf9f444303334f734cfcde9", "class_name": "RelatedNodeInfo"}}, "text": "# Callbacks\n\n## Concept\n\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library.\nUsing the callback manager, as many callbacks as needed can be added.\n\nIn addition to logging data related to events, you can also track the duration and number of occurrences\nof each event.\n\nFurthermore, a trace map of events is also recorded, and callbacks can use this data\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\nafter most operations.\n\n**Callback Event Types**\nWhile each callback may not leverage each event type, the following events are available to be tracked:\n\n- `CHUNKING` -> Logs for the before and after of text splitting.\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\n- `EMBEDDING` -> Logs for the number of texts embedded.\n- `LLM` -> Logs for the template and response of LLM calls.\n- `QUERY` -> Keeps track of the start and end of each query.\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\n- `TREE` -> Logs for the summary and level of summaries generated.\n- `SUB_QUESTION` -> Log for a generated sub question and answer.\n\nYou can implement your own callback to track and trace these events, or use an existing callback.\n\n## Modules\n\nCurrently supported callbacks are as follows:\n\n- [LangfuseCallbackHandler](../../../examples/callbacks/LangfuseCallbackHandler.ipynb) -> Tracking of events and traces using the open-source platform Langfuse. More details are in the linked notebook or in the [Langfuse docs](https://langfuse.com/docs)\n- [TokenCountingHandler](../../../examples/callbacks/TokenCountingHandler.ipynb) -> Flexible token counting for prompt, completion, and embedding token usage. See [the migration details](../callbacks/token_counting_migration.md)\n- [LlamaDebugHanlder](../../../examples/callbacks/LlamaDebugHandler.ipynb) -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\n- [WandbCallbackHandler](../../../examples/callbacks/WandbCallbackHandler.ipynb) -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at [Wandb](https://docs.wandb.ai/guides/prompts/quickstart)\n- [AimCallback](../../../examples/callbacks/AimCallback.ipynb) -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.\n- [OpenInferenceCallbackHandler](../../../examples/callbacks/OpenInferenceCallback.ipynb) -> Tracking of AI model inferences. Example usage can be found in the notebook below.\n- [OpenAIFineTuningHandler](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb) -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c94b2ee2-472b-4b75-b5b6-a867e5153535": {"__data__": {"id_": "c94b2ee2-472b-4b75-b5b6-a867e5153535", "embedding": null, "metadata": {"filename": "token_counting_migration.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adeda652217e9ca8b70ffc662c540458dfb35011", "node_type": "4", "metadata": {"filename": "token_counting_migration.md", "author": "LlamaIndex"}, "hash": "59179a0af40e68705bb2e757f20221ee42d7fbd6808f39077f9fe2d008858c11", "class_name": "RelatedNodeInfo"}}, "text": "# Token Counting - Migration Guide\n\nThe existing token counting implementation has been **deprecated**.\n\nWe know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition.\n\nPreviously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.\n\nGoing forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separate token counters for different indexes.\n\nHere is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:\n\n```python\nimport tiktoken\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index.core import Settings\n\n# you can set a tokenizer directly, or optionally let it default\n# to the same tokenizer that was used previously for token counting\n# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode,\n    verbose=False,  # set to true to see usage printed to the console\n)\n\nSettings.callback_manager = CallbackManager([token_counter])\n\ndocument = SimpleDirectoryReader(\"./data\").load_data()\n\n# if verbose is turned on, you will see embedding token usage printed\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\n\n# otherwise, you can access the count directly\nprint(token_counter.total_embedding_token_count)\n\n# reset the counts at your discretion!\ntoken_counter.reset_counts()\n\n# also track prompt, completion, and total LLM tokens, in addition to embeddings\nresponse = index.as_query_engine().query(\"What did the author do growing up?\")\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n)\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f69e6f63-7b75-4617-9f89-8046c04b91a7": {"__data__": {"id_": "f69e6f63-7b75-4617-9f89-8046c04b91a7", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e9b32b88d412df8b7c08fc69edbdc69b6f99b2b3e7b48ca476a9c24858a67c85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9881c9a2-7cd4-4693-adb4-f1893ffd6583", "node_type": "1", "metadata": {}, "hash": "f7409e450591ca47fa92304870252a467c88bea8ca96f461e4b9a6eb99425505", "class_name": "RelatedNodeInfo"}}, "text": "# Observability\n\nLlamaIndex provides **one-click observability** \ud83d\udd2d to allow you to build principled LLM applications in a production setting.\n\nA key requirement for principled development of LLM applications over your data (RAG systems, agents) is being able to observe, debug, and evaluate\nyour system - both as a whole and for each component.\n\nThis feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners.\nConfigure a variable once, and you'll be able to do things like the following:\n\n- View LLM/prompt inputs/outputs\n- Ensure that the outputs of any component (LLMs, embeddings) are performing as expected\n- View call traces for both indexing and querying\n\nEach provider has similarities and differences. Take a look below for the full set of guides for each one!\n\n**NOTE:**\n\nObservability is now being handled via the [`instrumentation` module](./instrumentation.md) (available in v0.10.20 and later.)\n\nA lot of the tooling and integrations mentioned in this page use our legacy `CallbackManager` or don't use `set_global_handler`. We've marked these integrations as such!\n\n\n## Usage Pattern\n\nTo toggle, you will generally just need to do the following:\n\n```python\nfrom llama_index.core import set_global_handler\n\n# general usage\nset_global_handler(\"<handler_name>\", **kwargs)\n```\n\nNote that all `kwargs` to `set_global_handler` are passed to the underlying callback handler.\n\nAnd that's it! Executions will get seamlessly piped to downstream service and you'll be able to access features such as viewing execution traces of your application.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9881c9a2-7cd4-4693-adb4-f1893ffd6583": {"__data__": {"id_": "9881c9a2-7cd4-4693-adb4-f1893ffd6583", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e9b32b88d412df8b7c08fc69edbdc69b6f99b2b3e7b48ca476a9c24858a67c85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f69e6f63-7b75-4617-9f89-8046c04b91a7", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "5d2dae53c38bda037c7d1684403663f0572c828917f94c11951ce3d3ea75bffd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271b31dd-0780-4d59-8afd-1564ec8174ab", "node_type": "1", "metadata": {}, "hash": "cdeca99efb116569aeb4a7706f115ef55228b4da9d017885064d6aa19743f86b", "class_name": "RelatedNodeInfo"}}, "text": "## Partner `One-Click` Integrations\n\n### LlamaTrace (Hosted Arize Phoenix)\n\nWe've partnered with Arize on [LlamaTrace](https://llamatrace.com/), a hosted tracing, observability, and evaluation platform that works natively with LlamaIndex open-source users and has integrations with LlamaCloud.\n\nThis is built upon the open-source Arize [Phoenix](https://github.com/Arize-ai/phoenix) project. Phoenix provides a notebook-first experience for monitoring your models and LLM Applications by providing:\n\n- LLM Traces - Trace through the execution of your LLM Application to understand the internals of your LLM Application and to troubleshoot problems related to things like retrieval and tool execution.\n- LLM Evals - Leverage the power of large language models to evaluate your generative model or application's relevance, toxicity, and more.\n\n#### Usage Pattern\n\nTo install the integration package, do `pip install -U llama-index-callbacks-arize-phoenix`.\n\nThen create an account on LlamaTrace: https://llamatrace.com/login. Create an API key and put it in the `PHOENIX_API_KEY` variable below.\n\nThen run the following code:\n\n```python\n# Phoenix can display in real time the traces automatically\n# collected from your LlamaIndex application.\n# Run all of your LlamaIndex applications as usual and traces\n# will be collected and displayed in Phoenix.\n\n# setup Arize Phoenix for logging/observability\nimport llama_index.core\nimport os\n\nPHOENIX_API_KEY = \"<PHOENIX_API_KEY>\"\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\nllama_index.core.set_global_handler(\n    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n)\n\n...\n```\n#### Guides\n\n- [LlamaCloud Agent with LlamaTrace](https://github.com/run-llama/llamacloud-demo/blob/main/examples/tracing/llamacloud_tracing_phoenix.ipynb)\n\n![](../../_static/integrations/arize_phoenix.png)\n\n### Arize Phoenix (local)\n\nYou can also choose to use a **local** instance of Phoenix through the open-source project.\n\nIn this case you don't need to create an account on LlamaTrace or set an API key for Phoenix. The phoenix server will launch locally.\n\n#### Usage Pattern\n\nTo install the integration package, do `pip install -U llama-index-callbacks-arize-phoenix`.\n\nThen run the following code:\n\n```python\n# Phoenix can display in real time the traces automatically\n# collected from your LlamaIndex application.\n# Run all of your LlamaIndex applications as usual and traces\n# will be collected and displayed in Phoenix.\n\nimport phoenix as px\n\n# Look for a URL in the output to open the App in a browser.\npx.launch_app()\n# The App is initially empty, but as you proceed with the steps below,\n# traces will appear automatically as your LlamaIndex application runs.\n\nimport llama_index.core\n\nllama_index.core.set_global_handler(\"arize_phoenix\")\n...\n```\n\n#### Example Guides\n\n- [Auto-Retrieval Guide with Pinecone and Arize Phoenix](https://docs.llamaindex.ai/en/latest/examples/vector_stores/pinecone_auto_retriever/?h=phoenix)\n- [Arize Phoenix Tracing Tutorial](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb)\n\n\n## Other Partner `One-Click` Integrations (Legacy Modules)\n\nThese partner integrations use our legacy `CallbackManager` or third-party calls.\n\n### Langfuse\n\n[Langfuse](https://langfuse.com/docs) is an open source LLM engineering platform to help teams collaboratively debug, analyze and iterate on their LLM Applications. With the Langfuse integration, you can seamlessly track and monitor performance, traces, and metrics of your LlamaIndex application. Detailed traces of the LlamaIndex context augmentation and the LLM querying processes are captured and can be inspected directly in the Langfuse UI.\n\n#### Usage Pattern\n\n```python\nfrom llama_index.core import set_global_handler\n\n# Make sure you've installed the 'llama-index-callbacks-langfuse' integration package.\n\n# NOTE: Set your environment variables 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' and 'LANGFUSE_HOST'\n# as shown in your langfuse.com project settings.\n\nset_global_handler(\"langfuse\")\n```\n\n#### Guides\n\n- [Langfuse Callback Handler](../../examples/callbacks/LangfuseCallbackHandler.ipynb)\n\n!", "mimetype": "text/plain", "start_char_idx": 1629, "end_char_idx": 5844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "271b31dd-0780-4d59-8afd-1564ec8174ab": {"__data__": {"id_": "271b31dd-0780-4d59-8afd-1564ec8174ab", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e9b32b88d412df8b7c08fc69edbdc69b6f99b2b3e7b48ca476a9c24858a67c85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9881c9a2-7cd4-4693-adb4-f1893ffd6583", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "65daede1e35e3e24674fa1d9ed7209c3733665b60dfdb2b83b7530ad7ad48d2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c2d0c50-c887-4f6c-adb1-c3ee46b634e5", "node_type": "1", "metadata": {}, "hash": "e848e672d6698910c0e4a9d5bc6649935b81a3cd8d235abfe8d88fba0618f854", "class_name": "RelatedNodeInfo"}}, "text": "[langfuse-tracing](https://static.langfuse.com/llamaindex-langfuse-docs.gif)\n\n### DeepEval\n\n[DeepEval (by Confident AI)](https://github.com/confident-ai/deepeval) is an open-source evaluation framework for LLM applications. As you \"unit test\" your LLM app using DeepEval's 14+ default metrics it currently offers (summarization, hallucination, answer relevancy, faithfulness, RAGAS, etc.), you can debug failing test cases through this tracing integration with LlamaIndex, or debug unsatisfactory evaluations in **production** through DeepEval's hosted evaluation platform, [Confident AI](https://confident-ai.com), that runs referenceless evaluations in production.\n\n#### Usage Pattern\n\n```python\nfrom llama_index.core import set_global_handler\n\nset_global_handler(\"deepeval\")\n\n# NOTE: Run 'deepeval login' in the CLI to log traces on Confident AI, DeepEval's hosted evaluation platform.\n# Run all of your LlamaIndex applications as usual and traces\n# will be collected and displayed on Confident AI whenever evaluations are ran.\n...\n```\n\n![tracing](https://d2lsxfc3p6r9rv.cloudfront.net/confident-tracing.gif)\n\n### Weights and Biases Prompts\n\nPrompts allows users to log/trace/inspect the execution flow of LlamaIndex during index construction and querying. It also allows users to version-control their indices.\n\n#### Usage Pattern\n\n```python\nfrom llama_index.core import set_global_handler\n\nset_global_handler(\"wandb\", run_args={\"project\": \"llamaindex\"})\n\n# NOTE: No need to do the following\nfrom llama_index.callbacks.wandb import WandbCallbackHandler\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core import Settings\n\n# wandb_callback = WandbCallbackHandler(run_args={\"project\": \"llamaindex\"})\n# Settings.callback_manager = CallbackManager([wandb_callback])\n\n# access additional methods on handler to persist index + load index\nimport llama_index.core\n\n# persist index\nllama_index.core.global_handler.persist_index(graph, index_name=\"my_index\")\n# load storage context\nstorage_context = llama_index.core.global_handler.load_storage_context(\n    artifact_url=\"ayut/llamaindex/my_index:v0\"\n)\n```\n\n![](../../_static/integrations/wandb.png)\n\n#### Guides\n\n- [Wandb Callback Handler](../../examples/callbacks/WandbCallbackHandler.ipynb)\n\n### OpenLLMetry\n\n[OpenLLMetry](https://github.com/traceloop/openllmetry) is an open-source project based on OpenTelemetry for tracing and monitoring\nLLM applications. It connects to [all major observability platforms](https://www.traceloop.com/docs/openllmetry/integrations/introduction) and installs in minutes.\n\n#### Usage Pattern\n\n```python\nfrom traceloop.sdk import Traceloop\n\nTraceloop.init()\n```\n\n#### Guides\n\n- [OpenLLMetry](../../examples/callbacks/OpenLLMetry.ipynb)\n\n![](../../_static/integrations/openllmetry.png)\n\n### OpenInference\n\n[OpenInference](https://github.com/Arize-ai/open-inference-spec) is an open standard for capturing and storing AI model inferences. It enables experimentation, visualization, and evaluation of LLM applications using LLM observability solutions such as [Phoenix](https://github.com/Arize-ai/phoenix).\n\n#### Usage Pattern\n\n```python\nimport llama_index.core\n\nllama_index.core.set_global_handler(\"openinference\")\n\n# NOTE: No need to do the following\nfrom llama_index.callbacks.openinference import OpenInferenceCallbackHandler\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core import Settings\n\n# callback_handler = OpenInferenceCallbackHandler()\n# Settings.callback_manager = CallbackManager([callback_handler])\n\n# Run your LlamaIndex application here...\nfor query in queries:\n    query_engine.query(query)\n\n# View your LLM app data as a dataframe in OpenInference format.\nfrom llama_index.core.callbacks.open_inference_callback import as_dataframe\n\nquery_data_buffer = llama_index.core.global_handler.flush_query_data_buffer()\nquery_dataframe = as_dataframe(query_data_buffer)\n```\n\n**NOTE**: To unlock capabilities of Phoenix, you will need to define additional steps to feed in query/ context dataframes. See below!", "mimetype": "text/plain", "start_char_idx": 5844, "end_char_idx": 9893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c2d0c50-c887-4f6c-adb1-c3ee46b634e5": {"__data__": {"id_": "6c2d0c50-c887-4f6c-adb1-c3ee46b634e5", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e9b32b88d412df8b7c08fc69edbdc69b6f99b2b3e7b48ca476a9c24858a67c85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271b31dd-0780-4d59-8afd-1564ec8174ab", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "998ecb1e902f236b7eb36dc4959c43d52375a5094ac0aecca8a39fbd70cc5207", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ea3e6dc-2ce2-402c-8900-85244acfa8dd", "node_type": "1", "metadata": {}, "hash": "a55f4a477acca22465a799d4af45f597e25e39c6d8ffd3c61e9ff60b8bc5456d", "class_name": "RelatedNodeInfo"}}, "text": "See below!\n\n#### Guides\n\n- [OpenInference Callback Handler](../../examples/callbacks/OpenInferenceCallback.ipynb)\n- [Evaluating Search and Retrieval with Arize Phoenix](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb)\n\n### TruEra TruLens\n\nTruLens allows users to instrument/evaluate LlamaIndex applications, through features such as feedback functions and tracing.\n\n#### Usage Pattern + Guides\n\n```python\n# use trulens\nfrom trulens_eval import TruLlama\n\ntru_query_engine = TruLlama(query_engine)\n\n# query\ntru_query_engine.query(\"What did the author do growing up?\")\n```\n\n![](../../_static/integrations/trulens.png)\n\n#### Guides\n\n- [Trulens Guide](../../community/integrations/trulens.md)\n- [Quickstart Guide with LlamaIndex + TruLens](https://github.com/truera/trulens/blob/trulens-eval-0.20.3/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)\n- [Google Colab](https://colab.research.google.com/github/truera/trulens/blob/trulens-eval-0.20.3/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)\n\n### HoneyHive\n\nHoneyHive allows users to trace the execution flow of any LLM pipeline. Users can then debug and analyze their traces, or customize feedback on specific trace events to create evaluation or fine-tuning datasets from production.\n\n#### Usage Pattern\n\n```python\nfrom llama_index.core import set_global_handler\n\nset_global_handler(\n    \"honeyhive\",\n    project=\"My HoneyHive Project\",\n    name=\"My LLM Pipeline Name\",\n    api_key=\"MY HONEYHIVE API KEY\",\n)\n\n# NOTE: No need to do the following\nfrom llama_index.core.callbacks import CallbackManager\n\n# from honeyhive.utils.llamaindex_tracer import HoneyHiveLlamaIndexTracer\nfrom llama_index.core import Settings\n\n# hh_tracer = HoneyHiveLlamaIndexTracer(\n#     project=\"My HoneyHive Project\",\n#     name=\"My LLM Pipeline Name\",\n#     api_key=\"MY HONEYHIVE API KEY\",\n# )\n# Settings.callback_manager = CallbackManager([hh_tracer])\n```\n\n![](../../_static/integrations/honeyhive.png)\n![](../../_static/integrations/perfetto.png)\n_Use Perfetto to debug and analyze your HoneyHive traces_\n\n#### Guides\n\n- [HoneyHive Callback Handler](../../examples/callbacks/HoneyHiveLlamaIndexTracer.ipynb)\n\n### PromptLayer\n\nPromptLayer allows you to track analytics across LLM calls, tagging, analyzing, and evaluating prompts for various use-cases. Use it with LlamaIndex to track the performance of your RAG prompts and more.\n\n#### Usage Pattern\n\n```python\nimport os\n\nos.environ[\"PROMPTLAYER_API_KEY\"] = \"pl_7db888a22d8171fb58aab3738aa525a7\"\n\nfrom llama_index.core import set_global_handler\n\n# pl_tags are optional, to help you organize your prompts and apps\nset_global_handler(\"promptlayer\", pl_tags=[\"paul graham\", \"essay\"])\n```\n\n#### Guides\n\n- [PromptLayer](../../examples/callbacks/PromptLayerHandler.ipynb)\n\n### Langtrace\n\n[Langtrace](https://github.com/Scale3-Labs/langtrace) is a robust open-source tool that supports OpenTelemetry and is designed to trace, evaluate, and manage LLM applications seamlessly. Langtrace integrates directly with LlamaIndex, offering detailed, real-time insights into performance metrics such as accuracy, evaluations, and latency.\n\n#### Install\n\n```shell\npip install langtrace-python-sdk\n```\n\n#### Usage Pattern\n\n```python\nfrom langtrace_python_sdk import (\n    langtrace,\n)  # Must precede any llm module imports\n\nlangtrace.init(api_key=\"<LANGTRACE_API_KEY>\")\n```\n\n![](../../_static/integrations/langtrace.gif)\n\n#### Guides\n\n- [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/llamaindex)\n\n### OpenLIT\n\n[OpenLIT](https://github.com/openlit/openlit) is an OpenTelemetry-native GenAI and LLM Application Observability tool. It's designed to make the integration process of observability into GenAI projects with just a single line of code. OpenLIT provides OpenTelemetry Auto instrumentation for various LLMs, VectorDBs and Frameworks like LlamaIndex.", "mimetype": "text/plain", "start_char_idx": 9883, "end_char_idx": 13835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ea3e6dc-2ce2-402c-8900-85244acfa8dd": {"__data__": {"id_": "9ea3e6dc-2ce2-402c-8900-85244acfa8dd", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "e9b32b88d412df8b7c08fc69edbdc69b6f99b2b3e7b48ca476a9c24858a67c85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c2d0c50-c887-4f6c-adb1-c3ee46b634e5", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d6453f6ac23463ce6a193387fc267a2c72348989239026d5861bdc4a3a1e44c0", "class_name": "RelatedNodeInfo"}}, "text": "OpenLIT provides OpenTelemetry Auto instrumentation for various LLMs, VectorDBs and Frameworks like LlamaIndex. OpenLIT provides insights into your LLM Applications performance, tracing of requests, over view metrics on usage like costs, tokens and a lot more.\n\n#### Install\n\n```shell\npip install openlit\n```\n\n#### Usage Pattern\n\n```python\nimport openlit\n\nopenlit.init()\n```\n\n![](../../_static/integrations/openlit.gif)\n\n#### Guides\n\n- [OpenLIT's Official Documentation](https://docs.openlit.io/latest/integrations/llama-index)\n\n### AgentOps\n\n[AgentOps](https://github.com/AgentOps-AI/agentops) helps developers build, evaluate,\nand monitor AI agents. AgentOps will help build agents from prototype to production,\nenabling agent monitoring, LLM cost tracking, benchmarking, and more.\n\n#### Install\n\n```shell\npip install llama-index-instrumentation-agentops\n```\n\n#### Usage Pattern\n\n```python\nfrom llama_index.core import set_global_handler\n\n# NOTE: Feel free to set your AgentOps environment variables (e.g., 'AGENTOPS_API_KEY')\n# as outlined in the AgentOps documentation, or pass the equivalent keyword arguments\n# anticipated by AgentOps' AOClient as **eval_params in set_global_handler.\n\nset_global_handler(\"agentops\")\n```\n\n### Simple (LLM Inputs/Outputs)\n\nThis simple observability tool prints every LLM input/output pair to the terminal. Most useful for when you need to quickly enable debug logging on your LLM application.\n\n#### Usage Pattern\n\n```python\nimport llama_index.core\n\nllama_index.core.set_global_handler(\"simple\")\n```\n\n\n## More observability\n\n- [Callbacks Guide](./callbacks/index.md)", "mimetype": "text/plain", "start_char_idx": 13724, "end_char_idx": 15327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b5966ce-1ee7-4440-810a-d8405e500f90": {"__data__": {"id_": "0b5966ce-1ee7-4440-810a-d8405e500f90", "embedding": null, "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "087f92b3f99e3b6f0bd593c8d761c192e801b497", "node_type": "4", "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "hash": "ae5f4797a93192a59a125714009576aaaa49c48178d35cc41e747ec85e5d1508", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a43cce5-d1d7-4b5c-a87a-2a69a8753985", "node_type": "1", "metadata": {}, "hash": "4b2ef3f99c36728afee44fcf99655082a17e2602ef50108a33d10eeeba9e26c9", "class_name": "RelatedNodeInfo"}}, "text": "# Instrumentation\n\n**NOTE**: The `instrumentation` module (available in llama-index v0.10.20 and later) is\nmeant to replace the legacy `callbacks` module. During the deprecation period,\nthe llama-index library supports both modules as a means to instrument your\nLLM application. However, at some point after all of the existing integrations\nhave moved over to the new `instrumentation` module, we will no longer support\n`callbacks` module.\n\nThe new `instrumentation` module allows for the instrumentation of `llama-index`\napplications. In particular, one can handle events and track spans using both\ncustom logic as well as those offered in the module. Users can also define their\nown events and specify where and when in the code logic that they should be emitted.\nListed below are the core classes as well as their brief description of the\n`instrumentation` module:\n\n- `Event` \u2014 represents a single moment in time that a certain occurrence took place within the execution of the application\u2019s code.\n- `EventHandler` \u2014 listen to the occurrences of `Event`'s and execute code logic at these moments in time.\n- `Span` \u2014 represents the execution flow of a particular part in the application\u2019s code and thus contains `Event`'s.\n- `SpanHandler` \u2014 is responsible for the entering, exiting, and dropping (i.e., early exiting due to error) of `Span`'s.\n- `Dispatcher` \u2014 emits `Event`'s as well as signals to enter/exit/drop a `Span` to the appropriate handlers.\n\n\n## Using the Instrumentation Module for Observability\n\nA core use case for instrumentation is observability. Our native instrumentation integrations with third-party partners allow you to get detailed traces across the entire call stack.\n\nCheck out our [observability guide](index.md) for more details on supported partners.\n\n\n## Usage\n\nUsing the new `instrumentation` module involves 3 high-level steps.\n\n1. Define a `dispatcher`\n2. (Optional) Define and attach your `EventHandler`'s to `dispatcher`\n3. (Optional) Define and attach your `SpanHandler` to `dispatcher`\n\nDoing so, would result in the ability to handle events and obtain spans that have\nbeen transmitted throughout the `llama-index` library and extension packages.\n\nFor example, if I wanted to track every LLM call made in the library:\n\n```python\nfrom typing import Dict, List\n\nfrom llama_index.core.instrumentation.events.llm import (\n    LLMChatEndEvent,\n    LLMChatStartEvent,\n    LLMChatInProgressEvent,\n)\n\n\nclass ExampleEventHandler(BaseEventHandler):\n    events: List[BaseEvent] = []\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"ExampleEventHandler\"\n\n    def handle(self, event: BaseEvent) -> None:\n        \"\"\"Logic for handling event.\"\"\"\n        print(\"-----------------------\")\n        # all events have these attributes\n        print(event.id_)\n        print(event.timestamp)\n        print(event.span_id)\n\n        # event specific attributes\n        if isinstance(event, LLMChatStartEvent):\n            # initial\n            print(event.messages)\n            print(event.additional_kwargs)\n            print(event.model_dict)\n        elif isinstance(event, LLMChatInProgressEvent):\n            # streaming\n            print(event.response.delta)\n        elif isinstance(event, LLMChatEndEvent):\n            # final response\n            print(event.response)\n\n        self.events.append(event)\n        print(\"-----------------------\")\n```\n\nSee the [full guide](../../examples/instrumentation/instrumentation_observability_rundown.ipynb) on all events logged in LlamaIndex, or visit the [api reference](../../api_reference/instrumentation/index.md) for more details.\n\n### Defining a custom `EventHandler`\n\nUsers can create their own custom handlers by subclassing `BaseEventHandler`\nand providing logic to the abstract method `handle()`.\n\n```python\nfrom llama_index.core.instrumentation.event_handlers.base import (\n    BaseEventHandler,\n)\n\n\nclass MyEventHandler(BaseEventHandler):\n    \"\"\"My custom EventHandler.\"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"MyEventHandler\"\n\n    def handle(self, event: BaseEvent, **kwargs) -> Any:\n        \"\"\"Logic for handling event.\"\"\"\n        print(event.class_name())", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a43cce5-d1d7-4b5c-a87a-2a69a8753985": {"__data__": {"id_": "5a43cce5-d1d7-4b5c-a87a-2a69a8753985", "embedding": null, "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "087f92b3f99e3b6f0bd593c8d761c192e801b497", "node_type": "4", "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "hash": "ae5f4797a93192a59a125714009576aaaa49c48178d35cc41e747ec85e5d1508", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b5966ce-1ee7-4440-810a-d8405e500f90", "node_type": "1", "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "hash": "be458e5528ce60782968aab1b22f9257c4abd2bbe9e491ae395c2ca4451ac491", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "440c4567-07d1-49dc-a3b0-b00ba048f8a9", "node_type": "1", "metadata": {}, "hash": "575f07c93c7cb80cf5bfcc2cf4d3e19b7a104878b02605e75a4feca165b3204a", "class_name": "RelatedNodeInfo"}}, "text": "my_event_handler = MyEventHandler()\n```\n\nAfter defining your handler, you can attach it to the desired dispatcher:\n\n```python\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\ndispatcher.add_event_handler(my_event_handler)\n```\n\n### Defining a custom `Event`\n\nUser can create their own custom events by subclassing `BaseEvent`. The\n`BaseEvent` class comes with a `timestamp` as well as an `id_` field. To add more\nitems to this event payload, simply add them in as new `Fields` (since they are\nsubclasses of `pydantic.BaseModel`).\n\n```python\nfrom llama_index.core.instrumentation.event.base import BaseEvent\n\n\nclass MyEvent(BaseEvent):\n    \"\"\"My custom Event.\"\"\"\n\n    new_field_1 = Field(...)\n    new_field_2 = Field(...)\n```\n\nOnce you have your custom event defined, you use a dispatcher to fire the event\nat desired instances throughout your application\u2019s code.\n\n```python\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\ndispatcher.event(MyEvent(new_field_1=..., new_field_2=...))\n```\n\n### Defining a custom `Span`\n\n`Span`\u2019s are like `Event`'s in that they are both structured data classes.\nUnlike `Event`'s though, `Span`'s as their name implies, span a duration of time\nwithin the programs execution flow. You can define a custom `Span` to store any\ninformation you would like.\n\n```python\nfrom typing import Any\nfrom llama_index.core.bridge.pydantic import Field\n\n\nclass MyCustomSpan(BaseSpan):\n    custom_field_1: Any = Field(...)\n    custom_field_2: Any = Field(...)\n```\n\nTo handle your new Span type, you need to also define your custom `SpanHandler`\nby subclassing the `BaseSpanHandler` class. Three abstract methods need to be\ndefined when subclass this base class, namely: `new_span()`, `prepare_to_exit_span()`,\nand `prepare_to_drop_span()`.\n\n```python\nfrom typing import Any, Optional\nfrom llama_index.core.instrumentation.span.base import BaseSpan\nfrom llama_index.core.instrumentation.span_handlers import BaseSpanHandler\n\n\nclass MyCustomSpanHandler(BaseSpanHandler[MyCustomSpan]):\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"MyCustomSpanHandler\"\n\n    def new_span(\n        self, id: str, parent_span_id: Optional[str], **kwargs\n    ) -> Optional[MyCustomSpan]:\n        \"\"\"Create a span.\"\"\"\n        # logic for creating a new MyCustomSpan\n        pass\n\n    def prepare_to_exit_span(\n        self, id: str, result: Optional[Any] = None, **kwargs\n    ) -> Any:\n        \"\"\"Logic for preparing to exit a span.\"\"\"\n        pass\n\n    def prepare_to_drop_span(\n        self, id: str, err: Optional[Exception], **kwargs\n    ) -> Any:\n        \"\"\"Logic for preparing to drop a span.\"\"\"\n        pass\n```\n\nTo make use of your new SpanHandler (and associated Span type), you simply need\nto add it to your desired dispatcher.\n\n```python\nimport llama_index.core.instrumentation as instrument\nfrom llama_index.core.instrumentation.span_handler import SimpleSpanHandler\n\ndispatcher = (\n    instrument.get_dispatcher()\n)  # with no name argument, defaults to root\n\nmy_span_handler = MyCustomSpanHandler()\ndispatcher.add_span_handler(my_span_handler)\n```\n\n### Entering/Exiting a `Span`\n\nTo send a signal to `SpanHandler`'s to enter/exit a `Span`, we use the `span_enter()`,\n`span_exit()` methods, respectively. There is also `span_drop()` method that could\nbe used to handle cases where `Span`'s are cut shorter than usual due to errors\nwithin the covered code\u2019s execution.\n\n```python\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\ndef func():\n    dispatcher.span_enter(...)\n    try:\n        val = ...\n    except:\n        ...\n        dispatcher.span_drop(...)\n    else:\n        dispatcher.span_exit(...)\n        return val\n\n\n# or, syntactic sugar via decorators", "mimetype": "text/plain", "start_char_idx": 4233, "end_char_idx": 8103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "440c4567-07d1-49dc-a3b0-b00ba048f8a9": {"__data__": {"id_": "440c4567-07d1-49dc-a3b0-b00ba048f8a9", "embedding": null, "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "087f92b3f99e3b6f0bd593c8d761c192e801b497", "node_type": "4", "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "hash": "ae5f4797a93192a59a125714009576aaaa49c48178d35cc41e747ec85e5d1508", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a43cce5-d1d7-4b5c-a87a-2a69a8753985", "node_type": "1", "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}, "hash": "d87c1b1b805fcc961a9af106fb898a6dafde9cac03e3ece55963f159dbb43518", "class_name": "RelatedNodeInfo"}}, "text": "# or, syntactic sugar via decorators\n\n\n@dispatcher.span\ndef func():\n    ...\n```\n\n### Making use of `dispatcher` hierarchy\n\nA similar hierarchy to that seen with the standard Python `logging` library and\nits `Logger` class exists for `dispatcher`. Specifically, all `dispatcher`\u2019s\nexcept for the root `dispatcher` has a parent, and when handling events or span\u2019s\ncan propagate them to its parent as well (this is the default behaviour). This\nhierarchical method of handling events and spans allows for defining \u201cglobal\u201d\nevent handlers as well as \u201clocal\u201d ones.\n\nConsider the project structure defined below. There are 3 `dispatcher`'s: one at\nthe top-level of the `project` and then two others at the individual sub-modules\n`llama1` and `llama2`. With this setup, any `EventHandler`\u2019s attached to the\nproject root\u2019s `dispatcher` will be be subscribed to all `Event`'s that occur in\nthe execution of code in `llama1` and `llama2`. On the other hand, `EventHandler`'s\ndefined in the respective `llama<x>` sub modules will only be subscribed to the\n`Event`'s that occur within their respective sub-module execution.\n\n```sh\nproject\n\u251c\u2500\u2500 __init__.py  # has a dispatcher=instrument.get_dispatcher(__name__)\n\u251c\u2500\u2500 llama1\n\u2502   \u251c\u2500\u2500 __init__.py  # has a dispatcher=instrument.get_dispatcher(__name__)\n\u2502   \u2514\u2500\u2500 app_query_engine.py\n\u2514\u2500\u2500 llama2\n    \u251c\u2500\u2500 __init__.py  # has a dispatcher=instrument.get_dispatcher(__name__)\n    \u2514\u2500\u2500 app_query_engine.py\n```\n\n## Notebook Guides:\n\n\n- [Basic Usage](../../examples/instrumentation/basic_usage.ipynb)\n- [Observing Model Calls](../../examples/instrumentation/observe_api_calls.ipynb)\n- [Observing All Events](../../examples/instrumentation/instrumentation_observability_rundown.ipynb)\n\n\n## API Reference\n\n- [Instrumentation API Reference](../../api_reference/instrumentation/index.md)", "mimetype": "text/plain", "start_char_idx": 8067, "end_char_idx": 9870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad32417d-ceea-4b77-acf5-32bc4d9510b4": {"__data__": {"id_": "ad32417d-ceea-4b77-acf5-32bc4d9510b4", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90c457c0f61b534d3528b3768c98c033adf65c76", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "5c5c8b767f252f59955a3bc41d141b7f2416294598bf2611ebba3f1f6abcaa50", "class_name": "RelatedNodeInfo"}}, "text": "# Querying\n\nQuerying is the most important part of your LLM application. To learn more about getting a final product that you can deploy, check out the [query engine](../deploying/query_engine/index.md), [chat engine](../deploying/chat_engines/index.md).\n\nIf you wish to combine advanced reasoning with tool use, check out our [agents](../deploying/agents/index.md) guide.\n\n## Query Pipeline\n\nYou can create query pipelines/chains with ease with our declarative `QueryPipeline` interface. Check out our [query pipeline guide](pipeline/index.md) for more details.\n\nOtherwise check out how to use our query modules as standalone components \ud83d\udc47.\n\n## Query Modules\n\n- [Query Engines](../deploying/query_engine/index.md)\n- [Chat Engines](../deploying/chat_engines/index.md)\n- [Agents](../deploying/agents/index.md)\n- [Retrievers](../querying/retriever/index.md)\n- [Response Synthesizers](../querying/response_synthesizers/index.md)\n- [Routers](../querying/router/index.md)\n- [Node Postprocessors](../querying/node_postprocessors/index.md)\n- [Structured Outputs](../querying/structured_outputs/index.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88fdeaf9-65be-4a04-943d-5ced50737a90": {"__data__": {"id_": "88fdeaf9-65be-4a04-943d-5ced50737a90", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7775a580b5808b1e16dcd3b82eca11a1cdaacaf1", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "9a95db5393cb11f1db8cb829fca960aa75086f47a9495dd9bdc500d663a1a734", "class_name": "RelatedNodeInfo"}}, "text": "# Node Postprocessor\n\n## Concept\n\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.\n\nIn LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.\n\nLlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.\n\n!!! tip\n    Confused about where node postprocessor fits in the pipeline? Read about [high-level concepts](../../../getting_started/concepts.md)\n\n## Usage Pattern\n\nAn example of using a node postprocessors is below:\n\n```python\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.core.data_structs import Node\nfrom llama_index.core.schema import NodeWithScore\n\nnodes = [\n    NodeWithScore(node=Node(text=\"text1\"), score=0.7),\n    NodeWithScore(node=Node(text=\"text2\"), score=0.8),\n]\n\n# similarity postprocessor: filter nodes below 0.75 similarity score\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\n# cohere rerank: rerank nodes given query using trained model\nreranker = CohereRerank(api_key=\"<COHERE_API_KEY>\", top_n=2)\nreranker.postprocess_nodes(nodes, query_str=\"<user_query>\")\n```\n\nNote that `postprocess_nodes` can take in either a `query_str` or `query_bundle` (`QueryBundle`), though not both.\n\n## Usage Pattern\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\n\n## Using with a Query Engine\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.postprocessor import TimeWeightedPostprocessor\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n    node_postprocessors=[\n        TimeWeightedPostprocessor(\n            time_decay=0.5, time_access_refresh=False, top_k=1\n        )\n    ]\n)\n\n# all node post-processors will be applied during each query\nresponse = query_engine.query(\"query string\")\n```\n\n## Using with Retrieved Nodes\n\nOr used as a standalone object for filtering retrieved nodes:\n\n```python\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n\nnodes = index.as_retriever().retrieve(\"test query str\")\n\n# filter nodes below 0.75 similarity score\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n```\n\n## Using with your own nodes\n\nAs you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.\n\n```python\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\nfrom llama_index.core.data_structs import Node\nfrom llama_index.core.schema import NodeWithScore\n\nnodes = [\n    NodeWithScore(node=Node(text=\"text\"), score=0.7),\n    NodeWithScore(node=Node(text=\"text\"), score=0.8),\n]\n\n# filter nodes below 0.75 similarity score\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n```\n\n(custom-node-postprocessor)=\n\n## Custom Node PostProcessor\n\nThe base class is `BaseNodePostprocessor`, and the API interface is very simple:\n\n```python\nclass BaseNodePostprocessor:\n    \"\"\"Node postprocessor.\"\"\"\n\n    @abstractmethod\n    def _postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n```\n\nA dummy node-postprocessor can be implemented in just a few lines of code:\n\n```python\nfrom llama_index.core import QueryBundle\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import NodeWithScore\n\n\nclass DummyNodePostprocessor(BaseNodePostprocessor):\n    def _postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        # subtracts 1 from the score\n        for n in nodes:\n            n.score -= 1\n\n        return nodes\n```\n\n## Modules\n\nSee the full [modules list](./node_postprocessors.md) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32b89b34-f75d-4e98-9f3b-b0d9ee92200a": {"__data__": {"id_": "32b89b34-f75d-4e98-9f3b-b0d9ee92200a", "embedding": null, "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c", "node_type": "4", "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "hash": "7a6ddaf6cc9426ede756c805065bf486d66bbf9b3f6b7ab9927be933a9633167", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "001aa0a5-bfb5-4ae2-8b97-26e646350e3e", "node_type": "1", "metadata": {}, "hash": "0cb86f419c88749744ce2ad30916afae7db27a00382e8b34285603c22fff22ff", "class_name": "RelatedNodeInfo"}}, "text": "# Node Postprocessor Modules\n\n## SimilarityPostprocessor\n\nUsed to remove nodes that are below a similarity score threshold.\n\n```python\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## KeywordNodePostprocessor\n\nUsed to ensure certain keywords are either excluded or included.\n\n```python\nfrom llama_index.core.postprocessor import KeywordNodePostprocessor\n\npostprocessor = KeywordNodePostprocessor(\n    required_keywords=[\"word1\", \"word2\"], exclude_keywords=[\"word3\", \"word4\"]\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## MetadataReplacementPostProcessor\n\nUsed to replace the node content with a field from the node metadata. If the field is not present in the metadata, then the node text remains unchanged. Most useful when used in combination with the `SentenceWindowNodeParser`.\n\n```python\nfrom llama_index.core.postprocessor import MetadataReplacementPostProcessor\n\npostprocessor = MetadataReplacementPostProcessor(\n    target_metadata_key=\"window\",\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## LongContextReorder\n\nModels struggle to access significant details found in the center of extended contexts. [A study](https://arxiv.org/abs/2307.03172) observed that the best performance typically arises when crucial data is positioned at the start or conclusion of the input context. Additionally, as the input context lengthens, performance drops notably, even in models designed for long contexts.\n\nThis module will re-order the retrieved nodes, which can be helpful in cases where a large top-k is needed.\n\n```python\nfrom llama_index.core.postprocessor import LongContextReorder\n\npostprocessor = LongContextReorder()\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## SentenceEmbeddingOptimizer\n\nThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).\n\nThe percentile cutoff is a measure for using the top percentage of relevant sentences.\n\nThe threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.\n\n```python\nfrom llama_index.core.postprocessor import SentenceEmbeddingOptimizer\n\npostprocessor = SentenceEmbeddingOptimizer(\n    embed_model=service_context.embed_model,\n    percentile_cutoff=0.5,\n    # threshold_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide can be found [here](../../../examples/node_postprocessor/OptimizerDemo.ipynb)\n\n## CohereRerank\n\nUses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N nodes.\n\n```python\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\n\npostprocessor = CohereRerank(\n    top_n=2, model=\"rerank-english-v2.0\", api_key=\"YOUR COHERE API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [here](../../../examples/node_postprocessor/CohereRerank.ipynb).\n\n## SentenceTransformerRerank\n\nUses the cross-encoders from the `sentence-transformer` package to re-order nodes, and returns the top N nodes.\n\n```python\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\n\n# We choose a model with relatively high speed and decent accuracy.\npostprocessor = SentenceTransformerRerank(\n    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [here](../../../examples/node_postprocessor/SentenceTransformerRerank.ipynb).\n\nPlease also refer to the [`sentence-transformer` docs](https://www.sbert.net/docs/pretrained-models/ce-msmarco.html) for a more complete list of models (and also shows tradeoffs in speed/accuracy). The default model is `cross-encoder/ms-marco-TinyBERT-L-2-v2`, which provides the most speed.\n\n## LLM Rerank\n\nUses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are. Returns the top N ranked nodes.\n\n```python\nfrom llama_index.core.postprocessor import LLMRerank\n\npostprocessor = LLMRerank(top_n=2, service_context=service_context)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [her for Gatsby](../../../examples/node_postprocessor/LLMReranker-Gatsby.ipynb) and [here for Lyft 10K documents](../../../examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb).\n\n## JinaRerank\n\nUses the \"Jina ReRank\" functionality to re-order nodes, and returns the top N nodes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "001aa0a5-bfb5-4ae2-8b97-26e646350e3e": {"__data__": {"id_": "001aa0a5-bfb5-4ae2-8b97-26e646350e3e", "embedding": null, "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c", "node_type": "4", "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "hash": "7a6ddaf6cc9426ede756c805065bf486d66bbf9b3f6b7ab9927be933a9633167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32b89b34-f75d-4e98-9f3b-b0d9ee92200a", "node_type": "1", "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "hash": "6c75a466c6a73a74f6aa53e7a9acfde1123416ac6e51e529a5caa6e71c7eb365", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56e75dc0-8c34-4684-8dc8-3a794abe9f5b", "node_type": "1", "metadata": {}, "hash": "e98254cf5945e99ffb4c932d5ff54576d0000104737a3b410d52febf2495a259", "class_name": "RelatedNodeInfo"}}, "text": "## JinaRerank\n\nUses the \"Jina ReRank\" functionality to re-order nodes, and returns the top N nodes.\n\n```python\nfrom llama_index.postprocessor.jinaai_rerank import JinaRerank\n\npostprocessor = JinaRerank(\n    top_n=2, model=\"jina-reranker-v1-base-en\", api_key=\"YOUR JINA API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [here](../../../examples/node_postprocessor/JinaRerank.ipynb).\n\n## FixedRecencyPostprocessor\n\nThis postproccesor returns the top K nodes sorted by date. This assumes there is a `date` field to parse in the metadata of each node.\n\n```python\nfrom llama_index.core.postprocessor import FixedRecencyPostprocessor\n\npostprocessor = FixedRecencyPostprocessor(\n    tok_k=1, date_key=\"date\"  # the key in the metadata to find the date\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n![](../../../_static/node_postprocessors/recency.png)\n\nA full notebook guide is available [here](../../..../../../examples/node_postprocessor/RecencyPostprocessorDemo.ipynb).\n\n## EmbeddingRecencyPostprocessor\n\nThis postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.\n\n```python\nfrom llama_index.core.postprocessor import EmbeddingRecencyPostprocessor\n\npostprocessor = EmbeddingRecencyPostprocessor(\n    service_context=service_context, date_key=\"date\", similarity_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide is available [here](../../..../../../examples/node_postprocessor/RecencyPostprocessorDemo.ipynb).\n\n## TimeWeightedPostprocessor\n\nThis postproccesor returns the top K nodes applying a time-weighted rerank to each node. Each time a node is retrieved, the time it was retrieved is recorded. This biases search to favor information that has not be returned in a query yet.\n\n```python\nfrom llama_index.core.postprocessor import TimeWeightedPostprocessor\n\npostprocessor = TimeWeightedPostprocessor(time_decay=0.99, top_k=1)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide is available [here](../../../examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb).\n\n## (Beta) PIINodePostprocessor\n\nThe PII (Personal Identifiable Information) postprocssor removes information that might be a security risk. It does this by using NER (either with a dedicated NER model, or with a local LLM model).\n\n### LLM Version\n\n```python\nfrom llama_index.core.postprocessor import PIINodePostprocessor\n\npostprocessor = PIINodePostprocessor(\n    service_context=service_context  # this should be setup with an LLM you trust\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n### NER Version\n\nThis version uses the default local model from Hugging Face that is loaded when you run `pipeline(\"ner\")`.\n\n```python\nfrom llama_index.core.postprocessor import NERPIINodePostprocessor\n\npostprocessor = NERPIINodePostprocessor()\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide for both can be found [here](../../../examples/node_postprocessor/PII.ipynb).\n\n## (Beta) PrevNextNodePostprocessor\n\nUses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.\n\nThis is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.\n\n```python\nfrom llama_index.core.postprocessor import PrevNextNodePostprocessor\n\npostprocessor = PrevNextNodePostprocessor(\n    docstore=index.docstore,\n    num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards\n    mode=\"next\",  # can be either 'next', 'previous', or 'both'\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n![](../../../_static/node_postprocessors/prev_next.png)\n\n## (Beta) AutoPrevNextNodePostprocessor\n\nThe same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).\n\n```python\nfrom llama_index.core.postprocessor import AutoPrevNextNodePostprocessor\n\npostprocessor = AutoPrevNextNodePostprocessor(\n    docstore=index.docstore,\n    service_context=service_context,\n    num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)\n)\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full example notebook is available [here](../../../examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb).", "mimetype": "text/plain", "start_char_idx": 4377, "end_char_idx": 8708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56e75dc0-8c34-4684-8dc8-3a794abe9f5b": {"__data__": {"id_": "56e75dc0-8c34-4684-8dc8-3a794abe9f5b", "embedding": null, "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c", "node_type": "4", "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "hash": "7a6ddaf6cc9426ede756c805065bf486d66bbf9b3f6b7ab9927be933a9633167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "001aa0a5-bfb5-4ae2-8b97-26e646350e3e", "node_type": "1", "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}, "hash": "929320f9b48de7c66728592597b4b0ea9f98f2a6bb6ced36a1075a8bac497fa5", "class_name": "RelatedNodeInfo"}}, "text": "## (Beta) RankGPT\n\nUses RankGPT agent to rerank documents according to relevance. Returns the top N ranked nodes.\n\n```python\nfrom llama_index.postprocessor.rankgpt_rerank import RankGPTRerank\n\npostprocessor = RankGPTRerank(top_n=3, llm=OpenAI(model=\"gpt-3.5-turbo-16k\"))\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [here](../../../examples/node_postprocessor/rankGPT.ipynb).\n\n## Colbert Reranker\n\nUses Colbert V2 model as a reranker to rerank documents according to the fine-grained similarity between query tokens and passage tokens. Returns the top N ranked nodes.\n\n```python\nfrom llama_index.postprocessor.colbert_rerank import ColbertRerank\n\ncolbert_reranker = ColbertRerank(\n    top_n=5,\n    model=\"colbert-ir/colbertv2.0\",\n    tokenizer=\"colbert-ir/colbertv2.0\",\n    keep_retrieval_score=True,\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=[colbert_reranker],\n)\nresponse = query_engine.query(\n    query_str,\n)\n```\n\nFull notebook guide is available [here](../../../examples/node_postprocessor/ColbertRerank.ipynb).\n\n## rankLLM\n\nUses models from [rankLLM](https://github.com/castorini/rank_llm) to rerank documents. Returns the top N ranked nodes.\n\n```python\nfrom llama_index.postprocessor import RankLLMRerank\n\npostprocessor = RankLLMRerank(top_n=5, model=\"zephyr\")\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full [notebook example is available](../../../examples/node_postprocessor/rankLLM.ipynb).\n\n## All Notebooks\n\n- [Sentence Optimizer](../../../examples/node_postprocessor/OptimizerDemo.ipynb)\n- [Cohere Rerank](../../../examples/node_postprocessor/CohereRerank.ipynb)\n- [LLM Reranker Lyft 10k](../../../examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb)\n- [LLM Reranker Gatsby](../../../examples/node_postprocessor/LLMReranker-Gatsby.ipynb)\n- [Recency](../../../examples/node_postprocessor/RecencyPostprocessorDemo.ipynb)\n- [Time Weighted](../../../examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb)\n- [PII](../../../examples/node_postprocessor/PII.ipynb)\n- [PrevNext](../../../examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb)\n- [Metadata Replacement](../../../examples/node_postprocessor/MetadataReplacementDemo.ipynb)\n- [Long Context Reorder](../../../examples/node_postprocessor/LongContextReorder.ipynb)\n- [RankGPT](../../../examples/node_postprocessor/rankGPT.ipynb)\n- [Colbert Rerank](../../../examples/node_postprocessor/ColbertRerank.ipynb)\n- [JinaAI Rerank](../../../examples/node_postprocessor/JinaRerank.ipynb)\n- [MixedBread Rerank](../../../examples/cookbooks/mixedbread_reranker.ipynb)\n- [RankLLM](../../../examples/node_postprocessor/rankLLM.ipynb)", "mimetype": "text/plain", "start_char_idx": 8710, "end_char_idx": 11395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd2c94fe-cb58-459d-9d5f-23d93c47e5bb": {"__data__": {"id_": "dd2c94fe-cb58-459d-9d5f-23d93c47e5bb", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6271897bdcd950cdf7bce0f307d1caf7b7bb03fd", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "4290a53bf29fd4cce6259a308fa1366e6117891373b2d25a7ba1e663f9473b70", "class_name": "RelatedNodeInfo"}}, "text": "# Query Pipeline\n\n## Concept\n\nLlamaIndex provides a declarative query API that allows you to chain together different modules in order to orchestrate simple-to-advanced workflows over your data.\n\nThis is centered around our `QueryPipeline` abstraction. Load in a variety of modules (from LLMs to prompts to retrievers to other pipelines), connect them all together into a sequential chain or DAG, and run it end2end.\n\n**NOTE**: You can orchestrate all these workflows without the declarative pipeline abstraction (by using the modules imperatively and writing your own functions). So what are the advantages of `QueryPipeline`?\n\n- Express common workflows with fewer lines of code/boilerplate\n- Greater readability\n- Greater parity / better integration points with common low-code / no-code solutions (e.g. LangFlow)\n- [In the future] A declarative interface allows easy serializability of pipeline components, providing portability of pipelines/easier deployment to different systems.\n\nOur query pipelines also propagate callbacks throughout all sub-modules, and these integrate with our [observability partners](../../observability/index.md).\n\n![](../../../_static/query/pipeline_rag_example.png)\n\nTo see an interactive example of `QueryPipeline` being put in use, check out the [RAG CLI](../../../getting_started/starter_tools/rag_cli.md).\n\n## Usage Pattern\n\nHere are two simple ways to setup a query pipeline - through a simplified syntax of setting up a sequential chain to setting up a full compute DAG.\n\n```python\nfrom llama_index.core.query_pipeline import QueryPipeline\n\n# sequential chain\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n\n# DAG\np = QueryPipeline(verbose=True)\np.add_modules({\"prompt_tmpl\": prompt_tmpl, \"llm\": llm})\np.add_link(\"prompt_tmpl\", \"llm\")\n\n# run pipeline\np.run(prompt_key1=\"<input1>\", ...)\n```\n\nMore information can be found in our usage pattern guides below.\n\n- [Usage Pattern Guide](./usage_pattern.md)\n- [Module Usage](./module_usage.md)\n\n## Module Guides\n\nCheck out our `QueryPipeline` [end-to-end guides](./modules.md) to learn standard to advanced ways to setup orchestration over your data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7344e602-b6a4-41d4-b350-9597cf183478": {"__data__": {"id_": "7344e602-b6a4-41d4-b350-9597cf183478", "embedding": null, "metadata": {"filename": "module_usage.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b1ee63acc0a41105f5f20f3a969f3b7a7ba7f0a", "node_type": "4", "metadata": {"filename": "module_usage.md", "author": "LlamaIndex"}, "hash": "d262af21f0659538500f6bba1e15b6d4d9ba2717e41ff5f4b6c0143e5ccbf221", "class_name": "RelatedNodeInfo"}}, "text": "# Module Usage\n\nCurrently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own!\n\n### LLMs (both completion and chat)\n\n- Base class: `LLM`\n- [Module Guide](../../models/llms.md)\n- If chat model:\n  - Input: `messages`. Takes in any `List[ChatMessage]` or any stringable input.\n  - Output: `output`. Outputs `ChatResponse` (stringable)\n- If completion model:\n  - Input: `prompt`. Takes in any stringable input.\n  - Output: `output`. Outputs `CompletionResponse` (stringable)\n\n### Prompts\n\n- Base class: `PromptTemplate`\n- [Module Guide](../../models/prompts/index.md)\n- Input: Prompt template variables. Each variable can be a stringable input.\n- Output: `output`. Outputs formatted prompt string (stringable)\n\n### Query Engines\n\n- Base class: `BaseQueryEngine`\n- [Module Guide](../../deploying/query_engine/index.md)\n- Input: `input`. Takes in any stringable input.\n- Output: `output`. Outputs `Response` (stringable)\n\n### Query Transforms\n\n- Base class: `BaseQueryTransform`\n- [Module Guide](../../../optimizing/advanced_retrieval/query_transformations.md)\n- Input: `query_str`, `metadata` (optional). `query_str` is any stringable input.\n- Output: `query_str`. Outputs string.\n\n### Retrievers\n\n- Base class: `BaseRetriever`\n- [Module Guide](../retriever/index.md)\n- Input: `input`. Takes in any stringable input.\n- Output: `output`. Outputs list of nodes `List[BaseNode]`.\n\n### Output Parsers\n\n- Base class: `BaseOutputParser`\n- [Module Guide](../structured_outputs/output_parser.md)\n- Input: `input`. Takes in any stringable input.\n- Output: `output`. Outputs whatever type output parser is supposed to parse out.\n\n### Postprocessors/Rerankers\n\n- Base class: `BaseNodePostprocessor`\n- [Module Guide](../node_postprocessors/index.md)\n- Input: `nodes`, `query_str` (optional). `nodes` is `List[BaseNode]`, `query_str` is any stringable input.\n- Output: `nodes`. Outputs list of nodes `List[BaseNode]`.\n\n### Response Synthesizers\n\n- Base class: `BaseSynthesizer`\n- [Module Guide]()\n- Input: `nodes`, `query_str`. `nodes` is `List[BaseNode]`, `query_str` is any stringable input.\n- Output: `output`. Outputs `Response` object (stringable).\n\n### Other QueryPipeline objects\n\nYou can define a `QueryPipeline` as a module within another query pipeline. This makes it easy for you to string together complex workflows.\n\n### Custom Components\n\nSee our [custom components guide](./usage_pattern.md#defining-a-custom-query-component) for more details.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e42fe406-f404-4183-b6d8-7298a465577c": {"__data__": {"id_": "e42fe406-f404-4183-b6d8-7298a465577c", "embedding": null, "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c0ab1e45f97c43b64548c664b4cf93fefd63c94", "node_type": "4", "metadata": {"filename": "modules.md", "author": "LlamaIndex"}, "hash": "a5678ea0c9ae7d06d04cf4f279dcc2b40e1be29f2bad46750e2dd61f82293b7e", "class_name": "RelatedNodeInfo"}}, "text": "# Module Guides\n\n- [Query Pipeline](../../../examples/pipeline/query_pipeline.ipynb)\n- [Async Query Pipeline](../../../examples/pipeline/query_pipeline_async.ipynb)\n- [Pandas Query Pipeline](../../../examples/pipeline/query_pipeline_pandas.ipynb)\n- [SQL Query Pipeline](../../../examples/pipeline/query_pipeline_sql.ipynb)\n- [Query Pipeline Agent](../../../examples/agent/agent_runner/query_pipeline_agent.ipynb)\n- [Query Pipeline with Memory](../../../examples/pipeline/query_pipeline_memory.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5ba981b-78f0-43c9-9d0d-a39343ee3110": {"__data__": {"id_": "e5ba981b-78f0-43c9-9d0d-a39343ee3110", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd2aac0f6299e5f3d78d6adc03033b85752cea84", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "486de77846776a65030124f55096f5823d47e50684508e35efb6f6c4a97eb146", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abb3f398-33ae-461f-bc6d-25976e8f0c1e", "node_type": "1", "metadata": {}, "hash": "40eb3158a40fc2e80f01984c2eb0db442898e28d4f8f2ae5f9f6c4c1c1dddd8a", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern\n\nThe usage pattern guide covers setup + usage of the `QueryPipeline` more in-depth.\n\n## Setting up a Pipeline\n\nHere we walk through a few different ways of setting up a query pipeline.\n\n### Defining a Sequential Chain\n\nSome simple pipelines are purely linear in nature - the output of the previous module directly goes into the input of the next module.\n\nSome examples:\n\n- prompt -> LLM -> output parsing\n- prompt -> LLM -> prompt -> LLM\n- retriever -> response synthesizer\n\nThese workflows can easily be expressed in the `QueryPipeline` through a simplified `chain` syntax.\n\n```python\nfrom llama_index.core.query_pipeline import QueryPipeline\n\n# try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n```\n\n### Defining a DAG\n\nMany pipelines will require you to setup a DAG (for instance, if you want to implement all the steps in a standard RAG pipeline).\n\nHere we offer a lower-level API to add modules along with their keys, and define links between previous module outputs to next\nmodule inputs.\n\n```python\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.core.response_synthesizers import TreeSummarize\n\n# define modules\nprompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\nretriever = index.as_retriever(similarity_top_k=3)\nreranker = CohereRerank()\nsummarizer = TreeSummarize(llm=llm)\n\n# define query pipeline\np = QueryPipeline(verbose=True)\np.add_modules(\n    {\n        \"llm\": llm,\n        \"prompt_tmpl\": prompt_tmpl,\n        \"retriever\": retriever,\n        \"summarizer\": summarizer,\n        \"reranker\": reranker,\n    }\n)\np.add_link(\"prompt_tmpl\", \"llm\")\np.add_link(\"llm\", \"retriever\")\np.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\np.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\np.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\np.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")\n```\n\n## Running the Pipeline\n\n### Single-Input/Single-Output\n\nThe input is the kwargs of the first component.\n\nIf the output of the last component is a single object (and not a dictionary of objects), then we return that directly.\n\nTaking the pipeline in the previous example, the output will be a `Response` object since the last step is the `TreeSummarize` response synthesis module.\n\n```python\noutput = p.run(topic=\"YC\")\n# output type is Response\ntype(output)\n```\n\n### Multi-Input/Multi-Output\n\nIf your DAG has multiple root nodes / and-or output nodes, you can try `run_multi`. Pass in an input dictionary containing module key -> input dict. Output is dictionary of module key -> output dict.\n\nIf we ran the prev example,\n\n```python\noutput_dict = p.run_multi({\"llm\": {\"topic\": \"YC\"}})\nprint(output_dict)\n\n# output dict is {\"summarizer\": {\"output\": response}}\n```\n\n### Defining partials\n\nIf you wish to prefill certain inputs for a module, you can do so with `partial`! Then the DAG would just hook into the unfilled inputs.\n\nYou may need to convert a module via `as_query_component`.\n\nHere's an example:\n\n```python\nsummarizer = TreeSummarize(llm=llm)\nsummarizer_c = summarizer.as_query_component(partial={\"nodes\": nodes})\n# can define a chain because llm output goes into query_str, nodes is pre-filled\np = QueryPipeline(chain=[prompt_tmpl, llm, summarizer_c])\n# run pipeline\np.run(topic=\"YC\")\n```\n\n### Batch Input\n\nIf you wish to run the pipeline for several rounds of single/multi-inputs, set `batch=True` in the function call - supported by `run`, `arun`, `run_multi`, and `arun_multi`. Pass in a list of individual single/multi-inputs you would like to run. `batch` mode will return a list of responses in the same order as the inputs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abb3f398-33ae-461f-bc6d-25976e8f0c1e": {"__data__": {"id_": "abb3f398-33ae-461f-bc6d-25976e8f0c1e", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd2aac0f6299e5f3d78d6adc03033b85752cea84", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "486de77846776a65030124f55096f5823d47e50684508e35efb6f6c4a97eb146", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5ba981b-78f0-43c9-9d0d-a39343ee3110", "node_type": "1", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "31e572a6fd23f402ac21797f4e278a18f7337e26a0e68bebf29862aa8445d4b0", "class_name": "RelatedNodeInfo"}}, "text": "`batch` mode will return a list of responses in the same order as the inputs.\n\nExample for single-input/single-output: `p.run(field=[in1: Any, in2: Any], batch=True)` --> `[out1: Any, out2: Any]`\n\n```python\noutput = p.run(topic=[\"YC\", \"RAG\", \"LlamaIndex\"], batch=True)\n# output is [ResponseYC, ResponseRAG, ResponseLlamaIndex]\nprint(output)\n```\n\nExample for multi-input/multi-output: `p.run_multi(\"root_node\": {\"field\": [in1: Any, in2, Any]}, batch=True)` --> `{\"output_node\": {\"field\": [out1: Any, out2: Any]}}`\n\n```python\noutput_dict = p.run_multi({\"llm\": {\"topic\": [\"YC\", \"RAG\", \"LlamaIndex\"]}})\nprint(output_dict)\n\n# output dict is {\"summarizer\": {\"output\": [ResponseYC, ResponseRAG, ResponseLlamaIndex]}}\n```\n\n\n### Intermediate outputs\n\nIf you wish to obtain the intermediate outputs of modules in QueryPipeline, you can use `run_with_intermediates` or `run_multi_with_intermediates` for single-input and multi-input, respectively.\n\nThe output will be a tuple of the normal output and a dictionary containing module key -> `ComponentIntermediates`. ComponentIntermediates has 2 fields: `inputs` dict and `outputs` dict.\n\n```python\noutput, intermediates = p.run_with_intermediates(topic=\"YC\")\nprint(output)\nprint(intermediates)\n\n# output is (Response, {\"module_key\": ComponentIntermediates(\"inputs\": {}, \"outputs\": {})})\n```\n\n## Defining a Custom Query Component\n\nYou can easily define a custom component: Either passing a function to a `FnComponent` or subclassing a `CustomQueryComponent`.\n\n### Passing a Function to `FnComponent`\n\nDefine any function and pass it to `FnComponent`. The positional argument names (`args`) will get converted to required input keys, and the keyword argument names (`kwargs`) will get converted to optional input keys.\n\n**NOTE**: We assume there is only a single output.\n\n```python\nfrom llama_index.core.query_pipeline import FnComponent\n\n\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers.\"\"\"\n    return a + b\n\n\nadd_component = FnComponent(fn=add, output_key=\"output\")\n\n# input keys to add_component are \"a\" and \"b\", output key is 'output'\n```\n\n### Subclassing a `CustomQueryComponent`\n\nSimply subclass a `CustomQueryComponent`, implement validation/run functions + some helpers, and plug it in.\n\n```python\nfrom llama_index.core.query_pipeline import CustomQueryComponent\nfrom typing import Dict, Any\n\n\nclass MyComponent(CustomQueryComponent):\n    \"\"\"My component.\"\"\"\n\n    # Pydantic class, put any attributes here\n    ...\n\n    def _validate_component_inputs(\n        self, input: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # NOTE: this is OPTIONAL but we show you here how to do validation as an example\n        return input\n\n    @property\n    def _input_keys(self) -> set:\n        \"\"\"Input keys dict.\"\"\"\n        return {\"input_key1\", ...}\n\n    @property\n    def _output_keys(self) -> set:\n        # can do multi-outputs too\n        return {\"output_key\"}\n\n    def _run_component(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Run the component.\"\"\"\n        # run logic\n        ...\n        return {\"output_key\": result}\n```\n\nFor more details check out our [in-depth query transformations guide](../../../examples/pipeline/query_pipeline.ipynb).\n\n## Ensuring outputs are compatible\n\nBy linking modules within a `QueryPipeline`, the output of one module goes into the input of the next module.\n\nGenerally you must make sure that for a link to work, the expected output and input types _roughly_ line up.\n\nWe say roughly because we do some magic on existing modules to make sure that \"stringable\" outputs can be passed into\ninputs that can be queried as a \"string\". Certain output types are treated as Stringable - `CompletionResponse`, `ChatResponse`, `Response`, `QueryBundle`, etc. Retrievers/query engines will automatically convert `string` inputs to `QueryBundle` objects.\n\nThis lets you do certain workflows that would otherwise require boilerplate string conversion if you were writing this yourself, for instance,\n\n- LLM -> prompt, LLM -> retriever, LLM -> query engine\n- query engine -> prompt, query engine -> retriever\n\nIf you are defining a custom component, you should use `_validate_component_inputs` to ensure that the inputs are the right type, and throw an error if they're not.", "mimetype": "text/plain", "start_char_idx": 3824, "end_char_idx": 8125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ceb340a-2197-4aa2-9042-1de2c606f12a": {"__data__": {"id_": "8ceb340a-2197-4aa2-9042-1de2c606f12a", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bde276b52e12284b88a00264f708a23e045215e", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f648633a89971342c0cfab51eb5ea59f459998cc4b084400d4e9d3d48927826e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "017ba432-8661-462e-ba0c-cc9858f88a91", "node_type": "1", "metadata": {}, "hash": "ba4cd5d9d9c6940e01cd091b5ac6b2973cb149943ab8f8996a0a0d022705c096", "class_name": "RelatedNodeInfo"}}, "text": "# Response Synthesizer\n\n## Concept\n\nA `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a `Response` object.\n\nThe method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.\n\nWhen used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.\n\n!!! tip\n    Confused about where response synthesizer fits in the pipeline? Read the [high-level concepts](../../../getting_started/concepts.md)\n\n## Usage Pattern\n\nUse a response synthesizer on it's own:\n\n```python\nfrom llama_index.core.data_structs import Node\nfrom llama_index.core.response_synthesizers import ResponseMode\nfrom llama_index.core import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=ResponseMode.COMPACT\n)\n\nresponse = response_synthesizer.synthesize(\n    \"query text\", nodes=[Node(text=\"text\"), ...]\n)\n```\n\nOr in a query engine after you've created an index:\n\n```python\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n```\n\nYou can find more details on all available response synthesizers, modes, and how to build your own below.\n\n## Usage Pattern\n\n## Get Started\n\nConfiguring the response synthesizer for a query engine using `response_mode`:\n\n```python\nfrom llama_index.core.data_structs import Node\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n\nresponse = response_synthesizer.synthesize(\n    \"query text\", nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ...]\n)\n```\n\nOr, more commonly, in a query engine after you've created an index:\n\n```python\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n```\n\n!!! tip\n    To learn how to build an index, see [Indexing](../../indexing/index.md)\n\n## Configuring the Response Mode\n\nResponse synthesizers are typically specified through a `response_mode` kwarg setting.\n\nSeveral response synthesizers are implemented already in LlamaIndex:\n\n- `refine`: **_create and refine_** an answer by sequentially going through each retrieved text chunk.\n  This makes a separate LLM call per Node/retrieved chunk.\n\n  **Details:** the first chunk is used in a query using the\n  `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used\n  in another query with the `refine_template` prompt. And so on until all chunks have been parsed.\n\n  If a chunk is too large to fit within the window (considering the prompt size), it is split using a `TokenTextSplitter`\n  (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks\n  of the original chunks collection (and thus queried with the `refine_template` as well).\n\n  Good for more detailed answers.\n\n- `compact` (default): similar to `refine` but **_compact_** (concatenate) the chunks beforehand, resulting in less LLM calls.\n\n  **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window\n  (considering the maximum prompt size between `text_qa_template` and `refine_template`).\n  If the text is too long to fit in one prompt, it is split in as many parts as needed\n  (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).\n\n  Each text part is considered a \"chunk\" and is sent to the `refine` synthesizer.\n\n  In short, it is like `refine`, but with less LLM calls.\n\n- `tree_summarize`: Query the LLM using the `summary_template` prompt as many times as needed so that all concatenated chunks\n  have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call\n  and so on, until there's only one chunk left, and thus only one final answer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "017ba432-8661-462e-ba0c-cc9858f88a91": {"__data__": {"id_": "017ba432-8661-462e-ba0c-cc9858f88a91", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bde276b52e12284b88a00264f708a23e045215e", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f648633a89971342c0cfab51eb5ea59f459998cc4b084400d4e9d3d48927826e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ceb340a-2197-4aa2-9042-1de2c606f12a", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "2ceb0a73b2d27433c026d2f7b4779220af5a1a29e3a6ac09cc80cc23ae815f44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "005dc515-1fd8-429c-9dc6-3c5ead7bd887", "node_type": "1", "metadata": {}, "hash": "a3da23c3c60ac336dd39aca5a1124f57193d9f50c70c1a5569311ff5765a20b5", "class_name": "RelatedNodeInfo"}}, "text": "**Details:** concatenate the chunks as much as possible to fit within the context window using the `summary_template` prompt,\n  and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against\n  `summary_template` (there is no **_refine_** query !) and get as many answers.\n\n  If there is only one answer (because there was only one chunk), then it's the final answer.\n\n  If there are more than one answer, these themselves are considered as chunks and sent recursively\n  to the `tree_summarize` process (concatenated/splitted-to-fit/queried).\n\n  Good for summarization purposes.\n\n- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick\n  summarization purposes, but may lose detail due to truncation.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,\n  without actually sending them. Then can be inspected by checking `response.source_nodes`.\n- `context_only`: Returns a concatenated string of all text chunks.\n- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n  chunk while accumulating the responses into an array. Returns a concatenated string of all\n  responses. Good for when you need to run the same query separately against each text\n  chunk.\n- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n  `compact`, and run the same query against each text chunk.\n\n## Custom Response Synthesizers\n\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`. The base API is extremely simple, which makes it easy to create your own response synthesizer.\n\nMaybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it's own.\n\nBelow we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement. The basic requirements are to process a query and text chunks, and return a string (or string generator) response.\n\n```python\nfrom llama_index.core import Settings", "mimetype": "text/plain", "start_char_idx": 4206, "end_char_idx": 6496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "005dc515-1fd8-429c-9dc6-3c5ead7bd887": {"__data__": {"id_": "005dc515-1fd8-429c-9dc6-3c5ead7bd887", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bde276b52e12284b88a00264f708a23e045215e", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f648633a89971342c0cfab51eb5ea59f459998cc4b084400d4e9d3d48927826e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "017ba432-8661-462e-ba0c-cc9858f88a91", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c91a3ebb489796ca4670257d90a2138f677fc7edf7fcd1b372a06e5535597595", "class_name": "RelatedNodeInfo"}}, "text": "The basic requirements are to process a query and text chunks, and return a string (or string generator) response.\n\n```python\nfrom llama_index.core import Settings\n\n\nclass BaseSynthesizer(ABC):\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._callback_manager = Settings.callback_manager\n        self._streaming = streaming\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n```\n\n## Using Structured Answer Filtering\n\nWhen using either the `\"refine\"` or `\"compact\"` response synthesis modules, you may find it beneficial to experiment with the `structured_answer_filtering` option.\n\n```\nfrom llama_index.core import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(structured_answer_filtering=True)\n```\n\nWith `structured_answer_filtering` set to `True`, our refine module is able to filter out any input nodes that are not relevant to the question being asked. This is particularly useful for RAG-based Q&A systems that involve retrieving chunks of text from external vector store for a given user query.\n\nThis option is particularly useful if you're using an [OpenAI model that supports function calling](https://openai.com/blog/function-calling-and-other-api-updates). Other LLM providers or models that don't have native function calling support may be less reliable in producing the structured response this feature relies on.\n\n## Using Custom Prompt Templates (with additional variables)\n\nYou may want to customize the prompts used in our response synthesizer, and also add additional variables during query-time.\n\nYou can specify these additional variables in the `**kwargs` for `get_response`.\n\nFor example,\n\n```python\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.response_synthesizers import TreeSummarize\n\n# NOTE: we add an extra tone_name variable here\nqa_prompt_tmpl = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Please also write the answer in the tone of {tone_name}.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_prompt = PromptTemplate(qa_prompt_tmpl)\n\n# initialize response synthesizer\nsummarizer = TreeSummarize(verbose=True, summary_template=qa_prompt)\n\n# get response\nresponse = summarizer.get_response(\n    \"who is Paul Graham?\", [text], tone_name=\"a Shakespeare play\"\n)\n```\n\n## Modules\n\nSee the full [module guide](./response_synthesizers.md) for more details.", "mimetype": "text/plain", "start_char_idx": 6333, "end_char_idx": 9409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95fd65dd-eb21-4164-bd0a-d9c6622f2041": {"__data__": {"id_": "95fd65dd-eb21-4164-bd0a-d9c6622f2041", "embedding": null, "metadata": {"filename": "response_synthesizers.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72bca5ffc14ac583cabd4f18a30c37965743159b", "node_type": "4", "metadata": {"filename": "response_synthesizers.md", "author": "LlamaIndex"}, "hash": "51077f3e04976163e0fcc9607030af7aed5f4ed78bee3019dbe4ee5f09c1b4b0", "class_name": "RelatedNodeInfo"}}, "text": "# Response Synthesis Modules\n\nDetailed inputs/outputs for each response synthesizer are found below.\n\n## API Example\n\nThe following shows the setup for utilizing all kwargs.\n\n- `response_mode` specifies which response synthesizer to use\n- `service_context` defines the LLM and related settings for synthesis\n- `text_qa_template` and `refine_template` are the prompts used at various stages\n- `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree\n- `streaming` configures whether to return a streaming response object or not\n- `structured_answer_filtering` enables the active filtering of text chunks that are not relevant to a given question\n\nIn the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.\n\n```python\nfrom llama_index.core.data_structs import Node\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"refine\",\n    service_context=service_context,\n    text_qa_template=text_qa_template,\n    refine_template=refine_template,\n    use_async=False,\n    streaming=False,\n)\n\n# synchronous\nresponse = response_synthesizer.synthesize(\n    \"query string\",\n    nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ...],\n    additional_source_nodes=[\n        NodeWithScore(node=Node(text=\"text\"), score=1.0),\n        ...,\n    ],\n)\n\n# asynchronous\nresponse = await response_synthesizer.asynthesize(\n    \"query string\",\n    nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ...],\n    additional_source_nodes=[\n        NodeWithScore(node=Node(text=\"text\"), score=1.0),\n        ...,\n    ],\n)\n```\n\nYou can also directly return a string, using the lower-level `get_response` and `aget_response` functions\n\n```python\nresponse_str = response_synthesizer.get_response(\n    \"query string\", text_chunks=[\"text1\", \"text2\", ...]\n)\n```\n\n## Example Notebooks\n\n- [Refine](../../../examples/response_synthesizers/refine.ipynb)\n- [Structured Refine](../../../examples/response_synthesizers/structured_refine.ipynb)\n- [Tree Summarize](../../../examples/response_synthesizers/tree_summarize.ipynb)\n- [Custom Prompting](../../../examples/response_synthesizers/custom_prompt_synthesizer.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30f35635-6c5a-42f1-91fb-1115acaa47c9": {"__data__": {"id_": "30f35635-6c5a-42f1-91fb-1115acaa47c9", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f4002b19f77e12dc75b5fcb4338ee6624d5f0e0", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c74efe37783513f16a015ce045d3cdcd374d59a859d32c381abd909d96a2a89a", "class_name": "RelatedNodeInfo"}}, "text": "# Retriever\n\n## Concept\n\nRetrievers are responsible for fetching the most relevant context given a user query (or chat message).\n\nIt can be built on top of [indexes](../../indexing/index.md), but can also be defined independently.\nIt is used as a key building block in [query engines](../../deploying/query_engine/index.md) (and [Chat Engines](../../deploying/chat_engines/index.md)) for retrieving relevant context.\n\n!!! tip\n    Confused about where retriever fits in the pipeline? Read about [high-level concepts](../../../getting_started/concepts.md)\n\n## Usage Pattern\n\nGet started with:\n\n```python\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"Who is Paul Graham?\")\n```\n\n## Get Started\n\nGet a retriever from index:\n\n```python\nretriever = index.as_retriever()\n```\n\nRetrieve relevant context for a question:\n\n```python\nnodes = retriever.retrieve(\"Who is Paul Graham?\")\n```\n\n> Note: To learn how to build an index, see [Indexing](../../indexing/index.md)\n\n## High-Level API\n\n### Selecting a Retriever\n\nYou can select the index-specific retriever class via `retriever_mode`.\nFor example, with a `SummaryIndex`:\n\n```python\nretriever = summary_index.as_retriever(\n    retriever_mode=\"llm\",\n)\n```\n\nThis creates a [SummaryIndexLLMRetriever](../../../api_reference/retrievers/summary.md) on top of the summary index.\n\nSee [**Retriever Modes**](retriever_modes.md) for a full list of (index-specific) retriever modes\nand the retriever classes they map to.\n\n### Configuring a Retriever\n\nIn the same way, you can pass kwargs to configure the selected retriever.\n\n> Note: take a look at the API reference for the selected retriever class' constructor parameters for a list of valid kwargs.\n\nFor example, if we selected the \"llm\" retriever mode, we might do the following:\n\n```python\nretriever = summary_index.as_retriever(\n    retriever_mode=\"llm\",\n    choice_batch_size=5,\n)\n```\n\n## Low-Level Composition API\n\nYou can use the low-level composition API if you need more granular control.\n\nTo achieve the same outcome as above, you can directly import and construct the desired retriever class:\n\n```python\nfrom llama_index.core.retrievers import SummaryIndexLLMRetriever\n\nretriever = SummaryIndexLLMRetriever(\n    index=summary_index,\n    choice_batch_size=5,\n)\n```\n\n## Examples\n\nSee more examples in the [retrievers guide](./retrievers.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d7add49-b129-435e-834d-7b315f696c35": {"__data__": {"id_": "4d7add49-b129-435e-834d-7b315f696c35", "embedding": null, "metadata": {"filename": "retriever_modes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd5a0040fe0758d794e9feaf60baa92ff4a4460e", "node_type": "4", "metadata": {"filename": "retriever_modes.md", "author": "LlamaIndex"}, "hash": "42b5109e2d8b4510ddf1a9b77f2415a820f8f2f96134dfa16094e686d7080f1e", "class_name": "RelatedNodeInfo"}}, "text": "# Retriever Modes\n\nHere we show the mapping from `retriever_mode` configuration to the selected retriever class.\n\n> Note that `retriever_mode` can mean different thing for different index classes.\n\n## Vector Index\n\nSpecifying `retriever_mode` has no effect (silently ignored).\n`vector_index.as_retriever(...)` always returns a VectorIndexRetriever.\n\n## Summary Index\n\n- `default`: SummaryIndexRetriever\n- `embedding`: SummaryIndexEmbeddingRetriever\n- `llm`: SummaryIndexLLMRetriever\n\n## Tree Index\n\n- `select_leaf`: TreeSelectLeafRetriever\n- `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever\n- `all_leaf`: TreeAllLeafRetriever\n- `root`: TreeRootRetriever\n\n## Keyword Table Index\n\n- `default`: KeywordTableGPTRetriever\n- `simple`: KeywordTableSimpleRetriever\n- `rake`: KeywordTableRAKERetriever\n\n## Knowledge Graph Index\n\n- `keyword`: KGTableRetriever\n- `embedding`: KGTableRetriever\n- `hybrid`: KGTableRetriever\n\n## Document Summary Index\n\n- `llm`: DocumentSummaryIndexLLMRetriever\n- `embedding`: DocumentSummaryIndexEmbeddingRetrievers", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56d9a115-a1cc-4148-911d-c3be4426c228": {"__data__": {"id_": "56d9a115-a1cc-4148-911d-c3be4426c228", "embedding": null, "metadata": {"filename": "retrievers.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dace94e70011355e27cec93dee718c3e229afed4", "node_type": "4", "metadata": {"filename": "retrievers.md", "author": "LlamaIndex"}, "hash": "c8b4103c6459157ed56229000e4ef930509b7b2e91a9f0d21d6d5602a9f15b2e", "class_name": "RelatedNodeInfo"}}, "text": "# Retriever Modules\n\nWe are actively adding more tailored retrieval guides.\nIn the meanwhile, please take a look at the [API References](../../../api_reference/retrievers/index.md).\n\n## Index Retrievers\n\nPlease see [the retriever modes](retriever_modes.md) for more details on how to get a retriever from any given index.\n\nIf you want to import the corresponding retrievers directly, please check out our [API reference](../../../api_reference/retrievers/index.md).\n\n## Comprehensive Retriever Guides\n\nCheck out our comprehensive guides on various retriever modules, many of which cover advanced concepts (auto-retrieval, routing, ensembling, and more).\n\n### Advanced Retrieval and Search\n\nThese guides contain advanced retrieval techniques. Some are common like keyword/hybrid search, reranking, and more.\nSome are specific to LLM + RAG pipelines, like small-to-big and auto-merging retrieval.\n\n- [Define Custom Retriever](../../../examples/query_engine/CustomRetrievers.ipynb)\n- [BM25 Hybrid Retriever](../../../examples/retrievers/bm25_retriever.ipynb)\n- [Simple Query Fusion](../../../examples/retrievers/simple_fusion.ipynb)\n- [Reciprocal Rerank Fusion](../../../examples/retrievers/reciprocal_rerank_fusion.ipynb)\n- [Auto Merging Retriever](../../../examples/retrievers/auto_merging_retriever.ipynb)\n- [Metadata Replacement](../../../examples/node_postprocessor/MetadataReplacementDemo.ipynb)\n- [Composable Retrievers](../../../examples/retrievers/composable_retrievers.ipynb)\n\n### Auto-Retrieval\n\nThese retrieval techniques perform **semi-structured** queries, combining semantic search with structured filtering.\n\n- [Auto-Retrieval (with Pinecone)](../../../examples/vector_stores/pinecone_auto_retriever.ipynb)\n- [Auto-Retrieval (with Lantern)](../../../examples/vector_stores/LanternAutoRetriever.ipynb)\n- [Auto-Retrieval (with Chroma)](../../../examples/vector_stores/chroma_auto_retriever.ipynb)\n- [Auto-Retrieval (with BagelDB)](../../../examples/vector_stores/BagelAutoRetriever.ipynb)\n- [Auto-Retrieval (with Vectara)](../../../examples/retrievers/vectara_auto_retriever.ipynb)\n- [Multi-Doc Auto-Retrieval](../../../examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.ipynb)\n\n### Knowledge Graph Retrievers\n\n- [Knowledge Graph RAG Retriever](../../../examples/query_engine/knowledge_graph_rag_query_engine.ipynb)\n\n### Composed Retrievers\n\nThese are retrieval techniques that are composed on top of other retrieval techniques - providing higher-level capabilities like\nhierarchical retrieval and query decomposition.\n\n- [Query Fusion](../../../examples/retrievers/reciprocal_rerank_fusion.ipynb)\n- [Recursive Table Retrieval](../../../examples/query_engine/pdf_tables/recursive_retriever.ipynb)\n- [Recursive Node Retrieval](../../../examples/retrievers/recursive_retriever_nodes.ipynb)\n- [Braintrust](../../../examples/retrievers/recurisve_retriever_nodes_braintrust.ipynb)\n- [Router Retriever](../../../examples/retrievers/router_retriever.ipynb)\n- [Ensemble Retriever](../../../examples/retrievers/ensemble_retrieval.ipynb)\n- [Multi-Doc Auto-Retrieval](../../../examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.ipynb)\n\n### Managed Retrievers\n\n- [Google](../../../examples/managed/GoogleDemo.ipynb)\n- [Vectara](../../../examples/managed/vectaraDemo.ipynb)\n- [VideoDB](../../../examples/retrievers/videodb_retriever.ipynb)\n- [Zilliz](../../../examples/managed/zcpDemo.ipynb)\n- [Amazon Bedrock](../../../examples/retrievers/bedrock_retriever.ipynb)\n\n### Other Retrievers\n\nThese are guides that don't fit neatly into a category but should be highlighted regardless.\n\n- [Multi-Doc Hybrid](../../../examples/retrievers/multi_doc_together_hybrid.ipynb)\n- [You Retriever](../../../examples/retrievers/you_retriever.ipynb)\n- [Text-to-SQL](../../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb)\n- [DeepMemory (Activeloop)](../../../examples/retrievers/deep_memory.ipynb)\n- [Pathway](../../../examples/retrievers/pathway_retriever.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a81b3d3-33df-4bf6-8fdc-1969fb80c46f": {"__data__": {"id_": "0a81b3d3-33df-4bf6-8fdc-1969fb80c46f", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9cff212012951393eb8ceb0352e73260c4eb3d8", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f16e2911d4aad5bd4d22f4e07a25f3dda9dcbb4a01e9c73135ad7e7c15c26b3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffdf68d7-4a10-409a-adbb-76892606a43e", "node_type": "1", "metadata": {}, "hash": "bd522ff7a8138ac08f163de968d0313a478f2a39d54635d69779cec645475a81", "class_name": "RelatedNodeInfo"}}, "text": "# Routers\n\n## Concept\n\nRouters are modules that take in a user query and a set of \"choices\" (defined by metadata), and returns one or more selected choices.\n\nThey can be used on their own (as \"selector modules\"), or used as a query engine or retriever (e.g. on top of other query engines/retrievers).\n\nThey are simple but powerful modules that use LLMs for decision making capabilities. They can be used for the following use cases and more:\n\n- Selecting the right data source among a diverse range of data sources\n- Deciding whether to do summarization (e.g. using summary index query engine) or semantic search (e.g. using vector index query engine)\n- Deciding whether to \"try\" out a bunch of choices at once and combine the results (using multi-routing capabilities).\n\nThe core router modules exist in the following forms:\n\n- LLM selectors put the choices as a text dump into a prompt and use LLM text completion endpoint to make decisions\n- Pydantic selectors pass choices as Pydantic schemas into a function calling endpoint, and return Pydantic objects\n\n## Usage Pattern\n\nA simple example of using our router module as part of a query engine is given below.\n\n```python\nfrom llama_index.core.query_engine import RouterQueryEngine\nfrom llama_index.core.selectors import PydanticSingleSelector\nfrom llama_index.core.tools import QueryEngineTool", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffdf68d7-4a10-409a-adbb-76892606a43e": {"__data__": {"id_": "ffdf68d7-4a10-409a-adbb-76892606a43e", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9cff212012951393eb8ceb0352e73260c4eb3d8", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f16e2911d4aad5bd4d22f4e07a25f3dda9dcbb4a01e9c73135ad7e7c15c26b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a81b3d3-33df-4bf6-8fdc-1969fb80c46f", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "177946a1ee3208ddf41f911ccb5d4b2fb223f6ba335a5cac69fcdcadee3def02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ffd4a76-91d4-424a-b913-520a0720f543", "node_type": "1", "metadata": {}, "hash": "e69d95a357d5ac829f8fccb4c21a0e0e2ad786552bd2ab66838707f95d9ae53c", "class_name": "RelatedNodeInfo"}}, "text": "list_tool = QueryEngineTool.from_defaults(\n    query_engine=list_query_engine,\n    description=\"Useful for summarization questions related to the data source\",\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=\"Useful for retrieving specific context related to the data source\",\n)\n\nquery_engine = RouterQueryEngine(\n    selector=PydanticSingleSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\nquery_engine.query(\"<query>\")\n```\n\n## Usage Pattern\n\nDefining a \"selector\" is at the core of defining a router.\n\nYou can easily use our routers as a query engine or a retriever. In these cases, the router will be responsible\nfor \"selecting\" query engine(s) or retriever(s) to route the user query to.\n\nWe also highlight our `ToolRetrieverRouterQueryEngine` for retrieval-augmented routing - this is the case\nwhere the set of choices themselves may be very big and may need to be indexed. **NOTE**: this is a beta feature.\n\nWe also highlight using our router as a standalone module.\n\n## Defining a selector\n\nSome examples are given below with LLM and Pydantic based single/multi selectors:\n\n```python\nfrom llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\nfrom llama_index.core.selectors import (\n    PydanticMultiSelector,\n    PydanticSingleSelector,\n)\n\n# pydantic selectors feed in pydantic objects to a function calling API\n# single selector (pydantic)\nselector = PydanticSingleSelector.from_defaults()\n# multi selector (pydantic)\nselector = PydanticMultiSelector.from_defaults()\n\n# LLM selectors use text completion endpoints\n# single selector (LLM)\nselector = LLMSingleSelector.from_defaults()\n# multi selector (LLM)\nselector = LLMMultiSelector.from_defaults()\n```\n\n## Using as a Query Engine\n\nA `RouterQueryEngine` is composed on top of other query engines as tools.\n\n```python\nfrom llama_index.core.query_engine import RouterQueryEngine\nfrom llama_index.core.selectors import PydanticSingleSelector\nfrom llama_index.core.selectors.pydantic_selectors import Pydantic\nfrom llama_index.core.tools import QueryEngineTool\nfrom llama_index.core import VectorStoreIndex, SummaryIndex\n\n# define query engines\n...\n\n# initialize tools\nlist_tool = QueryEngineTool.from_defaults(\n    query_engine=list_query_engine,\n    description=\"Useful for summarization questions related to the data source\",\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=\"Useful for retrieving specific context related to the data source\",\n)\n\n# initialize router query engine (single selection, pydantic)\nquery_engine = RouterQueryEngine(\n    selector=PydanticSingleSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\nquery_engine.query(\"<query>\")\n```\n\n## Using as a Retriever\n\nSimilarly, a `RouterRetriever` is composed on top of other retrievers as tools. An example is given below:\n\n```python\nfrom llama_index.core.query_engine import RouterQueryEngine\nfrom llama_index.core.selectors import PydanticSingleSelector\nfrom llama_index.core.tools import RetrieverTool\n\n# define indices\n...\n\n# define retrievers\nvector_retriever = vector_index.as_retriever()\nkeyword_retriever = keyword_index.as_retriever()\n\n# initialize tools\nvector_tool = RetrieverTool.from_defaults(\n    retriever=vector_retriever,\n    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n)\nkeyword_tool = RetrieverTool.from_defaults(\n    retriever=keyword_retriever,\n    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On (using entities mentioned in query)\",\n)\n\n# define retriever\nretriever = RouterRetriever(\n    selector=PydanticSingleSelector.from_defaults(llm=llm),\n    retriever_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\n```\n\n## Using selector as a standalone module\n\nYou can use the selectors as standalone modules. Define choices as either a list of `ToolMetadata` or as a list of strings.\n\n```python\nfrom llama_index.core.tools import ToolMetadata\nfrom llama_index.core.selectors import LLMSingleSelector", "mimetype": "text/plain", "start_char_idx": 1350, "end_char_idx": 5520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ffd4a76-91d4-424a-b913-520a0720f543": {"__data__": {"id_": "2ffd4a76-91d4-424a-b913-520a0720f543", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9cff212012951393eb8ceb0352e73260c4eb3d8", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "f16e2911d4aad5bd4d22f4e07a25f3dda9dcbb4a01e9c73135ad7e7c15c26b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffdf68d7-4a10-409a-adbb-76892606a43e", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "a03559b115b2b25cb2a2b15c7b13fdbad5cbc77147c57b98ffaff11f35f352ed", "class_name": "RelatedNodeInfo"}}, "text": "# choices as a list of tool metadata\nchoices = [\n    ToolMetadata(description=\"description for choice 1\", name=\"choice_1\"),\n    ToolMetadata(description=\"description for choice 2\", name=\"choice_2\"),\n]\n\n# choices as a list of strings\nchoices = [\n    \"choice 1 - description for choice 1\",\n    \"choice 2: description for choice 2\",\n]\n\nselector = LLMSingleSelector.from_defaults()\nselector_result = selector.select(\n    choices, query=\"What's revenue growth for IBM in 2007?\"\n)\nprint(selector_result.selections)\n```\n\nMore examples:\n\n- [Router Query Engine](../../../examples/query_engine/RouterQueryEngine.ipynb)\n- [Retriever Router Query Engine](../../../examples/query_engine/RetrieverRouterQueryEngine.ipynb)\n- [SQL Router Query Engine](../../../examples/query_engine/SQLRouterQueryEngine.ipynb)\n- [Router Retriever](../../../examples/retrievers/router_retriever.ipynb)", "mimetype": "text/plain", "start_char_idx": 5523, "end_char_idx": 6392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e10328c-b630-4d43-9868-a89922b86e7e": {"__data__": {"id_": "3e10328c-b630-4d43-9868-a89922b86e7e", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f63d7cc27bc4c0726384f55108bfb73209ec8365", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d081500ed463b67987456c12d671420c47c04dfe3d367601f82808a0d81556e6", "class_name": "RelatedNodeInfo"}}, "text": "# Structured Outputs\n\nThe ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values.\nLlamaIndex itself also relies on structured output in the following ways.\n\n- **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \"ANSWER: (number)\".\n- **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.)\n\nLlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format. We provide modules at different levels of abstraction:\n\n- **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint. They are not used with LLM function calling endpoints (since those contain structured outputs out of the box).\n- **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object. They may use function calling APIs or text completion APIs + output parsers. These can also be integrated with query engines.\n- **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).\n\nSee the sections below for an overview of output parsers and Pydantic programs.\n\n## \ud83d\udd2c Anatomy of a Structured Output Function\n\nHere we describe the different components of an LLM-powered structured output function. The pipeline depends on whether you're using a **generic LLM text completion API** or an **LLM function calling API**.\n\n![](../../../_static/structured_output/diagram1.png)\n\nWith generic completion APIs, the inputs and outputs are handled by text prompts. The output parser plays a role before and after the LLM call in ensuring structured outputs. Before the LLM call, the output parser can\nappend format instructions to the prompt. After the LLM call, the output parser can parse the output to the specified instructions.\n\nWith function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object. The structured output just needs to be cast in the right object format (e.g. Pydantic).\n\n## Resources\n\n- [Pydantic Programs](./pydantic_program.md)\n- [Structured Outputs + Query Engines](./query_engine.md)\n- [Output Parsers](./output_parser.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74a0d9ad-2451-4f8e-8e01-99d386398b2a": {"__data__": {"id_": "74a0d9ad-2451-4f8e-8e01-99d386398b2a", "embedding": null, "metadata": {"filename": "output_parser.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5048b1edf1ee1994496d53784f5453ba788c67c1", "node_type": "4", "metadata": {"filename": "output_parser.md", "author": "LlamaIndex"}, "hash": "5d788379195213185ff891b32f836015408c8b844fc34fb23d05871c4f13dfe9", "class_name": "RelatedNodeInfo"}}, "text": "# Output Parsing Modules\n\nLlamaIndex supports integrations with output parsing modules offered\nby other frameworks. These output parsing modules can be used in the following ways:\n\n- To provide formatting instructions for any prompt / query (through `output_parser.format`)\n- To provide \"parsing\" for LLM outputs (through `output_parser.parse`)\n\n### Guardrails\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers.guardrails import GuardrailsOutputParser\nfrom llama_index.llms.openai import OpenAI\n\n\n# load documents, build index\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\nindex = VectorStoreIndex(documents, chunk_size=512)\n\n# define query / output spec\nrail_spec = \"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.\n\n@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\"\n\n# define output parser\noutput_parser = GuardrailsOutputParser.from_rail_string(\n    rail_spec, llm=OpenAI()\n)\n\n# Attach output parser to LLM\nllm = OpenAI(output_parser=output_parser)\n\n# obtain a structured response\nquery_engine = index.as_query_engine(llm=llm)\nresponse = query_engine.query(\n    \"What are the three items the author did growing up?\",\n)\nprint(response)\n```\n\nOutput:\n\n```\n{'points': [{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}]}\n```\n\n### Langchain\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.output_parsers import LangchainOutputParser\nfrom llama_index.llms.openai import OpenAI\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\n\n# load documents, build index\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n# define output schema\nresponse_schemas = [\n    ResponseSchema(\n        name=\"Education\",\n        description=\"Describes the author's educational experience/background.\",\n    ),\n    ResponseSchema(\n        name=\"Work\",\n        description=\"Describes the author's work experience/background.\",\n    ),\n]\n\n# define output parser\nlc_output_parser = StructuredOutputParser.from_response_schemas(\n    response_schemas\n)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\n# Attach output parser to LLM\nllm = OpenAI(output_parser=output_parser)\n\n# obtain a structured response\nquery_engine = index.as_query_engine(llm=llm)\nresponse = query_engine.query(\n    \"What are a few things the author did growing up?\",\n)\nprint(str(response))\n```\n\nOutput:\n\n```\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}\n```\n\n### Guides\n\nMore examples:\n\n- [Guardrails](../../../examples/output_parsing/GuardrailsDemo.ipynb)\n- [Langchain](../../../examples/output_parsing/LangchainOutputParserDemo.ipynb)\n- [Guidance Pydantic Program](../../../examples/output_parsing/guidance_pydantic_program.ipynb)\n- [Guidance Sub-Question](../../../examples/output_parsing/guidance_sub_question.ipynb)\n- [Openai Pydantic Program](../../../examples/output_parsing/openai_pydantic_program.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03fc6590-bec3-4500-932b-31404888b9b2": {"__data__": {"id_": "03fc6590-bec3-4500-932b-31404888b9b2", "embedding": null, "metadata": {"filename": "pydantic_program.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "341efd4f1f6ce99a44b92bd8908e2e815a54ce3e", "node_type": "4", "metadata": {"filename": "pydantic_program.md", "author": "LlamaIndex"}, "hash": "ae8a3a04fe4b35f9f776919e610b916168b36da2ef79d9bbff63988b803e4d25", "class_name": "RelatedNodeInfo"}}, "text": "# Pydantic Program\n\nA pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.\n\nBecause this abstraction is so generic, it encompasses a broad range of LLM workflows. The programs are composable and be for more generic or specific use cases.\n\nThere's a few general types of Pydantic Programs:\n\n- **Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing.\n- **Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API.\n- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.\n\n## Text Completion Pydantic Programs\n\nSee the example notebook on [LLM Text Completion programs](../../../examples/output_parsing/llm_program.ipynb)\n\n## Function Calling Pydantic Programs\n\n- [Function Calling Pydantic Program](../../../examples/output_parsing/function_program.ipynb)\n- [OpenAI Pydantic Program](../../../examples/output_parsing/openai_pydantic_program.ipynb)\n- [Guidance Pydantic Program](../../../examples/output_parsing/guidance_pydantic_program.ipynb)\n- [Guidance Sub-Question Generator](../../../examples/output_parsing/guidance_sub_question.ipynb)\n\n## Prepackaged Pydantic Programs\n\n- [DF Program](../../../examples/output_parsing/df_program.ipynb)\n- [Evaporate Program](../../../examples/output_parsing/evaporate_program.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "208d18a1-e92e-42b3-b3b2-8d093ecb6f66": {"__data__": {"id_": "208d18a1-e92e-42b3-b3b2-8d093ecb6f66", "embedding": null, "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c305db40c58f76897f40776d7794d0eafa9cd2e", "node_type": "4", "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}, "hash": "cd5b2b185b327d2f38c0ac423696b4f590bc920f4406f3ea795af3bd19da1d82", "class_name": "RelatedNodeInfo"}}, "text": "# Query Engines + Pydantic Outputs\n\nUsing `index.as_query_engine()` and it's underlying `RetrieverQueryEngine`, we can support structured pydantic outputs without an additional LLM calls (in contrast to a typical output parser.)\n\nEvery query engine has support for integrated structured responses using the following `response_mode`s in `RetrieverQueryEngine`:\n\n- `refine`\n- `compact`\n- `tree_summarize`\n- `accumulate` (beta, requires extra parsing to convert to objects)\n- `compact_accumulate` (beta, requires extra parsing to convert to objects)\n\nUnder the hood, this uses `OpenAIPydanitcProgam` or `LLMTextCompletionProgram` depending on which LLM you've setup. If there are intermediate LLM responses (i.e. during `refine` or `tree_summarize` with multiple LLM calls), the pydantic object is injected into the next LLM prompt as a JSON object.\n\n## Usage Pattern\n\nFirst, you need to define the object you want to extract.\n\n```python\nfrom typing import List\nfrom pydantic import BaseModel\n\n\nclass Biography(BaseModel):\n    \"\"\"Data model for a biography.\"\"\"\n\n    name: str\n    best_known_for: List[str]\n    extra_info: str\n```\n\nThen, you create your query engine.\n\n```python\nquery_engine = index.as_query_engine(\n    response_mode=\"tree_summarize\", output_cls=Biography\n)\n```\n\nLastly, you can get a response and inspect the output.\n\n```python\nresponse = query_engine.query(\"Who is Paul Graham?\")\n\nprint(response.name)\n# > 'Paul Graham'\nprint(response.best_known_for)\n# > ['working on Bel', 'co-founding Viaweb', 'creating the programming language Arc']\nprint(response.extra_info)\n# > \"Paul Graham is a computer scientist, entrepreneur, and writer. He is best known      for ...\"\n```\n\n## Modules\n\nDetailed usage is available in the notebooks below:\n\n- [Structured Outputs with a Query Engine](../../../examples/query_engine/pydantic_query_engine.ipynb)\n- [Structured Outputs with a Tree Summarize](../../../examples/response_synthesizers/pydantic_tree_summarize.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a95e80af-b095-48e1-90c3-20a6f5f28076": {"__data__": {"id_": "a95e80af-b095-48e1-90c3-20a6f5f28076", "embedding": null, "metadata": {"filename": "chat_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e3c493bbf5db7cd81286fbc35667bf8ecb5988f", "node_type": "4", "metadata": {"filename": "chat_stores.md", "author": "LlamaIndex"}, "hash": "47ac69ed2dee4e3df2ce14c4f3d989a2c88f60b5831d960e7bde8fad479f5d0f", "class_name": "RelatedNodeInfo"}}, "text": "# Chat Stores\n\nA chat store serves as a centralized interface to store your chat history. Chat history is unique compared to other storage formats, since the order of messages is important for maintaining an overall conversation.\n\nChat stores can organize sequences of chat messages by keys (like `user_ids` or other unique identifiable strings), and handle `delete`, `insert`, and `get` operations.\n\n## SimpleChatStore\n\nThe most basic chat store is `SimpleChatStore`, which stores messages in memory and can save to/from disk, or can be serialized and stored elsewhere.\n\nTypically, you will instantiate a chat store and give it to a memory module. Memory modules that use chat stores will default to using `SimpleChatStore` if not provided.\n\n```python\nfrom llama_index.core.storage.chat_store import SimpleChatStore\nfrom llama_index.core.memory import ChatMemoryBuffer\n\nchat_store = SimpleChatStore()\n\nchat_memory = ChatMemoryBuffer.from_defaults(\n    token_limit=3000,\n    chat_store=chat_store,\n    chat_store_key=\"user1\",\n)\n```\n\nOnce you have the memory created, you might include it in an agent or chat engine:\n\n```python\nagent = OpenAIAgent.from_tools(tools, memory=memory)\n# OR\nchat_engine = index.as_chat_engine(memory=memory)\n```\n\nTo save the chat store for later, you can either save/load from disk\n\n```python\nchat_store.persist(persist_path=\"chat_store.json\")\nloaded_chat_store = SimpleChatStore.from_persist_path(\n    persist_path=\"chat_store.json\"\n)\n```\n\nOr you can convert to/from a string, saving the string somewhere else along the way\n\n```python\nchat_store_string = chat_store.json()\nloaded_chat_store = SimpleChatStore.parse_raw(chat_store_string)\n```\n\n## RedisChatStore\n\nUsing `RedisChatStore`, you can store your chat history remotely, without having to worry about manually persisting and loading the chat history.\n\n```python\nfrom llama_index.storage.chat_store.redis import RedisChatStore\nfrom llama_index.core.memory import ChatMemoryBuffer\n\nchat_store = RedisChatStore(redis_url=\"redis://localhost:6379\", ttl=300)\n\nchat_memory = ChatMemoryBuffer.from_defaults(\n    token_limit=3000,\n    chat_store=chat_store,\n    chat_store_key=\"user1\",\n)\n```\n\n## AzureChatStore\n\nUsing `AzureChatStore`, you can store your chat history remotely in Azure Table Storage or CosmosDB, without having to worry about manually persisting and loading the chat history.\n\n```\npip install llama-index\npip install llama-index-llms-azure-openai\npip install llama-index-storage-chat-store-azure\n```\n\n```python\nfrom llama_index.core.chat_engine import SimpleChatEngine\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.storage.chat_store.azure import AzureChatStore\n\nchat_store = AzureChatStore.from_account_and_key(\n    account_name=\"\",\n    account_key=\"\",\n    chat_table_name=\"ChatUser\",\n)\n\nmemory = ChatMemoryBuffer.from_defaults(\n    token_limit=3000,\n    chat_store=chat_store,\n    chat_store_key=\"conversation1\",\n)\n\nchat_engine = SimpleChatEngine(\n    memory=memory, llm=Settings.llm, prefix_messages=[]\n)\n\nresponse = chat_engine.chat(\"Hello.\")\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb4c93c5-d350-4ade-aa32-11424891af8c": {"__data__": {"id_": "cb4c93c5-d350-4ade-aa32-11424891af8c", "embedding": null, "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6057fd181fd26a8d792742d392167abf078e3126", "node_type": "4", "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "hash": "25d04afa43aaa9553f2870b780f621fac841c4ba0c6322c366cdd4932008fc09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dba5f0fd-3e3b-48b6-8ace-799a80257088", "node_type": "1", "metadata": {}, "hash": "b06affdec04789f1a8f898083ee5d3f0b3147a5b0cc409847c7081f95ee3b42c", "class_name": "RelatedNodeInfo"}}, "text": "# Customizing Storage\n\nBy default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code:\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Summarize the documents.\")\n```\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.\n\n![](../../_static/storage/storage.png)\n\n### Low-Level API\n\nTo do this, instead of the high-level API,\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nwe use a lower-level API that gives more granular control:\n\n```python\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.core.storage.index_store import SimpleIndexStore\nfrom llama_index.core.vector_stores import SimpleVectorStore\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# create parser and parse document into nodes\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create storage context using default stores\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n\n# create (or load) docstore and add nodes\nstorage_context.docstore.add_documents(nodes)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n# save index\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\n# can also set index_id to save multiple indexes to the same folder\nindex.set_index_id(\"<index_id>\")\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\n# to load index later, make sure you setup the storage context\n# this will loaded the persisted stores from persist_dir\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n\n# then load the index object\nfrom llama_index.core import load_index_from_storage\n\nloaded_index = load_index_from_storage(storage_context)\n\n# if loading an index from a persist_dir containing multiple indexes\nloaded_index = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\n# if loading multiple indexes from a persist dir\nloaded_indicies = load_index_from_storage(\n    storage_context, index_ids=[\"<index_id>\", ...]\n)\n```\n\nYou can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores.\nSee [Document Stores](./docstores.md), [Vector Stores](./vector_stores.md), [Index Stores](./index_stores.md) guides for more details.\n\n### Vector Store Integrations and Storage\n\nMost of our vector store integrations store the entire index (vectors + text) in the vector store itself. This comes with the major benefit of not having to explicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dba5f0fd-3e3b-48b6-8ace-799a80257088": {"__data__": {"id_": "dba5f0fd-3e3b-48b6-8ace-799a80257088", "embedding": null, "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6057fd181fd26a8d792742d392167abf078e3126", "node_type": "4", "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "hash": "25d04afa43aaa9553f2870b780f621fac841c4ba0c6322c366cdd4932008fc09", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb4c93c5-d350-4ade-aa32-11424891af8c", "node_type": "1", "metadata": {"filename": "customization.md", "author": "LlamaIndex"}, "hash": "59d5ca26154d5c79d9d6c62dacd32f5bd990da742092aebac918892477ca4ce3", "class_name": "RelatedNodeInfo"}}, "text": "The vector stores that support this practice are:\n\n- AzureAISearchVectorStore\n- ChatGPTRetrievalPluginClient\n- CassandraVectorStore\n- ChromaVectorStore\n- EpsillaVectorStore\n- DocArrayHnswVectorStore\n- DocArrayInMemoryVectorStore\n- JaguarVectorStore\n- LanceDBVectorStore\n- MetalVectorStore\n- MilvusVectorStore\n- MyScaleVectorStore\n- OpensearchVectorStore\n- PineconeVectorStore\n- QdrantVectorStore\n- RedisVectorStore\n- UpstashVectorStore\n- WeaviateVectorStore\n\nA small example using Pinecone is below:\n\n```python\nimport pinecone\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\n\n# Creating a Pinecone index\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\n# construct vector store\nvector_store = PineconeVectorStore(pinecone_index=index)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# create index, which will insert documents/vectors to pinecone\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n```\n\nIf you have an existing vector store with data already loaded in,\nyou can connect to it and directly create a `VectorStoreIndex` as follows:\n\n```python\nindex = pinecone.Index(\"quickstart\")\nvector_store = PineconeVectorStore(pinecone_index=index)\nloaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n```", "mimetype": "text/plain", "start_char_idx": 3079, "end_char_idx": 4723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d328b13-658c-4a29-b294-2a5819cd9cc8": {"__data__": {"id_": "2d328b13-658c-4a29-b294-2a5819cd9cc8", "embedding": null, "metadata": {"filename": "docstores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b0825184a1ecb80c940fa0aa1e55f17f4cacd01", "node_type": "4", "metadata": {"filename": "docstores.md", "author": "LlamaIndex"}, "hash": "c93261360db6df2c32a6aa0a3044b09d363b93ac198d2a2d0a699ef7c6b38277", "class_name": "RelatedNodeInfo"}}, "text": "# Document Stores\n\nDocument stores contain ingested document chunks, which we call `Node` objects.\n\nSee the [API Reference](../../api_reference/storage/docstore/index.md) for more details.\n\n### Simple Document Store\n\nBy default, the `SimpleDocumentStore` stores `Node` objects in-memory.\nThey can be persisted to (and loaded from) disk by calling `docstore.persist()` (and `SimpleDocumentStore.from_persist_path(...)` respectively).\n\nA more complete example can be found [here](../../examples/docstore/DocstoreDemo.ipynb)\n\n### MongoDB Document Store\n\nWe support MongoDB as an alternative document store backend that persists data as `Node` objects are ingested.\n\n```python\nfrom llama_index.storage.docstore.mongodb import MongoDocumentStore\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# create parser and parse document into nodes\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create (or load) docstore and add nodes\ndocstore = MongoDocumentStore.from_uri(uri=\"<mongodb+srv://...>\")\ndocstore.add_documents(nodes)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n```\n\nUnder the hood, `MongoDocumentStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your nodes.\n\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoDocumentStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `docstore.persist()`) when using an `MongoDocumentStore`\nsince data is persisted by default.\n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoDocumentStore` with an existing `db_name` and `collection_name`.\n\nA more complete example can be found [here](../../examples/docstore/MongoDocstoreDemo.ipynb)\n\n### Redis Document Store\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.\n\n```python\nfrom llama_index.storage.docstore.redis import RedisDocumentStore\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# create parser and parse document into nodes\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create (or load) docstore and add nodes\ndocstore = RedisDocumentStore.from_host_and_port(\n    host=\"127.0.0.1\", port=\"6379\", namespace=\"llama_index\"\n)\ndocstore.add_documents(nodes)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n```\n\nUnder the hood, `RedisDocumentStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/docs`.\n\n> Note: You can configure the `namespace` when instantiating `RedisDocumentStore`, otherwise it defaults `namespace=\"docstore\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisDocumentStore` with an existing `host`, `port`, and `namespace`.\n\nA more complete example can be found [here](../../examples/docstore/RedisDocstoreIndexStoreDemo.ipynb)\n\n### Firestore Document Store\n\nWe support Firestore as an alternative document store backend that persists data as `Node` objects are ingested.\n\n```python\nfrom llama_index.storage.docstore.firestore import FirestoreDocumentStore\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# create parser and parse document into nodes\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create (or load) docstore and add nodes\ndocstore = FirestoreDocumentStore.from_database(\n    project=\"project-id\",\n    database=\"(default)\",\n)\ndocstore.add_documents(nodes)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n```\n\nUnder the hood, `FirestoreDocumentStore` connects to a firestore database in Google Cloud and adds your nodes to a namespace stored under `{namespace}/docs`.\n\n> Note: You can configure the `namespace` when instantiating `FirestoreDocumentStore`, otherwise it defaults `namespace=\"docstore\"`.\n\nYou can easily reconnect to your Firestore database and reload the index by re-initializing a `FirestoreDocumentStore` with an existing `project`, `database`, and `namespace`.\n\nA more complete example can be found [here](../../examples/docstore/FirestoreDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45910725-7ea8-44e7-9703-a2a9057dd277": {"__data__": {"id_": "45910725-7ea8-44e7-9703-a2a9057dd277", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001f4ae9ca75b1982018cbfef5fe8d28a9dc1f71", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "1f59ca64b24a7d0db53bfeac4f0f8ccc080f07b04dcbdc726077d9356f733838", "class_name": "RelatedNodeInfo"}}, "text": "# Storing\n\n## Concept\n\nLlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n\nUnder the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n\n- **Document stores**: where ingested documents (i.e., `Node` objects) are stored,\n- **Index stores**: where index metadata are stored,\n- **Vector stores**: where embedding vectors are stored.\n- **Property Graph stores**: where knowledge graphs are stored (i.e. for `PropertyGraphIndex`).\n- **Chat Stores**: where chat messages are stored and organized.\n\nThe Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.\n\nLlamaIndex supports persisting data to any storage backend supported by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/index.html).\nWe have confirmed support for the following storage backends:\n\n- Local filesystem\n- AWS S3\n- Cloudflare R2\n\n![](../../_static/storage/storage.png)\n\n## Usage Pattern\n\nMany vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This _also_ means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.\n\n```python\n## build a new index\nfrom llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores.deeplake import DeepLakeVectorStore\n\n# construct vector store and customize storage context\nvector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n# Load documents and build index\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n\n\n## reload an existing one\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n```\n\nSee our [Vector Store Module Guide](vector_stores.md) below for more details.\n\nNote that in general to use storage abstractions, you need to define a `StorageContext` object:\n\n```python\nfrom llama_index.core.storage.docstore import SimpleDocumentStore\nfrom llama_index.core.storage.index_store import SimpleIndexStore\nfrom llama_index.core.vector_stores import SimpleVectorStore\nfrom llama_index.core import StorageContext\n\n# create storage context using default stores\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n```\n\nMore details on customization/persistence can be found in the guides below.\n\n- [Customization](./customization.md)\n- [Save/Load](./save_load.md)\n\n## Modules\n\nWe offer in-depth guides on the different storage components.\n\n- [Vector Stores](./vector_stores.md)\n- [Docstores](./docstores.md)\n- [Index Stores](./index_stores.md)\n- [Key-Val Stores](./kv_stores.md)\n- [Property Graph Stores](../indexing/lpg_index_guide.md#storage)\n- [ChatStores](./chat_stores.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdaf8922-814b-4224-907b-7a6515843a79": {"__data__": {"id_": "cdaf8922-814b-4224-907b-7a6515843a79", "embedding": null, "metadata": {"filename": "index_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5951f73f265447ea5618ce68a03a45c2c764d99", "node_type": "4", "metadata": {"filename": "index_stores.md", "author": "LlamaIndex"}, "hash": "28e042ac195285d8eaafd693062b9a55f92e19543eaa7c11ce34cedab68fa6be", "class_name": "RelatedNodeInfo"}}, "text": "# Index Stores\n\nIndex stores contains lightweight index metadata (i.e. additional state information created when building an index).\n\nSee the [API Reference](../../api_reference/storage/index_store/index.md) for more details.\n\n### Simple Index Store\n\nBy default, LlamaIndex uses a simple index store backed by an in-memory key-value store.\nThey can be persisted to (and loaded from) disk by calling `index_store.persist()` (and `SimpleIndexStore.from_persist_path(...)` respectively).\n\n### MongoDB Index Store\n\nSimilarly to document stores, we can also use `MongoDB` as the storage backend of the index store.\n\n```python\nfrom llama_index.storage.index_store.mongodb import MongoIndexStore\nfrom llama_index.core import VectorStoreIndex\n\n# create (or load) index store\nindex_store = MongoIndexStore.from_uri(uri=\"<mongodb+srv://...>\")\n\n# create storage context\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n# or alternatively, load index\nfrom llama_index.core import load_index_from_storage\n\nindex = load_index_from_storage(storage_context)\n```\n\nUnder the hood, `MongoIndexStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your index metadata.\n\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoIndexStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `index_store.persist()`) when using an `MongoIndexStore`\nsince data is persisted by default.\n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoIndexStore` with an existing `db_name` and `collection_name`.\n\nA more complete example can be found [here](../../examples/docstore/MongoDocstoreDemo.ipynb)\n\n### Redis Index Store\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.\n\n```python\nfrom llama_index.storage.index_store.redis import RedisIndexStore\nfrom llama_index.core import VectorStoreIndex\n\n# create (or load) docstore and add nodes\nindex_store = RedisIndexStore.from_host_and_port(\n    host=\"127.0.0.1\", port=\"6379\", namespace=\"llama_index\"\n)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n# or alternatively, load index\nfrom llama_index.core import load_index_from_storage\n\nindex = load_index_from_storage(storage_context)\n```\n\nUnder the hood, `RedisIndexStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/index`.\n\n> Note: You can configure the `namespace` when instantiating `RedisIndexStore`, otherwise it defaults `namespace=\"index_store\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisIndexStore` with an existing `host`, `port`, and `namespace`.\n\nA more complete example can be found [here](../../examples/docstore/RedisDocstoreIndexStoreDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a7448de-a0da-46a1-8c19-c069c3e8dcb2": {"__data__": {"id_": "5a7448de-a0da-46a1-8c19-c069c3e8dcb2", "embedding": null, "metadata": {"filename": "kv_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2eb4ab9b0553400c4a180c36963d238e6f8d58b3", "node_type": "4", "metadata": {"filename": "kv_stores.md", "author": "LlamaIndex"}, "hash": "c85c10076d70b7253c956a62ca9c49fcae44b76e3027ab5c3a725cb50b4ddd56", "class_name": "RelatedNodeInfo"}}, "text": "# Key-Value Stores\n\nKey-Value stores are the underlying storage abstractions that power our [Document Stores](./docstores.md) and [Index Stores](./index_stores.md).\n\nWe provide the following key-value stores:\n\n- **Simple Key-Value Store**: An in-memory KV store. The user can choose to call `persist` on this kv store to persist data to disk.\n- **MongoDB Key-Value Store**: A MongoDB KV store.\n\nSee the [API Reference](../../api_reference/storage/kvstore/index.md) for more details.\n\nNote: At the moment, these storage abstractions are not externally facing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf5521ff-45b4-4de9-baa1-0d65c9446e0c": {"__data__": {"id_": "bf5521ff-45b4-4de9-baa1-0d65c9446e0c", "embedding": null, "metadata": {"filename": "save_load.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2795506280f8e5a95edaa8833718e344f1432da", "node_type": "4", "metadata": {"filename": "save_load.md", "author": "LlamaIndex"}, "hash": "7d5b420b78d65560c0530d07027b9580da6eacb88e653d70a5f9f0733081dbb7", "class_name": "RelatedNodeInfo"}}, "text": "# Persisting & Loading Data\n\n## Persisting Data\n\nBy default, LlamaIndex stores data in-memory, and this data can be explicitly persisted if desired:\n\n```python\nstorage_context.persist(persist_dir=\"<persist_dir>\")\n```\n\nThis will persist data to disk, under the specified `persist_dir` (or `./storage` by default).\n\nMultiple indexes can be persisted and loaded from the same directory, assuming you keep track of index ID's for loading.\n\nUser can also configure alternative storage backends (e.g. `MongoDB`) that persist data by default.\nIn this case, calling `storage_context.persist()` will do nothing.\n\n## Loading Data\n\nTo load data, user simply needs to re-create the storage context using the same configuration (e.g. pass in the same `persist_dir` or vector store client).\n\n```python\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    vector_store=SimpleVectorStore.from_persist_dir(\n        persist_dir=\"<persist_dir>\"\n    ),\n    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n)\n```\n\nWe can then load specific indices from the `StorageContext` through some convenience functions below.\n\n```python\nfrom llama_index.core import (\n    load_index_from_storage,\n    load_indices_from_storage,\n    load_graph_from_storage,\n)\n\n# load a single index\n# need to specify index_id if multiple indexes are persisted to the same directory\nindex = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\n# don't need to specify index_id if there's only one index in storage context\nindex = load_index_from_storage(storage_context)\n\n# load multiple indices\nindices = load_indices_from_storage(storage_context)  # loads all indices\nindices = load_indices_from_storage(\n    storage_context, index_ids=[index_id1, ...]\n)  # loads specific indices\n\n# load composable graph\ngraph = load_graph_from_storage(\n    storage_context, root_id=\"<root_id>\"\n)  # loads graph with the specified root_id\n```\n\n## Using a remote backend\n\nBy default, LlamaIndex uses a local filesystem to load and save files. However, you can override this by passing a `fsspec.AbstractFileSystem` object.\n\nHere's a simple example, instantiating a vector store:\n\n```python\nimport dotenv\nimport s3fs\nimport os\n\ndotenv.load_dotenv(\"../../../.env\")\n\n# load documents\ndocuments = SimpleDirectoryReader(\n    \"../../../examples/paul_graham_essay/data/\"\n).load_data()\nprint(len(documents))\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nAt this point, everything has been the same. Now - let's instantiate a S3 filesystem and save / load from there.\n\n```python\n# set up s3fs\nAWS_KEY = os.environ[\"AWS_ACCESS_KEY_ID\"]\nAWS_SECRET = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nR2_ACCOUNT_ID = os.environ[\"R2_ACCOUNT_ID\"]\n\nassert AWS_KEY is not None and AWS_KEY != \"\"\n\ns3 = s3fs.S3FileSystem(\n    key=AWS_KEY,\n    secret=AWS_SECRET,\n    endpoint_url=f\"https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com\",\n    s3_additional_kwargs={\"ACL\": \"public-read\"},\n)\n\n# If you're using 2+ indexes with the same StorageContext,\n# run this to save the index to remote blob storage\nindex.set_index_id(\"vector_index\")\n\n# persist index to s3\ns3_bucket_name = \"llama-index/storage_demo\"  # {bucket_name}/{index_name}\nindex.storage_context.persist(persist_dir=s3_bucket_name, fs=s3)\n\n# load index from s3\nindex_from_s3 = load_index_from_storage(\n    StorageContext.from_defaults(persist_dir=s3_bucket_name, fs=s3),\n    index_id=\"vector_index\",\n)\n```\n\nBy default, if you do not pass a filesystem, we will assume a local filesystem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef18adb9-327b-46bc-b35e-c66fcfcff91a": {"__data__": {"id_": "ef18adb9-327b-46bc-b35e-c66fcfcff91a", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7056de4a07291707fa62602421e326f4d6ae3a8", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "139c4a6ab9511cb39da300ce42404b2853cf93d507e88cfadca98ce02a4c10ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4f7b06a-19df-4c55-8ab4-7ac2bd3c906d", "node_type": "1", "metadata": {}, "hash": "cc3f70cd75e01fbad80421edc41842bf63e733452f049a4f73a40f9f0a606596", "class_name": "RelatedNodeInfo"}}, "text": "# Vector Stores\n\nVector stores contain embedding vectors of ingested document chunks\n(and sometimes the document chunks as well).\n\n## Simple Vector Store\n\nBy default, LlamaIndex uses a simple in-memory vector store that's great for quick experimentation.\nThey can be persisted to (and loaded from) disk by calling `vector_store.persist()` (and `SimpleVectorStore.from_persist_path(...)` respectively).\n\n## Vector Store Options & Feature Support\n\nLlamaIndex supports over 20 different vector store options.\nWe are actively adding more integrations and improving feature coverage for each.\n\n| Vector Store             | Type                    | Metadata Filtering | Hybrid Search | Delete | Store Documents | Async |\n| ------------------------ | ----------------------- | ------------------ | ------------- | ------ | --------------- | ----- |\n| Alibaba Cloud OpenSearch | cloud                   | \u2713                  |               | \u2713      | \u2713               | \u2713     |\n| Apache Cassandra\u00ae       | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| Astra DB                 | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| Azure AI Search          | cloud                   | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Azure CosmosDB MongoDB   | cloud                   |                    |               | \u2713      | \u2713               |       |\n| BaiduVectorDB            | cloud                   | \u2713                  | \u2713             |        | \u2713               |       |\n| ChatGPT Retrieval Plugin | aggregator              |                    |               | \u2713      | \u2713               |       |\n| Chroma                   | self-hosted             | \u2713                  |               | \u2713      | \u2713               |       |\n| Couchbase                | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| DashVector               | cloud                   | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Databricks               | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| Deeplake                 | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| DocArray                 | aggregator              | \u2713                  |               | \u2713      | \u2713               |       |\n| DuckDB                   | in-memory / self-hosted | \u2713                  |               | \u2713      | \u2713               |       |\n| DynamoDB                 | cloud                   |                    |               | \u2713      |                 |       |\n| Elasticsearch            | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               | \u2713     |\n| FAISS                    | in-memory               |                    |               |        |                 |       |\n| txtai                    | in-memory               |                    |               |        |                 |       |\n| Jaguar                   | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| LanceDB                  | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| Lantern                  | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               | \u2713     |\n| Metal                    | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| MongoDB Atlas            | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| MyScale                  | cloud                   | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Milvus / Zilliz          | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Neo4jVector              | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| OpenSearch               | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               | \u2713     |\n| Pinecone                 | cloud                   | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Postgres                 | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               | \u2713     |\n| pgvecto.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4f7b06a-19df-4c55-8ab4-7ac2bd3c906d": {"__data__": {"id_": "c4f7b06a-19df-4c55-8ab4-7ac2bd3c906d", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7056de4a07291707fa62602421e326f4d6ae3a8", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "139c4a6ab9511cb39da300ce42404b2853cf93d507e88cfadca98ce02a4c10ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef18adb9-327b-46bc-b35e-c66fcfcff91a", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "c25ae8c8d44c5c696c7f2b35b28fad5149044a1a07ed84258cba792ed434ed81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4efaff5c-1f5a-4192-9486-1bdbdcd5e215", "node_type": "1", "metadata": {}, "hash": "40f01a429beacb9feaf3567da029e80cd200bfe94899ded86ae19cce9f20ed06", "class_name": "RelatedNodeInfo"}}, "text": "rs               | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Qdrant                   | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               | \u2713     |\n| Redis                    | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| Simple                   | in-memory               | \u2713                  |               | \u2713      |                 |       |\n| SingleStore              | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| Supabase                 | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| Tair                     | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| TiDB                     | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| TencentVectorDB          | cloud                   | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Timescale                |                         | \u2713                  |               | \u2713      | \u2713               | \u2713     |\n| Typesense                | self-hosted / cloud     | \u2713                  |               | \u2713      | \u2713               |       |\n| Upstash                  | cloud                   |                    |               |        | \u2713               |       |\n| Vearch                   | self-hosted             | \u2713                  |               | \u2713      | \u2713               |       |\n| Vespa                    | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Vertex AI Vector Search  | cloud                   | \u2713                  |               | \u2713      | \u2713               |       |\n| Weaviate                 | self-hosted / cloud     | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n\nFor more details, see [Vector Store Integrations](././community/integrations/vector_stores.md).", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4efaff5c-1f5a-4192-9486-1bdbdcd5e215": {"__data__": {"id_": "4efaff5c-1f5a-4192-9486-1bdbdcd5e215", "embedding": null, "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7056de4a07291707fa62602421e326f4d6ae3a8", "node_type": "4", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "139c4a6ab9511cb39da300ce42404b2853cf93d507e88cfadca98ce02a4c10ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4f7b06a-19df-4c55-8ab4-7ac2bd3c906d", "node_type": "1", "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}, "hash": "7a3d84dfecf4f2967fcd26dc9d211950cac46a942da0114772a96d6574aa2490", "class_name": "RelatedNodeInfo"}}, "text": "see [Vector Store Integrations](././community/integrations/vector_stores.md).\n\n## Example Notebooks\n\n- [Alibaba Cloud OpenSearch](../../examples/vector_stores/AlibabaCloudOpenSearchIndexDemo.ipynb)\n- [Astra DB](../../examples/vector_stores/AstraDBIndexDemo.ipynb)\n- [Async Index Creation](../../examples/vector_stores/AsyncIndexCreationDemo.ipynb)\n- [Azure AI Search](../../examples/vector_stores/AzureAISearchIndexDemo.ipynb)\n- [Azure Cosmos DB](../../examples/vector_stores/AzureCosmosDBMongoDBvCoreDemo.ipynb)\n- [Baidu](../../examples/vector_stores/BaiduVectorDBIndexDemo.ipynb)\n- [Caasandra](../../examples/vector_stores/CassandraIndexDemo.ipynb)\n- [Chromadb](../../examples/vector_stores/ChromaIndexDemo.ipynb)\n- [Couchbase](../../examples/vector_stores/CouchbaseVectorStoreDemo.ipynb)\n- [Dash](../../examples/vector_stores/DashvectorIndexDemo.ipynb)\n- [Databricks](../../examples/vector_stores/DatabricksVectorSearchDemo.ipynb)\n- [Deeplake](../../examples/vector_stores/DeepLakeIndexDemo.ipynb)\n- [DocArray HNSW](../../examples/vector_stores/DocArrayHnswIndexDemo.ipynb)\n- [DocArray in-Memory](../../examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb)\n- [DuckDB](../../examples/vector_stores/DuckDBDemo.ipynb)\n- [Espilla](../../examples/vector_stores/EpsillaIndexDemo.ipynb)\n- [Jaguar](../../examples/vector_stores/JaguarIndexDemo.ipynb)\n- [LanceDB](../../examples/vector_stores/LanceDBIndexDemo.ipynb)\n- [Lantern](../../examples/vector_stores/LanternIndexDemo.ipynb)\n- [Metal](../../examples/vector_stores/MetalIndexDemo.ipynb)\n- [Milvus](../../examples/vector_stores/MilvusIndexDemo.ipynb)\n- [Milvus Hybrid Search](../../examples/vector_stores/MilvusHybridIndexDemo.ipynb)\n- [MyScale](../../examples/vector_stores/MyScaleIndexDemo.ipynb)\n- [ElsaticSearch](../../examples/vector_stores/ElasticsearchIndexDemo.ipynb)\n- [FAISS](../../examples/vector_stores/FaissIndexDemo.ipynb)\n- [MongoDB Atlas](../../examples/vector_stores/MongoDBAtlasVectorSearch.ipynb)\n- [Neo4j](../../examples/vector_stores/Neo4jVectorDemo.ipynb)\n- [OpenSearch](../../examples/vector_stores/OpensearchDemo.ipynb)\n- [Pinecone](../../examples/vector_stores/PineconeIndexDemo.ipynb)\n- [Pinecone Hybrid Search](../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb)\n- [PGvectoRS](../../examples/vector_stores/PGVectoRsDemo.ipynb)\n- [Postgres](../../examples/vector_stores/postgres.ipynb)\n- [Redis](../../examples/vector_stores/RedisIndexDemo.ipynb)\n- [Qdrant](../../examples/vector_stores/QdrantIndexDemo.ipynb)\n- [Qdrant Hybrid Search](../../examples/vector_stores/qdrant_hybrid.ipynb)\n- [Rockset](../../examples/vector_stores/RocksetIndexDemo.ipynb)\n- [Simple](../../examples/vector_stores/SimpleIndexDemo.ipynb)\n- [Supabase](../../examples/vector_stores/SupabaseVectorIndexDemo.ipynb)\n- [Tair](../../examples/vector_stores/TairIndexDemo.ipynb)\n- [TiDB](../../examples/vector_stores/TiDBVector.ipynb)\n- [Tencent](../../examples/vector_stores/TencentVectorDBIndexDemo.ipynb)\n- [Timesacle](../../examples/vector_stores/Timescalevector.ipynb)\n- [Upstash](../../examples/vector_stores/UpstashVectorDemo.ipynb)\n- [Vearch](../../examples/vector_stores/VearchDemo.ipynb)\n- [Vespa](../../examples/vector_stores/VespaIndexDemo.ipynb)\n- [Vertex AI Vector Search](../../examples/vector_stores/VertexAIVectorSearchDemo.ipynb)\n- [Weaviate](../../examples/vector_stores/WeaviateIndexDemo.ipynb)\n- [Weaviate Hybrid Search](../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb)\n- [Zep](../../examples/vector_stores/ZepIndexDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "227f4bbe-a402-49c4-b50c-19feef22cb65": {"__data__": {"id_": "227f4bbe-a402-49c4-b50c-19feef22cb65", "embedding": null, "metadata": {"filename": "service_context_migration.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7e81ae86f06048d8cbebf43eb91533f569a167d", "node_type": "4", "metadata": {"filename": "service_context_migration.md", "author": "LlamaIndex"}, "hash": "a517672bd4f3027a6dc28116aabafb8c9668bc71ab236ced22e3b13b6af09248", "class_name": "RelatedNodeInfo"}}, "text": "# Migrating from ServiceContext to Settings\n\nIntroduced in v0.10.0, there is a new global `Settings` object intended to replace the old `ServiceContext` configuration.\n\nThe new `Settings` object is a global settings, with parameters that are lazily instantiated. Attributes like the LLM or embedding model are only loaded when they are actually required by an underlying module.\n\nPreviously with the service context, various modules often did not use it, and it also forced loading every component into memory at runtime (even if those components weren't used).\n\nConfiguring the global settings means you are change the default for EVERY module in LlamaIndex. This means if you aren't using OpenAI, an example config might look like:\n\n```python\nfrom llama_index.llms.ollama import Ollama\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings\n\nSettings.llm = Ollama(model=\"llama2\", request_timeout=120.0)\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n```\n\nNow with this settings, you can ensure OpenAI will never be used in the framework.\n\nThe `Settings` object supports nearly all the same attributes as the old `ServiceConext`. A complete list can be found in the [docs page](settings.md).\n\n### Complete Migration\n\nBelow is an example of completely migrating from `ServiceContext` to `Settings`:\n\n**Before**\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import ServiceContext, set_global_service_context\n\nservice_context = ServiceContext.from_defaults(\n    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n    node_parser=SentenceSplitter(chunk_size=512, chunk_overlap=20),\n    num_output=512,\n    context_window=3900,\n)\nset_global_service_context(service_context)\n```\n\n**After**\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\nSettings.llm = OpenAI(model=\"gpt-3.5-turbo\")\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\nSettings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\nSettings.num_output = 512\nSettings.context_window = 3900\n```\n\n## Local Config\n\nThe above covers global configuration. To config settings per-module, all module interfaces should be updated to accept kwargs for the objects that are being used.\n\nIf you are using an IDE, the kwargs should auto-populate with intellisense, but here are some examples below:\n\n```python\n# a vector store index only needs an embed model\nindex = VectorStoreIndex.from_documents(\n    documents, embed_model=embed_model, transformations=transformations\n)\n\n# ... until you create a query engine\nquery_engine = index.as_query_engine(llm=llm)\n```\n\n```python\n# a document summary index needs both an llm and embed model\n# for the constructor\nindex = DocumentSummaryIndex.from_documents(\n    documents, embed_model=embed_model, llm=llm\n)\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "286b3fc0-cf3c-48fa-a178-b9c49fe5a126": {"__data__": {"id_": "286b3fc0-cf3c-48fa-a178-b9c49fe5a126", "embedding": null, "metadata": {"filename": "settings.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dab4ed48ae8ad35ad4ff9e8366b381a8214fbeb5", "node_type": "4", "metadata": {"filename": "settings.md", "author": "LlamaIndex"}, "hash": "b704f22175acad2f794014bd46c8f9b9990dc09686c20424cbe85db495fcd32c", "class_name": "RelatedNodeInfo"}}, "text": "# Configuring Settings\n\nThe `Settings` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application.\n\nYou can use it to set the [global configuration](#setting-global-configuration). Local configurations (transformations, LLMs, embedding models) can be passed directly into the interfaces that make use of them.\n\nThe `Settings` is a simple singleton object that lives throughout your application. Whenever a particular component is not provided, the `Settings` object is used to provide it as a global default.\n\nThe following attributes can be configured on the `Settings` object:\n\n## LLM\n\nThe LLM is used to respond to prompts and queries, and is responsible for writing natural language responses.\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\nSettings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n```\n\n## Embed Model\n\nThe embedding model is used to convert text to numerical representationss, used for calculating similarity and top-k retrieval.\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core import Settings\n\nSettings.embed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\", embed_batch_size=100\n)\n```\n\n## Node Parser / Text Splitter\n\nThe node parser / text splitter is used to parse documents into smaller chunks, called nodes.\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core import Settings\n\nSettings.text_splitter = SentenceSplitter(chunk_size=1024)\n```\n\nIf you just want to change the chunk size or chunk overlap without changing the default splitter, this is also possible:\n\n```python\nSettings.chunk_size = 512\nSettings.chunk_overlap = 20\n```\n\n## Transformations\n\nTransformations are applied to `Document`s during ingestion. By default, the `node_parser`/`text_splitter` is used, but this can be overridden and customized further.\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core import Settings\n\nSettings.transformations = [SentenceSplitter(chunk_size=1024)]\n```\n\n## Tokenizer\n\nThe tokenizer is used to count tokens. This should be set to something that matches the LLM you are using.\n\n```python\nfrom llama_index.core import Settings\n\n# openai\nimport tiktoken\n\nSettings.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n\n# open-source\nfrom transformers import AutoTokenizer\n\nSettings.tokenzier = AutoTokenizer.from_pretrained(\n    \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n)\n```\n\n## Callbacks\n\nYou can set a global callback manager, which can be used to observe and consume events generated throughout the llama-index code\n\n```python\nfrom llama_index.core.callbacks import TokenCountingHandler, CallbackManager\nfrom llama_index.core import Settings\n\ntoken_counter = TokenCountingHandler()\nSettings.callback_manager = CallbackManager([token_counter])\n```\n\n## Prompt Helper Arguments\n\nA few specific arguments/values are used during querying, to ensure that the input prompts to the LLM have enough room to generate a certain number of tokens.\n\nTypically these are automatically configured using attributes from the LLM, but they can be overridden in special cases.\n\n```python\nfrom llama_index.core import Settings\n\n# maximum input size to the LLM\nSettings.context_window = 4096\n\n# number of tokens reserved for text generation.\nSettings.num_output = 256\n```\n\n!!! tip\n    Learn how to configure specific modules: - [LLM](../models/llms/usage_custom.md) - [Embedding Model](../models/embeddings.md) - [Node Parser/Text Splitters](../loading/node_parsers/index.md) - [Callbacks](../observability/callbacks/index.md)\n\n## Setting local configurations\n\nInterfaces that use specific parts of the settings can also accept local overrides.\n\n```python\nindex = VectorStoreIndex.from_documents(\n    documents, embed_model=embed_model, transformations=transformations\n)\n\nquery_engine = index.as_query_engine(llm=llm)\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "567a245f-5513-4645-83d5-15ff1ccd23eb": {"__data__": {"id_": "567a245f-5513-4645-83d5-15ff1ccd23eb", "embedding": null, "metadata": {"filename": "supporting_modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53cb322c58610c30a3904b7f7bcd79366c6aa029", "node_type": "4", "metadata": {"filename": "supporting_modules.md", "author": "LlamaIndex"}, "hash": "e28a87910bb50bf64852b6e073f69b479f49ecf99408d66f4ecb055016677f1e", "class_name": "RelatedNodeInfo"}}, "text": "# Supporting Modules\n\nWe have two configuration modules that can be configured separately and passed to individual indexes, or set globally.\n\n- The [Settings](settings.md) includes the LLM you're using, the embedding model, your node parser, your callback manager and more.\n- The `StorageContext` lets you specify where and how to store your documents, your vector embeddings, and your indexes. To learn more, read about [customizing storage](../storing/customization.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e2ce25d-1928-4ce8-8aa7-e83df2d8a391": {"__data__": {"id_": "6e2ce25d-1928-4ce8-8aa7-e83df2d8a391", "embedding": null, "metadata": {"filename": "advanced_retrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2bd3b4b59e759befd2a4bb027a80d643993a0059", "node_type": "4", "metadata": {"filename": "advanced_retrieval.md", "author": "LlamaIndex"}, "hash": "24df728acfc90324f63142be83aa664149b82b36f2c5643af8021653d31e4150", "class_name": "RelatedNodeInfo"}}, "text": "# Advanced Retrieval Strategies\n\n## Main Advanced Retrieval Strategies\n\nThere are a variety of more advanced retrieval strategies you may wish to try, each with different benefits:\n\n- [Reranking](../../examples/node_postprocessor/CohereRerank.ipynb)\n- [Recursive retrieval](../../examples/query_engine/pdf_tables/recursive_retriever.ipynb)\n- [Embedded tables](../../examples/query_engine/sec_tables/tesla_10q_table.ipynb)\n- [Small-to-big retrieval](../../examples/node_postprocessor/MetadataReplacementDemo.ipynb)\n\nSee our full [retrievers module guide](../../module_guides/querying/retriever/retrievers.md) for a comprehensive list of all retrieval strategies, broken down into different categories.\n\n- Basic retrieval from each index\n- Advanced retrieval and search\n- Auto-Retrieval\n- Knowledge Graph Retrievers\n- Composed/Hierarchical Retrievers\n- and more!\n\nMore resources are below.\n\n## Query Transformations\n\nA user query can be transformed before it enters a pipeline (query engine, agent, and more). See resources below on query transformations:\n\n- [Query Transform Cookbook](../../examples/query_transformations/query_transform_cookbook.ipynb)\n- [Query Transformations Docs](../../optimizing/advanced_retrieval/query_transformations.md)\n\n## Composable Retrievers\n\nEvery retriever is capable of retrieving and running other objects, including\n\n- other retrievers\n- query engines\n- query pipelines\n- other nodes\n\nFor more details, check out the guide below.\n\n- [Composable Retrievers](../../examples/retrievers/composable_retrievers.ipynb)\n\n## Third-Party Resources\n\nHere are some third-party resources on advanced retrieval strategies.\n\n- [DeepMemory (Activeloop)](../../examples/retrievers/deep_memory.ipynb)\n- [Weaviate Hybrid Search](../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb)\n- [Pinecone Hybrid Search](../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb)\n- [Milvus Hybrid Search](../../examples/vector_stores/MilvusHybridIndexDemo.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7442e7f0-c24f-4e3e-9898-951810821ab6": {"__data__": {"id_": "7442e7f0-c24f-4e3e-9898-951810821ab6", "embedding": null, "metadata": {"filename": "query_transformations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6898fbb0aafcc638d46e1b15b1bb424ca2867931", "node_type": "4", "metadata": {"filename": "query_transformations.md", "author": "LlamaIndex"}, "hash": "f4ea40aae7e80eaa5bd8810ad496d2ba7ce2eb6b622b97d21d1fa774ed71e481", "class_name": "RelatedNodeInfo"}}, "text": "# Query Transformations\n\nLlamaIndex allows you to perform _query transformations_ over your index structures.\nQuery transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index.\n\nThey can also be **multi-step**, as in:\n\n1. The query is transformed, executed against an index,\n2. The response is retrieved.\n3. Subsequent queries are transformed/executed in a sequential fashion.\n\nWe list some of our query transformations in more detail below.\n\n#### Use Cases\n\nQuery transformations have multiple use cases:\n\n- Transforming an initial query into a form that can be more easily embedded (e.g. HyDE)\n- Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\n- Breaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\n\n### HyDE (Hypothetical Document Embeddings)\n\n[HyDE](http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf) is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.\n\nTo use HyDE, an example code snippet is shown below.\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.indices.query.query_transform.base import (\n    HyDEQueryTransform,\n)\nfrom llama_index.core.query_engine import TransformQueryEngine\n\n# load documents, build index\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\nindex = VectorStoreIndex(documents)\n\n# run query with HyDE query transform\nquery_str = \"what did paul graham do after going to RISD\"\nhyde = HyDEQueryTransform(include_original=True)\nquery_engine = index.as_query_engine()\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\nresponse = query_engine.query(query_str)\nprint(response)\n```\n\nCheck out our [example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb) for a full walkthrough.\n\n### Multi-Step Query Transformations\n\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\n\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query.\nGiven the response (along with prior responses) and the query, follow-up questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\n\nAn example image is shown below.\n\n![](../../_static/query_transformations/multi_step_diagram.png)\n\nHere's a corresponding example code snippet.\n\n```python\nfrom llama_index.core.indices.query.query_transform.base import (\n    StepDecomposeQueryTransform,\n)\n\n# gpt-4\nstep_decompose_transform = StepDecomposeQueryTransform(llm, verbose=True)\n\nquery_engine = index.as_query_engine()\nquery_engine = MultiStepQueryEngine(\n    query_engine, query_transform=step_decompose_transform\n)\n\nresponse = query_engine.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n)\nprint(str(response))\n```\n\nCheck out our [example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb) for a full walkthrough.\n\n- [HyDE Query Transform](../../examples/query_transformations/HyDEQueryTransformDemo.ipynb)\n- [Multistep Query](../../examples/query_transformations/SimpleIndexDemo-multistep.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "707e0313-5c55-4df1-beed-54860b948737": {"__data__": {"id_": "707e0313-5c55-4df1-beed-54860b948737", "embedding": null, "metadata": {"filename": "agentic_strategies.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dea1bbdbdbc70740b749ce5983610afd9334c497", "node_type": "4", "metadata": {"filename": "agentic_strategies.md", "author": "LlamaIndex"}, "hash": "fb9037a69765d645c89c7802432e339522a018c847051fd7d26d1ce822bbebd8", "class_name": "RelatedNodeInfo"}}, "text": "# Agentic strategies\n\nYou can build agents on top of your existing LlamaIndex RAG pipeline to empower it with automated decision capabilities.\nA lot of modules (routing, query transformations, and more) are already agentic in nature in that they use LLMs for decision making.\n\n## Simpler Agentic Strategies\n\nThese include routing and query transformations.\n\n- [Routing](../../module_guides/querying/router/index.md)\n- [Query Transformations](../../optimizing/advanced_retrieval/query_transformations.md)\n- [Sub Question Query Engine (Intro)](../../examples/query_engine/sub_question_query_engine.ipynb)\n\n## Data Agents\n\nThis guides below show you how to deploy a full agent loop, capable of chain-of-thought and query planning, on top of existing RAG query engines as tools for more advanced decision making.\n\nMake sure to check out our [full module guide on Data Agents](../../module_guides/deploying/agents/index.md), which highlight these use cases and much more.\n\nOur [lower-level agent API](../../module_guides/deploying/agents/agent_runner.md) shows you the internals of how an agent works (with step-wise execution).\n\nExample guides below (using OpenAI function calling):\n\n- [OpenAIAgent](../../examples/agent/openai_agent.ipynb)\n- [OpenAIAgent with Query Engine Tools](../../examples/agent/openai_agent_with_query_engine.ipynb)\n- [OpenAIAgent Retrieval](../../examples/agent/openai_agent_retrieval.ipynb)\n- [OpenAIAgent Query Cookbook](../../examples/agent/openai_agent_query_cookbook.ipynb)\n- [OpenAIAgent Query Planning](../../examples/agent/openai_agent_query_plan.ipynb)\n- [OpenAIAgent Context Retrieval](../../examples/agent/openai_agent_context_retrieval.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5fa9e28-7781-4985-aaa5-c1eaa146de78": {"__data__": {"id_": "f5fa9e28-7781-4985-aaa5-c1eaa146de78", "embedding": null, "metadata": {"filename": "basic_strategies.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e7abbbb610c607fa279635a2db89b63e86b9186", "node_type": "4", "metadata": {"filename": "basic_strategies.md", "author": "LlamaIndex"}, "hash": "1ff3c80cf8bd1754cbd6d42a9c57b654711712f7ba0f09bd4ee82d9550774dd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa0c69e5-815b-4534-8f91-d150133becf7", "node_type": "1", "metadata": {}, "hash": "96c2f762676a452ffee97b9fecaa350d3ba3e7d62eeb9ea9455f0e9def62b35b", "class_name": "RelatedNodeInfo"}}, "text": "# Basic Strategies\n\nThere are many easy things to try, when you need to quickly squeeze out extra performance and optimize your RAG pipeline.\n\n## Prompt Engineering\n\nIf you're encountering failures related to the LLM, like hallucinations or poorly formatted outputs, then this\nshould be one of the first things you try.\n\nSome tasks are listed below, from simple to advanced.\n\n1. Try inspecting the prompts used in your RAG pipeline (e.g. the question\u2013answering prompt) and customizing it.\n\n- [Customizing Prompts](../../examples/prompts/prompt_mixin.ipynb)\n- [Advanced Prompts](../../examples/prompts/advanced_prompts.ipynb)\n\n2. Try adding **prompt functions**, allowing you to dynamically inject few-shot examples or process the injected inputs.\n\n- [Advanced Prompts](../../examples/prompts/advanced_prompts.ipynb)\n- [RAG Prompts](../../examples/prompts/prompts_rag.ipynb)\n\n## Embeddings\n\nChoosing the right embedding model plays a large role in overall performance.\n\n- Maybe you need something better than the default `text-embedding-ada-002` model from OpenAI?\n- Maybe you want to scale to a local server?\n- Maybe you need an embedding model that works well for a specific language?\n\nBeyond OpenAI, many options existing for embedding APIs, running your own embedding model locally, or even hosting your own server.\n\nA great resource to check on the current best overall embeddings models is the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), which ranks embeddings models on over 50 datasets and tasks.\n\n**NOTE:** Unlike an LLM (which you can change at any time), if you change your embedding model, you must re-index your data. Furthermore, you should ensure the same embedding model is used for both indexing and querying.\n\nWe have a list of [all supported embedding model integrations](../../module_guides/models/embeddings.md).\n\n## Chunk Sizes\n\nDepending on the type of data you are indexing, or the results from your retrieval, you may want to customize the chunk size or chunk overlap.\n\nWhen documents are ingested into an index, they are split into chunks with a certain amount of overlap. The default chunk size is 1024, while the default chunk overlap is 20.\n\nChanging either of these parameters will change the embeddings that are calculated. A smaller chunk size means the embeddings are more precise, while a larger chunk size means that the embeddings may be more general, but can miss fine-grained details.\n\nWe have done our own [initial evaluation on chunk sizes here](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5).\n\nFurthermore, when changing the chunk size for a vector index, you may also want to increase the `similarity_top_k` parameter to better represent the amount of data to retrieve for each query.\n\nHere is a full example:\n\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core import Settings\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nSettings.chunk_size = 512\nSettings.chunk_overlap = 50\n\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\n\nquery_engine = index.as_query_engine(similarity_top_k=4)\n```\n\nSince we halved the default chunk size, the example also doubles the `similarity_top_k` from the default of 2 to 4.\n\n## Hybrid Search\n\nHybrid search is a common term for retrieval that involves combining results from both semantic search (i.e. embedding similarity) and keyword search.\n\nEmbeddings are not perfect, and may fail to return text chunks with matching keywords in the retrieval step.\n\nThe solution to this issue is often hybrid search. In LlamaIndex, there are two main ways to achieve this:\n\n1. Use a vector database that has a hybrid search functionality (see [our complete list of supported vector stores](../../module_guides/storing/vector_stores.md)).\n2. Set up a local hybrid search mechanism with BM25.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa0c69e5-815b-4534-8f91-d150133becf7": {"__data__": {"id_": "aa0c69e5-815b-4534-8f91-d150133becf7", "embedding": null, "metadata": {"filename": "basic_strategies.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e7abbbb610c607fa279635a2db89b63e86b9186", "node_type": "4", "metadata": {"filename": "basic_strategies.md", "author": "LlamaIndex"}, "hash": "1ff3c80cf8bd1754cbd6d42a9c57b654711712f7ba0f09bd4ee82d9550774dd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5fa9e28-7781-4985-aaa5-c1eaa146de78", "node_type": "1", "metadata": {"filename": "basic_strategies.md", "author": "LlamaIndex"}, "hash": "b4686cb48eb9bf63991ddcb9fd53ce1e0fc52acc9291e1c34fe6a03870f98829", "class_name": "RelatedNodeInfo"}}, "text": "2. Set up a local hybrid search mechanism with BM25.\n\nRelevant guides with both approaches can be found below:\n\n- [BM25 Retriever](../../examples/retrievers/bm25_retriever.ipynb)\n- [Reciprocal Rerank Query Fusion](../../examples/retrievers/reciprocal_rerank_fusion.ipynb)\n- [Weaviate Hybrid Search](../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb)\n- [Pinecone Hybrid Search](../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb)\n- [Milvus Hybrid Search](../../examples/vector_stores/MilvusHybridIndexDemo.ipynb)\n\n## Metadata Filters\n\nBefore throwing your documents into a vector index, it can be useful to attach metadata to them. While this metadata can be used later on to help track the sources to answers from the `response` object, it can also be used at query time to filter data before performing the top-k similarity search.\n\nMetadata filters can be set manually, so that only nodes with the matching metadata are returned:\n\n```python\nfrom llama_index.core import VectorStoreIndex, Document\nfrom llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n\ndocuments = [\n    Document(text=\"text\", metadata={\"author\": \"LlamaIndex\"}),\n    Document(text=\"text\", metadata={\"author\": \"John Doe\"}),\n]\n\nfilters = MetadataFilters(\n    filters=[ExactMatchFilter(key=\"author\", value=\"John Doe\")]\n)\n\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine(filters=filters)\n```\n\nIf you are using an advanced LLM like GPT-4, and your [vector database supports filtering](../../module_guides/storing/vector_stores.md), you can get the LLM to write filters automatically at query time, using an `AutoVectorRetriever`.\n\n- [Vector Store Guide](../../module_guides/indexing/vector_store_guide.ipynb)\n\n## Document/Node Usage\n\nTake a look at our in-depth guides for more details on how to use Documents/Nodes.\n\n- [Documents Usage](../../module_guides/loading/documents_and_nodes/usage_documents.md)\n- [Nodes Usage](../../module_guides/loading/documents_and_nodes/usage_nodes.md)\n- [Metadata Extraction](../../module_guides/loading/documents_and_nodes/usage_metadata_extractor.md)\n\n## Multi-Tenancy RAG\n\nMulti-Tenancy in RAG systems is crucial for ensuring data security. It enables users to access exclusively their indexed documents, thereby preventing unauthorized sharing and safeguarding data privacy. Search operations are confined to the user's own data, protecting sensitive information. Implementation can be achieved with `VectorStoreIndex` and `VectorDB` providers through Metadata Filters.\n\nRefer the guides below for more details.\n\n- [Multi Tenancy RAG](../../examples/multi_tenancy/multi_tenancy_rag.ipynb)\n\nFor detailed guidance on implementing Multi-Tenancy RAG with LlamaIndex and Qdrant, refer to the [blog post](https://qdrant.tech/documentation/tutorials/llama-index-multitenancy/) released by Qdrant.", "mimetype": "text/plain", "start_char_idx": 3850, "end_char_idx": 6721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0499f9d9-a1cc-4303-903e-524c91532663": {"__data__": {"id_": "0499f9d9-a1cc-4303-903e-524c91532663", "embedding": null, "metadata": {"filename": "building_rag_from_scratch.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f041c5d9d10959ba897d229a632c1cd8e0c6991c", "node_type": "4", "metadata": {"filename": "building_rag_from_scratch.md", "author": "LlamaIndex"}, "hash": "1d00b878258497171b084f75ed84069d317addb95be2de5e1c5119f8731653ae", "class_name": "RelatedNodeInfo"}}, "text": "# Building RAG from Scratch (Lower-Level)\n\nThis doc is a hub for showing how you can build RAG and agent-based apps using only lower-level abstractions (e.g. LLMs, prompts, embedding models), and without using more \"packaged\" out of the box abstractions.\n\nOut of the box abstractions include:\n\n- High-level ingestion code e.g. `VectorStoreIndex.from_documents`\n- High-level query and retriever code e.g. `VectorStoreIndex.as_retriever()` and `VectorStoreIndex.as_query_engine()`\n- High-level agent abstractions e.g. `OpenAIAgent`\n\nInstead of using these, the goal here is to educate users on what's going on under the hood. By showing you the underlying algorithms for constructing RAG and agent pipelines, you can then be empowered to create your own custom LLM workflows (while still using LlamaIndex abstractions at any level of granularity that makes sense).\n\nWe show how to build an app from scratch, component by component. For the sake of focus, each tutorial will show how to build a specific component from scratch while using out-of-the-box abstractions for other components. **NOTE**: This is a WIP document, we're in the process of fleshing this out!\n\n## Building Ingestion from Scratch\n\nThis tutorial shows how you can define an ingestion pipeline into a vector store.\n\n- [Ingestion from scratch](../examples/low_level/ingestion.ipynb)\n\n## Building Vector Retrieval from Scratch\n\nThis tutorial shows you how to build a retriever to query a vector store.\n\n- [Vector Retrieval from Scratch](../examples/low_level/retrieval.ipynb)\n\n## Building Ingestion/Retrieval from Scratch (Open-Source/Local Components)\n\nThis tutoral shows you how to build an ingestion/retrieval pipeline using only\nopen-source components.\n\n- [Open Source RAG](../examples/low_level/oss_ingestion_retrieval.ipynb)\n\n## Building a (Very Simple) Vector Store from Scratch\n\nIf you want to learn more about how vector stores work, here's a tutorial showing you how to build a very simple vector store capable of dense search + metadata filtering.\n\nObviously not a replacement for production databases.\n\n- [Vector Store from Scratch](../examples/low_level/vector_store.ipynb)\n\n## Building Response Synthesis from Scratch\n\nThis tutorial shows you how to use the LLM to synthesize results given a set of retrieved context. Deals with context overflows, async calls, and source citations!\n\n- [Response Synthesis from Scratch](../examples/low_level/response_synthesis.ipynb)\n\n## Building Evaluation from Scratch\n\nLearn how to build common LLM-based eval modules (correctness, faithfulness) using LLMs and prompt modules; this will help you define your own custom evals!\n\n- [Evaluation from Scratch](../examples/low_level/evaluation.ipynb)\n\n## Building Advanced RAG from Scratch\n\nThese tutorials will show you how to build advanced functionality beyond the basic RAG pipeline. Especially helpful for advanced users with custom workflows / production needs.\n\n### Building Hybrid Search from Scratch\n\nHybrid search is an advanced retrieval feature supported by many vector databases. It allows you to combine **dense** retrieval with **sparse** retrieval with matching keywords.\n\n- [Building Hybrid Search from Scratch](../examples/vector_stores/qdrant_hybrid.ipynb)\n\n### Building a Router from Scratch\n\nBeyond the standard RAG pipeline, this takes you one step towards automated decision making with LLMs by showing you how to build a router module from scratch.\n\n- [Router from Scratch](../examples/low_level/router.ipynb)\n\n### Building RAG Fusion Retriever from Scratch\n\nHere we show you how to build an advanced retriever capable of query-rewriting, ensembling, dynamic retrieval.\n\n- [Fusion Retrieval from Scratch](../examples/low_level/fusion_retriever.ipynb)\n\n## Building QA over Structured Data from Scratch\n\nRAG as a framework is primarily focused on unstructured data. LlamaIndex also has out of the box support for structured data and semi-structured data as well.\n\nTake a look at our guides below to see how to build text-to-SQL and text-to-Pandas from scratch (using our Query Pipeline syntax).\n\n- [Text-to-SQL from Scratch](../examples/pipeline/query_pipeline_sql.ipynb)\n- [Text-to-Pandas from Scratch](../examples/pipeline/query_pipeline_pandas.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc024ccc-c066-4912-8fbf-c2c9f0945243": {"__data__": {"id_": "fc024ccc-c066-4912-8fbf-c2c9f0945243", "embedding": null, "metadata": {"filename": "custom_modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "972306828914dfa639c87d3c1a964daa88fca55e", "node_type": "4", "metadata": {"filename": "custom_modules.md", "author": "LlamaIndex"}, "hash": "a1b5003116ce28c8bb2066f42209a9a1363ea9f76c4c515f523b0062802aca26", "class_name": "RelatedNodeInfo"}}, "text": "# Writing Custom Modules\n\nA core design principle of LlamaIndex is that **almost every core module can be subclassed and customized**.\n\nThis allows you to use LlamaIndex for any advanced LLM use case, beyond the capabilities offered by our prepackaged modules. You're free to write as much custom code for any given module, but still take advantage of our lower-level abstractions and also plug this module along with other components.\n\nWe offer convenient/guided ways to subclass our modules, letting you write your custom logic without having to worry about having to define all boilerplate (for instance, [callbacks](../module_guides/observability/callbacks/index.md)).\n\nThis guide centralizes all the resources around writing custom modules in LlamaIndex. Check them out below \ud83d\udc47\n\n## Custom LLMs\n\n- [Custom LLMs](../module_guides/models/llms/usage_custom.md#example-using-a-custom-llm-model---advanced)\n\n## Custom Embeddings\n\n- [Custom Embedding Model](../module_guides/models/embeddings.md#custom-embedding-model)\n\n## Custom Output Parsers\n\n- [Custom Output Parsers](../examples/output_parsing/llm_program.ipynb)\n\n## Custom Transformations\n\n- [Custom Transformations](../module_guides/loading/ingestion_pipeline/transformations.md#custom-transformations)\n- [Custom Property Graph Extractors](../module_guides/indexing/lpg_index_guide.md#sub-classing-extractors)\n\n## Custom Retrievers\n\n- [Custom Retrievers](../examples/query_engine/CustomRetrievers.ipynb)\n- [Custom Property Graph Retrievers](../module_guides/indexing/lpg_index_guide.md#sub-classing-retrievers)\n\n## Custom Postprocessors/Rerankers\n\n- [Custom Node Postprocessor](./custom_modules.md#custom-postprocessorsrerankers)\n\n## Custom Query Engines\n\n- [Custom Query Engine](../examples/query_engine/custom_query_engine.ipynb)\n\n## Custom Agents\n\n- [Custom Agents](../examples/agent/custom_agent.ipynb)\n\n## Custom Query Components (for use in Query Pipeline)\n\n- [Custom Query Component](../module_guides/querying/pipeline/usage_pattern.md#defining-a-custom-query-component)\n\n## Other Ways of Customization\n\nSome modules can be customized heavily within your workflows but not through subclassing (and instead through parameters or functions we expose). We list these in guides below:\n\n- [Customizing Documents](../module_guides/loading/documents_and_nodes/usage_documents.md)\n- [Customizing Nodes](../module_guides/loading/documents_and_nodes/usage_nodes.md)\n- [Customizing Prompts within Higher-Level Modules](../examples/prompts/prompt_mixin.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21c4b589-5fbb-479c-92c2-3715a68125b3": {"__data__": {"id_": "21c4b589-5fbb-479c-92c2-3715a68125b3", "embedding": null, "metadata": {"filename": "component_wise_evaluation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c435a0cbf93fc8d2a21024417ba7efefbf636d40", "node_type": "4", "metadata": {"filename": "component_wise_evaluation.md", "author": "LlamaIndex"}, "hash": "ae96042a0db1ca7fcc91420f8af0454633d5ead068feaf81ee719e11ff5e8867", "class_name": "RelatedNodeInfo"}}, "text": "# Component Wise Evaluation\n\nTo do more in-depth evaluation of your pipeline, it helps to break it down into an evaluation of individual components.\n\nFor instance, a particular failure case may be due to a combination of not retrieving the right documents and also the LLM misunderstanding the context and hallucinating an incorrect result. Being able to isolate and deal with these issues separately can help reduce complexity and guide you in a step-by-step manner to a more satisfactory overall result.\n\n## Utilizing public benchmarks\n\nWhen doing initial model selection, it helps to look at how well the model is performing on a standardized, diverse set of domains or tasks.\n\nA useful benchmark for embeddings is the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n## Evaluating Retrieval\n\n### BEIR dataset\n\nBEIR is useful for benchmarking if a particular retrieval model generalize well to niche domains in a zero-shot setting.\n\nSince most publically-available embedding and retrieval models are already benchmarked against BEIR (e.g. through the MTEB benchmark), utilizing BEIR is more helpful when you have a unique model that you want to evaluate.\n\nFor instance, after fine-tuning an embedding model on your dataset, it may be helpful to view whether and by how much its performance degrades on a diverse set of domains. This can be an indication of how much data drift may affect your retrieval accuracy, such as if you add documents to your RAG system outside of your fine-tuning training distribution.\n\nHere is a notebook showing how the BEIR dataset can be used with your retrieval pipeline.\n\n- [BEIR Evaluation](../../examples/evaluation/BeirEvaluation.ipynb)\n\nWe will be adding more methods to evaluate retrieval soon. This includes evaluating retrieval on your own dataset.\n\n## Evaluating the Query Engine Components (e.g. Without Retrieval)\n\nIn this case, we may want to evaluate how specific components of a query engine (one which may generate sub-questions or follow-up questions) may perform on a standard benchmark. It can help give an indication of how far behind or ahead your retrieval pipeline is compared to alternate pipelines or models.\n\n### HotpotQA Dataset\n\nThe HotpotQA dataset is useful for evaluating queries that require multiple retrieval steps.\n\nExample:\n\n- [HotpotQA Eval](../../examples/evaluation/HotpotQADistractor.ipynb)\n\nLimitations:\n\n1. HotpotQA is evaluated on a Wikipedia corpus. LLMs, especially GPT4, tend to have memorized information from Wikipedia relatively well. Hence, the benchmark is not particularly good for evaluating retrieval + rerank systems with knowledgeable models like GPT4.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee03346f-d784-4531-b1a3-d6c994dbd7f1": {"__data__": {"id_": "ee03346f-d784-4531-b1a3-d6c994dbd7f1", "embedding": null, "metadata": {"filename": "e2e_evaluation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb057e089363ffddbbda67e822923a7d173ca225", "node_type": "4", "metadata": {"filename": "e2e_evaluation.md", "author": "LlamaIndex"}, "hash": "1007b7ec1ffcb6a9e46514b80290fb857d159a6a97ca7337ceac3262729b886b", "class_name": "RelatedNodeInfo"}}, "text": "# End-to-End Evaluation\n\nEnd-to-End evaluation should be the guiding signal for your RAG application - will my pipeline generate the right responses given the data sources and a set of queries?\n\nWhile it helps initially to individually inspect queries and responses, as you deal with more failure and corner cases, it may stop being feasible to look at each query individually, and rather it may help instead to define a set of summary metrics or automated evaluation, and gain an intuition for what they might be telling you and where you might dive deeper.\n\n## Setting up an Evaluation Set\n\nIt is helpful to start off with a small but diverse set of queries, and build up more examples as one discovers problematic queries or interactions.\n\nWe've created some tools that automatically generate a dataset for you given a set of documents to query. (See example below).\n\n- [Question Generation](../../examples/evaluation/QuestionGeneration.ipynb)\n\nIn the future, we will also be able to create datasets automatically against tools.\n\n## The Spectrum of Evaluation Options\n\nQuantitative eval is more useful when evaluating applications where there is a correct answer - for instance, validating that the choice of tools and their inputs are correct given the plan, or retrieving specific pieces of information, or attempting to produce intermediate output of a certain schema (e.g. JSON fields).\n\nQualitative eval is more useful when generating long-form responses that are meant to be _helpful_ but not necessarily completely accurate.\n\nThere is a spectrum of evaluation options ranging from metrics, cheaper models, more expensive models (GPT4), and human evaluation.\n\nBelow is some example usage of the [evaluation modules](evaluation.md):\n\n- [Batch Eval Runner](../../examples/evaluation/batch_eval.ipynb)\n- [Correctness Eval](../../examples/evaluation/correctness_eval.ipynb)\n- [Faithfulness Eval](../../examples/evaluation/faithfulness_eval.ipynb)\n- [Guideline Eval](../../examples/evaluation/guideline_eval.ipynb)\n- [Pairwise Eval](../../examples/evaluation/pairwise_eval.ipynb)\n- [Relevancy Eval](../../examples/evaluation/relevancy_eval.ipynb)\n- [Semantic Similarity Eval](../../examples/evaluation/semantic_similarity_eval.ipynb)\n\n## Discovery - Sensitivity Testing\n\nWith a complex pipeline, it may be unclear which parts of the pipeline are affecting your results.\n\nSensitivity testing can be a good inroad into choosing which components to individually test or tweak more thoroughly, or which parts of your dataset (e.g. queries) may be producing problematic results.\n\nMore details on how to discover issues automatically with methods such as sensitivity testing will come soon.\n\nExamples of this in the more traditional ML domain include [Giskard](https://docs.giskard.ai/en/latest/getting-started/quickstart.html).\n\n## Metrics Ensembling\n\nIt may be expensive to use GPT-4 to carry out evaluation especially as your dev set grows large.\n\nMetrics ensembling uses an ensemble of weaker signals (exact match, F1, ROUGE, BLEU, BERT-NLI and BERT-similarity) to predict the output of a more expensive evaluation methods that are closer to the gold labels (human-labelled/GPT-4).\n\nIt is intended for two purposes:\n\n1. Evaluating changes cheaply and quickly across a large dataset during the development stage.\n2. Flagging outliers for further evaluation (GPT-4 / human alerting) during the production monitoring stage.\n\nWe also want the metrics ensembling to be interpretable - the correlation and weighting scores should give an indication of which metrics best capture the evaluation criteria.\n\nWe will discuss more about the methodology in future updates.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9800936-57a0-4a2e-a038-fab77a504e3d": {"__data__": {"id_": "c9800936-57a0-4a2e-a038-fab77a504e3d", "embedding": null, "metadata": {"filename": "evaluation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dcc118424e9f2e4f070197ca13909f86a7471ae", "node_type": "4", "metadata": {"filename": "evaluation.md", "author": "LlamaIndex"}, "hash": "b03037e3234bab9be69dd7c9a5f2f6e5675de0d3466c563f4320280a79f05816", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluation\n\n## Setting the Stage\n\nLlamaIndex is meant to connect your data to your LLM applications.\n\nSometimes, even after diagnosing and fixing bugs by looking at traces, more fine-grained evaluation is required to systematically diagnose issues.\n\nLlamaIndex aims to provide those tools to make identifying issues and receiving useful diagnostic signals easy.\n\nClosely tied to evaluation are the concepts of experimentation and experiment tracking.\n\n## General Strategy\n\nWhen developing your LLM application, it could help to first define an end-to-end evaluation workflow, and then once you've started collecting failure or corner cases and getting an intuition for what is or isn't going well, you may dive deeper into evaluating and improving specific components.\n\nThe analogy with software testing is integration tests and unit tests. You should probably start writing unit tests once you start fiddling with individual components. Equally, your gold standard on whether things are working well together are integration tests. Both are equally important.\n\n- [End-to-end Evaluation](./e2e_evaluation.md)\n- [Component-Wise Evaluation](./component_wise_evaluation.md)\n\nHere is an overview of the existing modules for evaluation. We will be adding more modules and support over time.\n\n- [Evaluation Overview](../../module_guides/evaluating/index.md)\n\n### E2E or Component-Wise - Which Do I Start With?\n\nIf you want to get an overall idea of how your system is doing as you iterate upon it, it makes sense to start with centering your core development loop around the e2e eval - as an overall sanity/vibe check.\n\nIf you have an idea of what you're doing and want to iterate step by step on each component, building it up as things go - you may want to start with a component-wise eval. However this may run the risk of premature optimization - making model selection or parameter choices without assessing the overall application needs. You may have to revisit these choices when creating your final application.\n\n## Diving Deeper into Evaluation\n\nEvaluation is a controversial topic, and as the field of NLP has evolved, so have the methods of evaluation.\n\nIn a world where powerful foundation models are now performing annotation tasks better than human annotators, the best practices around evaluation are constantly changing. Previous methods of evaluation which were used to bootstrap and evaluate today's models such as BLEU or F1 have been shown to have poor correlation with human judgements, and need to be applied prudently.\n\nTypically, generation-heavy, open-ended tasks and requiring judgement or opinion and harder to evaluate automatically than factual questions due to their subjective nature. We will aim to provide more guides and case-studies for which methods are appropriate in a given scenario.\n\n### Standard Metrics\n\nAgainst annotated datasets, whether your own data or an academic benchmark, there are a number of standard metrics that it helps to be aware of:\n\n1. **Exact Match (EM):** The percentage of queries that are answered exactly correctly.\n2. **Recall:** The percentage of queries that are answered correctly, regardless of the number of answers returned.\n3. **Precision:** The percentage of queries that are answered correctly, divided by the number of answers returned.\n4. **F1:** The F1 score is the harmonic mean of precision and recall. It thus symmetrically represents both precision and recall in one metric, considering both false positives and false negatives.\n\nThis [towardsdatascience article](https://towardsdatascience.com/ranking-evaluation-metrics-for-recommender-systems-263d0a66ef54) covers more technical metrics like NDCG, MAP and MRR in greater depth.\n\n## Case Studies and Resources\n\n1. (Course) [Data-Centric AI (MIT), 2023](https://www.youtube.com/playlist?list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5)\n2. [Scale's Approach to LLM Testing and Evaluation](https://scale.com/llm-test-evaluation)\n3. [LLM Patterns by Eugene Yan](https://eugeneyan.com/writing/llm-patterns/)\n\n## Resources\n\n- [Component-Wise Evaluation](./component_wise_evaluation.md)\n- [End-to-end Evaluation](./e2e_evaluation.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40cd32cf-499a-4950-8c89-e1c8249ee062": {"__data__": {"id_": "40cd32cf-499a-4950-8c89-e1c8249ee062", "embedding": null, "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96c6062b9c4e89910918ec78aa542f3a707d3996", "node_type": "4", "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "hash": "a1a281f4511bfcc7c8e2a4438a1f4b5cb587ee3cbfab43746d1ee8cabd649d73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e697071-27d8-401e-ac2e-9e00dd24303f", "node_type": "1", "metadata": {}, "hash": "96f246c8314acb5432181e7e8e4a745df6c2ab8ff189568be754a94845ca12c1", "class_name": "RelatedNodeInfo"}}, "text": "# Fine-tuning\n\n## Overview\n\nFinetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost.\n\nThe core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves.\n\nWhile finetuning can be also used to \"augment\" a model with external data, finetuning can complement retrieval augmentation in a variety of ways:\n\n#### Embedding Finetuning Benefits\n\n- Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data --> leads to better retrieval performance.\n\n#### LLM Finetuning Benefits\n\n- Allow it to learn a style over a given dataset\n- Allow it to learn a DSL that might be less represented in the training data (e.g. SQL)\n- Allow it to correct hallucinations/errors that might be hard to fix through prompt engineering\n- Allow it to distill a better model (e.g. GPT-4) into a simpler/cheaper model (e.g. gpt-3.5, Llama 2)\n\n## Integrations with LlamaIndex\n\nThis is an evolving guide, and there are currently three key integrations with LlamaIndex. Please check out the sections below for more details!\n\n- Finetuning embeddings for better retrieval performance\n- Finetuning Llama 2 for better text-to-SQL\n- Finetuning gpt-3.5-turbo to distill gpt-4\n\n## Finetuning Embeddings\n\nWe've created comprehensive guides showing you how to finetune embeddings in different ways, whether that's the model itself (in this case, `bge`) over an unstructured text corpus, or an adapter over any black-box embedding. It consists of the following steps:\n\n1. Generating a synthetic question/answer dataset using LlamaIndex over any unstructured context.\n2. Finetuning the model\n3. Evaluating the model.\n\nFinetuning gives you a 5-10% increase in retrieval evaluation metrics. You can then plug this fine-tuned model into your RAG application with LlamaIndex.\n\n- [Fine-tuning an Adapter](../../examples/finetuning/embeddings/finetune_embedding_adapter.ipynb)\n- [Embedding Fine-tuning Guide](../../examples/finetuning/embeddings/finetune_embedding.ipynb)\n- [Router Fine-tuning](../../examples/finetuning/router/router_finetune.ipynb)\n\n**Old**\n\n- [Embedding Fine-tuning Repo](https://github.com/run-llama/finetune-embedding)\n- [Embedding Fine-tuning Blog](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)\n\n## Fine-tuning LLMs\n\n### Fine-tuning GPT-3.5 to distill GPT-4\n\nWe have multiple guides showing how to use OpenAI's finetuning endpoints to fine-tune gpt-3.5-turbo to output GPT-4 responses for RAG/agents.\n\nWe use GPT-4 to automatically generate questions from any unstructured context, and use a GPT-4 query engine pipeline to generate \"ground-truth\" answers. Our `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset.\n\nWe then launch a finetuning job, and get back a distilled model. We can evaluate this model with [Ragas](https://github.com/explodinggradients/ragas) to benchmark against a naive GPT-3.5 pipeline.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e697071-27d8-401e-ac2e-9e00dd24303f": {"__data__": {"id_": "1e697071-27d8-401e-ac2e-9e00dd24303f", "embedding": null, "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96c6062b9c4e89910918ec78aa542f3a707d3996", "node_type": "4", "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "hash": "a1a281f4511bfcc7c8e2a4438a1f4b5cb587ee3cbfab43746d1ee8cabd649d73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40cd32cf-499a-4950-8c89-e1c8249ee062", "node_type": "1", "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "hash": "80813a8b3488d24441c7df78870c53f5766d0310a499d8c0c861225731468086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9c95829-907b-4e36-8a72-7eea66638902", "node_type": "1", "metadata": {}, "hash": "c34aa6435f7f39afd257ddd9f4e381b7b3b05ac84e3abfb64b1cc7cea1c252f8", "class_name": "RelatedNodeInfo"}}, "text": "- [GPT-3.5 Fine-tuning Notebook (Colab)](https://colab.research.google.com/drive/1NgyCJVyrC2xcZ5lxt2frTU862v6eJHlc?usp=sharing)\n- [GPT-3.5 Fine-tuning Notebook (Notebook link)](../../examples/finetuning/openai_fine_tuning.ipynb)\n  /examples/finetuning/react_agent/react_agent_finetune.ipynb\n- [[WIP] Function Calling Fine-tuning](../../examples/finetuning/openai_fine_tuning_functions.ipynb)\n\n**Old**\n\n- [GPT-3.5 Fine-tuning Notebook (Colab)](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing)\n- [GPT-3.5 Fine-tuning Notebook (in Repo)](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb)\n\n### Fine-tuning for Better Structured Outputs\n\nAnother use case for fine-tuning is to make the model better at outputting structured data.\nWe can do this for both OpenAI and Llama2.\n\n- [OpenAI Function Calling Fine-tuning](../../examples/finetuning/openai_fine_tuning_functions.ipynb)\n- [Llama2 Structured Output Fine-tuning](../../examples/finetuning/gradient/gradient_structured.ipynb)\n\n### Fine-tuning Llama 2 for Better Text-to-SQL\n\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using LlamaIndex abstractions.\n\nThe stack includes `sql-create-context` as the training dataset, OpenLLaMa as the base model, PEFT for finetuning, Modal for cloud compute, LlamaIndex for inference abstractions.\n\n- [Llama 2 Text-to-SQL Fine-tuning (w/ Gradient.AI)](../../examples/finetuning/gradient/gradient_fine_tuning.ipynb)\n- [Llama 2 Text-to-SQL Fine-tuning (w/ Modal, Repo)](https://github.com/run-llama/modal_finetune_sql)\n- [Llama 2 Text-to-SQL Fine-tuning (w/ Modal, Notebook)](https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb)\n\n### Fine-tuning An Evaluator\n\nIn these tutorials, we aim to distill a GPT-4 judge (or evaluator) onto a GPT-3.5 judge. It has\nbeen recently observed that GPT-4 judges can reach high levels of agreement with human evaluators (e.g.,\nsee https://arxiv.org/pdf/2306.05685.pdf).\n\nThus, by fine-tuning a GPT-3.5 judge, we may be able to reach GPT-4 levels (and\nby proxy, agreement with humans) at a lower cost.\n\n- [Finetune Correctness Judge](../../examples/finetuning/llm_judge/correctness/finetune_llm_judge_single_grading_correctness.ipynb)\n- [Finetune LLM Judge](../../examples/finetuning/llm_judge/pairwise/finetune_llm_judge.ipynb)\n\n## Fine-tuning Cross-Encoders for Re-Ranking\n\nBy finetuning a cross encoder, we can attempt to improve re-ranking performance on our own private data.\n\nRe-ranking is key step in advanced retrieval, where retrieved nodes from many sources are re-ranked using a separate model, so that the most relevant nodes\nare first.\n\nIn this example, we use the `sentence-transformers` package to help finetune a crossencoder model, using a dataset that is generated based on the `QASPER` dataset.", "mimetype": "text/plain", "start_char_idx": 3238, "end_char_idx": 6187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9c95829-907b-4e36-8a72-7eea66638902": {"__data__": {"id_": "f9c95829-907b-4e36-8a72-7eea66638902", "embedding": null, "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96c6062b9c4e89910918ec78aa542f3a707d3996", "node_type": "4", "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "hash": "a1a281f4511bfcc7c8e2a4438a1f4b5cb587ee3cbfab43746d1ee8cabd649d73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e697071-27d8-401e-ac2e-9e00dd24303f", "node_type": "1", "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}, "hash": "720db87f6f59eb51834449d7e9dd7b502e2d313e1fa2986f7225f1897e31b576", "class_name": "RelatedNodeInfo"}}, "text": "- [Cross-Encoder Finetuning](../../examples/finetuning/cross_encoder_finetuning/cross_encoder_finetuning.ipynb)\n- [Finetuning Llama 2 for Text-to-SQL](https://medium.com/llamaindex-blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d)\n- [Finetuning GPT-3.5 to Distill GPT-4](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing)\n\n## Cohere Custom Reranker\n\nBy training a custom reranker with CohereAI, we can attempt to improve re-ranking performance on our own private data.\n\nRe-ranking is a crucial step in advanced retrieval processes. This step involves using a separate model to re-organize nodes retrieved from initial retrieval phase. The goal is to ensure that the most relevant nodes are prioritized and appear first.\n\nIn this example, we use the `cohere` custom reranker training module to create a reranker on your domain or specific dataset to improve retrieval performance.\n\n- [Cohere Custom Reranker](../../examples/finetuning/rerankers/cohere_custom_reranker.ipynb)", "mimetype": "text/plain", "start_char_idx": 6189, "end_char_idx": 7225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76f70d8d-828a-4342-b948-4e982571e87c": {"__data__": {"id_": "76f70d8d-828a-4342-b948-4e982571e87c", "embedding": null, "metadata": {"filename": "production_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab61d80aa6d929518257d345d5c406ceb6fdb6c5", "node_type": "4", "metadata": {"filename": "production_rag.md", "author": "LlamaIndex"}, "hash": "60cbddfd21ad55eb87b28f00839117c2e7e205ca0daabc4c2c3657aff1d38959", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75ca9196-00d2-432e-84dc-4b7014baaa36", "node_type": "1", "metadata": {}, "hash": "913d5a58c9e0dad82dea74ce4dcbc3718f18b6218170019a100f387befec15f2", "class_name": "RelatedNodeInfo"}}, "text": "# Building Performant RAG Applications for Production\n\nPrototyping a RAG application is easy, but making it performant, robust, and scalable to a large knowledge corpus is hard.\n\nThis guide contains a variety of tips and tricks to improve the performance of your RAG pipeline. We first outline\nsome general techniques - they are loosely ordered in terms of most straightforward to most challenging.\nWe then dive a bit more deeply into each technique, the use cases that it solves,\nand how to implement it with LlamaIndex!\n\nThe end goal is to optimize your retrieval and generation performance to answer more\nqueries over more complex datasets accurately and without hallucinations.\n\n## General Techniques for Building Production-Grade RAG\n\nHere are some top Considerations for Building Production-Grade RAG\n\n- Decoupling chunks used for retrieval vs. chunks used for synthesis\n- Structured Retrieval for Larger Document Sets\n- Dynamically Retrieve Chunks Depending on your Task\n- Optimize context embeddings\n\nWe discussed this and more during our [Production RAG Webinar](https://www.youtube.com/watch?v=Zj5RCweUHIk).\nCheck out [this Tweet thread](https://twitter.com/jerryjliu0/status/1692931028963221929?s=20) for more synthesized details.\n\n## Decoupling Chunks Used for Retrieval vs. Chunks Used for Synthesis\n\nA key technique for better retrieval is to decouple chunks used for retrieval with those that are used for synthesis.\n\n![](../_static/production_rag/decouple_chunks.png)\n\n#### Motivation\n\nThe optimal chunk representation for retrieval might be different than the optimal consideration used for synthesis.\nFor instance, a raw text chunk may contain needed details for the LLM to synthesize a more detailed answer given a query. However, it\nmay contain filler words/info that may bias the embedding representation, or it may lack global context and not be retrieved at all\nwhen a relevant query comes in.\n\n#### Key Techniques\n\nThere\u2019s two main ways to take advantage of this idea:\n\n**1. Embed a document summary, which links to chunks associated with the document.**\n\nThis can help retrieve relevant documents at a high-level before retrieving chunks vs. retrieving chunks directly (that might be in irrelevant documents).\n\nResources:\n\n- [Table Recursive Retrieval](../examples/query_engine/pdf_tables/recursive_retriever.ipynb)\n- [Document Summary Index](../examples/index_structs/doc_summary/DocSummary.ipynb)\n\n**2. Embed a sentence, which then links to a window around the sentence.**\n\nThis allows for finer-grained retrieval of relevant context (embedding giant chunks leads to \u201clost in the middle\u201d problems), but also ensures enough context for LLM synthesis.\n\nResources:\n\n- [Metadata Replacement Postprocessor](../examples/node_postprocessor/MetadataReplacementDemo.ipynb)\n\n## Structured Retrieval for Larger Document Sets\n\n![](../_static/production_rag/structured_retrieval.png)\n\n#### Motivation\n\nA big issue with the standard RAG stack (top-k retrieval + basic text splitting) is that it doesn\u2019t do well as the number of documents scales up - e.g. if you have 100 different PDFs.\nIn this setting, given a query you may want to use structured information to help with more precise retrieval; for instance, if you ask a question that's only relevant to two PDFs,\nusing structured information to ensure those two PDFs get returned beyond raw embedding similarity with chunks.\n\n#### Key Techniques\n\nThere\u2019s a few ways of performing more structured tagging/retrieval for production-quality RAG systems, each with their own pros/cons.\n\n**1. Metadata Filters + Auto Retrieval**\nTag each document with metadata and then store in a vector database. During inference time, use the LLM to infer the right metadata filters to query the vector db in addition to the semantic query string.\n\n- Pros \u2705: Supported in major vector dbs. Can filter document via multiple dimensions.\n- Cons \ud83d\udeab: Can be hard to define the right tags. Tags may not contain enough relevant information for more precise retrieval. Also tags represent keyword search at the document-level, doesn\u2019t allow for semantic lookups.\n\nResources:\n**2. Store Document Hierarchies (summaries -> raw chunks) + Recursive Retrieval**\nEmbed document summaries and map to chunks per document. Fetch at the document-level first before chunk level.\n\n- Pros \u2705: Allows for semantic lookups at the document level.\n- Cons \ud83d\udeab: Doesn\u2019t allow for keyword lookups by structured tags (can be more precise than semantic search). Also autogenerating summaries can be expensive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75ca9196-00d2-432e-84dc-4b7014baaa36": {"__data__": {"id_": "75ca9196-00d2-432e-84dc-4b7014baaa36", "embedding": null, "metadata": {"filename": "production_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab61d80aa6d929518257d345d5c406ceb6fdb6c5", "node_type": "4", "metadata": {"filename": "production_rag.md", "author": "LlamaIndex"}, "hash": "60cbddfd21ad55eb87b28f00839117c2e7e205ca0daabc4c2c3657aff1d38959", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76f70d8d-828a-4342-b948-4e982571e87c", "node_type": "1", "metadata": {"filename": "production_rag.md", "author": "LlamaIndex"}, "hash": "6f8441c22378ea59a1f35107d52953aa9c61bd7afbace023d3ee6e4ad1342814", "class_name": "RelatedNodeInfo"}}, "text": "Also autogenerating summaries can be expensive.\n\n**Resources**\n\n- [Chroma Auto-Retrieval](../examples/vector_stores/chroma_auto_retriever.ipynb)\n- [Document Summary Index](../examples/index_structs/doc_summary/DocSummary.ipynb)\n- [Recursive Retriever](../examples/query_engine/recursive_retriever_agents.ipynb)\n- [Auto-Retriever vs. Recursive Retriever](../examples/retrievers/auto_vs_recursive_retriever.ipynb)\n\n## Dynamically Retrieve Chunks Depending on your Task\n\n![](../_static/production_rag/joint_qa_summary.png)\n\n#### Motivation\n\nRAG isn't just about question-answering about specific facts, which top-k similarity is optimized for. There can be a broad range of queries that a user might ask. Queries that are handled by naive RAG stacks include ones that ask about specific facts e.g. \"Tell me about the D&I initiatives for this company in 2023\" or \"What did the narrator do during his time at Google\". But queries can also include summarization e.g. \"Can you give me a high-level overview of this document\", or comparisons \"Can you compare/contrast X and Y\". All of these use cases may require different retrieval techniques.\n\n#### Key Techniques\n\nLlamaIndex provides some core abstractions to help you do task-specific retrieval. This includes our [router](../module_guides/querying/router/index.md) module as well as our [data agent](../module_guides/deploying/agents/index.md) module.\nThis also includes some advanced query engine modules.\nThis also include other modules that join structured and unstructured data.\n\nYou can use these modules to do joint question-answering and summarization, or even combine structured queries with unstructured queries.\n\n**Core Module Resources**\n\n- [Query engine](../module_guides/deploying/query_engine/index.md)\n- [Agents](../module_guides/deploying/agents/index.md)\n- [Router](../module_guides/querying/router/index.md)\n\n**Detailed Guide Resources**\n\n- [Sub-Question Query Engine](../examples/query_engine/sub_question_query_engine.ipynb)\n- [Joint QA-Summary](../examples/query_engine/JointQASummary.ipynb)\n- [Recursive Retriever Agents](../examples/query_engine/recursive_retriever_agents.ipynb)\n- [Router Query Engine](../examples/query_engine/RouterQueryEngine.ipynb)\n- [OpenAI Agent Cookbook](../examples/agent/openai_agent_query_cookbook.ipynb)\n- [OpenAIAgent Query Planning](../examples/agent/openai_agent_query_plan.ipynb)\n\n## Optimize Context Embeddings\n\n#### Motivation\n\nThis is related to the motivation described above in \"decoupling chunks used for retrieval vs. synthesis\".\nWe want to make sure that the embeddings are optimized for better retrieval over your specific data corpus.\nPre-trained models may not capture the salient properties of the data relevant to your use case.\n\n### Key Techniques\n\nBeyond some of the techniques listed above, we can also try finetuning the embedding model.\nWe can actually do this over an unstructured text corpus, in a label-free way.\n\nCheck out our guides here:\n\n- [Embedding Fine-tuning Guide](../examples/finetuning/embeddings/finetune_embedding.ipynb)", "mimetype": "text/plain", "start_char_idx": 4476, "end_char_idx": 7533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45c94491-734f-47d5-8f41-6014f931b4b3": {"__data__": {"id_": "45c94491-734f-47d5-8f41-6014f931b4b3", "embedding": null, "metadata": {"filename": "past_presentations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "976a4a7632a2f9e7ba16cd956f7dbc3b76f67b25", "node_type": "4", "metadata": {"filename": "past_presentations.md", "author": "LlamaIndex"}, "hash": "a2133295e0ad9eea2539c496e9643b3ccacc38dd3104c0a3ec36337194e3ce51", "class_name": "RelatedNodeInfo"}}, "text": "# List of Past Presentations\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/presentations/materials/2024-06-22-genai-philippines.ipynb\n/presentations/materials/2024-06-19-georgian-genai-bootcamp.ipynb\n/presentations/materials/2024-06-13-vector-ess-oss-tools.ipynb\n/presentations/materials/2024-05-10-rbc-llm-workshop.ipynb\n/presentations/materials/2024-04-04-vector-pe-lab.ipynb\n/presentations/materials/2024-04-02-otpp.ipynb\n/presentations/materials/2024-02-28-rag-bootcamp-vector-institute.ipynb\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e54c64d-d182-4632-b174-e929896d7d58": {"__data__": {"id_": "3e54c64d-d182-4632-b174-e929896d7d58", "embedding": null, "metadata": {"filename": "basic_agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4c73cf633f2552255eb077668754504eae8a1ac", "node_type": "4", "metadata": {"filename": "basic_agent.md", "author": "LlamaIndex"}, "hash": "ac5506682eefa6dd0d17598c907b47956a543f9ab7411ab5fffeb5617e75a148", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d594115-9d6f-4831-9134-b60bf087739a", "node_type": "1", "metadata": {}, "hash": "de04df73ac3079bb82321f12ee6f9eeccd2b8409bba33cc147a59222b3fea425", "class_name": "RelatedNodeInfo"}}, "text": "# Building a basic agent\n\nIn LlamaIndex, an agent is a semi-autonomous piece of software powered by an LLM that is given a task and executes a series of steps towards solving that task. It is given a set of tools, which can be anything from arbitrary functions up to full LlamaIndex query engines, and it selects the best available tool to complete each step. When each step is completed, the agent judges whether the task is now complete, in which case it returns a result to the user, or whether it needs to take another step, in which case it loops back to the start.\n\n![agent flow](./agent_flow.png)\n\n## Getting started\n\nYou can find all of this code in [the tutorial repo](https://github.com/run-llama/python-agents-tutorial).\n\nTo avoid conflicts and keep things clean, we'll start a new Python virtual environment. You can use any virtual environment manager, but we'll use `poetry` here:\n\n```bash\npoetry init\npoetry shell\n```\n\nAnd then we'll install the LlamaIndex library and some other dependencies that will come in handy:\n\n```bash\npip install llama-index python-dotenv\n```\n\nIf any of this gives you trouble, check out our more detailed [installation guide](../getting_started/installation/).\n\n## OpenAI Key\n\nOur agent will be powered by OpenAI's `GPT-3.5-Turbo` LLM, so you'll need an [API key](https://platform.openai.com/). Once you have your key, you can put it in a `.env` file in the root of your project:\n\n```bash\nOPENAI_API_KEY=sk-proj-xxxx\n```\n\nIf you don't want to use OpenAI, we'll show you how to use other models later.\n\n## Bring in dependencies\n\nWe'll start by importing the components of LlamaIndex we need, as well as loading the environment variables from our `.env` file:\n\n```python\nfrom dotenv import load_dotenv\n\nload_dotenv()\nfrom llama_index.core.agent import ReActAgent\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.tools import FunctionTool\n```\n\n## Create basic tools\n\nFor this simple example we'll be creating two tools: one that knows how to multiply numbers together, and one that knows how to add them.\n\n```python\ndef multiply(a: float, b: float) -> float:\n    \"\"\"Multiply two numbers and returns the product\"\"\"\n    return a * b\n\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n\ndef add(a: float, b: float) -> float:\n    \"\"\"Add two numbers and returns the sum\"\"\"\n    return a + b", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d594115-9d6f-4831-9134-b60bf087739a": {"__data__": {"id_": "9d594115-9d6f-4831-9134-b60bf087739a", "embedding": null, "metadata": {"filename": "basic_agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4c73cf633f2552255eb077668754504eae8a1ac", "node_type": "4", "metadata": {"filename": "basic_agent.md", "author": "LlamaIndex"}, "hash": "ac5506682eefa6dd0d17598c907b47956a543f9ab7411ab5fffeb5617e75a148", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e54c64d-d182-4632-b174-e929896d7d58", "node_type": "1", "metadata": {"filename": "basic_agent.md", "author": "LlamaIndex"}, "hash": "40f6736fd8b29e5f2f0cc7128784822e66125b1ad4410539ea257964b31c0f5b", "class_name": "RelatedNodeInfo"}}, "text": "def add(a: float, b: float) -> float:\n    \"\"\"Add two numbers and returns the sum\"\"\"\n    return a + b\n\n\nadd_tool = FunctionTool.from_defaults(fn=add)\n```\n\nAs you can see, these are regular vanilla Python functions. The docstring comments provide metadata to the agent about what the tool does: if your LLM is having trouble figuring out which tool to use, these docstrings are what you should tweak first.\n\nAfter each function is defined we create `FunctionTool` objects from these functions, which wrap them in a way that the agent can understand.\n\n## Initialize the LLM\n\n`GPT-3.5-Turbo` is going to be doing the work today:\n\n```python\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n```\n\nYou could also pick another popular model accessible via API, such as those from [Mistral](../examples/llm/mistralai/), [Claude from Anthropic](../examples/llm/anthropic/) or [Gemini from Google](../examples/llm/gemini/).\n\n## Initialize the agent\n\nNow we create our agent. In this case, this is a [ReAct agent](https://klu.ai/glossary/react-agent-model), a relatively simple but powerful agent. We give it an array containing our two tools, the LLM we just created, and set `verbose=True` so we can see what's going on:\n\n```python\nagent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n```\n\n## Ask a question\n\nWe specify that it should use a tool, as this is pretty simple and GPT-3.5 doesn't really need this tool to get the answer.\n\n```python\nresponse = agent.chat(\"What is 20+(2*4)? Use a tool to calculate every step.\")\n```\n\nThis should give you output similar to the following:\n\n```\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: multiply\nAction Input: {'a': 2, 'b': 4}\nObservation: 8\nThought: I need to add 20 to the result of the multiplication.\nAction: add\nAction Input: {'a': 20, 'b': 8}\nObservation: 28\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The result of 20 + (2 * 4) is 28.\nThe result of 20 + (2 * 4) is 28.\n```\n\nAs you can see, the agent picks the correct tools one after the other and combines the answers to give the final result. Check the [repo](https://github.com/run-llama/python-agents-tutorial/blob/main/1_basic_agent.py) to see what the final code should look like.\n\nCongratulations! You've built the most basic kind of agent. Next you can find out how to use [local models](./local_models.md) or skip to [adding RAG to your agent](./rag_agent.md).", "mimetype": "text/plain", "start_char_idx": 2246, "end_char_idx": 4762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a30ebdb5-6c98-4f36-ac2a-bb40d622f5ae": {"__data__": {"id_": "a30ebdb5-6c98-4f36-ac2a-bb40d622f5ae", "embedding": null, "metadata": {"filename": "llamaparse.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a87dd1768aa384a4a3009d61c7ebfb0e6435889", "node_type": "4", "metadata": {"filename": "llamaparse.md", "author": "LlamaIndex"}, "hash": "2c68ae5ee9030c02df874f6b71add2869cd06176d9b00b81c0a16806172880b5", "class_name": "RelatedNodeInfo"}}, "text": "# Enhancing with LlamaParse\n\nIn the previous example we asked a very basic question of our document, about the total amount of the budget. Let's instead ask a more complicated question about a specific fact in the document:\n\n```python\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\n    \"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\"\n)\nprint(response)\n```\n\nWe unfortunately get an unhelpful answer:\n\n```\nThe budget allocated funds to a new green investments tax credit, but the exact amount was not specified in the provided context information.\n```\n\nThis is bad, because we happen to know the exact number is in the document! But the PDF is complicated, with tables and multi-column layout, and the LLM is missing the answer. Luckily, we can use LlamaParse to help us out.\n\nFirst, you need a LlamaCloud API key. You can [get one for free](https://cloud.llamaindex.ai/) by signing up for LlamaCloud. Then put it in your `.env` file just like your OpenAI key:\n\n```bash\nLLAMA_CLOUD_API_KEY=llx-xxxxx\n```\n\nNow you're ready to use LlamaParse in your code. Let's bring it in as as import:\n\n```python\nfrom llama_parse import LlamaParse\n```\n\nAnd let's put in a second attempt to parse and query the file (note that this uses `documents2`, `index2`, etc.) and see if we get a better answer to the exact same question:\n\n```python\ndocuments2 = LlamaParse(result_type=\"markdown\").load_data(\n    \"./data/2023_canadian_budget.pdf\"\n)\nindex2 = VectorStoreIndex.from_documents(documents2)\nquery_engine2 = index2.as_query_engine()\n\nresponse2 = query_engine2.query(\n    \"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\"\n)\nprint(response2)\n```\n\nWe do!\n\n```\n$20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\n```\n\nYou can always check [the repo](https://github.com/run-llama/python-agents-tutorial/blob/main/4_llamaparse.py) to what this code looks like.\n\nAs you can see, parsing quality makes a big difference to what the LLM can understand, even for relatively simple questions. Next let's see how [memory](./memory.md) can help us with more complex questions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41ea0afc-4f7d-473d-b1a3-205993586bd7": {"__data__": {"id_": "41ea0afc-4f7d-473d-b1a3-205993586bd7", "embedding": null, "metadata": {"filename": "local_models.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b2d6332c6226c8a1ef662e0a627c426d8b2c0ee", "node_type": "4", "metadata": {"filename": "local_models.md", "author": "LlamaIndex"}, "hash": "3e470895fa9932329cfd0f48444b484240a045d8ba564c85509f3c6370ea1add", "class_name": "RelatedNodeInfo"}}, "text": "# Agents with local models\r\n\r\nIf you're happy using OpenAI or another remote model, you can skip this section, but many people are interested in using models they run themselves. The easiest way to do this is via the great work of our friends at [Ollama](https://ollama.com/), who provide a simple to use client that will download, install and run a [growing range of models](https://ollama.com/library) for you.\r\n\r\n## Install Ollama\r\n\r\nThey provide a one-click installer for Mac, Linux and Windows on their [home page](https://ollama.com/).\r\n\r\n## Pick and run a model\r\n\r\nSince we're going to be doing agentic work, we'll need a very capable model, but the largest models are hard to run on a laptop. We think `mixtral 8x7b` is a good balance between power and resources, but `llama3` is another great option. You can run Mixtral by running\r\n\r\n```bash\r\nollama run mixtral:8x7b\r\n```\r\n\r\nThe first time you run, it will also automatically download and install the model for you, which can take a while.\r\n\r\n## Switch to local agent\r\n\r\nTo switch to Mixtral, you'll need to bring in the Ollama integration:\r\n\r\n```bash\r\npip install llama-index-llms-ollama\r\n```\r\n\r\nThen modify your dependencies to bring in Ollama instead of OpenAI:\r\n\r\n```python\r\nfrom llama_index.llms.ollama import Ollama\r\n```\r\n\r\nAnd finally initialize Mixtral as your LLM instead:\r\n\r\n```python\r\nllm = Ollama(model=\"mixtral:8x7b\", request_timeout=120.0)\r\n```\r\n\r\n## Ask the question again\r\n\r\n```python\r\nresponse = agent.chat(\"What is 20+(2*4)? Calculate step by step.\")\r\n```\r\n\r\nThe exact output looks different from OpenAI (it makes a mistake the first time it tries), but Mixtral gets the right answer:\r\n\r\n```\r\nThought: The current language of the user is: English. The user wants to calculate the value of 20+(2*4). I need to break down this task into subtasks and use appropriate tools to solve each subtask.\r\nAction: multiply\r\nAction Input: {'a': 2, 'b': 4}\r\nObservation: 8\r\nThought: The user has calculated the multiplication part of the expression, which is (2*4), and got 8 as a result. Now I need to add this value to 20 by using the 'add' tool.\r\nAction: add\r\nAction Input: {'a': 20, 'b': 8}\r\nObservation: 28\r\nThought: The user has calculated the sum of 20+(2*4) and got 28 as a result. Now I can answer without using any more tools.\r\nAnswer: The solution to the expression 20+(2*4) is 28.\r\nThe solution to the expression 20+(2*4) is 28.\r\n```\r\n\r\nCheck the [repo](https://github.com/run-llama/python-agents-tutorial/blob/main/2_local_agent.py) to see what this final code looks like.\r\n\r\nYou can now continue the rest of the tutorial with a local model if you prefer. We'll keep using OpenAI as we move on to [adding RAG to your agent](./rag_agent.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78c646e0-f29b-45e3-97ed-f0001c59ba9b": {"__data__": {"id_": "78c646e0-f29b-45e3-97ed-f0001c59ba9b", "embedding": null, "metadata": {"filename": "memory.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bdfbeb720c9784aac369b677c942a5120965a285", "node_type": "4", "metadata": {"filename": "memory.md", "author": "LlamaIndex"}, "hash": "b1cadfb5129d1625d40d7af922533173513aeed3518e5e985b05596abf418fb8", "class_name": "RelatedNodeInfo"}}, "text": "# Memory\n\nWe've now made several additions and subtractions to our code. To make it clear what we're using, you can see [the current code for our agent](https://github.com/run-llama/python-agents-tutorial/blob/main/5_memory.py) in the repo. It's using OpenAI for the LLM and LlamaParse to enhance parsing.\n\nWe've also added 3 questions in a row. Let's see how the agent handles them:\n\n```python\nresponse = agent.chat(\n    \"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\"\n)\n\nprint(response)\n\nresponse = agent.chat(\n    \"How much was allocated to a implement a means-tested dental care program in the 2023 Canadian federal budget?\"\n)\n\nprint(response)\n\nresponse = agent.chat(\n    \"How much was the total of those two allocations added together? Use a tool to answer any questions.\"\n)\n\nprint(response)\n```\n\nThis is demonstrating a powerful feature of agents in LlamaIndex: memory. Let's see what the output looks like:\n\n```\nStarted parsing the file under job_id cac11eca-45e0-4ea9-968a-25f1ac9b8f99\nThought: The current language of the user is English. I need to use a tool to help me answer the question.\nAction: canadian_budget_2023\nAction Input: {'input': 'How much was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?'}\nObservation: $20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: $20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\n$20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: canadian_budget_2023\nAction Input: {'input': 'How much was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget?'}\nObservation: $13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: $13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\n$13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: add\nAction Input: {'a': 20, 'b': 13}\nObservation: 33\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The total of the allocations for the tax credit to promote investment in green technologies and the means-tested dental care program in the 2023 Canadian federal budget is $33 billion.\nThe total of the allocations for the tax credit to promote investment in green technologies and the means-tested dental care program in the 2023 Canadian federal budget is $33 billion.\n```\n\nThe agent remembers that it already has the budget allocations from previous questions, and can answer a contextual question like \"add those two allocations together\" without needing to specify which allocations exactly. It even correctly uses the other addition tool to sum up the numbers.\n\nHaving demonstrated how memory helps, let's [add some more complex tools](./tools.md) to our agent.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdfb9cbf-ba3e-491e-a172-028b1ee06f2e": {"__data__": {"id_": "bdfb9cbf-ba3e-491e-a172-028b1ee06f2e", "embedding": null, "metadata": {"filename": "rag_agent.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06a667d42dac0856ec12be4bff99b2143826bd2f", "node_type": "4", "metadata": {"filename": "rag_agent.md", "author": "LlamaIndex"}, "hash": "d622313ed12018eda2e430bf8ac2b33a30166f8ebd38192549ac218cc573a1bf", "class_name": "RelatedNodeInfo"}}, "text": "# Adding RAG to an agent\n\nTo demonstrate using RAG engines as a tool in an agent, we're going to create a very simple RAG query engine. Our source data is going to be the [Wikipedia page about the 2023 Canadian federal budget](https://en.wikipedia.org/wiki/2023_Canadian_federal_budget) that we've [printed as a PDF](https://www.dropbox.com/scl/fi/rop435rax7mn91p3r8zj3/2023_canadian_budget.pdf?rlkey=z8j6sab5p6i54qa9tr39a43l7&dl=0).\n\n## Bring in new dependencies\n\nTo read the PDF and index it, we'll need a few new dependencies. They were installed along with the rest of LlamaIndex, so we just need to import them:\n\n```python\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n```\n\n## Add LLM to settings\n\nWe were previously passing the LLM directly, but now we need to use it in multiple places, so we'll add it to the global settings.\n\n```python\nSettings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n```\n\nPlace this line near the top of the file; you can delete the other `llm` assignment.\n\n## Load and index documents\n\nWe'll now do 3 things in quick succession: we'll load the PDF from a folder called \"data\", index and embed it using the `VectorStoreIndex`, and then create a query engine from that index:\n\n```python\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\n```\n\nWe can run a quick smoke-test to make sure the engine is working:\n\n```python\nresponse = query_engine.query(\n    \"What was the total amount of the 2023 Canadian federal budget?\"\n)\nprint(response)\n```\n\nThe response is fast:\n\n```\nThe total amount of the 2023 Canadian federal budget was $496.9 billion.\n```\n\n## Add a query engine tool\n\nThis requires one more import:\n\n```python\nfrom llama_index.core.tools import QueryEngineTool\n```\n\nNow we turn our query engine into a tool by supplying the appropriate metadata (for the python functions, this was being automatically extracted so we didn't need to add it):\n\n```python\nbudget_tool = QueryEngineTool.from_defaults(\n    query_engine,\n    name=\"canadian_budget_2023\",\n    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n)\n```\n\nWe modify our agent by adding this engine to our array of tools (we also remove the `llm` parameter, since it's now provided by settings):\n\n```python\nagent = ReActAgent.from_tools(\n    [multiply_tool, add_tool, budget_tool], verbose=True\n)\n```\n\n## Ask a question using multiple tools\n\nThis is kind of a silly question, we'll ask something more useful later:\n\n```python\nresponse = agent.chat(\n    \"What is the total amount of the 2023 Canadian federal budget multiplied by 3? Go step by step, using a tool to do any math.\"\n)\n\nprint(response)\n```\n\nWe get a perfect answer:\n\n```\nThought: The current language of the user is English. I need to use the tools to help me answer the question.\nAction: canadian_budget_2023\nAction Input: {'input': 'total'}\nObservation: $496.9 billion\nThought: I need to multiply the total amount of the 2023 Canadian federal budget by 3.\nAction: multiply\nAction Input: {'a': 496.9, 'b': 3}\nObservation: 1490.6999999999998\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The total amount of the 2023 Canadian federal budget multiplied by 3 is $1,490.70 billion.\nThe total amount of the 2023 Canadian federal budget multiplied by 3 is $1,490.70 billion.\n```\n\nAs usual, you can check the [repo](https://github.com/run-llama/python-agents-tutorial/blob/main/3_rag_agent.py) to see this code all together.\n\nExcellent! Your agent can now use any arbitrarily advanced query engine to help answer questions. You can also add as many different RAG engines as you need to consult different data sources. Next, we'll look at how we can answer more advanced questions [using LlamaParse](./llamaparse.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e790141d-9464-4384-a282-0810c1e86e2f": {"__data__": {"id_": "e790141d-9464-4384-a282-0810c1e86e2f", "embedding": null, "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48eeef24eaa808ab07adea1985c470b3591c81f1", "node_type": "4", "metadata": {"filename": "tools.md", "author": "LlamaIndex"}, "hash": "604a673d3c8f172172519f56683b3903dacbfcb66829c95733b1eae6d22484c2", "class_name": "RelatedNodeInfo"}}, "text": "# Adding other tools\n\nNow that you've built a capable agent, we hope you're excited about all it can do. The core of expanding agent capabilities is the tools available, and we have good news: [LlamaHub](https://llamahub.ai) from LlamaIndex has hundreds of integrations, including [dozens of existing agent tools](https://llamahub.ai/?tab=tools) that you can use right away. We'll show you how to use one of the existing tools, and also how to build and contribute your own.\n\n## Using an existing tool from LlamaHub\n\nFor our example, we're going to use the [Yahoo Finance tool](https://llamahub.ai/l/tools/llama-index-tools-yahoo-finance?from=tools) from LlamaHub. It provides a set of six agent tools that look up a variety of information about stock ticker symbols.\n\nFirst we need to install the tool:\n\n```bash\npip install llama-index-tools-yahoo-finance\n```\n\nThen we can set up our dependencies. This is exactly the same as our previous examples, except for the final import:\n\n```python\nfrom dotenv import load_dotenv\n\nload_dotenv()\nfrom llama_index.core.agent import ReActAgent\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.core import Settings\nfrom llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n```\n\nTo show how custom tools and LlamaHub tools can work together, we'll include the code from our previous examples the defines a \"multiple\" tool. We'll also take this opportunity to set up the LLM:\n\n```python\n# settings\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0)\n\n\n# function tools\ndef multiply(a: float, b: float) -> float:\n    \"\"\"Multiply two numbers and returns the product\"\"\"\n    return a * b\n\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n\ndef add(a: float, b: float) -> float:\n    \"\"\"Add two numbers and returns the sum\"\"\"\n    return a + b\n\n\nadd_tool = FunctionTool.from_defaults(fn=add)\n```\n\nNow we'll do the new step, which is to fetch the array of tools:\n\n```python\nfinance_tools = YahooFinanceToolSpec().to_tool_list()\n```\n\nThis is just a regular array, so we can use Python's `extend` method to add our own tools to the mix:\n\n```python\nfinance_tools.extend([multiply_tool, add_tool])\n```\n\nThen we set up the agent as usual, and ask a question:\n\n```python\nagent = ReActAgent.from_tools(finance_tools, verbose=True)\n\nresponse = agent.chat(\"What is the current price of NVDA?\")\n\nprint(response)\n```\n\nThe response is very wordy, so we've truncated it:\n\n```\nThought: The current language of the user is English. I need to use a tool to help me answer the question.\nAction: stock_basic_info\nAction Input: {'ticker': 'NVDA'}\nObservation: Info:\n{'address1': '2788 San Tomas Expressway'\n...\n'currentPrice': 135.58\n...}\nThought: I have obtained the current price of NVDA from the stock basic info.\nAnswer: The current price of NVDA (NVIDIA Corporation) is $135.58.\nThe current price of NVDA (NVIDIA Corporation) is $135.58.\n```\n\nPerfect! As you can see, using existing tools is a snap.\n\nAs always, you can check [the repo](https://github.com/run-llama/python-agents-tutorial/blob/main/6_tools.py) to see this code all in one place.\n\n## Building and contributing your own tools\n\nWe love open source contributions of new tools! You can see an example of [what the code of the Yahoo finance tool looks like](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-yahoo-finance/llama_index/tools/yahoo_finance/base.py):\n* A class that extends `BaseToolSpec`\n* A set of arbitrary Python functions\n* A `spec_functions` list that maps the functions to the tool's API\n\nOnce you've got a tool working, follow our [contributing guide](https://github.com/run-llama/llama_index/blob/main/CONTRIBUTING.md#2--contribute-a-pack-reader-tool-or-dataset-formerly-from-llama-hub) for instructions on correctly setting metadata and submitting a pull request.\n\nCongratulations! You've completed our guide to building agents with LlamaIndex. We can't wait to see what use-cases you build!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bd7db2a-3554-4ecb-883f-6afd072cd25a": {"__data__": {"id_": "7bd7db2a-3554-4ecb-883f-6afd072cd25a", "embedding": null, "metadata": {"filename": "deployment.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0b064d3453212b574464941c25b04b017509200", "node_type": "4", "metadata": {"filename": "deployment.md", "author": "LlamaIndex"}, "hash": "07155a6fb84850d7feb6ab817c83bdc87b30e8b02edddb3ff1ed2bbcdce24cd3", "class_name": "RelatedNodeInfo"}}, "text": "# Deployment\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 18, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b3d00d1-2498-4933-827f-faf5a9f48cdf": {"__data__": {"id_": "3b3d00d1-2498-4933-827f-faf5a9f48cdf", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "774bb04cb1c31b33aa85b57be0c5ae7a8ae18e77", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "702282cddbb2b102759804fc284b6fa1b4e99602ad6c12fcc108d50294943243", "class_name": "RelatedNodeInfo"}}, "text": "# Cost Analysis\n\n## Concept\n\nEach call to an LLM will cost some amount of money - for instance, OpenAI's gpt-3.5-turbo costs $0.002 / 1k tokens. The cost of building an index and querying depends on\n\n- the type of LLM used\n- the type of data structure used\n- parameters used during building\n- parameters used during querying\n\nThe cost of building and querying each index is a TODO in the reference documentation. In the meantime, we provide the following information:\n\n1. A high-level overview of the cost structure of the indices.\n2. A token predictor that you can use directly within LlamaIndex!\n\n### Overview of Cost Structure\n\n#### Indices with no LLM calls\n\nThe following indices don't require LLM calls at all during building (0 cost):\n\n- `SummaryIndex`\n- `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document\n- `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\n\n#### Indices with LLM calls\n\nThe following indices do require LLM calls during build time:\n\n- `TreeIndex` - use LLM to hierarchically summarize the text to build the tree\n- `KeywordTableIndex` - use LLM to extract keywords from each document\n\n### Query Time\n\nThere will always be >= 1 LLM call during query time, in order to synthesize the final answer.\nSome indices contain cost tradeoffs between index building and querying. `SummaryIndex`, for instance,\nis free to build, but running a query over a summary index (without filtering or embedding lookups), will\ncall the LLM {math}`N` times.\n\nHere are some notes regarding each of the indices:\n\n- `SummaryIndex`: by default requires {math}`N` LLM calls, where N is the number of nodes.\n- `TreeIndex`: by default requires {math}`\\log (N)` LLM calls, where N is the number of leaf nodes.\n  - Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node.\n- `KeywordTableIndex`: by default requires an LLM call to extract query keywords.\n  - Can do `index.as_retriever(retriever_mode=\"simple\")` or `index.as_retriever(retriever_mode=\"rake\")` to also use regex/RAKE keyword extractors on your query text.\n- `VectorStoreIndex`: by default, requires one LLM call per query. If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.\n\n## Usage Pattern\n\nLlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls.\nThis allows you to estimate your costs during 1) index construction, and 2) index querying, before\nany respective LLM calls are made.\n\nTokens are counted using the `TokenCountingHandler` callback. See the [example notebook](../../../examples/callbacks/TokenCountingHandler.ipynb) for details on the setup.\n\n### Using MockLLM\n\nTo predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \"worst case\" prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.\n\n```python\nfrom llama_index.core.llms import MockLLM\nfrom llama_index.core import Settings\n\n# use a mock llm globally\nSettings.llm = MockLLM(max_tokens=256)\n```\n\nYou can then use this predictor during both index construction and querying.\n\n### Using MockEmbedding\n\nYou may also predict the token usage of embedding calls with `MockEmbedding`.\n\n```python\nfrom llama_index.core import MockEmbedding\nfrom llama_index.core import Settings\n\n# use a mock embedding globally\nSettings.embed_model = MockEmbedding(embed_dim=1536)\n```\n\n## Usage Pattern\n\nRead about the [full usage pattern](./usage_pattern.md) for more details!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d8b94bf-6cb0-454f-a75e-f4dfb5a6d2d8": {"__data__": {"id_": "7d8b94bf-6cb0-454f-a75e-f4dfb5a6d2d8", "embedding": null, "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b22daad3974506b6a282e44c164634cec28a52e", "node_type": "4", "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}, "hash": "2bd48e72cdae5302e3c03261784d60d88e81e24db13d69e74f722c9017768d16", "class_name": "RelatedNodeInfo"}}, "text": "# Usage Pattern\n\n## Estimating LLM and Embedding Token Counts\n\nIn order to measure LLM and Embedding token counts, you'll need to\n\n1. Setup `MockLLM` and `MockEmbedding` objects\n\n```python\nfrom llama_index.core.llms import MockLLM\nfrom llama_index.core import MockEmbedding\n\nllm = MockLLM(max_tokens=256)\nembed_model = MockEmbedding(embed_dim=1536)\n```\n\n2. Setup the `TokenCountingCallback` handler\n\n```python\nimport tiktoken\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n```\n\n3. Add them to the global `Settings`\n\n```python\nfrom llama_index.core import Settings\n\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.callback_manager = callback_manager\n```\n\n4. Construct an Index\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\n    \"./docs/examples/data/paul_graham\"\n).load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n5. Measure the counts!\n\n```python\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n\n# reset counts\ntoken_counter.reset_counts()\n```\n\n6. Run a query, measure again\n\n```python\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query\")\n\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n```", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61188a5f-28ec-4b41-86f4-a46e41d33106": {"__data__": {"id_": "61188a5f-28ec-4b41-86f4-a46e41d33106", "embedding": null, "metadata": {"filename": "evaluating.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2de1b4ec5dfa08e732d8f4473e58d35c93dbc3e7", "node_type": "4", "metadata": {"filename": "evaluating.md", "author": "LlamaIndex"}, "hash": "61d018273dde5588188573ab6fab46c497e994bb87f84ff4056eef33d8206f96", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluating\n\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\n\nLlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality. You can learn more about how evaluation works in LlamaIndex in our [module guides](../../module_guides/evaluating/index.md).\n\n## Response Evaluation\n\nDoes the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines? Here's a simple example that evaluates a single response for Faithfulness, i.e. whether the response is aligned to the context, such as being free from hallucinations:\n\n```python\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.evaluation import FaithfulnessEvaluator\n\n# create llm\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\n\n# build index\n...\nvector_index = VectorStoreIndex(...)\n\n# define evaluator\nevaluator = FaithfulnessEvaluator(llm=llm)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\n    \"What battles took place in New York City in the American Revolution?\"\n)\neval_result = evaluator.evaluate_response(response=response)\nprint(str(eval_result.passing))\n```\n\nThe response contains both the response and the source from which the response was generated; the evaluator compares them and determines if the response is faithful to the source.\n\nYou can learn more in our module guides about [response evaluation](../../module_guides/evaluating/usage_pattern.md).\n\n## Retrieval Evaluation\n\nAre the retrieved sources relevant to the query? This is a simple example that evaluates a single retrieval:\n\n```python\nfrom llama_index.core.evaluation import RetrieverEvaluator\n\n# define retriever somewhere (e.g. from index)\n# retriever = index.as_retriever(similarity_top_k=2)\nretriever = ...\n\nretriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [\"mrr\", \"hit_rate\"], retriever=retriever\n)\n\nretriever_evaluator.evaluate(\n    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n)\n```\n\nThis compares what was retrieved for the query to a set of nodes that were expected to be retrieved.\n\nIn reality you would want to evaluate a whole batch of retrievals; you can learn how do this in our module guide on [retrieval evaluation](../../module_guides/evaluating/usage_pattern_retrieval.md).\n\n## Related concepts\n\nYou may be interested in [analyzing the cost of your application](cost_analysis/index.md) if you are making calls to a hosted, remote LLM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c10ea44-83b4-44be-9bf9-79ff1cf414c2": {"__data__": {"id_": "1c10ea44-83b4-44be-9bf9-79ff1cf414c2", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e02b57a874d53519207d53f185ae648374b1f85", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "18f7d126f0996d6618036483893057af1f194b7b84bf679b54873ba0dd000fa8", "class_name": "RelatedNodeInfo"}}, "text": "# Building an LLM application\n\nWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\n\n## Key steps in building an LLM application\n\n!!! tip\n    If you've already read our [high-level concepts](../getting_started/concepts.md) page you'll recognize several of these steps.\n\nThis tutorial has two main parts: **Building a RAG pipeline** and **Building an agent**, with some smaller sections before and after. Here's what to expect:\n\n- **[Using LLMs](./using_llms/using_llms.md)**: hit the ground running by getting started working with LLMs. We'll show you how to use any of our [dozens of supported LLMs](../module_guides/models/llms/modules/), whether via remote API calls or running locally on your machine.\n\n- **Building a RAG pipeline**: Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes:\n\n    - **[Loading & Ingestion](./loading/loading.md)**: Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at [LlamaHub](https://llamahub.ai/).\n\n    - **[Indexing and Embedding](./indexing/indexing.md)**: Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\n\n    - **[Storing](./storing/storing.md)**: You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a `Vector Store` (see below). You can also store your indexes, metadata and more.\n\n    - **[Querying](./querying/querying.md)**: Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\n\n- **Building an agent**: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can be RAG engines such as you learned how to build in the previous section, or any arbitrary code. This tutorial includes:\n\n    - **[Building a basic agent](./agent/basic_agent.md)**: We show you how to build a simple agent that can interact with the world via a set of tools.\n\n    - **[Using local models with agents](./agent/local_models.md)**: Agents can be built to use local models, which can be important for performance or privacy reasons.\n\n    - **[Adding RAG to an agent](./agent/rag_agent.md)**: The RAG pipelines you built in the previous tutorial can be used as a tool by an agent, giving your agent powerful information-retrieval capabilities.\n\n    - **[Adding other tools](./agent/tools.md)**: Let's add more sophisticated tools to your agent, such as API integrations.\n\n- **[Putting it all together](./putting_it_all_together/index.md)**: whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\n\n- **[Tracing and debugging](./tracing_and_debugging/tracing_and_debugging.md)**: also called **observability**, it's especially important with LLM applications to be able to look into the inner workings of what's going on to help you debug problems and spot places to improve.\n\n- **[Evaluating](./evaluating/evaluating.md)**: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.\n\n## Let's get started!\n\nReady to dive in? Head to [using LLMs](./using_llms/using_llms.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57f16fd3-792e-4f7c-ae44-c5933f25f59d": {"__data__": {"id_": "57f16fd3-792e-4f7c-ae44-c5933f25f59d", "embedding": null, "metadata": {"filename": "indexing.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4a022fa95ff87a103eb867d929db544435c103", "node_type": "4", "metadata": {"filename": "indexing.md", "author": "LlamaIndex"}, "hash": "410ff0cd4829dd5a494ebc71cc5cd8cc0a4de0b142f7fa06ea34b93d37c3cb55", "class_name": "RelatedNodeInfo"}}, "text": "# Indexing\n\nWith your data loaded, you now have a list of Document objects (or a list of Nodes). It's time to build an `Index` over these objects so you can start querying them.\n\n## What is an Index?\n\nIn LlamaIndex terms, an `Index` is a data structure composed of `Document` objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n\nLlamaIndex offers several different index types. We'll cover the two most common here.\n\n## Vector Store Index\n\nA `VectorStoreIndex` is by far the most frequent type of Index you'll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates `vector embeddings` of the text of every node, ready to be queried by an LLM.\n\n### What is an embedding?\n\n`Vector embeddings` are central to how LLM applications function.\n\nA `vector embedding`, often just called an embedding, is a **numerical representation of the semantics, or meaning of your text**. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\n\nThis mathematical relationship enables **semantic search**, where a user provides query terms and LlamaIndex can locate text that is related to the **meaning of the query terms** rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\n\nThere are [many types of embeddings](../../module_guides/models/embeddings.md), and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses `text-embedding-ada-002`, which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\n\n### Vector Store Index embeds your documents\n\nVector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it \"embeds your text\". If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\n\nWhen you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\n\n### Top K Retrieval\n\nOnce the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as `k`, so the parameter controlling how many embeddings to return is known as `top_k`. This whole type of search is often referred to as \"top-k semantic retrieval\" for this reason.\n\nTop-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the [querying](../querying/querying.md) section.\n\n### Using Vector Store Index\n\nTo use the Vector Store Index, pass it the list of Documents you created during the loading stage:\n\n```python\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n!!! tip\n    `from_documents` also takes an optional argument `show_progress`. Set it to `True` to display a progress bar during index construction.\n\nYou can also choose to build an index over a list of Node objects directly:\n\n```python\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex(nodes)\n```\n\nWith your text indexed, it is now technically ready for [querying](../querying/querying.md)! However, embedding all your text can be time-consuming and, if you are using a hosted LLM, it can also be expensive. To save time and money you will want to [store your embeddings](../storing/storing.md) first.\n\n## Summary Index\n\nA Summary Index is a simpler form of Index best suited to queries where, as the name suggests, you are trying to generate a summary of the text in your Documents. It simply stores all of the Documents and returns all of them to your query engine.\n\n## Further Reading\n\nIf your data is a set of interconnected concepts (in computer science terms, a \"graph\") then you may be interested in our [knowledge graph index](../../examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3530afe5-a11c-47c6-b590-1cc33ab36cb0": {"__data__": {"id_": "3530afe5-a11c-47c6-b590-1cc33ab36cb0", "embedding": null, "metadata": {"filename": "llamahub.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd0b8ccc8d3b9c8780fcc91930ab64f9a69834e7", "node_type": "4", "metadata": {"filename": "llamahub.md", "author": "LlamaIndex"}, "hash": "9c4abf9b71b1a6e7edd9538c83be3439233cf471c3ade1058afad124b44b4d8c", "class_name": "RelatedNodeInfo"}}, "text": "# LlamaHub\n\nOur data connectors are offered through [LlamaHub](https://llamahub.ai/) \ud83e\udd99.\nLlamaHub contains a registry of open-source data connectors that you can easily plug into any LlamaIndex application (+ Agent Tools, and Llama Packs).\n\n![](../../_static/data_connectors/llamahub.png)\n\n## Usage Pattern\n\nGet started with:\n\n```python\nfrom llama_index.core import download_loader\n\nfrom llama_index.readers.google import GoogleDocsReader\n\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=[...])\n```\n\n## Built-in connector: SimpleDirectoryReader\n\n`SimpleDirectoryReader`. Can support parsing a wide range of file types including `.md`, `.pdf`, `.jpg`, `.png`, `.docx`, as well as audio and video types. It is available directly as part of LlamaIndex:\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n```\n\n## Available connectors\n\nBrowse [LlamaHub](https://llamahub.ai/) directly to see the hundreds of connectors available, including:\n\n- [Notion](https://developers.notion.com/) (`NotionPageReader`)\n- [Google Docs](https://developers.google.com/docs/api) (`GoogleDocsReader`)\n- [Slack](https://api.slack.com/) (`SlackReader`)\n- [Discord](https://discord.com/developers/docs/intro) (`DiscordReader`)\n- [Apify Actors](https://llamahub.ai/l/apify-actor) (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3e48f27-b046-472a-a792-390a2efeecf8": {"__data__": {"id_": "c3e48f27-b046-472a-a792-390a2efeecf8", "embedding": null, "metadata": {"filename": "loading.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0a21f7c3d8d525947d227d1d691e47fb9f4502e", "node_type": "4", "metadata": {"filename": "loading.md", "author": "LlamaIndex"}, "hash": "5a14a0c08cedb7c5ab6b0b805f59e07b830643a991f60dbfa79657ba0aeb0b8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cd9fb45-8eac-4efa-80df-93577844949c", "node_type": "1", "metadata": {}, "hash": "1d74dadcc5d187ca415d923e39869acb8628bcace8fe653a1d8b37fbed0de15d", "class_name": "RelatedNodeInfo"}}, "text": "# Loading Data (Ingestion)\n\nBefore your chosen LLM can act on your data, you first need to process the data and load it. This has parallels to data cleaning/feature engineering pipelines in the ML world, or ETL pipelines in the traditional data setting.\n\nThis ingestion pipeline typically consists of three main stages:\n\n1. Load the data\n2. Transform the data\n3. Index and store the data\n\nWe cover indexing/storage in [future](../indexing/indexing.md) [sections](../storing/storing.md). In this guide we'll mostly talk about loaders and transformations.\n\n## Loaders\n\nBefore your chosen LLM can act on your data you need to load it. The way LlamaIndex does this is via data connectors, also called `Reader`. Data connectors ingest data from different data sources and format the data into `Document` objects. A `Document` is a collection of data (currently text, and in future, images and audio) and metadata about that data.\n\n### Loading using SimpleDirectoryReader\n\nThe easiest reader to use is our SimpleDirectoryReader, which creates documents out of every file in a given directory. It is built in to LlamaIndex and can read a variety of formats including Markdown, PDFs, Word documents, PowerPoint decks, images, audio and video.\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n```\n\n### Using Readers from LlamaHub\n\nBecause there are so many possible places to get data, they are not all built-in. Instead, you download them from our registry of data connectors, [LlamaHub](llamahub.md).\n\nIn this example LlamaIndex downloads and installs the connector called [DatabaseReader](https://llamahub.ai/l/readers/llama-index-readers-database), which runs a query against a SQL database and returns every row of the results as a `Document`:\n\n```python\nfrom llama_index.core import download_loader\n\nfrom llama_index.readers.database import DatabaseReader\n\nreader = DatabaseReader(\n    scheme=os.getenv(\"DB_SCHEME\"),\n    host=os.getenv(\"DB_HOST\"),\n    port=os.getenv(\"DB_PORT\"),\n    user=os.getenv(\"DB_USER\"),\n    password=os.getenv(\"DB_PASS\"),\n    dbname=os.getenv(\"DB_NAME\"),\n)\n\nquery = \"SELECT * FROM users\"\ndocuments = reader.load_data(query=query)\n```\n\nThere are hundreds of connectors to use on [LlamaHub](https://llamahub.ai)!\n\n### Creating Documents directly\n\nInstead of using a loader, you can also use a Document directly.\n\n```python\nfrom llama_index.core import Document\n\ndoc = Document(text=\"text\")\n```\n\n## Transformations\n\nAfter the data is loaded, you then need to process and transform your data before putting it into a storage system. These transformations include chunking, extracting metadata, and embedding each chunk. This is necessary to make sure that the data can be retrieved, and used optimally by the LLM.\n\nTransformation input/outputs are `Node` objects (a `Document` is a subclass of a `Node`). Transformations can also be stacked and reordered.\n\nWe have both a high-level and lower-level API for transforming documents.\n\n### High-Level Transformation API\n\nIndexes have a `.from_documents()` method which accepts an array of Document objects and will correctly parse and chunk them up. However, sometimes you will want greater control over how your documents are split up.\n\n```python\nfrom llama_index.core import VectorStoreIndex\n\nvector_index = VectorStoreIndex.from_documents(documents)\nvector_index.as_query_engine()\n```\n\nUnder the hood, this splits your Document into Node objects, which are similar to Documents (they contain text and metadata) but have a relationship to their parent Document.\n\nIf you want to customize core components, like the text splitter, through this abstraction you can pass in a custom `transformations` list or apply to the global `Settings`:\n\n```python\nfrom llama_index.core.node_parser import SentenceSplitter\n\ntext_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n\n# global\nfrom llama_index.core import Settings\n\nSettings.text_splitter = text_splitter\n\n# per-index\nindex = VectorStoreIndex.from_documents(\n    documents, transformations=[text_splitter]\n)\n```\n\n### Lower-Level Transformation API\n\nYou can also define these steps explicitly.\n\nYou can do this by either using our transformation modules (text splitters, metadata extractors, etc.) as standalone components, or compose them in our declarative [Transformation Pipeline interface](../../module_guides/loading/ingestion_pipeline/index.md).\n\nLet's walk through the steps below.\n\n#### Splitting Your Documents into Nodes\n\nA key step to process your documents is to split them into \"chunks\"/Node objects.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cd9fb45-8eac-4efa-80df-93577844949c": {"__data__": {"id_": "7cd9fb45-8eac-4efa-80df-93577844949c", "embedding": null, "metadata": {"filename": "loading.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0a21f7c3d8d525947d227d1d691e47fb9f4502e", "node_type": "4", "metadata": {"filename": "loading.md", "author": "LlamaIndex"}, "hash": "5a14a0c08cedb7c5ab6b0b805f59e07b830643a991f60dbfa79657ba0aeb0b8c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3e48f27-b046-472a-a792-390a2efeecf8", "node_type": "1", "metadata": {"filename": "loading.md", "author": "LlamaIndex"}, "hash": "5c89339cd81921c7e96aeda8ba68d9ac5d3afba36c5b7eebf426bd7c5f63a063", "class_name": "RelatedNodeInfo"}}, "text": "#### Splitting Your Documents into Nodes\n\nA key step to process your documents is to split them into \"chunks\"/Node objects. The key idea is to process your data into bite-sized pieces that can be retrieved / fed to the LLM.\n\nLlamaIndex has support for a wide range of [text splitters](../../module_guides/loading/node_parsers/modules.md), ranging from paragraph/sentence/token based splitters to file-based splitters like HTML, JSON.\n\nThese can be [used on their own or as part of an ingestion pipeline](../../module_guides/loading/node_parsers/index.md).\n\n```python\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import TokenTextSplitter\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\npipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n\nnodes = pipeline.run(documents=documents)\n```\n\n### Adding Metadata\n\nYou can also choose to add metadata to your documents and nodes. This can be done either manually or with [automatic metadata extractors](../../module_guides/loading/documents_and_nodes/usage_metadata_extractor.md).\n\nHere are guides on 1) [how to customize Documents](../../module_guides/loading/documents_and_nodes/usage_documents.md), and 2) [how to customize Nodes](../../module_guides/loading/documents_and_nodes/usage_nodes.md).\n\n```python\ndocument = Document(\n    text=\"text\",\n    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n)\n```\n\n### Adding Embeddings\n\nTo insert a node into a vector index, it should have an embedding. See our [ingestion pipeline](../../module_guides/loading/ingestion_pipeline/index.md) or our [embeddings guide](../../module_guides/models/embeddings.md) for more details.\n\n### Creating and passing Nodes directly\n\nIf you want to, you can create nodes directly and pass a list of Nodes directly to an indexer:\n\n```python\nfrom llama_index.core.schema import TextNode\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n\nindex = VectorStoreIndex([node1, node2])\n```", "mimetype": "text/plain", "start_char_idx": 4481, "end_char_idx": 6588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6eb34d70-5ecb-45ae-b991-0d3680d18164": {"__data__": {"id_": "6eb34d70-5ecb-45ae-b991-0d3680d18164", "embedding": null, "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "729325bb7f1845950d83b12f6e9fc8dfb284ab7b", "node_type": "4", "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "hash": "e9ab5182e1438ab1e0069d96c2559fae81c5ad7c53129a1a5cb4e73d72b96702", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a9f3d9a-cb31-4f4e-9701-8f574e686bdf", "node_type": "1", "metadata": {}, "hash": "b95d36ebfef250bc5d67582bdb2b039c78cbdf56d7efc4eb4515d1ed032bf33f", "class_name": "RelatedNodeInfo"}}, "text": "# Agents\n\nPutting together an agent in LlamaIndex can be done by defining a set of tools and providing them to our ReActAgent implementation. We're using it here with OpenAI, but it can be used with any sufficiently capable LLM:\n\n```python\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.agent import ReActAgent\n\n\n# define sample Tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers and returns the result integer\"\"\"\n    return a * b", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a9f3d9a-cb31-4f4e-9701-8f574e686bdf": {"__data__": {"id_": "4a9f3d9a-cb31-4f4e-9701-8f574e686bdf", "embedding": null, "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "729325bb7f1845950d83b12f6e9fc8dfb284ab7b", "node_type": "4", "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "hash": "e9ab5182e1438ab1e0069d96c2559fae81c5ad7c53129a1a5cb4e73d72b96702", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6eb34d70-5ecb-45ae-b991-0d3680d18164", "node_type": "1", "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "hash": "a5251e957d765821e442c4aee4c180969b49c8a3d2b22c971fb844ee640bc42b", "class_name": "RelatedNodeInfo"}}, "text": "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n# initialize ReAct agent\nagent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\n```\n\nThese tools can be Python functions as shown above, or they can be LlamaIndex query engines:\n\n```python\nfrom llama_index.core.tools import QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sql_agent,\n        metadata=ToolMetadata(\n            name=\"sql_agent\", description=\"Agent that can execute SQL queries.\"\n        ),\n    ),\n]\n\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n```\n\nYou can learn more in our [Agent Module Guide](../../module_guides/deploying/agents/index.md).\n\n## Native OpenAIAgent\n\nWe have an `OpenAIAgent` implementation built on the [OpenAI API for function calling](https://openai.com/blog/function-calling-and-other-api-updates) that allows you to rapidly build agents:\n\n- [OpenAIAgent](../../examples/agent/openai_agent.ipynb)\n- [OpenAIAgent with Query Engine Tools](../../examples/agent/openai_agent_with_query_engine.ipynb)\n- [OpenAIAgent Query Planning](../../examples/agent/openai_agent_query_plan.ipynb)\n- [OpenAI Assistant](../../examples/agent/openai_assistant_agent.ipynb)\n- [OpenAI Assistant Cookbook](../../examples/agent/openai_assistant_query_cookbook.ipynb)\n- [Forced Function Calling](../../examples/agent/openai_forced_function_call.ipynb)\n- [Parallel Function Calling](../../examples/agent/openai_agent_parallel_function_calling.ipynb)\n- [Context Retrieval](../../examples/agent/openai_agent_context_retrieval.ipynb)\n\n## Agentic Components within LlamaIndex\n\nLlamaIndex provides core modules capable of automated reasoning for different use cases over your data which makes them essentially Agents. Some of these core modules are shown below along with example tutorials.\n\n**SubQuestionQueryEngine for Multi Document Analysis**\n\n- [Sub Question Query Engine (Intro)](../../examples/query_engine/sub_question_query_engine.ipynb)\n- [10Q Analysis (Uber)](../../examples/usecases/10q_sub_question.ipynb)\n- [10K Analysis (Uber and Lyft)](../../examples/usecases/10k_sub_question.ipynb)\n\n**Query Transformations**\n\n- [How-To](../../optimizing/advanced_retrieval/query_transformations.md)\n- [Multi-Step Query Decomposition](../../examples/query_transformations/HyDEQueryTransformDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\n\n**Routing**\n\n- [Usage](../../module_guides/querying/router/index.md)\n- [Router Query Engine Guide](../../examples/query_engine/RouterQueryEngine.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs../../examples/query_engine/RouterQueryEngine.ipynb))\n\n**LLM Reranking**\n\n- [Second Stage Processing How-To](../../module_guides/querying/node_postprocessors/index.md)\n- [LLM Reranking Guide (Great Gatsby)](../../examples/node_postprocessor/LLMReranker-Gatsby.ipynb)\n\n**Chat Engines**\n\n- [Chat Engines How-To](../../module_guides/deploying/chat_engines/index.md)\n\n## Using LlamaIndex as as Tool within an Agent Framework\n\nLlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\n\n### LangChain\n\nWe have deep integrations with LangChain.\nLlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!\n\n**Resources**\n\n- [Building a Chatbot Tutorial](chatbots/building_a_chatbot.md)\n- [OnDemandLoaderTool Tutorial](../../examples/tools/OnDemandLoaderTool.ipynb)\n\n### ChatGPT\n\nLlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).\n\n**Resources**\n\n- [LlamaIndex ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin#llamaindex)", "mimetype": "text/plain", "start_char_idx": 519, "end_char_idx": 4485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "347c8135-ffbe-4ca8-b8ff-f9bbff31eb6a": {"__data__": {"id_": "347c8135-ffbe-4ca8-b8ff-f9bbff31eb6a", "embedding": null, "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded", "node_type": "4", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "a437a2d8efb83626fca8f320f41c1d3f6862e351df90e386c213eb1606602644", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3a10881-f73a-478d-9993-9cc186892259", "node_type": "1", "metadata": {}, "hash": "d8f2ebf07d3d2a86bd1b73e910b339988efc4bfb77d8b0d4644baeae24968d66", "class_name": "RelatedNodeInfo"}}, "text": "# A Guide to Building a Full-Stack Web App with LLamaIndex\n\nLlamaIndex is a python library, which means that integrating it with a full-stack web application will be a little different than what you might be used to.\n\nThis guide seeks to walk through the steps needed to create a basic API service written in python, and how this interacts with a TypeScript+React frontend.\n\nAll code examples here are available from the [llama_index_starter_pack](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react) in the flask_react folder.\n\nThe main technologies used in this guide are as follows:\n\n- python3.11\n- llama_index\n- flask\n- typescript\n- react\n\n## Flask Backend\n\nFor this guide, our backend will use a [Flask](https://flask.palletsprojects.com/en/2.2.x/) API server to communicate with our frontend code. If you prefer, you can also easily translate this to a [FastAPI](https://fastapi.tiangolo.com/) server, or any other python server library of your choice.\n\nSetting up a server using Flask is easy. You import the package, create the app object, and then create your endpoints. Let's create a basic skeleton for the server first:\n\n```python\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5601)\n```\n\n_flask_demo.py_\n\nIf you run this file (`python flask_demo.py`), it will launch a server on port 5601. If you visit `http://localhost:5601/`, you will see the \"Hello World!\" text rendered in your browser. Nice!\n\nThe next step is deciding what functions we want to include in our server, and to start using LlamaIndex.\n\nTo keep things simple, the most basic operation we can provide is querying an existing index. Using the [paul graham essay](https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt) from LlamaIndex, create a documents folder and download+place the essay text file inside of it.\n\n### Basic Flask - Handling User Index Queries\n\nNow, let's write some code to initialize our index:\n\n```python\nimport os\nfrom llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    StorageContext,\n    load_index_from_storage,\n)\n\n# NOTE: for local testing only, do NOT deploy with your key hardcoded\nos.environ[\"OPENAI_API_KEY\"] = \"your key here\"\n\nindex = None\n\n\ndef initialize_index():\n    global index\n    storage_context = StorageContext.from_defaults()\n    index_dir = \"./.index\"\n    if os.path.exists(index_dir):\n        index = load_index_from_storage(storage_context)\n    else:\n        documents = SimpleDirectoryReader(\"./documents\").load_data()\n        index = VectorStoreIndex.from_documents(\n            documents, storage_context=storage_context\n        )\n        storage_context.persist(index_dir)\n```\n\nThis function will initialize our index. If we call this just before starting the flask server in the `main` function, then our index will be ready for user queries!\n\nOur query endpoint will accept `GET` requests with the query text as a parameter. Here's what the full endpoint function will look like:\n\n```python\nfrom flask import request", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3a10881-f73a-478d-9993-9cc186892259": {"__data__": {"id_": "f3a10881-f73a-478d-9993-9cc186892259", "embedding": null, "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded", "node_type": "4", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "a437a2d8efb83626fca8f320f41c1d3f6862e351df90e386c213eb1606602644", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "347c8135-ffbe-4ca8-b8ff-f9bbff31eb6a", "node_type": "1", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "8f4b46f6bdad3be107600b9684698ca2ee2249de0a88b6cf6e5dafc3d012d399", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbc4eba6-cb60-416b-9d7e-e1afbe8512ef", "node_type": "1", "metadata": {}, "hash": "8b20a64c161a47f003f9c2e8a802875ffdf6e16c0f91dc1f91aeb964ae794bb7", "class_name": "RelatedNodeInfo"}}, "text": "@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n    global index\n    query_text = request.args.get(\"text\", None)\n    if query_text is None:\n        return (\n            \"No text found, please include a ?text=blah parameter in the URL\",\n            400,\n        )\n    query_engine = index.as_query_engine()\n    response = query_engine.query(query_text)\n    return str(response), 200\n```\n\nNow, we've introduced a few new concepts to our server:\n\n- a new `/query` endpoint, defined by the function decorator\n- a new import from flask, `request`, which is used to get parameters from the request\n- if the `text` parameter is missing, then we return an error message and an appropriate HTML response code\n- otherwise, we query the index, and return the response as a string\n\nA full query example that you can test in your browser might look something like this: `http://localhost:5601/query?text=what did the author do growing up` (once you press enter, the browser will convert the spaces into \"%20\" characters).\n\nThings are looking pretty good! We now have a functional API. Using your own documents, you can easily provide an interface for any application to call the flask API and get answers to queries.\n\n### Advanced Flask - Handling User Document Uploads\n\nThings are looking pretty cool, but how can we take this a step further? What if we want to allow users to build their own indexes by uploading their own documents? Have no fear, Flask can handle it all :muscle:.\n\nTo let users upload documents, we have to take some extra precautions. Instead of querying an existing index, the index will become **mutable**. If you have many users adding to the same index, we need to think about how to handle concurrency. Our Flask server is threaded, which means multiple users can ping the server with requests which will be handled at the same time.\n\nOne option might be to create an index for each user or group, and store and fetch things from S3. But for this example, we will assume there is one locally stored index that users are interacting with.\n\nTo handle concurrent uploads and ensure sequential inserts into the index, we can use the `BaseManager` python package to provide sequential access to the index using a separate server and locks. This sounds scary, but it's not so bad! We will just move all our index operations (initializing, querying, inserting) into the `BaseManager` \"index_server\", which will be called from our Flask server.\n\nHere's a basic example of what our `index_server.py` will look like after we've moved our code:\n\n```python\nimport os\nfrom multiprocessing import Lock\nfrom multiprocessing.managers import BaseManager\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n\n# NOTE: for local testing only, do NOT deploy with your key hardcoded\nos.environ[\"OPENAI_API_KEY\"] = \"your key here\"\n\nindex = None\nlock = Lock()\n\n\ndef initialize_index():\n    global index\n\n    with lock:\n        # same as before ...\n        pass\n\n\ndef query_index(query_text):\n    global index\n    query_engine = index.as_query_engine()\n    response = query_engine.query(query_text)\n    return str(response)\n\n\nif __name__ == \"__main__\":\n    # init the global index\n    print(\"initializing index...\")\n    initialize_index()\n\n    # setup server\n    # NOTE: you might want to handle the password in a less hardcoded way\n    manager = BaseManager((\"\", 5602), b\"password\")\n    manager.register(\"query_index\", query_index)\n    server = manager.get_server()\n\n    print(\"starting server...\")\n    server.serve_forever()\n```\n\n_index_server.py_\n\nSo, we've moved our functions, introduced the `Lock` object which ensures sequential access to the global index, registered our single function in the server, and started the server on port 5602 with the password `password`.\n\nThen, we can adjust our flask code as follows:\n\n```python\nfrom multiprocessing.managers import BaseManager\nfrom flask import Flask, request\n\n# initialize manager connection\n# NOTE: you might want to handle the password in a less hardcoded way\nmanager = BaseManager((\"\", 5602), b\"password\")\nmanager.register(\"query_index\")\nmanager.connect()\n\n\n@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n    global index\n    query_text = request.args.get(\"text\", None)\n    if query_text is None:\n        return (\n            \"No text found, please include a ?text=blah parameter in the URL\",\n            400,\n        )\n    response = manager.query_index(query_text)._getvalue()\n    return str(response), 200\n\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"", "mimetype": "text/plain", "start_char_idx": 3175, "end_char_idx": 7733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbc4eba6-cb60-416b-9d7e-e1afbe8512ef": {"__data__": {"id_": "bbc4eba6-cb60-416b-9d7e-e1afbe8512ef", "embedding": null, "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded", "node_type": "4", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "a437a2d8efb83626fca8f320f41c1d3f6862e351df90e386c213eb1606602644", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3a10881-f73a-478d-9993-9cc186892259", "node_type": "1", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "cdd9ee504b28cb7523bfaf2c8f77f40c224761dc73da0e10596fe283033014bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd914d53-d2ce-4a99-b12e-cd80392ec556", "node_type": "1", "metadata": {}, "hash": "014c33d7d7d724c2b9ea7ad8d09578acfa3668e532681afaabf137b38a455700", "class_name": "RelatedNodeInfo"}}, "text": "@app.route(\"/\")\ndef home():\n    return \"Hello World!\"\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5601)\n```\n\n_flask_demo.py_\n\nThe two main changes are connecting to our existing `BaseManager` server and registering the functions, as well as calling the function through the manager in the `/query` endpoint.\n\nOne special thing to note is that `BaseManager` servers don't return objects quite as we expect. To resolve the return value into it's original object, we call the `_getvalue()` function.\n\nIf we allow users to upload their own documents, we should probably remove the Paul Graham essay from the documents folder, so let's do that first. Then, let's add an endpoint to upload files! First, let's define our Flask endpoint function:\n\n```python\n...\nmanager.register(\"insert_into_index\")\n...\n\n\n@app.route(\"/uploadFile\", methods=[\"POST\"])\ndef upload_file():\n    global manager\n    if \"file\" not in request.files:\n        return \"Please send a POST request with a file\", 400\n\n    filepath = None\n    try:\n        uploaded_file = request.files[\"file\"]\n        filename = secure_filename(uploaded_file.filename)\n        filepath = os.path.join(\"documents\", os.path.basename(filename))\n        uploaded_file.save(filepath)\n\n        if request.form.get(\"filename_as_doc_id\", None) is not None:\n            manager.insert_into_index(filepath, doc_id=filename)\n        else:\n            manager.insert_into_index(filepath)\n    except Exception as e:\n        # cleanup temp file\n        if filepath is not None and os.path.exists(filepath):\n            os.remove(filepath)\n        return \"Error: {}\".format(str(e)), 500\n\n    # cleanup temp file\n    if filepath is not None and os.path.exists(filepath):\n        os.remove(filepath)\n\n    return \"File inserted!\", 200\n```\n\nNot too bad! You will notice that we write the file to disk. We could skip this if we only accept basic file formats like `txt` files, but written to disk we can take advantage of LlamaIndex's `SimpleDirectoryReader` to take care of a bunch of more complex file formats. Optionally, we also use a second `POST` argument to either use the filename as a doc_id or let LlamaIndex generate one for us. This will make more sense once we implement the frontend.\n\nWith these more complicated requests, I also suggest using a tool like [Postman](https://www.postman.com/downloads/?utm_source=postman-home). Examples of using postman to test our endpoints are in the [repository for this project](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react/postman_examples).\n\nLastly, you'll notice we added a new function to the manager. Let's implement that inside `index_server.py`:\n\n```python\ndef insert_into_index(doc_text, doc_id=None):\n    global index\n    document = SimpleDirectoryReader(input_files=[doc_text]).load_data()[0]\n    if doc_id is not None:\n        document.doc_id = doc_id\n\n    with lock:\n        index.insert(document)\n        index.storage_context.persist()\n\n\n...\nmanager.register(\"insert_into_index\", insert_into_index)\n...\n```\n\nEasy! If we launch both the `index_server.py` and then the `flask_demo.py` python files, we have a Flask API server that can handle multiple requests to insert documents into a vector index and respond to user queries!\n\nTo support some functionality in the frontend, I've adjusted what some responses look like from the Flask API, as well as added some functionality to keep track of which documents are stored in the index (LlamaIndex doesn't currently support this in a user-friendly way, but we can augment it ourselves!). Lastly, I had to add CORS support to the server using the `Flask-cors` python package.\n\nCheck out the complete `flask_demo.py` and `index_server.py` scripts in the [repository](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react) for the final minor changes, the`requirements.txt` file, and a sample `Dockerfile` to help with deployment.\n\n## React Frontend\n\nGenerally, React and Typescript are one of the most popular libraries and languages for writing webapps today. This guide will assume you are familiar with how these tools work, because otherwise this guide will triple in length :smile:.\n\nIn the [repository](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react), the frontend code is organized inside of the `react_frontend` folder.\n\nThe most relevant part of the frontend will be the `src/apis` folder.", "mimetype": "text/plain", "start_char_idx": 7680, "end_char_idx": 12129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd914d53-d2ce-4a99-b12e-cd80392ec556": {"__data__": {"id_": "bd914d53-d2ce-4a99-b12e-cd80392ec556", "embedding": null, "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded", "node_type": "4", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "a437a2d8efb83626fca8f320f41c1d3f6862e351df90e386c213eb1606602644", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbc4eba6-cb60-416b-9d7e-e1afbe8512ef", "node_type": "1", "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "92274e6c2bee3e2a305e38465ea72c4206cb4787b0ffbfde9ac1a1cd7020c160", "class_name": "RelatedNodeInfo"}}, "text": "The most relevant part of the frontend will be the `src/apis` folder. This is where we make calls to the Flask server, supporting the following queries:\n\n- `/query` -- make a query to the existing index\n- `/uploadFile` -- upload a file to the flask server for insertion into the index\n- `/getDocuments` -- list the current document titles and a portion of their texts\n\nUsing these three queries, we can build a robust frontend that allows users to upload and keep track of their files, query the index, and view the query response and information about which text nodes were used to form the response.\n\n### fetchDocuments.tsx\n\nThis file contains the function to, you guessed it, fetch the list of current documents in the index. The code is as follows:\n\n```typescript\nexport type Document = {\n  id: string;\n  text: string;\n};\n\nconst fetchDocuments = async (): Promise<Document[]> => {\n  const response = await fetch(\"http://localhost:5601/getDocuments\", {\n    mode: \"cors\",\n  });\n\n  if (!response.ok) {\n    return [];\n  }\n\n  const documentList = (await response.json()) as Document[];\n  return documentList;\n};\n```\n\nAs you can see, we make a query to the Flask server (here, it assumes running on localhost). Notice that we need to include the `mode: 'cors'` option, as we are making an external request.\n\nThen, we check if the response was ok, and if so, get the response json and return it. Here, the response json is a list of `Document` objects that are defined in the same file.\n\n### queryIndex.tsx\n\nThis file sends the user query to the flask server, and gets the response back, as well as details about which nodes in our index provided the response.\n\n```typescript\nexport type ResponseSources = {\n  text: string;\n  doc_id: string;\n  start: number;\n  end: number;\n  similarity: number;\n};\n\nexport type QueryResponse = {\n  text: string;\n  sources: ResponseSources[];\n};\n\nconst queryIndex = async (query: string): Promise<QueryResponse> => {\n  const queryURL = new URL(\"http://localhost:5601/query?text=1\");\n  queryURL.searchParams.append(\"text\", query);\n\n  const response = await fetch(queryURL, { mode: \"cors\" });\n  if (!response.ok) {\n    return { text: \"Error in query\", sources: [] };\n  }\n\n  const queryResponse = (await response.json()) as QueryResponse;\n\n  return queryResponse;\n};\n\nexport default queryIndex;\n```\n\nThis is similar to the `fetchDocuments.tsx` file, with the main difference being we include the query text as a parameter in the URL. Then, we check if the response is ok and return it with the appropriate typescript type.\n\n### insertDocument.tsx\n\nProbably the most complex API call is uploading a document. The function here accepts a file object and constructs a `POST` request using `FormData`.\n\nThe actual response text is not used in the app but could be utilized to provide some user feedback on if the file failed to upload or not.\n\n```typescript\nconst insertDocument = async (file: File) => {\n  const formData = new FormData();\n  formData.append(\"file\", file);\n  formData.append(\"filename_as_doc_id\", \"true\");\n\n  const response = await fetch(\"http://localhost:5601/uploadFile\", {\n    mode: \"cors\",\n    method: \"POST\",\n    body: formData,\n  });\n\n  const responseText = response.text();\n  return responseText;\n};\n\nexport default insertDocument;\n```\n\n### All the Other Frontend Good-ness\n\nAnd that pretty much wraps up the frontend portion! The rest of the react frontend code is some pretty basic react components, and my best attempt to make it look at least a little nice :smile:.\n\nI encourage to read the rest of the [codebase](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react/react_frontend) and submit any PRs for improvements!\n\n## Conclusion\n\nThis guide has covered a ton of information. We went from a basic \"Hello World\" Flask server written in python, to a fully functioning LlamaIndex powered backend and how to connect that to a frontend application.\n\nAs you can see, we can easily augment and wrap the services provided by LlamaIndex (like the little external document tracker) to help provide a good user experience on the frontend.\n\nYou could take this and add many features (multi-index/user support, saving objects into S3, adding a Pinecone vector server, etc.). And when you build an app after reading this, be sure to share the final result in the Discord! Good Luck! :muscle:", "mimetype": "text/plain", "start_char_idx": 12060, "end_char_idx": 16421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "422a6791-d891-4b2f-8466-cd9d8136c974": {"__data__": {"id_": "422a6791-d891-4b2f-8466-cd9d8136c974", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65aa08ac-e70b-4514-ab0e-a78f87004eba", "node_type": "1", "metadata": {}, "hash": "87adf258c8e9b404c2adfd6ee14ca9eecc9f5d287d3edecfc2279e9439002c29", "class_name": "RelatedNodeInfo"}}, "text": "# A Guide to Building a Full-Stack LlamaIndex Web App with Delphic\n\nThis guide seeks to walk you through using LlamaIndex with a production-ready web app starter template\ncalled [Delphic](https://github.com/JSv4/Delphic). All code examples here are available from\nthe [Delphic](https://github.com/JSv4/Delphic) repo\n\n## What We're Building\n\nHere's a quick demo of the out-of-the-box functionality of Delphic:\n\nhttps://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\n\n## Architectural Overview\n\nDelphic leverages the LlamaIndex python library to let users to create their own document collections they can then\nquery in a responsive frontend.\n\nWe chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python\nprocessing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional\nfunctionality upon.\n\nThe core libraries are:\n\n1. [Django](https://www.djangoproject.com/)\n2. [Django Channels](https://channels.readthedocs.io/en/stable/)\n3. [Django Ninja](https://django-ninja.rest-framework.com/)\n4. [Redis](https://redis.io/)\n5. [Celery](https://docs.celeryq.dev/en/stable/getting-started/introduction.html)\n6. [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/)\n7. [Langchain](https://python.langchain.com/en/latest/index.html)\n8. [React](https://github.com/facebook/react)\n9. Docker & Docker Compose\n\nThanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined\ndeveloper experience, built-in authentication and user management, asynchronous vector store processing, and\nweb-socket-based query connections for a responsive UI. In addition, our frontend is built with TypeScript and is based\non MUI React for a responsive and modern user interface.\n\n## System Requirements\n\nCelery doesn't work on Windows. It may be deployable with Windows Subsystem for Linux, but configuring that is beyond\nthe scope of this tutorial. For this reason, we recommend you only follow this tutorial if you're running Linux or OSX.\nYou will need Docker and Docker Compose installed to deploy the application. Local development will require node version\nmanager (nvm).\n\n## Django Backend\n\n### Project Directory Overview\n\nThe Delphic application has a structured backend directory organization that follows common Django project conventions.\nFrom the repo root, in the `./delphic` subfolder, the main folders are:\n\n1. `contrib`: This directory contains custom modifications or additions to Django's built-in `contrib` apps.\n2. `indexes`: This directory contains the core functionality related to document indexing and LLM integration. It\n   includes:\n\n- `admin.py`: Django admin configuration for the app\n- `apps.py`: Application configuration\n- `models.py`: Contains the app's database models\n- `migrations`: Directory containing database schema migrations for the app\n- `signals.py`: Defines any signals for the app\n- `tests.py`: Unit tests for the app\n\n3. `tasks`: This directory contains tasks for asynchronous processing using Celery. The `index_tasks.py` file includes\n   the tasks for creating vector indexes.\n4. `users`: This directory is dedicated to user management, including:\n5. `utils`: This directory contains utility modules and functions that are used across the application, such as custom\n   storage backends, path helpers, and collection-related utilities.\n\n### Database Models\n\nThe Delphic application has two core models: `Document` and `Collection`. These models represent the central entities\nthe application deals with when indexing and querying documents using LLMs. They're defined in\n[`./delphic/indexes/models.py`](https://github.com/JSv4/Delphic/blob/main/delphic/indexes/models.py).\n\n1. `Collection`:\n\n- `api_key`: A foreign key that links a collection to an API key. This helps associate jobs with the source API key.\n- `title`: A character field that provides a title for the collection.\n- `description`: A text field that provides a description of the collection.\n- `status`: A character field that stores the processing status of the collection, utilizing the `CollectionStatus`\n  enumeration.\n- `created`: A datetime field that records when the collection was created.\n- `modified`: A datetime field that records the last modification time of the collection.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65aa08ac-e70b-4514-ab0e-a78f87004eba": {"__data__": {"id_": "65aa08ac-e70b-4514-ab0e-a78f87004eba", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "422a6791-d891-4b2f-8466-cd9d8136c974", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "126e0a9e37293840e6708116e7871b7884f52c101ddf2dcc6b14a17f51e1b9b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71886cb0-dbe9-41fd-8281-2dab901dd1c2", "node_type": "1", "metadata": {}, "hash": "7b380df02a5b5dd551618b3da17ff10f13212e91cfe1b190a5326c00ec6eee00", "class_name": "RelatedNodeInfo"}}, "text": "- `created`: A datetime field that records when the collection was created.\n- `modified`: A datetime field that records the last modification time of the collection.\n- `model`: A file field that stores the model associated with the collection.\n- `processing`: A boolean field that indicates if the collection is currently being processed.\n\n2. `Document`:\n\n- `collection`: A foreign key that links a document to a collection. This represents the relationship between documents\n  and collections.\n- `file`: A file field that stores the uploaded document file.\n- `description`: A text field that provides a description of the document.\n- `created`: A datetime field that records when the document was created.\n- `modified`: A datetime field that records the last modification time of the document.\n\nThese models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.\n\n### Django Ninja API\n\nDjango Ninja is a web framework for building APIs with Django and Python 3.7+ type hints. It provides a simple,\nintuitive, and expressive way of defining API endpoints, leveraging Python\u2019s type hints to automatically generate input\nvalidation, serialization, and documentation.\n\nIn the Delphic repo,\nthe [`./config/api/endpoints.py`](https://github.com/JSv4/Delphic/blob/main/config/api/endpoints.py)\nfile contains the API routes and logic for the API endpoints. Now, let\u2019s briefly address the purpose of each endpoint\nin the `endpoints.py` file:\n\n1. `/heartbeat`: A simple GET endpoint to check if the API is up and running. Returns `True` if the API is accessible.\n   This is helpful for Kubernetes setups that expect to be able to query your container to ensure it's up and running.\n\n2. `/collections/create`: A POST endpoint to create a new `Collection`. Accepts form parameters such\n   as `title`, `description`, and a list of `files`. Creates a new `Collection` and `Document` instances for each file,\n   and schedules a Celery task to create an index.\n\n```python\n@collections_router.post(\"/create\")\nasync def create_collection(\n    request,\n    title: str = Form(...),\n    description: str = Form(...),\n    files: list[UploadedFile] = File(...),\n):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collection_instance = Collection(\n        api_key=key,\n        title=title,\n        description=description,\n        status=CollectionStatusEnum.QUEUED,\n    )\n\n    await sync_to_async(collection_instance.save)()\n\n    for uploaded_file in files:\n        doc_data = uploaded_file.file.read()\n        doc_file = ContentFile(doc_data, uploaded_file.name)\n        document = Document(collection=collection_instance, file=doc_file)\n        await sync_to_async(document.save)()\n\n    create_index.si(collection_instance.id).apply_async()\n\n    return await sync_to_async(CollectionModelSchema)(...)\n```\n\n3. `/collections/query` \u2014 a POST endpoint to query a document collection using the LLM. Accepts a JSON payload\n   containing `collection_id` and `query_str`, and returns a response generated by querying the collection. We don't\n   actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate\n   to this REST endpoint to query a specific collection.\n\n```python\n@collections_router.post(\n    \"/query\",\n    response=CollectionQueryOutput,\n    summary=\"Ask a question of a document collection\",\n)\ndef query_collection_view(\n    request: HttpRequest, query_input: CollectionQueryInput\n):\n    collection_id = query_input.collection_id\n    query_str = query_input.query_str\n    response = query_collection(collection_id, query_str)\n    return {\"response\": response}\n```\n\n4. `/collections/available`: A GET endpoint that returns a list of all collections created with the user's API key. The\n   output is serialized using the `CollectionModelSchema`.\n\n```python\n@collections_router.get(\n    \"/available\",\n    response=list[CollectionModelSchema],\n    summary=\"Get a list of all of the collections created with my api_key\",\n)\nasync def get_my_collections_view(request: HttpRequest):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collections = Collection.objects.filter(api_key=key)\n\n    return [{...} async for collection in collections]\n```\n\n5. `/collections/{collection_id}/add_file`: A POST endpoint to add a file to an existing collection. Accepts\n   a `collection_id` path parameter, and form parameters such as `file` and `description`.", "mimetype": "text/plain", "start_char_idx": 4204, "end_char_idx": 8793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71886cb0-dbe9-41fd-8281-2dab901dd1c2": {"__data__": {"id_": "71886cb0-dbe9-41fd-8281-2dab901dd1c2", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65aa08ac-e70b-4514-ab0e-a78f87004eba", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "755fdafe43dec70cd38a4cc2a0fdbbf8bc9c8b93294c4a07886a35b0696a6013", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2425e5b1-624d-4255-a2aa-cbee41ebd9f1", "node_type": "1", "metadata": {}, "hash": "c648435c5f95d89c3c6697f8c6c61c8b17a519adc30472290eb1a3a9972349bb", "class_name": "RelatedNodeInfo"}}, "text": "Accepts\n   a `collection_id` path parameter, and form parameters such as `file` and `description`. Adds the file as a `Document`\n   instance associated with the specified collection.\n\n```python\n@collections_router.post(\n    \"/{collection_id}/add_file\", summary=\"Add a file to a collection\"\n)\nasync def add_file_to_collection(\n    request,\n    collection_id: int,\n    file: UploadedFile = File(...),\n    description: str = Form(...),\n):\n    collection = await sync_to_async(Collection.objects.get)(id=collection_id)\n```\n\n### Intro to Websockets\n\nWebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a\nserver over a single, long-lived connection. The WebSocket protocol is designed to work over the same ports as HTTP and\nHTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection. Once the\nconnection is established, data can be sent in both directions as \u201cframes\u201d without the need to reestablish the\nconnection each time, unlike traditional HTTP requests.\n\nThere are several reasons to use WebSockets, particularly when working with code that takes a long time to load into\nmemory but is quick to run once loaded:\n\n1. **Performance**: WebSockets eliminate the overhead associated with opening and closing multiple connections for each\n   request, reducing latency.\n2. **Efficiency**: WebSockets allow for real-time communication without the need for polling, resulting in more\n   efficient use of resources and better responsiveness.\n3. **Scalability**: WebSockets can handle a large number of simultaneous connections, making it ideal for applications\n   that require high concurrency.\n\nIn the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory.\nBy establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be\nprocessed quickly without the need to reload the model each time.\n\nThe ASGI configuration file [`./config/asgi.py`](https://github.com/JSv4/Delphic/blob/main/config/asgi.py) defines how\nthe application should handle incoming connections, using the Django Channels `ProtocolTypeRouter` to route connections\nbased on their protocol type. In this case, we have two protocol types: \"http\" and \"websocket\".\n\nThe \u201chttp\u201d protocol type uses the standard Django ASGI application to handle HTTP requests, while the \u201cwebsocket\u201d\nprotocol type uses a custom `TokenAuthMiddleware` to authenticate WebSocket connections. The `URLRouter` within\nthe `TokenAuthMiddleware` defines a URL pattern for the `CollectionQueryConsumer`, which is responsible for handling\nWebSocket connections related to querying document collections.\n\n```python\napplication = ProtocolTypeRouter(\n    {\n        \"http\": get_asgi_application(),\n        \"websocket\": TokenAuthMiddleware(\n            URLRouter(\n                [\n                    re_path(\n                        r\"ws/collections/(?P<collection_id>\\w+)/query/$\",\n                        CollectionQueryConsumer.as_asgi(),\n                    ),\n                ]\n            )\n        ),\n    }\n)\n```\n\nThis configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query\ndocument collections using the LLMs, without the need to reload the models for each request.\n\n### Websocket Handler\n\nThe `CollectionQueryConsumer` class\nin [`config/api/websockets/queries.py`](https://github.com/JSv4/Delphic/blob/main/config/api/websockets/queries.py) is\nresponsible for handling WebSocket connections related to querying document collections. It inherits from\nthe `AsyncWebsocketConsumer` class provided by Django Channels.\n\nThe `CollectionQueryConsumer` class has three main methods:\n\n1. `connect`: Called when a WebSocket is handshaking as part of the connection process.\n2. `disconnect`: Called when a WebSocket closes for any reason.\n3. `receive`: Called when the server receives a message from the WebSocket.\n\n#### Websocket connect listener\n\nThe `connect` method is responsible for establishing the connection, extracting the collection ID from the connection\npath, loading the collection model, and accepting the connection.\n\n```python\nasync def connect(self):\n    try:\n        self.collection_id = extract_connection_id(self.scope[\"path\"])\n        self.index = await load_collection_model(self.collection_id)\n        await self.accept()\n\n    except ValueError as e:\n        await self.accept()\n        await self.close(code=4000)\n    except Exception as e:\n        pass\n```\n\n#### Websocket disconnect listener\n\nThe `disconnect` method is empty in this case, as there are no additional actions to be taken when the WebSocket is\nclosed.\n\n#### Websocket receive listener\n\nThe `receive` method is responsible for processing incoming messages from the WebSocket.", "mimetype": "text/plain", "start_char_idx": 8695, "end_char_idx": 13549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2425e5b1-624d-4255-a2aa-cbee41ebd9f1": {"__data__": {"id_": "2425e5b1-624d-4255-a2aa-cbee41ebd9f1", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71886cb0-dbe9-41fd-8281-2dab901dd1c2", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "dec9c5b5f81e4c486b20f7084ea07fd3452038a252150e787d098e2c01c5d773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33a4ab75-08f5-4b2b-958e-dec9292dcf95", "node_type": "1", "metadata": {}, "hash": "0eb5d65118b120801383f8f4ebdffc8874ce1ce63eccd797d4d9336b7891a166", "class_name": "RelatedNodeInfo"}}, "text": "#### Websocket receive listener\n\nThe `receive` method is responsible for processing incoming messages from the WebSocket. It takes the incoming message,\ndecodes it, and then queries the loaded collection model using the provided query. The response is then formatted as a\nmarkdown string and sent back to the client over the WebSocket connection.\n\n```python\nasync def receive(self, text_data):\n    text_data_json = json.loads(text_data)\n\n    if self.index is not None:\n        query_str = text_data_json[\"query\"]\n        modified_query_str = f\"Please return a nicely formatted markdown string to this request:\\n\\n{query_str}\"\n        query_engine = self.index.as_query_engine()\n        response = query_engine.query(modified_query_str)\n\n        markdown_response = f\"## Response\\n\\n{response}\\n\\n\"\n        if response.source_nodes:\n            markdown_sources = (\n                f\"## Sources\\n\\n{response.get_formatted_sources()}\"\n            )\n        else:\n            markdown_sources = \"\"\n\n        formatted_response = f\"{markdown_response}{markdown_sources}\"\n\n        await self.send(json.dumps({\"response\": formatted_response}, indent=4))\n    else:\n        await self.send(\n            json.dumps(\n                {\"error\": \"No index loaded for this connection.\"}, indent=4\n            )\n        )\n```\n\nTo load the collection model, the `load_collection_model` function is used, which can be found\nin [`delphic/utils/collections.py`](https://github.com/JSv4/Delphic/blob/main/delphic/utils/collections.py). This\nfunction retrieves the collection object with the given collection ID, checks if a JSON file for the collection model\nexists, and if not, creates one. Then, it sets up the `LLM` and `Settings` before loading\nthe `VectorStoreIndex` using the cache file.\n\n```python\nfrom llama_index.core import Settings\n\n\nasync def load_collection_model(collection_id: str | int) -> VectorStoreIndex:\n    \"\"\"\n    Load the Collection model from cache or the database, and return the index.\n\n    Args:\n        collection_id (Union[str, int]): The ID of the Collection model instance.\n\n    Returns:\n        VectorStoreIndex: The loaded index.\n\n    This function performs the following steps:\n    1. Retrieve the Collection object with the given collection_id.\n    2. Check if a JSON file with the name '/cache/model_{collection_id}.json' exists.\n    3. If the JSON file doesn't exist, load the JSON from the Collection.model FileField and save it to\n       '/cache/model_{collection_id}.json'.\n    4. Call VectorStoreIndex.load_from_disk with the cache_file_path.\n    \"\"\"\n    # Retrieve the Collection object\n    collection = await Collection.objects.aget(id=collection_id)\n    logger.info(f\"load_collection_model() - loaded collection {collection_id}\")\n\n    # Make sure there's a model\n    if collection.model.name:\n        logger.info(\"load_collection_model() - Setup local json index file\")\n\n        # Check if the JSON file exists\n        cache_dir = Path(settings.BASE_DIR) / \"cache\"\n        cache_file_path = cache_dir / f\"model_{collection_id}.json\"\n        if not cache_file_path.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n            with collection.model.open(\"rb\") as model_file:\n                with cache_file_path.open(\n                    \"w+\", encoding=\"utf-8\"\n                ) as cache_file:\n                    cache_file.write(model_file.read().decode(\"utf-8\"))\n\n        # define LLM\n        logger.info(\n            f\"load_collection_model() - Setup Settings with tokens {settings.MAX_TOKENS} and \"\n            f\"model {settings.MODEL_NAME}\"\n        )\n        Settings.llm = OpenAI(\n            temperature=0, model=\"gpt-3.5-turbo\", max_tokens=512\n        )\n\n        # Call VectorStoreIndex.load_from_disk\n        logger.info(\"load_collection_model() - Load llama index\")\n        index = VectorStoreIndex.load_from_disk(\n            cache_file_path,\n        )\n        logger.info(\n            \"load_collection_model() - Llamaindex loaded and ready for query...\"\n        )\n\n    else:\n        logger.error(\n            f\"load_collection_model() - collection {collection_id} has no model!\"\n        )\n        raise ValueError(\"No model exists for this collection!\")\n\n    return index\n```\n\n## React Frontend\n\n### Overview\n\nWe chose to use TypeScript, React and Material-UI (MUI) for the Delphic project\u2019s frontend for a couple reasons. First,\nas the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this\nproject accessible to a huge community of developers.", "mimetype": "text/plain", "start_char_idx": 13428, "end_char_idx": 17979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33a4ab75-08f5-4b2b-958e-dec9292dcf95": {"__data__": {"id_": "33a4ab75-08f5-4b2b-958e-dec9292dcf95", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2425e5b1-624d-4255-a2aa-cbee41ebd9f1", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "0ceddf68ae5ae825403af34fb2e3d19d18d09801f6dfc25f775bcd0fbe45c953", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf7f510d-543d-4b4a-8494-3ec40abf0c94", "node_type": "1", "metadata": {}, "hash": "0a965c48de8b96decda5ea278d8f9a4a3f84c8fe7738c3af48c9a13032f2524d", "class_name": "RelatedNodeInfo"}}, "text": "Second, React is, at this point, a stable and generally well-liked\nframework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in\nour opinion, pretty easy to learn, again making it accessible.\n\n### Frontend Project Structure\n\nThe frontend can be found in the [`/frontend`](https://github.com/JSv4/Delphic/tree/main/frontend) directory of the\nrepo, with the React-related components being in `/frontend/src` . You\u2019ll notice there is a DockerFile in the `frontend`\ndirectory and several folders and files related to configuring our frontend web\nserver \u2014 [nginx](https://www.nginx.com/).\n\nThe `/frontend/src/App.tsx` file serves as the entry point of the application. It defines the main components, such as\nthe login form, the drawer layout, and the collection create modal. The main components are conditionally rendered based\non whether the user is logged in and has an authentication token.\n\nThe DrawerLayout2 component is defined in the`DrawerLayour2.tsx` file. This component manages the layout of the\napplication and provides the navigation and main content areas.\n\nSince the application is relatively simple, we can get away with not using a complex state management solution like\nRedux and just use React\u2019s useState hooks.\n\n### Grabbing Collections from the Backend\n\nThe collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component. The process\ncan be broken down into the following steps:\n\n1. Initializing state variables:\n\n```tsx\nconst [collections, setCollections] = useState<CollectionModelSchema[]>([]);\nconst [loading, setLoading] = useState(true);\n```\n\nHere, we initialize two state variables: `collections` to store the list of collections and `loading` to track whether\nthe collections are being fetched.\n\n2. Collections are fetched for the logged-in user with the `fetchCollections()` function:\n\n```tsx\nconst\nfetchCollections = async () = > {\ntry {\nconst accessToken = localStorage.getItem(\"accessToken\");\nif (accessToken) {\nconst response = await getMyCollections(accessToken);\nsetCollections(response.data);\n}\n} catch (error) {\nconsole.error(error);\n} finally {\nsetLoading(false);\n}\n};\n```\n\nThe `fetchCollections` function retrieves the collections for the logged-in user by calling the `getMyCollections` API\nfunction with the user's access token. It then updates the `collections` state with the retrieved data and sets\nthe `loading` state to `false` to indicate that fetching is complete.\n\n### Displaying Collections\n\nThe latest collectios are displayed in the drawer like this:\n\n```tsx\n< List >\n{collections.map((collection) = > (\n    < div key={collection.id} >\n    < ListItem disablePadding >\n    < ListItemButton\n    disabled={\n    collection.status != = CollectionStatus.COMPLETE | |\n    !collection.has_model\n    }\n    onClick={() = > handleCollectionClick(collection)}\nselected = {\n    selectedCollection & &\n    selectedCollection.id == = collection.id\n}\n>\n< ListItemText\nprimary = {collection.title} / >\n          {collection.status == = CollectionStatus.RUNNING ? (\n    < CircularProgress\n    size={24}\n    style={{position: \"absolute\", right: 16}}\n    / >\n): null}\n< / ListItemButton >\n    < / ListItem >\n        < / div >\n))}\n< / List >\n```\n\nYou\u2019ll notice that the `disabled` property of a collection\u2019s `ListItemButton` is set based on whether the collection's\nstatus is not `CollectionStatus.COMPLETE` or the collection does not have a model (`!collection.has_model`). If either\nof these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less\ncollection. Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.\n\nIn a separate `useEffect` hook, we check if any collection in the `collections` state has a status\nof `CollectionStatus.RUNNING` or `CollectionStatus.QUEUED`. If so, we set up an interval to repeatedly call\nthe `fetchCollections` function every 15 seconds (15,000 milliseconds) to update the collection statuses. This way, the\napplication periodically checks for completed collections, and the UI is updated accordingly when the processing is\ndone.", "mimetype": "text/plain", "start_char_idx": 17980, "end_char_idx": 22152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf7f510d-543d-4b4a-8494-3ec40abf0c94": {"__data__": {"id_": "bf7f510d-543d-4b4a-8494-3ec40abf0c94", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33a4ab75-08f5-4b2b-958e-dec9292dcf95", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "4809c5ccdc894e55826ab250265ef58d13cc1ee208f94cccca1a77fec9e374da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f50c83d1-e9b4-4654-b695-286cd79d7597", "node_type": "1", "metadata": {}, "hash": "9d28cc3b869f7971f3de8205a5b4741bf21dc16141445aebf59f0a314f0eb271", "class_name": "RelatedNodeInfo"}}, "text": "This way, the\napplication periodically checks for completed collections, and the UI is updated accordingly when the processing is\ndone.\n\n```tsx\nuseEffect(() = > {\n    let\ninterval: NodeJS.Timeout;\nif (\n    collections.some(\n        (collection) = >\ncollection.status == = CollectionStatus.RUNNING | |\ncollection.status == = CollectionStatus.QUEUED\n)\n) {\n    interval = setInterval(() = > {\n    fetchCollections();\n}, 15000);\n}\nreturn () = > clearInterval(interval);\n}, [collections]);\n```\n\n### Chat View Component\n\nThe `ChatView` component in `frontend/src/chat/ChatView.tsx` is responsible for handling and displaying a chat interface\nfor a user to interact with a collection. The component establishes a WebSocket connection to communicate in real-time\nwith the server, sending and receiving messages.\n\nKey features of the `ChatView` component include:\n\n1. Establishing and managing the WebSocket connection with the server.\n2. Displaying messages from the user and the server in a chat-like format.\n3. Handling user input to send messages to the server.\n4. Updating the messages state and UI based on received messages from the server.\n5. Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors\n   while loading a collection.\n\nTogether, all of this allows users to interact with their selected collection with a very smooth, low-latency\nexperience.\n\n#### Chat Websocket Client\n\nThe WebSocket connection in the `ChatView` component is used to establish real-time communication between the client and\nthe server. The WebSocket connection is set up and managed in the `ChatView` component as follows:\n\nFirst, we want to initialize the WebSocket reference:\n\nconst websocket = useRef<WebSocket | null>(null);\n\nA `websocket` reference is created using `useRef`, which holds the WebSocket object that will be used for\ncommunication. `useRef` is a hook in React that allows you to create a mutable reference object that persists across\nrenders. It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket\nconnection, without causing unnecessary re-renders.\n\nIn the `ChatView` component, the WebSocket connection needs to be established and maintained throughout the lifetime of\nthe component, and it should not trigger a re-render when the connection state changes. By using `useRef`, you ensure\nthat the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state\nchanges, such as updating messages or displaying errors.\n\nThe `setupWebsocket` function is responsible for establishing the WebSocket connection and setting up event handlers to\nhandle different WebSocket events.\n\nOverall, the setupWebsocket function looks like this:\n\n```tsx\nconst setupWebsocket = () => {\n  setConnecting(true);\n  // Here, a new WebSocket object is created using the specified URL, which includes the\n  // selected collection's ID and the user's authentication token.\n\n  websocket.current = new WebSocket(\n    `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`,\n  );\n\n  websocket.current.onopen = (event) => {\n    //...\n  };\n\n  websocket.current.onmessage = (event) => {\n    //...\n  };\n\n  websocket.current.onclose = (event) => {\n    //...\n  };\n\n  websocket.current.onerror = (event) => {\n    //...\n  };\n\n  return () => {\n    websocket.current?.close();\n  };\n};\n```\n\nNotice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.\n\nWhen the component first opens and we try to establish a connection, the `onopen` listener is triggered. In the\ncallback, the component updates the states to reflect that the connection is established, any previous errors are\ncleared, and no messages are awaiting responses:\n\n```tsx\nwebsocket.current.onopen = (event) => {\n  setError(false);\n  setConnecting(false);\n  setAwaitingMessage(false);\n\n  console.log(\"WebSocket connected:\", event);\n};\n```\n\n`onmessage`is triggered when a new message is received from the server through the WebSocket connection.", "mimetype": "text/plain", "start_char_idx": 22017, "end_char_idx": 26113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f50c83d1-e9b4-4654-b695-286cd79d7597": {"__data__": {"id_": "f50c83d1-e9b4-4654-b695-286cd79d7597", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf7f510d-543d-4b4a-8494-3ec40abf0c94", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "6ce51aedb7720ceb30f7869d0466573df05c9a1cc944dc0ba61a62fb07bfa10c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "817c954f-e6bd-407e-a8b5-543d7e2bddd8", "node_type": "1", "metadata": {}, "hash": "9ed06cafaf6e9a3eacd3c9a9a3a9755bfc3d96aea68e7cb3a6327e6f26df5690", "class_name": "RelatedNodeInfo"}}, "text": "In the\ncallback, the received data is parsed and the `messages` state is updated with the new message from the server:\n\n```\nwebsocket.current.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  console.log(\"WebSocket message received:\", data);\n  setAwaitingMessage(false);\n\n  if (data.response) {\n    // Update the messages state with the new message from the server\n    setMessages((prevMessages) => [\n      ...prevMessages,\n      {\n        sender_id: \"server\",\n        message: data.response,\n        timestamp: new Date().toLocaleTimeString(),\n      },\n    ]);\n  }\n};\n```\n\n`onclose`is triggered when the WebSocket connection is closed. In the callback, the component checks for a specific\nclose code (`4000`) to display a warning toast and update the component states accordingly. It also logs the close\nevent:\n\n```tsx\nwebsocket.current.onclose = (event) => {\n  if (event.code === 4000) {\n    toast.warning(\n      \"Selected collection's model is unavailable. Was it created properly?\",\n    );\n    setError(true);\n    setConnecting(false);\n    setAwaitingMessage(false);\n  }\n  console.log(\"WebSocket closed:\", event);\n};\n```\n\nFinally, `onerror` is triggered when an error occurs with the WebSocket connection. In the callback, the component\nupdates the states to reflect the error and logs the error event:\n\n```tsx\nwebsocket.current.onerror = (event) => {\n  setError(true);\n  setConnecting(false);\n  setAwaitingMessage(false);\n\n  console.error(\"WebSocket error:\", event);\n};\n```\n\n#### Rendering our Chat Messages\n\nIn the `ChatView` component, the layout is determined using CSS styling and Material-UI components. The main layout\nconsists of a container with a `flex` display and a column-oriented `flexDirection`. This ensures that the content\nwithin the container is arranged vertically.\n\nThere are three primary sections within the layout:\n\n1. The chat messages area: This section takes up most of the available space and displays a list of messages exchanged\n   between the user and the server. It has an overflow-y set to \u2018auto\u2019, which allows scrolling when the content\n   overflows the available space. The messages are rendered using the `ChatMessage` component for each message and\n   a `ChatMessageLoading` component to show the loading state while waiting for a server response.\n2. The divider: A Material-UI `Divider` component is used to separate the chat messages area from the input area,\n   creating a clear visual distinction between the two sections.\n3. The input area: This section is located at the bottom and allows the user to type and send messages. It contains\n   a `TextField` component from Material-UI, which is set to accept multiline input with a maximum of 2 rows. The input\n   area also includes a `Button` component to send the message. The user can either click the \"Send\" button or press \"\n   Enter\" on their keyboard to send the message.\n\nThe user inputs accepted in the `ChatView` component are text messages that the user types in the `TextField`. The\ncomponent processes these text inputs and sends them to the server through the WebSocket connection.\n\n## Deployment\n\n### Prerequisites\n\nTo deploy the app, you're going to need Docker and Docker Compose installed. If you're on Ubuntu or another, common\nLinux distribution, DigitalOcean has\na [great Docker tutorial](https://www.digitalocean.com/community/tutorial_collections/how-to-install-and-use-docker) and\nanother great tutorial\nfor [Docker Compose](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04)\nyou can follow. If those don't work for you, try\nthe [official docker documentation.](https://docs.docker.com/engine/install/)\n\n### Build and Deploy\n\nThe project is based on django-cookiecutter, and it\u2019s pretty easy to get it deployed on a VM and configured to serve\nHTTPs traffic for a specific domain. The configuration is somewhat involved, however \u2014 not because of this project, but\nit\u2019s just a fairly involved topic to configure your certificates, DNS, etc.\n\nFor the purposes of this guide, let\u2019s just get running locally. Perhaps we\u2019ll release a guide on production deployment.\nIn the meantime, check out\nthe [Django Cookiecutter project docs](https://cookiecutter-django.readthedocs.io/en/latest/deployment-with-docker.html)\nfor starters.\n\nThis guide assumes your goal is to get the application up and running for use.", "mimetype": "text/plain", "start_char_idx": 26114, "end_char_idx": 30501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "817c954f-e6bd-407e-a8b5-543d7e2bddd8": {"__data__": {"id_": "817c954f-e6bd-407e-a8b5-543d7e2bddd8", "embedding": null, "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d", "node_type": "4", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "056f51ee97d5ce2dd299621bd61ac1bd30e4cff5e27e85581d5a2d8610ba9fb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f50c83d1-e9b4-4654-b695-286cd79d7597", "node_type": "1", "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "283a55b000a154f9841bbc59d9cf74b275d02d8b91747945955672e4215704ba", "class_name": "RelatedNodeInfo"}}, "text": "This guide assumes your goal is to get the application up and running for use. If you want to develop, most likely you\nwon\u2019t want to launch the compose stack with the \u2014 profiles fullstack flag and will instead want to launch the react\nfrontend using the node development server.\n\nTo deploy, first clone the repo:\n\n```commandline\ngit clone https://github.com/yourusername/delphic.git\n```\n\nChange into the project directory:\n\n```commandline\ncd delphic\n```\n\nCopy the sample environment files:\n\n```commandline\nmkdir -p ./.envs/.local/\ncp -a ./docs/sample_envs/local/.frontend ./frontend\ncp -a ./docs/sample_envs/local/.django ./.envs/.local\ncp -a ./docs/sample_envs/local/.postgres ./.envs/.local\n```\n\nEdit the `.django` and `.postgres` configuration files to include your OpenAI API key and set a unique password for your\ndatabase user. You can also set the response token limit in the .django file or switch which OpenAI model you want to\nuse. GPT4 is supported, assuming you\u2019re authorized to access it.\n\nBuild the docker compose stack with the `--profiles fullstack` flag:\n\n```commandline\nsudo docker-compose --profiles fullstack -f local.yml build\n```\n\nThe fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched\nalong with all of the needed, backend containers. It takes a long time to build a production React container, however,\nso we don\u2019t recommend you develop this way. Follow\nthe [instructions in the project readme.md](https://github.com/JSv4/Delphic#development) for development environment\nsetup instructions.\n\nFinally, bring up the application:\n\n```commandline\nsudo docker-compose -f local.yml up\n```\n\nNow, visit `localhost:3000` in your browser to see the frontend, and use the Delphic application locally.\n\n## Using the Application\n\n### Setup Users\n\nIn order to actually use the application (at the moment, we intend to make it possible to share certain models with\nunauthenticated users), you need a login. You can use either a superuser or non-superuser. In either case, someone needs\nto first create a superuser using the console:\n\n**Why set up a Django superuser?** A Django superuser has all the permissions in the application and can manage all\naspects of the system, including creating, modifying, and deleting users, collections, and other data. Setting up a\nsuperuser allows you to fully control and manage the application.\n\n**How to create a Django superuser:**\n\n1 Run the following command to create a superuser:\n\nsudo docker-compose -f local.yml run django python manage.py createsuperuser\n\n2 You will be prompted to provide a username, email address, and password for the superuser. Enter the required\ninformation.\n\n**How to create additional users using Django admin:**\n\n1. Start your Delphic application locally following the deployment instructions.\n2. Visit the Django admin interface by navigating to `http://localhost:8000/admin` in your browser.\n3. Log in with the superuser credentials you created earlier.\n4. Click on \u201cUsers\u201d under the \u201cAuthentication and Authorization\u201d section.\n5. Click on the \u201cAdd user +\u201d button in the top right corner.\n6. Enter the required information for the new user, such as username and password. Click \u201cSave\u201d to create the user.\n7. To grant the new user additional permissions or make them a superuser, click on their username in the user list,\n   scroll down to the \u201cPermissions\u201d section, and configure their permissions accordingly. Save your changes.", "mimetype": "text/plain", "start_char_idx": 30423, "end_char_idx": 33895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a74955e-4ee7-4173-85c9-ef2b803c265e": {"__data__": {"id_": "0a74955e-4ee7-4173-85c9-ef2b803c265e", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e77465268f6d2163e2e83ad1735a1dfda606c08e", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "38d6592463e6a1db61c419345971aa16573850f2becf5da8be46a3a3fdc4b5af", "class_name": "RelatedNodeInfo"}}, "text": "# Full-Stack Web Application\n\nLlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.\n\nWe provide tutorials and resources to help you get started in this area:\n\n- [Fullstack Application Guide](./fullstack_app_guide.md) shows you how to build an app with LlamaIndex as an API and a TypeScript+React frontend\n- [Fullstack Application with Delphic](./fullstack_with_delphic.md) walks you through using LlamaIndex with a production-ready web app starter template called Delphic.\n- The [LlamaIndex Starter Pack](https://github.com/logan-markewich/llama_index_starter_pack) provides very basic flask, streamlit, and docker examples for LlamaIndex.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10dbf4b5-7391-4676-82b3-558a725f7666": {"__data__": {"id_": "10dbf4b5-7391-4676-82b3-558a725f7666", "embedding": null, "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf22c259939ad9077de8acb6503b790e2775207", "node_type": "4", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "442ae365460d40a7b9ab6d7a86374280a9aadc2ec153465d25374d0bb609146a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbb33c2b-debf-4108-869a-ebddf5e92b0a", "node_type": "1", "metadata": {}, "hash": "1afebd238de189407557137b6141fcf3dc07300c5bf653d49fd23e4c23e563bb", "class_name": "RelatedNodeInfo"}}, "text": "# How to Build a Chatbot\n\nLlamaIndex serves as a bridge between your data and Large Language Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\n\nIn this tutorial, we'll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n\n**Note**: This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n\n### Context\n\nIn this guide, we\u2019ll build a \"10-K Chatbot\" that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings.\n\n### Preparation\n\n```python\nimport os\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n### Ingest Data\n\nLet's first download the raw 10-k files, from 2019-2022.\n\n```\n# NOTE: the code examples assume you're operating within a Jupyter notebook.\n# download files\n!mkdir data\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n!unzip data/UBER.zip -d data\n```\n\nTo parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\n\nFirst we install the necessary packages:\n\n```\n!pip install llama-hub unstructured\n```\n\nThen we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects.\n\n```python\nfrom llama_index.readers.file import UnstructuredReader\nfrom pathlib import Path\n\nyears = [2022, 2021, 2020, 2019]\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nfor year in years:\n    year_docs = loader.load_data(\n        file=Path(f\"./data/UBER/UBER_{year}.html\"), split_documents=False\n    )\n    # insert year metadata into each year\n    for d in year_docs:\n        d.metadata = {\"year\": year}\n    doc_set[year] = year_docs\n    all_docs.extend(year_docs)\n```\n\n### Setting up Vector Indices for each year\n\nWe first setup a vector index for each year. Each vector index allows us\nto ask questions about the 10-K filing of a given year.\n\nWe build each index and save it to disk.\n\n```python\n# initialize simple vector indices\nfrom llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.core import Settings\n\nSettings.chunk_size = 512\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults()\n    cur_index = VectorStoreIndex.from_documents(\n        doc_set[year],\n        storage_context=storage_context,\n    )\n    index_set[year] = cur_index\n    storage_context.persist(persist_dir=f\"./storage/{year}\")\n```\n\nTo load an index from disk, do the following\n\n```python\n# Load indices from disk\nfrom llama_index.core import load_index_from_storage\n\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults(\n        persist_dir=f\"./storage/{year}\"\n    )\n    cur_index = load_index_from_storage(\n        storage_context,\n    )\n    index_set[year] = cur_index\n```\n\n### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\n\nTo address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbb33c2b-debf-4108-869a-ebddf5e92b0a": {"__data__": {"id_": "fbb33c2b-debf-4108-869a-ebddf5e92b0a", "embedding": null, "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf22c259939ad9077de8acb6503b790e2775207", "node_type": "4", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "442ae365460d40a7b9ab6d7a86374280a9aadc2ec153465d25374d0bb609146a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10dbf4b5-7391-4676-82b3-558a725f7666", "node_type": "1", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "90803f096c17192e80487abcb8bf854c6896d785888ca29714edb7db7ea2b355", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dfb79e8-a648-4f51-ba8d-66776f20d2f4", "node_type": "1", "metadata": {}, "hash": "f80398f49a1a7e55d3134aaa27f5d8a6f6554ec365f9b93c37959c0933b7998e", "class_name": "RelatedNodeInfo"}}, "text": "It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\n\nLlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index.\nEach tool has a name and a description; these are what the LLM agent sees to decide which tool to choose.\n\n```python\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\nindividual_query_engine_tools = [\n    QueryEngineTool(\n        query_engine=index_set[year].as_query_engine(),\n        metadata=ToolMetadata(\n            name=f\"vector_index_{year}\",\n            description=f\"useful for when you want to answer queries about the {year} SEC 10-K for Uber\",\n        ),\n    )\n    for year in years\n]\n```\n\nNow we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above, as well as an `llm` that will be used to run the subqueries.\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=individual_query_engine_tools,\n    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n)\n```\n\n### Setting up the Chatbot Agent\n\nWe use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use an OpenAIAgent, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\n\nFirst we define a `QueryEngineTool` for the sub question query engine:\n\n```python\nquery_engine_tool = QueryEngineTool(\n    query_engine=query_engine,\n    metadata=ToolMetadata(\n        name=\"sub_question_query_engine\",\n        description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber\",\n    ),\n)\n```\n\nThen, we combine the Tools we defined above into a single list of tools for the agent:\n\n```python\ntools = individual_query_engine_tools + [query_engine_tool]\n```\n\nFinally, we call `OpenAIAgent.from_tools` to create the agent, passing in the list of tools we defined above.\n\n```python\nfrom llama_index.agent.openai import OpenAIAgent\n\nagent = OpenAIAgent.from_tools(tools, verbose=True)\n```\n\n### Testing the Agent\n\nWe can now test the agent with various queries.\n\nIf we test it with a simple \"hello\" query, the agent does not use any Tools.\n\n```python\nresponse = agent.chat(\"hi, i am bob\")\nprint(str(response))\n```\n\n```\nHello Bob! How can I assist you today?\n```\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use\nthe relevant vector index Tool.\n\n```python\nresponse = agent.chat(\n    \"What were some of the biggest risk factors in 2020 for Uber?\"\n)\nprint(str(response))\n```\n\n```\n=== Calling Function ===\nCalling function: vector_index_2020 with args: {\n  \"input\": \"biggest risk factors\"\n}\nGot output: The biggest risk factors mentioned in the context are:\n1. The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n2. The potential reclassification of drivers as employees, workers, or quasi-employees instead of independent contractors.\n3. Intense competition in the mobility, delivery, and logistics industries, with low-cost alternatives and well-capitalized competitors.\n4. The need to lower fares or service fees and offer driver incentives and consumer discounts to remain competitive.\n5. Significant losses incurred and the uncertainty of achieving profitability.\n6. The risk of not attracting or maintaining a critical mass of platform users.\n7. Operational, compliance, and cultural challenges related to the workplace culture and forward-leaning approach.\n8. The potential negative impact of international investments and the challenges of conducting business in foreign countries.\n9. Risks associated with operational and compliance challenges, localization, laws and regulations, competition, social acceptance, technological compatibility, improper business practices, liability uncertainty, managing international operations, currency fluctuations, cash transactions, tax consequences, and payment fraud.\n========================\nSome of the biggest risk factors for Uber in 2020 were:\n\n1. The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n2.", "mimetype": "text/plain", "start_char_idx": 4093, "end_char_idx": 8683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dfb79e8-a648-4f51-ba8d-66776f20d2f4": {"__data__": {"id_": "1dfb79e8-a648-4f51-ba8d-66776f20d2f4", "embedding": null, "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf22c259939ad9077de8acb6503b790e2775207", "node_type": "4", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "442ae365460d40a7b9ab6d7a86374280a9aadc2ec153465d25374d0bb609146a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbb33c2b-debf-4108-869a-ebddf5e92b0a", "node_type": "1", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "806db92afccf94e5c2db4ed895c091535a139300f6db1aa70d438b13d094b7a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebef20a2-ce27-487b-893f-605bc94bd753", "node_type": "1", "metadata": {}, "hash": "b36bbd3891ee75c5016199a98ac7f8908c8dc4bda3e4ba80bd7cd9d173a965d4", "class_name": "RelatedNodeInfo"}}, "text": "The adverse impact of the COVID-19 pandemic and actions taken to mitigate it on the business.\n2. The potential reclassification of drivers as employees, workers, or quasi-employees instead of independent contractors.\n3. Intense competition in the mobility, delivery, and logistics industries, with low-cost alternatives and well-capitalized competitors.\n4. The need to lower fares or service fees and offer driver incentives and consumer discounts to remain competitive.\n5. Significant losses incurred and the uncertainty of achieving profitability.\n6. The risk of not attracting or maintaining a critical mass of platform users.\n7. Operational, compliance, and cultural challenges related to the workplace culture and forward-leaning approach.\n8. The potential negative impact of international investments and the challenges of conducting business in foreign countries.\n9. Risks associated with operational and compliance challenges, localization, laws and regulations, competition, social acceptance, technological compatibility, improper business practices, liability uncertainty, managing international operations, currency fluctuations, cash transactions, tax consequences, and payment fraud.\n\nThese risk factors highlight the challenges and uncertainties that Uber faced in 2020.\n```\n\nFinally, if we test it with a query to compare/contrast risk factors across years,\nthe agent will use the Sub Question Query Engine Tool.\n\n```python\ncross_query_str = \"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n\nresponse = agent.chat(cross_query_str)\nprint(str(response))\n```\n\n```\n=== Calling Function ===\nCalling function: sub_question_query_engine with args: {\n  \"input\": \"Compare/contrast the risk factors described in the Uber 10-K across years\"\n}\nGenerated 4 sub questions.\n[vector_index_2022] Q: What are the risk factors described in the 2022 SEC 10-K for Uber?\n[vector_index_2021] Q: What are the risk factors described in the 2021 SEC 10-K for Uber?\n[vector_index_2020] Q: What are the risk factors described in the 2020 SEC 10-K for Uber?\n[vector_index_2019] Q: What are the risk factors described in the 2019 SEC 10-K for Uber?\n[vector_index_2021] A: The risk factors described in the 2021 SEC 10-K for Uber include the adverse impact of the COVID-19 pandemic on their business, the potential reclassification of drivers as employees instead of independent contractors, intense competition in the mobility, delivery, and logistics industries, the need to lower fares and offer incentives to remain competitive, significant losses incurred by the company, the importance of attracting and maintaining a critical mass of platform users, and the ongoing legal challenges regarding driver classification.\n[vector_index_2020] A: The risk factors described in the 2020 SEC 10-K for Uber include the adverse impact of the COVID-19 pandemic on their business, the potential reclassification of drivers as employees instead of independent contractors, intense competition in the mobility, delivery, and logistics industries, the need to lower fares and offer incentives to remain competitive, significant losses and the uncertainty of achieving profitability, the importance of attracting and retaining a critical mass of drivers and users, and the challenges associated with their workplace culture and operational compliance.\n[vector_index_2022] A: The risk factors described in the 2022 SEC 10-K for Uber include the potential adverse effect on their business if drivers were classified as employees instead of independent contractors, the highly competitive nature of the mobility, delivery, and logistics industries, the need to lower fares or service fees to remain competitive in certain markets, the company's history of significant losses and the expectation of increased operating expenses in the future, and the potential impact on their platform if they are unable to attract or maintain a critical mass of drivers, consumers, merchants, shippers, and carriers.\n[vector_index_2019] A: The risk factors described in the 2019 SEC 10-K for Uber include the loss of their license to operate in London, the complexity of their business and operating model due to regulatory uncertainties, the potential for additional regulations for their other products in the Other Bets segment, the evolving laws and regulations regarding the development and deployment of autonomous vehicles, and the increasing number of data protection and privacy laws around the world. Additionally, there are legal proceedings, litigation, claims, and government investigations that Uber is involved in, which could impose a burden on management and employees and come with defense costs or unfavorable rulings.\nGot output: The risk factors described in the Uber 10-K reports across the years include the potential reclassification of drivers as employees instead of independent contractors, intense competition in the mobility, delivery, and logistics industries, the need to lower fares and offer incentives to remain competitive, significant losses incurred by the company, the importance of attracting and maintaining a critical mass of platform users, and the ongoing legal challenges regarding driver classification.", "mimetype": "text/plain", "start_char_idx": 8587, "end_char_idx": 13848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebef20a2-ce27-487b-893f-605bc94bd753": {"__data__": {"id_": "ebef20a2-ce27-487b-893f-605bc94bd753", "embedding": null, "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf22c259939ad9077de8acb6503b790e2775207", "node_type": "4", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "442ae365460d40a7b9ab6d7a86374280a9aadc2ec153465d25374d0bb609146a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dfb79e8-a648-4f51-ba8d-66776f20d2f4", "node_type": "1", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "053318339869c914b6a54756da5ec44303a5840c1b896d323ddf483bdc815e5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bab0d9af-e377-40e1-bebf-2bf0a0ff9766", "node_type": "1", "metadata": {}, "hash": "ec86b5335a904452f5c942f663dc4de094e5baf391f985ecd0ef07ae1ce9407e", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, there are specific risk factors mentioned in each year's report, such as the adverse impact of the COVID-19 pandemic in 2020 and 2021, the loss of their license to operate in London in 2019, and the evolving laws and regulations regarding autonomous vehicles in 2019. Overall, while there are some similarities in the risk factors mentioned, there are also specific factors that vary across the years.\n========================\n=== Calling Function ===\nCalling function: vector_index_2022 with args: {\n  \"input\": \"risk factors\"\n}\nGot output: Some of the risk factors mentioned in the context include the potential adverse effect on the business if drivers were classified as employees instead of independent contractors, the highly competitive nature of the mobility, delivery, and logistics industries, the need to lower fares or service fees to remain competitive, the company's history of significant losses and the expectation of increased operating expenses, the impact of future pandemics or disease outbreaks on the business and financial results, and the potential harm to the business due to economic conditions and their effect on discretionary consumer spending.\n========================\n=== Calling Function ===\nCalling function: vector_index_2021 with args: {\n  \"input\": \"risk factors\"\n}\nGot output: The COVID-19 pandemic and the impact of actions to mitigate the pandemic have adversely affected and may continue to adversely affect parts of our business. Our business would be adversely affected if Drivers were classified as employees, workers or quasi-employees instead of independent contractors. The mobility, delivery, and logistics industries are highly competitive, with well-established and low-cost alternatives that have been available for decades, low barriers to entry, low switching costs, and well-capitalized competitors in nearly every major geographic region. To remain competitive in certain markets, we have in the past lowered, and may continue to lower, fares or service fees, and we have in the past offered, and may continue to offer, significant Driver incentives and consumer discounts and promotions. We have incurred significant losses since inception, including in the United States and other major markets. We expect our operating expenses to increase significantly in the foreseeable future, and we may not achieve or maintain profitability. If we are unable to attract or maintain a critical mass of Drivers, consumers, merchants, shippers, and carriers, whether as a result of competition or other factors, our platform will become less appealing to platform users.\n========================\n=== Calling Function ===\nCalling function: vector_index_2020 with args: {\n  \"input\": \"risk factors\"\n}\nGot output: The risk factors mentioned in the context include the adverse impact of the COVID-19 pandemic on the business, the potential reclassification of drivers as employees, the highly competitive nature of the mobility, delivery, and logistics industries, the need to lower fares or service fees to remain competitive, the company's history of significant losses and potential future expenses, the importance of attracting and maintaining a critical mass of platform users, and the operational and cultural challenges faced by the company.\n========================\n=== Calling Function ===\nCalling function: vector_index_2019 with args: {\n  \"input\": \"risk factors\"\n}\nGot output: The risk factors mentioned in the context include competition with local companies, differing levels of social acceptance, technological compatibility issues, exposure to improper business practices, legal uncertainty, difficulties in managing international operations, fluctuations in currency exchange rates, regulations governing local currencies, tax consequences, financial accounting burdens, difficulties in implementing financial systems, import and export restrictions, political and economic instability, public health concerns, reduced protection for intellectual property rights, limited influence over minority-owned affiliates, and regulatory complexities. These risk factors could adversely affect the international operations, business, financial condition, and operating results of the company.\n========================\nHere is a comparison of the risk factors described in the Uber 10-K reports across years:\n\n2022 Risk Factors:\n- Potential adverse effect if drivers were classified as employees instead of independent contractors.\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n- Need to lower fares or service fees to remain competitive.\n- History of significant losses and expectation of increased operating expenses.\n- Impact of future pandemics or disease outbreaks on the business and financial results.\n- Potential harm to the business due to economic conditions and their effect on discretionary consumer spending.\n\n2021 Risk Factors:\n- Adverse impact of the COVID-19 pandemic and actions to mitigate it on the business.\n- Potential reclassification of drivers as employees instead of independent contractors.\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n- Need to lower fares or service fees and offer incentives to remain competitive.\n- History of significant losses and uncertainty of achieving profitability.\n- Importance of attracting and maintaining a critical mass of platform users.\n\n2020 Risk Factors:\n- Adverse impact of the COVID-19 pandemic on the business.\n- Potential reclassification of drivers as employees.\n- Highly competitive nature of the mobility, delivery, and logistics industries.", "mimetype": "text/plain", "start_char_idx": 13849, "end_char_idx": 19487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bab0d9af-e377-40e1-bebf-2bf0a0ff9766": {"__data__": {"id_": "bab0d9af-e377-40e1-bebf-2bf0a0ff9766", "embedding": null, "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdf22c259939ad9077de8acb6503b790e2775207", "node_type": "4", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "442ae365460d40a7b9ab6d7a86374280a9aadc2ec153465d25374d0bb609146a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebef20a2-ce27-487b-893f-605bc94bd753", "node_type": "1", "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "55833acbf2da5ce2bb3e7d4dd553bb970ce4e8a5a1d6c8a2a98de77871390697", "class_name": "RelatedNodeInfo"}}, "text": "- Potential reclassification of drivers as employees.\n- Highly competitive nature of the mobility, delivery, and logistics industries.\n- Need to lower fares or service fees to remain competitive.\n- History of significant losses and potential future expenses.\n- Importance of attracting and maintaining a critical mass of platform users.\n- Operational and cultural challenges faced by the company.\n\n2019 Risk Factors:\n- Competition with local companies.\n- Differing levels of social acceptance.\n- Technological compatibility issues.\n- Exposure to improper business practices.\n- Legal uncertainty.\n- Difficulties in managing international operations.\n- Fluctuations in currency exchange rates.\n- Regulations governing local currencies.\n- Tax consequences.\n- Financial accounting burdens.\n- Difficulties in implementing financial systems.\n- Import and export restrictions.\n- Political and economic instability.\n- Public health concerns.\n- Reduced protection for intellectual property rights.\n- Limited influence over minority-owned affiliates.\n- Regulatory complexities.\n\nThese comparisons highlight both common and unique risk factors that Uber faced in different years.\n```\n\n### Setting up the Chatbot Loop\n\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!\n\n```python\nagent = OpenAIAgent.from_tools(tools)  # verbose=False by default\n\nwhile True:\n    text_input = input(\"User: \")\n    if text_input == \"exit\":\n        break\n    response = agent.chat(text_input)\n    print(f\"Agent: {response}\")\n```\n\nHere's an example of the loop in action:\n\n```\nUser:  What were some of the legal proceedings against Uber in 2022?\nAgent: In 2022, Uber faced several legal proceedings. Some of the notable ones include:\n\n1. Petition against Proposition 22: A petition was filed in California alleging that Proposition 22, which classifies app-based drivers as independent contractors, is unconstitutional.\n\n2. Lawsuit by Massachusetts Attorney General: The Massachusetts Attorney General filed a lawsuit against Uber, claiming that drivers should be classified as employees and entitled to protections under wage and labor laws.\n\n3. Allegations by New York Attorney General: The New York Attorney General made allegations against Uber regarding the misclassification of drivers and related employment violations.\n\n4. Swiss social security rulings: Swiss social security rulings classified Uber drivers as employees, which could have implications for Uber's operations in Switzerland.\n\n5. Class action lawsuits in Australia: Uber faced class action lawsuits in Australia, with allegations that the company conspired to harm participants in the taxi, hire-car, and limousine industries.\n\nIt's important to note that the outcomes of these legal proceedings are uncertain and may vary.\n\nUser:\n\n```\n\n### Notebook\n\nTake a look at our [corresponding notebook](../../../examples/agent/Chatbot_SEC.ipynb).", "mimetype": "text/plain", "start_char_idx": 19353, "end_char_idx": 22321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91115a6a-6c16-4f21-9873-969b6468bb8b": {"__data__": {"id_": "91115a6a-6c16-4f21-9873-969b6468bb8b", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27b13e6441729713e8f50968e11ef423af665228", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "c77412096aab8f981f6c6b5908c59261b7e37f5bbdf9990b0615cb501bfcf55a", "class_name": "RelatedNodeInfo"}}, "text": "# Putting It All Together\n\nCongratulations! You've loaded your data, indexed it, stored your index, and queried your index. Now you've got to ship something to production. We can show you how to do that!\n\n- In [Q&A Patterns](q_and_a.md) we'll go into some of the more advanced and subtle ways you can build a query engine beyond the basics.\n  - The [terms definition tutorial](q_and_a/terms_definitions_tutorial.md) is a detailed, step-by-step tutorial on creating a subtle query application including defining your prompts and supporting images as input.\n  - We have a guide to [creating a unified query framework over your indexes](../../examples/retrievers/reciprocal_rerank_fusion.ipynb) which shows you how to run queries across multiple indexes.\n  - And also over [structured data like SQL](structured_data.md)\n- We have a guide on [how to build a chatbot](chatbots/building_a_chatbot.md)\n- We talk about [building agents in LlamaIndex](agents.md)\n- We have a complete guide to using [property graphs for indexing and retrieval](../../module_guides/indexing/lpg_index_guide.md)\n- And last but not least we show you how to build [a full stack web application](apps.md) using LlamaIndex\n\nLlamaIndex also provides some tools / project templates to help you build a full-stack template. For instance, [`create-llama`](https://github.com/run-llama/LlamaIndexTS/tree/main/packages/create-llama) spins up a full-stack scaffold for you.\n\nCheck out our [Full-Stack Projects](../../community/full_stack_projects.md) page for more details.\n\nWe also have the [`llamaindex-cli rag` CLI tool](../../getting_started/starter_tools/rag_cli.md) that combines some of the above concepts into an easy to use tool for chatting with files from your terminal!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8fcbccc4-1ac1-4ed9-bb9a-25017ad1ab19": {"__data__": {"id_": "8fcbccc4-1ac1-4ed9-bb9a-25017ad1ab19", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "643a6616d589550cad7e3db6883dcbea8df513de", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "9f2bb260af6c2959404f22b7783dc5173652780a793eb4b0474003cb58c82b6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddcbf9d6-45c0-47ee-a6d5-728ae4a82244", "node_type": "1", "metadata": {}, "hash": "43b43733b90f14ecde424eab46a17ddda1ab3de7aa1e8ccd7b035525eb0b4f8c", "class_name": "RelatedNodeInfo"}}, "text": "# Q&A patterns\n\n## Semantic Search\n\nThe most basic example usage of LlamaIndex is through semantic search. We provide a simple in-memory vector store for you to get started, but you can also choose to use any one of our [vector store integrations](../../community/integrations/vector_stores.md):\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\n**Tutorials**\n\n- [Starter Tutorial](../../getting_started/starter_example.md)\n- [Basic Usage Pattern](../querying/querying.md)\n\n**Guides**\n\n- [Example](../../examples/vector_stores/SimpleIndexDemo.ipynb) ([Notebook](https://github.com/run-llama/llama_index/tree/main/docs../../examples/vector_stores/SimpleIndexDemo.ipynb))\n\n## Summarization\n\nA summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.\nFor instance, a summarization query could look like one of the following:\n\n- \"What is a summary of this collection of text?\"\n- \"Give me a summary of person X's experience with the company.\"\n\nIn general, a summary index would be suited for this use case. A summary index by default goes through all the data.\n\nEmpirically, setting `response_mode=\"tree_summarize\"` also leads to better summarization results.\n\n```python\nindex = SummaryIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(response_mode=\"tree_summarize\")\nresponse = query_engine.query(\"<summarization_query>\")\n```\n\n## Queries over Structured Data\n\nLlamaIndex supports queries over structured data, whether that's a Pandas DataFrame or a SQL Database.\n\nHere are some relevant resources:\n\n**Tutorials**\n\n- [Guide on Text-to-SQL](structured_data.md)\n\n**Guides**\n\n- [SQL Guide (Core)](../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb))\n- [Pandas Demo](../../examples/query_engine/pandas_query_engine.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs../../examples/query_engine/pandas_query_engine.ipynb))\n\n## Routing over Heterogeneous Data\n\nLlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \"route\" a query to an\nunderlying Document or a sub-index.\n\nTo do this, first build the sub-indices over different data sources.\nThen construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.\n\n```python\nfrom llama_index.core import TreeIndex, VectorStoreIndex\nfrom llama_index.core.tools import QueryEngineTool\n\n...\n\n# define sub-indices\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\n# define query engines and tools\ntool1 = QueryEngineTool.from_defaults(\n    query_engine=index1.as_query_engine(),\n    description=\"Use this query engine to do...\",\n)\ntool2 = QueryEngineTool.from_defaults(\n    query_engine=index2.as_query_engine(),\n    description=\"Use this query engine for something else...\",\n)\n```\n\nThen, we define a `RouterQueryEngine` over them.\nBy default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.\n\n```python\nfrom llama_index.core.query_engine import RouterQueryEngine\n\nquery_engine = RouterQueryEngine.from_defaults(\n    query_engine_tools=[tool1, tool2]\n)\n\nresponse = query_engine.query(\n    \"In Notion, give me a summary of the product roadmap.\"\n)\n```\n\n**Guides**\n\n- [Router Query Engine Guide](../../examples/query_engine/RouterQueryEngine.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs../../examples/query_engine/RouterQueryEngine.ipynb))\n\n## Compare/Contrast Queries\n\nYou can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.\n\n```python\nfrom llama_index.core.query.query_transform.base import DecomposeQueryTransform\n\ndecompose_transform = DecomposeQueryTransform(\n    service_context.llm, verbose=True\n)\n```\n\nThis module will help break down a complex query into a simpler one over your existing index structure.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddcbf9d6-45c0-47ee-a6d5-728ae4a82244": {"__data__": {"id_": "ddcbf9d6-45c0-47ee-a6d5-728ae4a82244", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "643a6616d589550cad7e3db6883dcbea8df513de", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "9f2bb260af6c2959404f22b7783dc5173652780a793eb4b0474003cb58c82b6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fcbccc4-1ac1-4ed9-bb9a-25017ad1ab19", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "25ff51ecb94542dbbcf5a4c03b55b49edd9e61a97b8036253754db962c357d2e", "class_name": "RelatedNodeInfo"}}, "text": "**Guides**\n\n- [Query Transformations](../../optimizing/advanced_retrieval/query_transformations.md)\n\nYou can also rely on the LLM to _infer_ whether to perform compare/contrast queries (see Multi Document Queries below).\n\n## Multi Document Queries\n\nBesides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well.\nIt can do this through our `SubQuestionQueryEngine` class. Given a query, this query engine will generate a \"query plan\" containing\nsub-queries against sub-documents before synthesizing the final answer.\n\nTo do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):\n\n```python\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sept_engine,\n        metadata=ToolMetadata(\n            name=\"sept_22\",\n            description=\"Provides information about Uber quarterly financials ending September 2022\",\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=june_engine,\n        metadata=ToolMetadata(\n            name=\"june_22\",\n            description=\"Provides information about Uber quarterly financials ending June 2022\",\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=march_engine,\n        metadata=ToolMetadata(\n            name=\"march_22\",\n            description=\"Provides information about Uber quarterly financials ending March 2022\",\n        ),\n    ),\n]\n```\n\nThen, we define a `SubQuestionQueryEngine` over these tools:\n\n```python\nfrom llama_index.core.query_engine import SubQuestionQueryEngine\n\nquery_engine = SubQuestionQueryEngine.from_defaults(\n    query_engine_tools=query_engine_tools\n)\n```\n\nThis query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer.\nThis makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.\n\n**Guides**\n\n- [Sub Question Query Engine (Intro)](../../examples/query_engine/sub_question_query_engine.ipynb)\n- [10Q Analysis (Uber)](../../examples/usecases/10q_sub_question.ipynb)\n- [10K Analysis (Uber and Lyft)](../../examples/usecases/10k_sub_question.ipynb)\n\n## Multi-Step Queries\n\nLlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions,\nand sequentially generate subquestions based on returned answers until the final answer is returned.\n\nFor instance, given a question \"Who was in the first batch of the accelerator program the author started?\",\nthe module will first decompose the query into a simpler initial question \"What was the accelerator program the author started?\",\nquery the index, and then ask followup questions.\n\n**Guides**\n\n- [Query Transformations](../../optimizing/advanced_retrieval/query_transformations.md)\n- [Multi-Step Query Decomposition](../../examples/query_transformations/HyDEQueryTransformDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\n\n## Temporal Queries\n\nLlamaIndex can support queries that require an understanding of time. It can do this in two ways:\n\n- Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question.\n- Sort by recency and filter outdated context.\n\n**Guides**\n\n- [Postprocessing Guide](../../module_guides/querying/node_postprocessors/node_postprocessors.md)\n- [Prev/Next Postprocessing](../../examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb)\n- [Recency Postprocessing](../../examples/node_postprocessor/RecencyPostprocessorDemo.ipynb)\n\n## Additional Resources\n\n- [A Guide to Extracting Terms and Definitions](q_and_a/terms_definitions_tutorial.md)\n- [SEC 10k Analysis](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d)", "mimetype": "text/plain", "start_char_idx": 4416, "end_char_idx": 8480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "409d0cba-7e87-4ae9-a286-83aa1cf69abc": {"__data__": {"id_": "409d0cba-7e87-4ae9-a286-83aa1cf69abc", "embedding": null, "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a", "node_type": "4", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "4adb61017eb8959f761ecf33c3d4784b0e397ec7337bad94dae768b73d5ea068", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f96ed7e3-7f7f-4859-a2e0-82b36621c1f7", "node_type": "1", "metadata": {}, "hash": "5b2aa374c069f4fbfc9d80922fc1ad607e74f33dd86dc584b45a3ca5b65994e6", "class_name": "RelatedNodeInfo"}}, "text": "# A Guide to Extracting Terms and Definitions\n\nLlama Index has many use cases (semantic search, summarization, etc.) that are well documented. However, this doesn't mean we can't apply Llama Index to very specific use cases!\n\nIn this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later. Using [Streamlit](https://streamlit.io/), we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.\n\nThis tutorial assumes you have Python3.9+ and the following packages installed:\n\n- llama-index\n- streamlit\n\nAt the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions. The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.\n\nThe final version of this tutorial can be found [here](https://github.com/abdulasiraj/A-Guide-to-Extracting-Terms-and-Definitions) and a live hosted demo is available on [Huggingface Spaces](https://huggingface.co/spaces/Nobody4591/Llama_Index_Term_Extractor).\n\n## Uploading Text\n\nStep one is giving users a way to input text manually. Let\u2019s write some code using Streamlit to provide the interface for this! Use the following code and launch the app with `streamlit run app.py`.\n\n```python\nimport streamlit as st\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\ndocument_text = st.text_area(\"Enter raw text\")\nif st.button(\"Extract Terms and Definitions\") and document_text:\n    with st.spinner(\"Extracting...\"):\n        extracted_terms = document_text  # this is a placeholder!\n    st.write(extracted_terms)\n```\n\nSuper simple right! But you'll notice that the app doesn't do anything useful yet. To use llama_index, we also need to setup our OpenAI LLM. There are a bunch of possible settings for the LLM, so we can let the user figure out what's best. We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).\n\n## LLM Settings\n\nThis next step introduces some tabs to our app, to separate it into different panes that provide different features. Let's create a tab for LLM settings and for uploading text:\n\n```python\nimport os\nimport streamlit as st\n\nDEFAULT_TERM_STR = (\n    \"Make a list of terms and definitions that are defined in the context, \"\n    \"with one pair on each line. \"\n    \"If a term is missing it's definition, use your best judgment. \"\n    \"Write each line as as follows:\\nTerm: <term> Definition: <definition>\"\n)\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\nsetup_tab, upload_tab = st.tabs([\"Setup\", \"Upload/Extract Terms\"])\n\nwith setup_tab:\n    st.subheader(\"LLM Setup\")\n    api_key = st.text_input(\"Enter your OpenAI API key here\", type=\"password\")\n    llm_name = st.selectbox(\"Which LLM?\", [\"gpt-3.5-turbo\", \"gpt-4\"])\n    model_temperature = st.slider(\n        \"LLM Temperature\", min_value=0.0, max_value=1.0, step=0.1\n    )\n    term_extract_str = st.text_area(\n        \"The query to extract terms and definitions with.\",\n        value=DEFAULT_TERM_STR,\n    )\n\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document_text = st.text_area(\"Enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document_text:\n        with st.spinner(\"Extracting...\"):\n            extracted_terms = document_text  # this is a placeholder!\n        st.write(extracted_terms)\n```\n\nNow our app has two tabs, which really helps with the organization. You'll also noticed I added a default prompt to extract terms -- you can change this later once you try extracting some terms, it's just the prompt I arrived at after experimenting a bit.\n\nSpeaking of extracting terms, it's time to add some functions to do just that!\n\n## Extracting and Storing Terms\n\nNow that we are able to define LLM settings and input text, we can try using Llama Index to extract the terms from text for us!\n\nWe can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.\n\n```python\nfrom llama_index.core import Document, SummaryIndex, load_index_from_storage\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f96ed7e3-7f7f-4859-a2e0-82b36621c1f7": {"__data__": {"id_": "f96ed7e3-7f7f-4859-a2e0-82b36621c1f7", "embedding": null, "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a", "node_type": "4", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "4adb61017eb8959f761ecf33c3d4784b0e397ec7337bad94dae768b73d5ea068", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "409d0cba-7e87-4ae9-a286-83aa1cf69abc", "node_type": "1", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "58d17e104c9716f8a84cf731d8949c0739033a10f696efe7f2bed6d6723d45d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb56746a-c810-4001-97e2-a985b07c1fdd", "node_type": "1", "metadata": {}, "hash": "ed3642b9d48b1d38848ca420dfc297145a60a080a879548c3eb99ad8b12556df", "class_name": "RelatedNodeInfo"}}, "text": "def get_llm(llm_name, model_temperature, api_key, max_tokens=256):\n    os.environ[\"OPENAI_API_KEY\"] = api_key\n    return OpenAI(\n        temperature=model_temperature, model=llm_name, max_tokens=max_tokens\n    )\n\n\ndef extract_terms(\n    documents, term_extract_str, llm_name, model_temperature, api_key\n):\n    llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)\n\n    temp_index = SummaryIndex.from_documents(\n        documents,\n    )\n    query_engine = temp_index.as_query_engine(\n        response_mode=\"tree_summarize\", llm=llm\n    )\n    terms_definitions = str(query_engine.query(term_extract_str))\n    terms_definitions = [\n        x\n        for x in terms_definitions.split(\"\\n\")\n        if x and \"Term:\" in x and \"Definition:\" in x\n    ]\n    # parse the text into a dict\n    terms_to_definition = {\n        x.split(\"Definition:\")[0]\n        .split(\"Term:\")[-1]\n        .strip(): x.split(\"Definition:\")[-1]\n        .strip()\n        for x in terms_definitions\n    }\n    return terms_to_definition\n```\n\nNow, using the new functions, we can finally extract our terms!\n\n```python\n...\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document_text = st.text_area(\"Enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document_text:\n        with st.spinner(\"Extracting...\"):\n            extracted_terms = extract_terms(\n                [Document(text=document_text)],\n                term_extract_str,\n                llm_name,\n                model_temperature,\n                api_key,\n            )\n        st.write(extracted_terms)\n```\n\nThere's a lot going on now, let's take a moment to go over what is happening.\n\n`get_llm()` is instantiating the LLM based on the user configuration from the setup tab. Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).\n\n`extract_terms()` is where all the good stuff happens. First, we call `get_llm()` with `max_tokens=1024`, since we don't want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set). Then, we define our `Settings` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output. When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.\n\nNext, we create a temporary summary index and pass in our llm. A summary index will read every single piece of text in our index, which is perfect for extracting terms. Finally, we use our pre-defined query text to extract terms, using `response_mode=\"tree_summarize`. This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children. Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.\n\nLastly, we do some minor post processing. We assume the model followed instructions and put a term/definition pair on each line. If a line is missing the `Term:` or `Definition:` labels, we skip it. Then, we convert this to a dictionary for easy storage!\n\n## Saving Extracted Terms\n\nNow that we can extract terms, we need to put them somewhere so that we can query for them later. A `VectorStoreIndex` should be a perfect choice for now! But in addition, our app should also keep track of which terms are inserted into the index so that we can inspect them later. Using `st.session_state`, we can store the current list of terms in a session dict, unique to each user!\n\nFirst things first though, let's add a feature to initialize a global vector index and another function to insert the extracted terms.\n\n```python\nfrom llama_index.core import Settings, VectorStoreIndex\n\n...\nif \"all_terms\" not in st.session_state:\n    st.session_state[\"all_terms\"] = DEFAULT_TERMS\n...\n\n\ndef insert_terms(terms_to_definition):\n    for term, definition in terms_to_definition.items():\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\n        st.session_state[\"llama_index\"].insert(doc)\n\n\n@st.cache_resource\ndef initialize_index(llm_name, model_temperature, api_key):\n    \"\"\"Create the VectorStoreIndex object.\"\"\"\n    Settings.llm = get_llm(llm_name, model_temperature, api_key)\n\n    index = VectorStoreIndex([])\n\n    return index, llm", "mimetype": "text/plain", "start_char_idx": 4375, "end_char_idx": 8711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb56746a-c810-4001-97e2-a985b07c1fdd": {"__data__": {"id_": "cb56746a-c810-4001-97e2-a985b07c1fdd", "embedding": null, "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a", "node_type": "4", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "4adb61017eb8959f761ecf33c3d4784b0e397ec7337bad94dae768b73d5ea068", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f96ed7e3-7f7f-4859-a2e0-82b36621c1f7", "node_type": "1", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "df9d5f9b8c345bac326d3631f0a3d8d9b5d26c4c9ca2472d7386ca07291662c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58eacbce-2989-4e1f-9581-bf0328b1d5b4", "node_type": "1", "metadata": {}, "hash": "e70f81804e425a6ca5f8abf78ac5973259ca91eb971de3b56c423c427c3605d0", "class_name": "RelatedNodeInfo"}}, "text": "...\n\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = {}\n\n    if \"llama_index\" in st.session_state:\n        st.markdown(\n            \"Either upload an image/screenshot of a document, or enter the text manually.\"\n        )\n        document_text = st.text_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (\n            uploaded_file or document_text\n        ):\n            st.session_state[\"terms\"] = {}\n            terms_docs = {}\n            with st.spinner(\"Extracting...\"):\n                terms_docs.update(\n                    extract_terms(\n                        [Document(text=document_text)],\n                        term_extract_str,\n                        llm_name,\n                        model_temperature,\n                        api_key,\n                    )\n                )\n            st.session_state[\"terms\"].update(terms_docs)\n\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]:\n            st.markdown(\"Extracted terms\")\n            st.json(st.session_state[\"terms\"])\n\n            if st.button(\"Insert terms?\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert_terms(st.session_state[\"terms\"])\n                st.session_state[\"all_terms\"].update(st.session_state[\"terms\"])\n                st.session_state[\"terms\"] = {}\n                st.experimental_rerun()\n```\n\nNow you are really starting to leverage the power of streamlit! Let's start with the code under the upload tab. We added a button to initialize the vector index, and we store it in the global streamlit state dictionary, as well as resetting the currently extracted terms. Then, after extracting terms from the input text, we store it the extracted terms in the global state again and give the user a chance to review them before inserting. If the insert button is pressed, then we call our insert terms function, update our global tracking of inserted terms, and remove the most recently extracted terms from the session state.\n\n## Querying for Extracted Terms/Definitions\n\nWith the terms and definitions extracted and saved, how can we use them? And how will the user even remember what's previously been saved?? We can simply add some more tabs to the app to handle these features.\n\n```python\n...\nsetup_tab, terms_tab, upload_tab, query_tab = st.tabs(\n    [\"Setup\", \"All Terms\", \"Upload/Extract Terms\", \"Query Terms\"]\n)\n...\nwith terms_tab:\n    with terms_tab:\n        st.subheader(\"Current Extracted Terms and Definitions\")\n        st.json(st.session_state[\"all_terms\"])\n...\nwith query_tab:\n    st.subheader(\"Query for Terms/Definitions!\")\n    st.markdown(\n        (\n            \"The LLM will attempt to answer your query, and augment it's answers using the terms/definitions you've inserted. \"\n            \"If a term is not in the index, it will answer using it's internal knowledge.\"\n        )\n    )\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_2\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = {}\n\n    if \"llama_index\" in st.session_state:\n        query_text = st.text_input(\"Ask about a term or definition:\")\n        if query_text:\n            query_text = (\n                query_text\n                + \"\\nIf you can't find the answer, answer the query with the best of your knowledge.\"\n            )\n            with st.spinner(\"Generating answer...\"):\n                response = (\n                    st.session_state[\"llama_index\"]\n                    .as_query_engine(\n                        similarity_top_k=5,\n                        response_mode=\"compact\",\n                        text_qa_template=TEXT_QA_TEMPLATE,\n                        refine_template=DEFAULT_REFINE_PROMPT,\n                    )\n                    .query(query_text)\n                )\n            st.markdown(str(response))\n```\n\nWhile this is mostly basic, some important things to note:\n\n- Our initialize button has the same text as our other button. Streamlit will complain about this, so we provide a unique key instead.\n- Some additional text has been added to the query! This is to try and compensate for times when the index does not have the answer.\n- In our index query, we've specified two options:\n  - `similarity_top_k=5` means the index will fetch the top 5 closest matching terms/definitions to the query.\n  - `response_mode=\"compact\"` means as much text as possible from the 5 matching terms/definitions will be used in each LLM call. Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.", "mimetype": "text/plain", "start_char_idx": 8714, "end_char_idx": 13600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58eacbce-2989-4e1f-9581-bf0328b1d5b4": {"__data__": {"id_": "58eacbce-2989-4e1f-9581-bf0328b1d5b4", "embedding": null, "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a", "node_type": "4", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "4adb61017eb8959f761ecf33c3d4784b0e397ec7337bad94dae768b73d5ea068", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb56746a-c810-4001-97e2-a985b07c1fdd", "node_type": "1", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c12eda279d5833503c3c413d793a346eba3a2a88ab46a4b48c4aa6bbaf35e2ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec5f532d-fbfd-4be4-a8c8-48dc9cf14170", "node_type": "1", "metadata": {}, "hash": "e83284a04ee90452e7169944f0a628622ae88f89ae8bb4855bf6b751620e9cd1", "class_name": "RelatedNodeInfo"}}, "text": "Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.\n\n## Dry Run Test\n\nWell, actually I hope you've been testing as we went. But now, let's try one complete test.\n\n1. Refresh the app\n2. Enter your LLM settings\n3. Head over to the query tab\n4. Ask the following: `What is a bunnyhug?`\n5. The app should give some nonsense response. If you didn't know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies!\n6. Let's add this definition to the app. Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.`\n7. Click the extract button. After a few moments, the app should display the correctly extracted term/definition. Click the insert term button to save it!\n8. If we open the terms tab, the term and definition we just extracted should be displayed\n9. Go back to the query tab and try asking what a bunnyhug is. Now, the answer should be correct!\n\n## Improvement #1 - Create a Starting Index\n\nWith our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app's query capabilities? We can do just that! First, let's make a small change to our app so that we save the index to disk after every upload:\n\n```python\ndef insert_terms(terms_to_definition):\n    for term, definition in terms_to_definition.items():\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\n        st.session_state[\"llama_index\"].insert(doc)\n    # TEMPORARY - save to disk\n    st.session_state[\"llama_index\"].storage_context.persist()\n```\n\nNow, we need some document to extract from! The repository for this project used the wikipedia page on New York City, and you can find the text [here](https://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/data/nyc_text.txt).\n\nIf you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms. Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index! We will need them in a second.\n\nAfter inserting, remove the line of code we used to save the index to disk. With a starting index now saved, we can modify our `initialize_index` function to look like this:\n\n```python\n@st.cache_resource\ndef initialize_index(llm_name, model_temperature, api_key):\n    \"\"\"Load the Index object.\"\"\"\n    Settings.llm = get_llm(llm_name, model_temperature, api_key)\n\n    index = load_index_from_storage(storage_context)\n\n    return index\n```\n\nDid you remember to save that giant list of extracted terms in a notepad? Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:\n\n```python\n...\nif \"all_terms\" not in st.session_state:\n    st.session_state[\"all_terms\"] = DEFAULT_TERMS\n...\n```\n\nRepeat the above anywhere where we were previously resetting the `all_terms` values.\n\n## Improvement #2 - (Refining) Better Prompts\n\nIf you play around with the app a bit now, you might notice that it stopped following our prompt! Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge. But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.\n\nThis is due to the concept of \"refining\" answers in Llama Index. Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt! OpenAI models typically have a max input size of 4097 tokens. So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt. After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.\n\nSo, the refine process seems to be messing with our results! Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts!", "mimetype": "text/plain", "start_char_idx": 13496, "end_char_idx": 17649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec5f532d-fbfd-4be4-a8c8-48dc9cf14170": {"__data__": {"id_": "ec5f532d-fbfd-4be4-a8c8-48dc9cf14170", "embedding": null, "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a", "node_type": "4", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "4adb61017eb8959f761ecf33c3d4784b0e397ec7337bad94dae768b73d5ea068", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58eacbce-2989-4e1f-9581-bf0328b1d5b4", "node_type": "1", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "9dbab604c08221ec9adbd2d874009d89ac3c6c5218d3c2a39081ff8c796aca75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71bb5190-fd8f-4cdc-9530-39076b17a983", "node_type": "1", "metadata": {}, "hash": "b1ab005eee1dabe86772c3d25c1816d1c48b4f4876092c05dd4a708652a188ff", "class_name": "RelatedNodeInfo"}}, "text": "Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts! Let's create those now, using the [default prompts](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py) and [chat specific prompts](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py) as a guide. Using a new file `constants.py`, let's create some new query templates:\n\n```python\nfrom llama_index.core import (\n    PromptTemplate,\n    SelectorPromptTemplate,\n    ChatPromptTemplate,\n)\nfrom llama_index.core.prompts.utils import is_chat_model\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\n# Text QA templates\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information answer the following question \"\n    \"(if you don't know the answer, use the best of your knowledge): {query_str}\\n\"\n)\nTEXT_QA_TEMPLATE = PromptTemplate(DEFAULT_TEXT_QA_PROMPT_TMPL)\n\n# Refine templates\nDEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n    \"If you can't improve the existing answer, just repeat it again.\"\n)\nDEFAULT_REFINE_PROMPT = PromptTemplate(DEFAULT_REFINE_PROMPT_TMPL)\n\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\n    ChatMessage(content=\"{query_str}\", role=MessageRole.USER),\n    ChatMessage(content=\"{existing_answer}\", role=MessageRole.ASSISTANT),\n    ChatMessage(\n        content=\"We have the opportunity to refine the above answer \"\n        \"(only if needed) with some more context below.\\n\"\n        \"------------\\n\"\n        \"{context_msg}\\n\"\n        \"------------\\n\"\n        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n        \"If you can't improve the existing answer, just repeat it again.\",\n        role=MessageRole.USER,\n    ),\n]\n\nCHAT_REFINE_PROMPT = ChatPromptTemplate(CHAT_REFINE_PROMPT_TMPL_MSGS)\n\n# refine prompt selector\nREFINE_TEMPLATE = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_PROMPT,\n    conditionals=[(is_chat_model, CHAT_REFINE_PROMPT)],\n)\n```\n\nThat seems like a lot of code, but it's not too bad! If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models. Continuing that trend, we do the same for our custom prompts. Then, using a prompt selector, we can combine both prompts into a single object. If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used. Otherwise, use the normal prompt templates.\n\nAnother thing to note is that we only defined one QA template. In a chat model, this will be converted to a single \"human\" message.\n\nSo, now we can import these prompts into our app and use them during the query.\n\n```python\nfrom constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE\n\n...\nif \"llama_index\" in st.session_state:\n    query_text = st.text_input(\"Ask about a term or definition:\")\n    if query_text:\n        query_text = query_text  # Notice we removed the old instructions\n        with st.spinner(\"Generating answer...\"):\n            response = (\n                st.session_state[\"llama_index\"]\n                .as_query_engine(\n                    similarity_top_k=5,\n                    response_mode=\"compact\",\n                    text_qa_template=TEXT_QA_TEMPLATE,\n                    refine_template=DEFAULT_REFINE_PROMPT,\n                )\n                .query(query_text)\n            )\n        st.markdown(str(response))\n...\n```\n\nIf you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!\n\n## Improvement #3 - Image Support\n\nLlama index also supports images! Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text. We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.", "mimetype": "text/plain", "start_char_idx": 17516, "end_char_idx": 21904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71bb5190-fd8f-4cdc-9530-39076b17a983": {"__data__": {"id_": "71bb5190-fd8f-4cdc-9530-39076b17a983", "embedding": null, "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a", "node_type": "4", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "4adb61017eb8959f761ecf33c3d4784b0e397ec7337bad94dae768b73d5ea068", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec5f532d-fbfd-4be4-a8c8-48dc9cf14170", "node_type": "1", "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "813c2614ac9144596d01ce5e164590c1369d3a4617cb1797bfbfe2616546f12e", "class_name": "RelatedNodeInfo"}}, "text": "We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.\n\nIf you get an import error about PIL, install it using `pip install Pillow` first.\n\n```python\nfrom PIL import Image\nfrom llama_index.readers.file import ImageReader\n\n\n@st.cache_resource\ndef get_file_extractor():\n    image_parser = ImageReader(keep_image=True, parse_text=True)\n    file_extractor = {\n        \".jpg\": image_parser,\n        \".png\": image_parser,\n        \".jpeg\": image_parser,\n    }\n    return file_extractor\n\n\nfile_extractor = get_file_extractor()\n...\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_1\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = DEFAULT_TERMS\n\n    if \"llama_index\" in st.session_state:\n        st.markdown(\n            \"Either upload an image/screenshot of a document, or enter the text manually.\"\n        )\n        uploaded_file = st.file_uploader(\n            \"Upload an image/screenshot of a document:\",\n            type=[\"png\", \"jpg\", \"jpeg\"],\n        )\n        document_text = st.text_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (\n            uploaded_file or document_text\n        ):\n            st.session_state[\"terms\"] = {}\n            terms_docs = {}\n            with st.spinner(\"Extracting (images may be slow)...\"):\n                if document_text:\n                    terms_docs.update(\n                        extract_terms(\n                            [Document(text=document_text)],\n                            term_extract_str,\n                            llm_name,\n                            model_temperature,\n                            api_key,\n                        )\n                    )\n                if uploaded_file:\n                    Image.open(uploaded_file).convert(\"RGB\").save(\"temp.png\")\n                    img_reader = SimpleDirectoryReader(\n                        input_files=[\"temp.png\"], file_extractor=file_extractor\n                    )\n                    img_docs = img_reader.load_data()\n                    os.remove(\"temp.png\")\n                    terms_docs.update(\n                        extract_terms(\n                            img_docs,\n                            term_extract_str,\n                            llm_name,\n                            model_temperature,\n                            api_key,\n                        )\n                    )\n            st.session_state[\"terms\"].update(terms_docs)\n\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]:\n            st.markdown(\"Extracted terms\")\n            st.json(st.session_state[\"terms\"])\n\n            if st.button(\"Insert terms?\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert_terms(st.session_state[\"terms\"])\n                st.session_state[\"all_terms\"].update(st.session_state[\"terms\"])\n                st.session_state[\"terms\"] = {}\n                st.experimental_rerun()\n```\n\nHere, we added the option to upload a file using Streamlit. Then the image is opened and saved to disk (this seems hacky but it keeps things simple). Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.\n\nNow that we have the documents, we can call `extract_terms()` the same as before.\n\n## Conclusion/TLDR\n\nIn this tutorial, we covered a ton of information, while solving some common issues and problems along the way:\n\n- Using different indexes for different use cases (List vs. Vector index)\n- Storing global state values with Streamlit's `session_state` concept\n- Customizing internal prompts with Llama Index\n- Reading text from images with Llama Index\n\nThe final version of this tutorial can be found [here](https://github.com/abdulasiraj/A-Guide-to-Extracting-Terms-and-Definitions) and a live hosted demo is available on [Huggingface Spaces](https://huggingface.co/spaces/Nobody4591/Llama_Index_Term_Extractor).", "mimetype": "text/plain", "start_char_idx": 21783, "end_char_idx": 25912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fea0e53a-241e-4fc0-a2a6-ab1932e636a0": {"__data__": {"id_": "fea0e53a-241e-4fc0-a2a6-ab1932e636a0", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7befaf646f70f91314c80d8097a8d148fe1015a1", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d7200691def01ce3a568eb0b2cf3b7e6ec672529faaed51a056f73a2e0db4d75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9b30e7b-2038-4bb2-966a-ec443e335c35", "node_type": "1", "metadata": {}, "hash": "e7aec750adc1a622e47f81d8816be9f3ebc5341fe79e5262c84dcd0f4c78a909", "class_name": "RelatedNodeInfo"}}, "text": "# Structured Data\n\n# A Guide to LlamaIndex + Structured Data\n\nA lot of modern data systems depend on structured data, such as a Postgres DB or a Snowflake data warehouse.\nLlamaIndex provides a lot of advanced features, powered by LLM's, to both create structured data from\nunstructured data, as well as analyze this structured data through augmented text-to-SQL capabilities.\n\n**NOTE:** Any Text-to-SQL application should be aware that executing\narbitrary SQL queries can be a security risk. It is recommended to\ntake precautions as needed, such as using restricted roles, read-only\ndatabases, sandboxing, etc.\n\nThis guide helps walk through each of these capabilities. Specifically, we cover the following topics:\n\n- **Setup**: Defining up our example SQL Table.\n- **Building our Table Index**: How to go from sql database to a Table Schema Index\n- **Using natural language SQL queries**: How to query our SQL database using natural language.\n\nWe will walk through a toy example table which contains city/population/country information.\nA notebook for this tutorial is [available here](../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb).\n\n## Setup\n\nFirst, we use SQLAlchemy to setup a simple sqlite db:\n\n```python\nfrom sqlalchemy import (\n    create_engine,\n    MetaData,\n    Table,\n    Column,\n    String,\n    Integer,\n    select,\n    column,\n)\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n```\n\nWe then create a toy `city_stats` table:\n\n```python\n# create city SQL table\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all(engine)\n```\n\nNow it's time to insert some datapoints!\n\nIf you want to look into filling into this table by inferring structured datapoints\nfrom unstructured data, take a look at the below section. Otherwise, you can choose\nto directly populate this table:\n\n```python\nfrom sqlalchemy import insert\n\nrows = [\n    {\"city_name\": \"Toronto\", \"population\": 2731571, \"country\": \"Canada\"},\n    {\"city_name\": \"Tokyo\", \"population\": 13929286, \"country\": \"Japan\"},\n    {\"city_name\": \"Berlin\", \"population\": 600000, \"country\": \"Germany\"},\n]\nfor row in rows:\n    stmt = insert(city_stats_table).values(**row)\n    with engine.begin() as connection:\n        cursor = connection.execute(stmt)\n```\n\nFinally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper;\nthis allows the db to be used within LlamaIndex:\n\n```python\nfrom llama_index.core import SQLDatabase\n\nsql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n```\n\n## Natural language SQL\n\nOnce we have constructed our SQL database, we can use the NLSQLTableQueryEngine to\nconstruct natural language queries that are synthesized into SQL queries.\n\nNote that we need to specify the tables we want to use with this query engine.\nIf we don't the query engine will pull all the schema context, which could\noverflow the context window of the LLM.\n\n```python\nfrom llama_index.core.query_engine import NLSQLTableQueryEngine\n\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"city_stats\"],\n)\nquery_str = \"Which city has the highest population?\"\nresponse = query_engine.query(query_str)\n```\n\nThis query engine should used in any case where you can specify the tables you want\nto query over beforehand, or the total size of all the table schema plus the rest of\nthe prompt fits your context window.\n\n## Building our Table Index\n\nIf we don't know ahead of time which table we would like to use, and the total size of\nthe table schema overflows your context window size, we should store the table schema\nin an index so that during query time we can retrieve the right schema.\n\nThe way we can do this is using the SQLTableNodeMapping object, which takes in a\nSQLDatabase and produces a Node object for each SQLTableSchema object passed\ninto the ObjectIndex constructor.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9b30e7b-2038-4bb2-966a-ec443e335c35": {"__data__": {"id_": "c9b30e7b-2038-4bb2-966a-ec443e335c35", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7befaf646f70f91314c80d8097a8d148fe1015a1", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "d7200691def01ce3a568eb0b2cf3b7e6ec672529faaed51a056f73a2e0db4d75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fea0e53a-241e-4fc0-a2a6-ab1932e636a0", "node_type": "1", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "1a83c445ee17e6b6fc6f33d2d81be5be10dae4cccd0be1676b0575b1178bd52b", "class_name": "RelatedNodeInfo"}}, "text": "```python\nfrom llama_index.core.objects import (\n    SQLTableNodeMapping,\n    ObjectIndex,\n    SQLTableSchema,\n)\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [\n    (SQLTableSchema(table_name=\"city_stats\")),\n    ...,\n]  # one SQLTableSchema for each table\n\nobj_index = ObjectIndex.from_objects(\n    table_schema_objs,\n    table_node_mapping,\n    VectorStoreIndex,\n)\n```\n\nHere you can see we define our table_node_mapping, and a single SQLTableSchema with the\n\"city_stats\" table name. We pass these into the ObjectIndex constructor, along with the\nVectorStoreIndex class definition we want to use. This will give us a VectorStoreIndex where\neach Node contains table schema and other context information. You can also add any additional\ncontext information you'd like.\n\n```python\n# manually set extra context text\ncity_stats_text = (\n    \"This table gives information regarding the population and country of a given city.\\n\"\n    \"The user will query with codewords, where 'foo' corresponds to population and 'bar'\"\n    \"corresponds to city.\"\n)\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [\n    (SQLTableSchema(table_name=\"city_stats\", context_str=city_stats_text))\n]\n```\n\n## Using natural language SQL queries\n\nOnce we have defined our table schema index obj_index, we can construct a SQLTableRetrieverQueryEngine\nby passing in our SQLDatabase, and a retriever constructed from our object index.\n\n```python\nfrom llama_index.core.indices.struct_store import SQLTableRetrieverQueryEngine\n\nquery_engine = SQLTableRetrieverQueryEngine(\n    sql_database, obj_index.as_retriever(similarity_top_k=1)\n)\nresponse = query_engine.query(\"Which city has the highest population?\")\nprint(response)\n```\n\nNow when we query the retriever query engine, it will retrieve the relevant table schema\nand synthesize a SQL query and a response from the results of that query.\n\n## Concluding Thoughts\n\nThis is it for now! We're constantly looking for ways to improve our structured data support.\nIf you have any questions let us know in [our Discord](https://discord.gg/dGcwcsnxhU).\n\nRelevant Resources:\n\n- [Airbyte SQL Index Guide](./structured_data/Airbyte_demo.ipynb)", "mimetype": "text/plain", "start_char_idx": 4018, "end_char_idx": 6222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f606be89-b4a0-42bb-bb34-d9c433cb21aa": {"__data__": {"id_": "f606be89-b4a0-42bb-bb34-d9c433cb21aa", "embedding": null, "metadata": {"filename": "querying.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5fb0bbdb8aa45dde964666f84cfa8fea01418b6c", "node_type": "4", "metadata": {"filename": "querying.md", "author": "LlamaIndex"}, "hash": "cbebef8b44ca6647de7c301f73da28da070d918330ddf98e157e450cf93fcb1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "305cd500-0951-4741-9bf9-0928cb7d0e97", "node_type": "1", "metadata": {}, "hash": "4678057ec1b4df6f62e498218fb8349ee1ad5576ee46817f3a4a3ebb80103440", "class_name": "RelatedNodeInfo"}}, "text": "# Querying\n\nNow you've loaded your data, built an index, and stored that index for later, you're ready to get to the most significant part of an LLM application: querying.\n\nAt its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\n\nMore complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components.\n\n## Getting started\n\nThe basis of all querying is the `QueryEngine`. The simplest way to get a QueryEngine is to get your index to create one for you, like this:\n\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\n    \"Write an email to the user given their background information.\"\n)\nprint(response)\n```\n\n## Stages of querying\n\nHowever, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\n\n- **Retrieval** is when you find and return the most relevant documents for your query from your `Index`. As previously discussed in [indexing](../indexing/indexing.md), the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies.\n- **Postprocessing** is when the `Node`s retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\n- **Response synthesis** is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response.\n\n!!! tip\n    You can find out about [how to attach metadata to documents](../../module_guides/loading/documents_and_nodes/usage_documents.md) and [nodes](../../module_guides/loading/documents_and_nodes/usage_nodes.md).\n\n## Customizing the stages of querying\n\nLlamaIndex features a low-level composition API that gives you granular control over your querying.\n\nIn this example, we customize our retriever to use a different number for `top_k` and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant.\n\n```python\nfrom llama_index.core import VectorStoreIndex, get_response_synthesizer\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=10,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer()\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n)\n\n# query\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\nYou can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\n\nFor a full list of implemented components and the supported configurations, check out our [reference docs](../../api_reference/index.md).\n\nLet's go into more detail about customizing each step:\n\n### Configuring retriever\n\n```python\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=10,\n)\n```\n\nThere are a huge variety of retrievers that you can learn about in our [module guide on retrievers](../../module_guides/querying/retriever/index.md).\n\n### Configuring node postprocessors\n\nWe support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects.\nThis can help reduce the time/number of LLM calls/cost or improve response quality.\n\nFor example:\n\n- `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`.\n- `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n- `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.\n\nThe full list of node postprocessors is documented in the [Node Postprocessor Reference](../../api_reference/postprocessor/index.md).\n\nTo configure the desired node postprocessors:\n\n```python\nnode_postprocessors = [\n    KeywordNodePostprocessor(\n        required_keywords=[\"Combinator\"], exclude_keywords=[\"Italy\"]\n    )\n]\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=node_postprocessors\n)\nresponse = query_engine.query(\"What did the author do growing up?\")", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "305cd500-0951-4741-9bf9-0928cb7d0e97": {"__data__": {"id_": "305cd500-0951-4741-9bf9-0928cb7d0e97", "embedding": null, "metadata": {"filename": "querying.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5fb0bbdb8aa45dde964666f84cfa8fea01418b6c", "node_type": "4", "metadata": {"filename": "querying.md", "author": "LlamaIndex"}, "hash": "cbebef8b44ca6647de7c301f73da28da070d918330ddf98e157e450cf93fcb1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f606be89-b4a0-42bb-bb34-d9c433cb21aa", "node_type": "1", "metadata": {"filename": "querying.md", "author": "LlamaIndex"}, "hash": "226da76f6c24f0f659b05e964c6292b54b749118fddccee7a0c81069f21d1f48", "class_name": "RelatedNodeInfo"}}, "text": "```\n\n### Configuring response synthesis\n\nAfter a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.\n\nYou can configure it via\n\n```python\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever, response_mode=response_mode\n)\n```\n\nRight now, we support the following options:\n\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`;\n  This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact`: \"compact\" the prompt during each LLM call by stuffing as\n  many `Node` text chunks that can fit within the maximum prompt size. If there are\n  too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n  multiple prompts.\n- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree\n  and return the root node as the response. Good for summarization purposes.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,\n  without actually sending them. Then can be inspected by checking `response.source_nodes`.\n  The response object is covered in more detail in Section 5.\n- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n  chunk while accumulating the responses into an array. Returns a concatenated string of all\n  responses. Good for when you need to run the same query separately against each text\n  chunk.\n\n## Structured Outputs\n\nYou may want to ensure your output is structured. See our [Query Engines + Pydantic Outputs](../../module_guides/querying/structured_outputs/query_engine.md) to see how to extract a Pydantic object from a query engine class.\n\nAlso make sure to check out our entire [Structured Outputs](../../module_guides/querying/structured_outputs/index.md) guide.\n\n## Creating your own Query Pipeline\n\nIf you want to design complex query flows, you can compose your own query pipeline across many different modules, from prompts/LLMs/output parsers to retrievers to response synthesizers to your own custom components.\n\nTake a look at our [Query Pipelines Module Guide](../../module_guides/querying/pipeline/index.md) for more details.", "mimetype": "text/plain", "start_char_idx": 4774, "end_char_idx": 7003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e27520ab-6cc1-4379-a9e3-810e27aa43d4": {"__data__": {"id_": "e27520ab-6cc1-4379-a9e3-810e27aa43d4", "embedding": null, "metadata": {"filename": "retrieval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06dc6f52bf6a6823f17a22e22673134108d60673", "node_type": "4", "metadata": {"filename": "retrieval.md", "author": "LlamaIndex"}, "hash": "a41525e07c3e49b3124220dc4300b9f07cb33c22b894983e8e333f0f9cfc5835", "class_name": "RelatedNodeInfo"}}, "text": "# Retrieval & Postprocessing\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 34, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66e4ab5e-7c95-4c3c-b965-86bc1092396f": {"__data__": {"id_": "66e4ab5e-7c95-4c3c-b965-86bc1092396f", "embedding": null, "metadata": {"filename": "storing.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c969568f721b247d9defebb6ae03a5c5d8ff92a", "node_type": "4", "metadata": {"filename": "storing.md", "author": "LlamaIndex"}, "hash": "46476089965a6cf762667c41940a1c919c1fb6cf3bcc09809b729ab3c4687109", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c876828-10e0-45b5-b7de-28cb190564b2", "node_type": "1", "metadata": {}, "hash": "6781e5a855475e20bb3921e3d1bb1b4a12a47f44c2b6ad6c1d2a7944eab72995", "class_name": "RelatedNodeInfo"}}, "text": "# Storing\n\nOnce you have data [loaded](../loading/loading.md) and [indexed](../indexing/indexing.md), you will probably want to store it to avoid the time and cost of re-indexing it. By default, your indexed data is stored only in memory.\n\n## Persisting to disk\n\nThe simplest way to store your indexed data is to use the built-in `.persist()` method of every Index, which writes all the data to disk at the location specified. This works for any type of index.\n\n```python\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n```\n\nHere is an example of a Composable Graph:\n\n```python\ngraph.root_index.storage_context.persist(persist_dir=\"<persist_dir>\")\n```\n\nYou can then avoid re-loading and re-indexing your data by loading the persisted index like this:\n\n```python\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n\n# load index\nindex = load_index_from_storage(storage_context)\n```\n\n!!! tip\n    Important: if you had initialized your index with a custom `transformations`, `embed_model`, etc., you will need to pass in the same options during `load_index_from_storage`, or have it set as the [global settings](../../module_guides/supporting_modules/settings.md).\n\n## Using Vector Stores\n\nAs discussed in [indexing](../indexing/indexing.md), one of the most common types of Index is the VectorStoreIndex. The API calls to create the {ref}`embeddings <what-is-an-embedding>` in a VectorStoreIndex can be expensive in terms of time and money, so you will want to store them to avoid having to constantly re-index things.\n\nLlamaIndex supports a [huge number of vector stores](../../module_guides/storing/vector_stores.md) which vary in architecture, complexity and cost. In this example we'll be using Chroma, an open-source vector store.\n\nFirst you will need to install chroma:\n\n```\npip install chromadb\n```\n\nTo use Chroma to store the embeddings from a VectorStoreIndex, you need to:\n\n- initialize the Chroma client\n- create a Collection to store your data in Chroma\n- assign Chroma as the `vector_store` in a `StorageContext`\n- initialize your VectorStoreIndex using that StorageContext\n\nHere's what that looks like, with a sneak peek at actually querying the data:\n\n```python\nimport chromadb\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nfrom llama_index.core import StorageContext\n\n# load some documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# initialize client, setting path to save data\ndb = chromadb.PersistentClient(path=\"./chroma_db\")\n\n# create collection\nchroma_collection = db.get_or_create_collection(\"quickstart\")\n\n# assign chroma as the vector_store to the context\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# create your index\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n\n# create a query engine and query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is the meaning of life?\")\nprint(response)\n```\n\nIf you've already created and stored your embeddings, you'll want to load them directly without loading your documents or creating a new VectorStoreIndex:\n\n```python\nimport chromadb\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nfrom llama_index.core import StorageContext\n\n# initialize client\ndb = chromadb.PersistentClient(path=\"./chroma_db\")\n\n# get collection\nchroma_collection = db.get_or_create_collection(\"quickstart\")\n\n# assign chroma as the vector_store to the context\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# load your index from stored vectors\nindex = VectorStoreIndex.from_vector_store(\n    vector_store, storage_context=storage_context\n)\n\n# create a query engine\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is llama2?\")\nprint(response)\n```\n\n!!! tip\n    We have a [more thorough example of using Chroma](../../examples/vector_stores/ChromaIndexDemo.ipynb) if you want to go deeper on this store.\n\n### You're ready to query!\n\nNow you have loaded data, indexed it, and stored that index, you're ready to [query your data](../querying/querying.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c876828-10e0-45b5-b7de-28cb190564b2": {"__data__": {"id_": "7c876828-10e0-45b5-b7de-28cb190564b2", "embedding": null, "metadata": {"filename": "storing.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c969568f721b247d9defebb6ae03a5c5d8ff92a", "node_type": "4", "metadata": {"filename": "storing.md", "author": "LlamaIndex"}, "hash": "46476089965a6cf762667c41940a1c919c1fb6cf3bcc09809b729ab3c4687109", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66e4ab5e-7c95-4c3c-b965-86bc1092396f", "node_type": "1", "metadata": {"filename": "storing.md", "author": "LlamaIndex"}, "hash": "3f94324295b6d3909c3982fb1a975e10d4f720b05f14bb8f2d7f04c5653e58c9", "class_name": "RelatedNodeInfo"}}, "text": "Now you have loaded data, indexed it, and stored that index, you're ready to [query your data](../querying/querying.md).\n\n## Inserting Documents or Nodes\n\nIf you've already created an index, you can add new documents to your index using the `insert` method.\n\n```python\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex([])\nfor doc in documents:\n    index.insert(doc)\n```\n\nSee the [document management how-to](../../module_guides/indexing/document_management.md) for more details on managing documents and an example notebook.", "mimetype": "text/plain", "start_char_idx": 4358, "end_char_idx": 4905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a85e69aa-3f37-46b8-8218-a3bfa369ffb2": {"__data__": {"id_": "a85e69aa-3f37-46b8-8218-a3bfa369ffb2", "embedding": null, "metadata": {"filename": "synthesis.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df7eace4860b2d8c98f8868c2f6841e161548b4e", "node_type": "4", "metadata": {"filename": "synthesis.md", "author": "LlamaIndex"}, "hash": "2ab1d025a58a2102d1c47afe2863e09ff38e87c64f60a307c366b511674ad04c", "class_name": "RelatedNodeInfo"}}, "text": "# Prompting & Synthesis\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 29, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0208844a-7048-4b01-ad10-7f2163d2f9a9": {"__data__": {"id_": "0208844a-7048-4b01-ad10-7f2163d2f9a9", "embedding": null, "metadata": {"filename": "tracing_and_debugging.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e766bd372640230aa294aea2c7fd035c4e0d195c", "node_type": "4", "metadata": {"filename": "tracing_and_debugging.md", "author": "LlamaIndex"}, "hash": "1fc54290820ad5ea0f4e38b8550c530ee3135e5f500f8609d540c805dde94ece", "class_name": "RelatedNodeInfo"}}, "text": "# Tracing and Debugging\n\nDebugging and tracing the operation of your application is key to understanding and optimizing it. LlamaIndex provides a variety of ways to do this.\n\n## Basic logging\n\nThe simplest possible way to look into what your application is doing is to turn on debug logging. That can be done anywhere in your application like this:\n\n```python\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n```\n\n## Callback handler\n\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. Using the callback manager, as many callbacks as needed can be added.\n\nIn addition to logging data related to events, you can also track the duration and number of occurrences\nof each event.\n\nFurthermore, a trace map of events is also recorded, and callbacks can use this data however they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events after most operations.\n\nYou can get a simple callback handler like this:\n\n```python\nimport llama_index.core\n\nllama_index.core.set_global_handler(\"simple\")\n```\n\nYou can also learn how to [build you own custom callback handler](../../module_guides/observability/callbacks/index.md).\n\n## Observability\n\nLlamaIndex provides **one-click observability** to allow you to build principled LLM applications in a production setting.\n\nThis feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners. Configure a variable once, and you'll be able to do things like the following:\n\n- View LLM/prompt inputs/outputs\n- Ensure that the outputs of any component (LLMs, embeddings) are performing as expected\n- View call traces for both indexing and querying\n\nTo learn more, check out our [observability docs](../../module_guides/observability/index.md)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8326b09a-f7d7-4f3f-a4c0-5297e98e487f": {"__data__": {"id_": "8326b09a-f7d7-4f3f-a4c0-5297e98e487f", "embedding": null, "metadata": {"filename": "privacy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a4d3488f00d7f794a7bffb539aa3d1148a98bd2", "node_type": "4", "metadata": {"filename": "privacy.md", "author": "LlamaIndex"}, "hash": "c0e9169097e57a123f2e577d18f35747d848ee8f66242f3f413b9c97014aebd9", "class_name": "RelatedNodeInfo"}}, "text": "# Privacy and Security\n\nBy default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\n\n## Data Privacy\n\nRegarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI's policies. And each custom service other than OpenAI has its policies as well.\n\n## Vector stores\n\nLLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how it handles or uses your data. Also by default, LLamaIndex has a default option to store your embeddings locally.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdba2273-d567-4e1d-843e-03b10bbf2b38": {"__data__": {"id_": "cdba2273-d567-4e1d-843e-03b10bbf2b38", "embedding": null, "metadata": {"filename": "using_llms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb3e042eec51f285bd52d38b82135d4bd18d0e5a", "node_type": "4", "metadata": {"filename": "using_llms.md", "author": "LlamaIndex"}, "hash": "c57d8253259d8998a137b49b2b1eda5df4e9254c672693486f91b89074b6b11d", "class_name": "RelatedNodeInfo"}}, "text": "# Using LLMs\n\n!!! tip\n    For a list of our supported LLMs and a comparison of their functionality, check out our [LLM module guide](../../module_guides/models/llms.md).\n\nOne of the first steps when building an LLM-based application is which LLM to use; you can also use more than one if you wish.\n\nLLMs are used at multiple different stages of your pipeline:\n\n- During **Indexing** you may use an LLM to determine the relevance of data (whether to index it at all) or you may use an LLM to summarize the raw data and index the summaries instead.\n- During **Querying** LLMs can be used in two ways:\n  - During **Retrieval** (fetching data from your index) LLMs can be given an array of options (such as multiple different indices) and make decisions about where best to find the information you're looking for. An agentic LLM can also use _tools_ at this stage to query different data sources.\n  - During **Response Synthesis** (turning the retrieved data into an answer) an LLM can combine answers to multiple sub-queries into a single coherent answer, or it can transform data, such as from unstructured text to JSON or another programmatic output format.\n\nLlamaIndex provides a single interface to a large number of different LLMs, allowing you to pass in any LLM you choose to any stage of the pipeline. It can be as simple as this:\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\nresponse = OpenAI().complete(\"Paul Graham is \")\nprint(response)\n```\n\nUsually, you will instantiate an LLM and pass it to `Settings`, which you then pass to other stages of the pipeline, as in this example:\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\nSettings.llm = OpenAI(temperature=0.2, model=\"gpt-4\")\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\n```\n\nIn this case, you've instantiated OpenAI and customized it to use the `gpt-4` model instead of the default `gpt-3.5-turbo`, and also modified the `temperature`. The `VectorStoreIndex` will now use gpt-4 to answer questions when querying.\n\n!!! tip\n    The `Settings` is a bundle of configuration data that you pass into different parts of LlamaIndex. You can [learn more about Settings](../../module_guides/supporting_modules/settings.md) and how to customize it.\n\n## Available LLMs\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more. Check out our [module guide to LLMs](../../module_guides/models/llms.md) for a full list, including how to run a local model.\n\n!!! tip\n    A general note on privacy and LLMs can be found on the [privacy page](./privacy.md).\n\n### Using a local LLM\n\nLlamaIndex doesn't just support hosted LLM APIs; you can also [run a local model such as Llama2 locally](https://replicate.com/blog/run-llama-locally).\n\nFor example, if you have [Ollama](https://github.com/ollama/ollama) installed and running:\n\n```python\nfrom llama_index.llms.ollama import Ollama\nfrom llama_index.core import Settings\n\nSettings.llm = Ollama(model=\"llama2\", request_timeout=60.0)\n```\n\nSee the [custom LLM's How-To](../../module_guides/models/llms/usage_custom.md) for more details.\n\n## Prompts\n\nBy default LlamaIndex comes with a great set of built-in, battle-tested prompts that handle the tricky work of getting a specific LLM to correctly handle and format data. This is one of the biggest benefits of using LlamaIndex. If you want to, you can [customize the prompts](../../module_guides/models/prompts/index.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adf8d937-1604-4169-8ad3-ee24d77d21c0": {"__data__": {"id_": "adf8d937-1604-4169-8ad3-ee24d77d21c0", "embedding": null, "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4d7ec9b218757f8f4d3e28151a50802643dab81", "node_type": "4", "metadata": {"filename": "agents.md", "author": "LlamaIndex"}, "hash": "3cdbb8ff3eeb8bcaad3c6981cef6851542ff624d8570960ead80c6c45279c6a8", "class_name": "RelatedNodeInfo"}}, "text": "# Agents\n\nAn \"agent\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing\nthat query in order to return the correct result. The key agent components can include, but are not limited to:\n\n- Breaking down a complex question into smaller ones\n- Choosing an external Tool to use + coming up with parameters for calling the Tool\n- Planning out a set of tasks\n- Storing previously completed tasks in a memory module\n\nLlamaIndex provides a comprehensive framework for building agents. This includes the following components:\n\n- Using agents with tools at a high-level to build agentic RAG and workflow automation use cases\n- Low-level components for building and debugging agents\n- Core agent ingredients that can be used as standalone modules: query planning, tool use, and more.\n\n\n## Use Cases\n\nThe scope of possible use cases for agents is vast and ever-expanding. That said, here are some practical use cases that can deliver immediate value.\n\n- **Agentic RAG**: Build a context-augmented research assistant over your data that not only answers simple questions, but complex research tasks. Here are two resources ([resource 1](../understanding/putting_it_all_together/agents.md), [resource 2](../optimizing/agentic_strategies/agentic_strategies.md)) to help you get started.\n\n- **SQL Agent**: A subset of the above is a \"text-to-SQL assistant\" that can interact with a structured database. Check out [this guide](https://docs.llamaindex.ai/en/stable/examples/agent/agent_runner/query_pipeline_agent/?h=sql+agent#setup-simple-retry-agent-pipeline-for-text-to-sql) to see how to build an agent from scratch.\n\n- **Workflow Assistant**: Build an agent that can operate over common workflow tools like email, calendar. Check out our [GSuite agent tutorial](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-google/examples/advanced_tools_usage.ipynb).\n\n- **Coding Assistant**: Build an agent that can operate over code. Check out our [code interpreter tutorial](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-code-interpreter/examples/code_interpreter.ipynb).\n\n\n## Resources\n\n**Using Agents with Tools**\n\nThe following component guides are the central hubs for getting started in building with agents:\n\n- [Agents](../module_guides/deploying/agents/index.md)\n- [Tools](../module_guides/deploying/agents/tools.md)\n\n\n**Building Custom Agents**\n\nIf you're interested in building custom agents, check out the following resources.\n\n- [Custom Agent](../examples/agent/custom_agent.ipynb)\n- [Custom Agent with Query Pipelines](../examples/agent/agent_runner/query_pipeline_agent.ipynb)\n\n**Building with Agentic Ingredients**\n\nLlamaIndex has robust abstractions for every agent sub-ingredient.\n\n- **Query Planning**: [Routing](../module_guides/querying/router/index.md), [Sub-Questions](../examples/query_engine/sub_question_query_engine.ipynb), [Query Transformations](../optimizing/advanced_retrieval/query_transformations.md).\n- **Function Calling and Tool Use**: Check out our [OpenAI](../examples/llm/openai.ipynb), [Mistral](../examples/llm/mistralai.ipynb) guides as examples.\n- **Memory**: [Example guide for adding memory to RAG](../examples/pipeline/query_pipeline_memory/).\n\n## LlamaHub\n\nWe offer a collection of 40+ agent tools for use with your agent in [LlamaHub](https://llamahub.ai/) \ud83e\udd99.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34944126-3440-49c7-9ca9-b5dba48a79e4": {"__data__": {"id_": "34944126-3440-49c7-9ca9-b5dba48a79e4", "embedding": null, "metadata": {"filename": "chatbots.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65964fb76fdbb78a66531039d7973ef96d047da0", "node_type": "4", "metadata": {"filename": "chatbots.md", "author": "LlamaIndex"}, "hash": "4b79bab9acd62f3c3bb020d6baff0a0f39f3a74315ae0298fe2dc357ac42ede4", "class_name": "RelatedNodeInfo"}}, "text": "# Chatbots\n\nChatbots are another extremely popular use case for LLMs. Instead of single-shot question-answering, a chatbot can handle multiple back-and-forth queries and answers, getting clarification or answering follow-up questions.\n\nLlamaIndex gives you the tools to build knowledge-augmented chatbots and agents. This use case builds upon the [QA](q_and_a/index.md) use case, make sure to check that out first!\n\n## Resources\n\nThe central module guide you'll want to check out is our [Chat Engines](../module_guides/deploying/chat_engines/index.md).\n\nHere are some additional relevant resources to build full-stack chatbot apps:\n\n- [Building a chatbot](../understanding/putting_it_all_together/chatbots/building_a_chatbot.md) tutorial\n- [create-llama](https://blog.llamaindex.ai/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191), a command line tool that generates a full-stack chatbot application for you\n- [SECinsights.ai](https://www.secinsights.ai/), an open-source application that uses LlamaIndex to build a chatbot that answers questions about SEC filings\n- [RAGs](https://blog.llamaindex.ai/introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1), a project inspired by OpenAI's GPTs that lets you build a low-code chatbot over your data using Streamlit\n- Our [OpenAI agents](../module_guides/deploying/agents/modules.md) are all chat bots in nature\n\n## External sources\n\n- [Building a chatbot with Streamlit](https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99887808-c943-4eb2-85ab-f170ecb3e83c": {"__data__": {"id_": "99887808-c943-4eb2-85ab-f170ecb3e83c", "embedding": null, "metadata": {"filename": "extraction.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0653601b8b1944540c09fd36e4d3733b2c61375", "node_type": "4", "metadata": {"filename": "extraction.md", "author": "LlamaIndex"}, "hash": "630c7e28132cb65a7063dd1dbe15a95615929626c66f3f458a7e5d17ee9cd758", "class_name": "RelatedNodeInfo"}}, "text": "# Structured Data Extraction\n\nLLMs are capable of ingesting large amounts of unstructured data and returning it in structured formats, and LlamaIndex is set up to make this easy.\n\nUsing LlamaIndex, you can get an LLM to read natural language and identify semantically important details such as names, dates, addresses, and figures, and return them in a consistent structured format regardless of the source format.\n\nThis can be especially useful when you have unstructured source material like chat logs and conversation transcripts.\n\nOnce you have structured data you can send them to a database, or you can parse structured outputs in code to automate workflows.\n\n## Core Guides\n\nCheck out our Structured Output guide for a comprehensive overview of structured data extraction with LlamaIndex. Do it in a standalone fashion (Pydantic program) or as part of a RAG pipeline. We also have standalone output parsing modules that you can use yourself with an LLM / prompt.\n\n- [Structured Outputs](../module_guides/querying/structured_outputs/index.md)\n- [Pydantic Program](../module_guides/querying/structured_outputs/pydantic_program.md)\n- [Output Parsing](../module_guides/querying/structured_outputs/output_parser.md)\n\nWe also have multi-modal structured data extraction. [Check it out](../use_cases/multimodal.md#simple-evaluation-of-multi-modal-rag).\n\n## Misc Examples\n\nSome additional miscellaneous examples highlighting use cases:\n\n- [Extracting names and locations from descriptions of people](../examples/output_parsing/df_program.ipynb)\n- [Extracting album data from music reviews](../examples/llm/llama_api.ipynb)\n- [Extracting information from emails](../examples/usecases/email_data_extraction.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fd7ad37-3c45-4cca-ab65-69bb95ec1d60": {"__data__": {"id_": "7fd7ad37-3c45-4cca-ab65-69bb95ec1d60", "embedding": null, "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eda87abe70cf3fbe7f79f8366385815870f30363", "node_type": "4", "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "hash": "35c2a1ec68a466c318533db22ffe34ebb6ab7d21249b617a64d398abb3879905", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef0b6a9e-9ce0-4028-aec5-0d13a854eb69", "node_type": "1", "metadata": {}, "hash": "435f806df34b8da0a71f05fedf9d0e281bbd1016743a41f56f337bdd2ce03cdc", "class_name": "RelatedNodeInfo"}}, "text": "# Fine-tuning\n\n## Overview\n\nFinetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost.\n\nThe core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves.\n\nWhile finetuning can be also used to \"augment\" a model with external data, finetuning can complement retrieval augmentation in a variety of ways:\n\n#### Embedding Finetuning Benefits\n\n- Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data --> leads to better retrieval performance.\n\n#### LLM Finetuning Benefits\n\n- Allow it to learn a style over a given dataset\n- Allow it to learn a DSL that might be less represented in the training data (e.g. SQL)\n- Allow it to correct hallucinations/errors that might be hard to fix through prompt engineering\n- Allow it to distill a better model (e.g. GPT-4) into a simpler/cheaper model (e.g. gpt-3.5, Llama 2)\n\n## Integrations with LlamaIndex\n\nThis is an evolving guide, and there are currently three key integrations with LlamaIndex. Please check out the sections below for more details!\n\n- Finetuning embeddings for better retrieval performance\n- Finetuning Llama 2 for better text-to-SQL\n- Finetuning gpt-3.5-turbo to distill gpt-4\n\n## Finetuning Embeddings\n\nWe've created comprehensive guides showing you how to finetune embeddings in different ways, whether that's the model itself (in this case, `bge`) over an unstructured text corpus, or an adapter over any black-box embedding. It consists of the following steps:\n\n1. Generating a synthetic question/answer dataset using LlamaIndex over any unstructured context.\n2. Finetuning the model\n3. Evaluating the model.\n\nFinetuning gives you a 5-10% increase in retrieval evaluation metrics. You can then plug this fine-tuned model into your RAG application with LlamaIndex.\n\n- [Fine-tuning an Adapter](../examples/finetuning/embeddings/finetune_embedding_adapter.ipynb)\n- [Embedding Fine-tuning Guide](../examples/finetuning/embeddings/finetune_embedding.ipynb)\n- [Router Fine-tuning](../examples/finetuning/router/router_finetune.ipynb)\n\n**Old**\n\n- [Embedding Fine-tuning Repo](https://github.com/run-llama/finetune-embedding)\n- [Embedding Fine-tuning Blog](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)\n\n## Fine-tuning LLMs\n\n### Fine-tuning GPT-3.5 to distill GPT-4\n\nWe have multiple guides showing how to use OpenAI's finetuning endpoints to fine-tune gpt-3.5-turbo to output GPT-4 responses for RAG/agents.\n\nWe use GPT-4 to automatically generate questions from any unstructured context, and use a GPT-4 query engine pipeline to generate \"ground-truth\" answers. Our `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset.\n\nWe then launch a finetuning job, and get back a distilled model. We can evaluate this model with [Ragas](https://github.com/explodinggradients/ragas) to benchmark against a naive GPT-3.5 pipeline.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef0b6a9e-9ce0-4028-aec5-0d13a854eb69": {"__data__": {"id_": "ef0b6a9e-9ce0-4028-aec5-0d13a854eb69", "embedding": null, "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eda87abe70cf3fbe7f79f8366385815870f30363", "node_type": "4", "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "hash": "35c2a1ec68a466c318533db22ffe34ebb6ab7d21249b617a64d398abb3879905", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fd7ad37-3c45-4cca-ab65-69bb95ec1d60", "node_type": "1", "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "hash": "ec43d7b25767f3ef79fab298e47a05440447a127206a8b090bdac9d0ffc29a3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9346abed-ce4a-4c13-b768-fd32a4f0e171", "node_type": "1", "metadata": {}, "hash": "aa7d2ebbec26fa9e78aa1b2549bf90cc557b5cf4dc0494120f8c31edd8b5f382", "class_name": "RelatedNodeInfo"}}, "text": "- [GPT-3.5 Fine-tuning Notebook (Colab)](https://colab.research.google.com/drive/1NgyCJVyrC2xcZ5lxt2frTU862v6eJHlc?usp=sharing)\n- [GPT-3.5 Fine-tuning Notebook (Notebook link)](../examples/finetuning/openai_fine_tuning.ipynb)\n- [React Agent Finetuning](../examples/finetuning/react_agent/react_agent_finetune.ipynb)\n- [[WIP] Function Calling Fine-tuning](../examples/finetuning/openai_fine_tuning_functions.ipynb)\n\n**Old**\n\n- [GPT-3.5 Fine-tuning Notebook (Colab)](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing)\n- [GPT-3.5 Fine-tuning Notebook (in Repo)](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb)\n\n### Fine-tuning for Better Structured Outputs\n\nAnother use case for fine-tuning is to make the model better at outputting structured data.\nWe can do this for both OpenAI and Llama2.\n\n- [OpenAI Function Calling Fine-tuning](../examples/finetuning/openai_fine_tuning_functions.ipynb)\n- [Llama2 Structured Output Fine-tuning](../examples/finetuning/gradient/gradient_structured.ipynb)\n\n### Fine-tuning Llama 2 for Better Text-to-SQL\n\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using LlamaIndex abstractions.\n\nThe stack includes `sql-create-context` as the training dataset, OpenLLaMa as the base model, PEFT for finetuning, Modal for cloud compute, LlamaIndex for inference abstractions.\n\n- [Llama 2 Text-to-SQL Fine-tuning (w/ Gradient.AI)](../examples/finetuning/gradient/gradient_fine_tuning.ipynb)\n- [Llama 2 Text-to-SQL Fine-tuning (w/ Modal, Repo)](https://github.com/run-llama/modal_finetune_sql)\n- [Llama 2 Text-to-SQL Fine-tuning (w/ Modal, Notebook)](https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb)\n\n### Fine-tuning An Evaluator\n\nIn these tutorials, we aim to distill a GPT-4 judge (or evaluator) onto a GPT-3.5 judge. It has\nbeen recently observed that GPT-4 judges can reach high levels of agreement with human evaluators (e.g.,\nsee https://arxiv.org/pdf/2306.05685.pdf).\n\nThus, by fine-tuning a GPT-3.5 judge, we may be able to reach GPT-4 levels (and\nby proxy, agreement with humans) at a lower cost.\n\n- [Fine-tune LLM Correctness Judge](../examples/finetuning/llm_judge/correctness/finetune_llm_judge_single_grading_correctness.ipynb)\n- [Fine-tune LLM Judge](../examples/finetuning/llm_judge/pairwise/finetune_llm_judge.ipynb)\n\n## Fine-tuning Cross-Encoders for Re-Ranking\n\nBy finetuning a cross encoder, we can attempt to improve re-ranking performance on our own private data.\n\nRe-ranking is key step in advanced retrieval, where retrieved nodes from many sources are re-ranked using a separate model, so that the most relevant nodes\nare first.\n\nIn this example, we use the `sentence-transformers` package to help finetune a crossencoder model, using a dataset that is generated based on the `QASPER` dataset.", "mimetype": "text/plain", "start_char_idx": 3229, "end_char_idx": 6191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9346abed-ce4a-4c13-b768-fd32a4f0e171": {"__data__": {"id_": "9346abed-ce4a-4c13-b768-fd32a4f0e171", "embedding": null, "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eda87abe70cf3fbe7f79f8366385815870f30363", "node_type": "4", "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "hash": "35c2a1ec68a466c318533db22ffe34ebb6ab7d21249b617a64d398abb3879905", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef0b6a9e-9ce0-4028-aec5-0d13a854eb69", "node_type": "1", "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}, "hash": "9ee058e26f21ec798978b1aa6c371d277feec0f60768d987659dbd5632060a4e", "class_name": "RelatedNodeInfo"}}, "text": "- [Cross-Encoder Finetuning](../examples/finetuning/cross_encoder_finetuning/cross_encoder_finetuning.ipynb)\n- [Finetuning Llama 2 for Text-to-SQL](https://medium.com/llamaindex-blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d)\n- [Finetuning GPT-3.5 to Distill GPT-4](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing)\n\n## Cohere Custom Reranker\n\nBy training a custom reranker with CohereAI, we can attempt to improve re-ranking performance on our own private data.\n\nRe-ranking is a crucial step in advanced retrieval processes. This step involves using a separate model to re-organize nodes retrieved from initial retrieval phase. The goal is to ensure that the most relevant nodes are prioritized and appear first.\n\nIn this example, we use the `cohere` custom reranker training module to create a reranker on your domain or specific dataset to improve retrieval performance.\n\n- [Cohere Custom Reranker](../examples/finetuning/rerankers/cohere_custom_reranker.ipynb)", "mimetype": "text/plain", "start_char_idx": 6193, "end_char_idx": 7223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b29b78e6-2217-42be-99c7-fa1ce32b36cf": {"__data__": {"id_": "b29b78e6-2217-42be-99c7-fa1ce32b36cf", "embedding": null, "metadata": {"filename": "graph_querying.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35c676db7a821c2b5816ef06d9205d88ac3df846", "node_type": "4", "metadata": {"filename": "graph_querying.md", "author": "LlamaIndex"}, "hash": "7c0d2da042868608dbfe9f3d52100cd9cecf03fe1e786950f85196b5dae9a5df", "class_name": "RelatedNodeInfo"}}, "text": "# Querying Graphs\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 23, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc56db05-0ebd-4f44-abcd-c7edeabd0966": {"__data__": {"id_": "dc56db05-0ebd-4f44-abcd-c7edeabd0966", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e2796b6f98168047d342c043567c094f98c95a3", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "410929820fa4195f0f44a7db408c7cbb24e41a2a8bbddbdadd810b9f0c7d1053", "class_name": "RelatedNodeInfo"}}, "text": "# Use Cases\n\nSee the navigation on the left to explore the use-cases with LlamaIndex!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e47d823-d3f0-4e85-b927-0cd89c68ae02": {"__data__": {"id_": "0e47d823-d3f0-4e85-b927-0cd89c68ae02", "embedding": null, "metadata": {"filename": "multimodal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8c82613334827ad40a42c0a42a6a985ac09ce2b", "node_type": "4", "metadata": {"filename": "multimodal.md", "author": "LlamaIndex"}, "hash": "57e02f3298d97a791fb2361af6f922d523c277963e07eeba7ff54aa5a45af9db", "class_name": "RelatedNodeInfo"}}, "text": "# Multi-modal\n\nLlamaIndex offers capabilities to not only build language-based applications but also **multi-modal** applications - combining language and images.\n\n## Types of Multi-modal Use Cases\n\nThis space is actively being explored right now, but some fascinating use cases are popping up.\n\n### RAG (Retrieval Augmented Generation)\n\nAll the core RAG concepts: indexing, retrieval, and synthesis, can be extended into the image setting.\n\n- The input could be text or image.\n- The stored knowledge base can consist of text or images.\n- The inputs to response generation can be text or image.\n- The final response can be text or image.\n\nCheck out our guides below:\n\n- [GPT-4V Multi Modal](../examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb)\n- [Multi-modal retrieval with CLIP](../examples/multi_modal/multi_modal_retrieval.ipynb)\n- [Image to Image Retrieval](../examples/multi_modal/image_to_image_retrieval.ipynb)\n- [Structured Image Retrieval](../examples/multi_modal/structured_image_retrieval.ipynb)\n- [Chroma Multi-Modal](../examples/multi_modal/ChromaMultiModalDemo.ipynb)\n- [Gemini Multi-Modal](../examples/multi_modal/gemini.ipynb)\n- [Ollama Multi-Modal](../examples/multi_modal/ollama_cookbook.ipynb)\n\n### Structured Outputs\n\nYou can generate a `structured` output with the new OpenAI GPT4V via LlamaIndex. The user just needs to specify a Pydantic object to define the structure of the output.\n\nCheck out the guide below:\n\n- [Multi-Modal Pydantic Program](../examples/multi_modal/multi_modal_pydantic.ipynb)\n\n### Retrieval-Augmented Image Captioning\n\nOftentimes understanding an image requires looking up information from a knowledge base. A flow here is retrieval-augmented image captioning - first caption the image with a multi-modal model, then refine the caption by retrieving it from a text corpus.\n\nCheck out our guides below:\n\n- [Llava + Testla 10Q](../examples/multi_modal/llava_multi_modal_tesla_10q.ipynb)\n\n### Agents\n\nHere are some initial works demonstrating agentic capabilities with GPT-4V.\n\n- [Multi-Modal Agents](../examples/multi_modal/mm_agent.ipynb)\n- [GPT-4V Experiments](../examples/multi_modal/gpt4v_experiments_cot.ipynb)\n\n## Evaluations and Comparisons\n\nThese sections show comparisons between different multi-modal models for different use cases.\n\n### LLaVa-13, Fuyu-8B, and MiniGPT-4 Multi-Modal LLM Models Comparison for Image Reasoning\n\nThese notebooks show how to use different Multi-Modal LLM models for image understanding/reasoning. The various model inferences are supported by Replicate or OpenAI GPT4-V API. We compared several popular Multi-Modal LLMs:\n\n- GPT4-V (OpenAI API)\n- LLava-13B (Replicate)\n- Fuyu-8B (Replicate)\n- MiniGPT-4 (Replicate)\n- CogVLM (Replicate)\n\nCheck out our guides below:\n\n- [Replicate Multi-Modal](../examples/multi_modal/replicate_multi_modal.ipynb)\n- [GPT4-V](../examples/multi_modal/openai_multi_modal.ipynb)\n\n### Simple Evaluation of Multi-Modal RAG\n\nIn this notebook guide, we'll demonstrate how to evaluate a Multi-Modal RAG system. As in the text-only case, we will consider the evaluation of Retrievers and Generators separately. As we alluded to in our blog on the topic of Evaluating Multi-Modal RAGs, our approach here involves the application of adapted versions of the usual techniques for evaluating both Retriever and Generator (used for the text-only case). These adapted versions are part of the llama-index library (i.e., evaluation module), and this notebook will walk you through how you can apply them to your evaluation use cases.\n\n- [Multi-Modal RAG Evaluation](../examples/evaluation/multi_modal/multi_modal_rag_evaluation.ipynb)\n\n## Model Guides\n\nHere are notebook guides showing you how to interact with different multimodal model providers.\n\n- [OpenAI Multi-Modal](../examples/multi_modal/openai_multi_modal.ipynb)\n- [Replicate Multi-Modal](../examples/multi_modal/replicate_multi_modal.ipynb)\n- [Ollama Multi-Modal](../examples/multi_modal/ollama_cookbook.ipynb)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "331ce9e5-8087-4cc4-9f94-5fbe5999b419": {"__data__": {"id_": "331ce9e5-8087-4cc4-9f94-5fbe5999b419", "embedding": null, "metadata": {"filename": "prompting.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9390547c52f8e0469706addf6e38ed7870144bdd", "node_type": "4", "metadata": {"filename": "prompting.md", "author": "LlamaIndex"}, "hash": "7a7e6f11b80b1c7b428e1a4fa12adfe9c7e2fd3762bc2a647bc3dfc63413e5f0", "class_name": "RelatedNodeInfo"}}, "text": "# Prompting\n\nPrompting LLMs is a fundamental unit of any LLM application. You can build an entire application entirely around prompting, or orchestrate with other modules (e.g. retrieval) to build RAG, agents, and more.\n\nLlamaIndex supports LLM abstractions and simple-to-advanced prompt abstractions to make complex prompt workflows possible.\n\n## LLM Integrations\n\nLlamaIndex supports 40+ LLM integrations, from proprietary model providers like OpenAI, Anthropic to open-source models/model providers like Mistral, Ollama, Replicate. It provides all the tools to standardize interface around common LLM usage patterns, including but not limited to async, streaming, function calling.\n\nHere's the [full module guide for LLMs](../module_guides/models/llms.md).\n\n## Prompts\n\nLlamaIndex has robust prompt abstractions that capture all the common interaction patterns with LLMs.\n\nHere's the [full module guide for prompts](../module_guides/models/prompts/index.md).\n\n### Table Stakes\n- [Text Completion Prompts](../examples/customization/prompts/completion_prompts.ipynb)\n- [Chat Prompts](../examples/customization/prompts/chat_prompts.ipynb)\n\n### Advanced\n- [Variable Mappings, Functions, Partials](../examples/prompts/advanced_prompts.ipynb)\n- [Few-shot Examples, RAG](../examples/prompts/prompts_rag.ipynb)\n\n## Prompt Chains and Pipelines\n\nLlamaIndex has robust abstractions for creating sequential prompt chains, as well as general DAGs to orchestrate prompts with any other component. This allows you to build complex workflows, including RAG with multi-hop query understanding layers, as well as agents.\n\nThese pipelines are integrated with [observability partners](../module_guides/observability/index.md) out of the box.\n\nThe central guide for prompt chains and pipelines is through our [Query Pipelines](../module_guides/querying/pipeline/index.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "619b2dc5-eaa9-4ed0-8cbe-886ec0901d52": {"__data__": {"id_": "619b2dc5-eaa9-4ed0-8cbe-886ec0901d52", "embedding": null, "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "759650789e1100aad523f9b1b24b557e3b201696", "node_type": "4", "metadata": {"filename": "index.md", "author": "LlamaIndex"}, "hash": "a383bbdc134d1d6356ef11736b10ff3a670e8a0a981936cf920bf5f9e04b55bc", "class_name": "RelatedNodeInfo"}}, "text": "# Question-Answering (RAG)\n\nOne of the most common use-cases for LLMs is to answer questions over a set of data. This data is oftentimes in the form of unstructured documents (e.g. PDFs, HTML), but can also be semi-structured or structured.\n\nThe predominant framework for enabling QA with LLMs is Retrieval Augmented Generation (RAG). LlamaIndex offers simple-to-advanced RAG techniques to tackle simple-to-advanced questions over different volumes and types of data.\n\nThere are different subtypes of question-answering.\n\n## RAG over Unstructured Documents\nLlamaIndex can pull in unstructured text, PDFs, Notion and Slack documents and more and index the data within them.\n\nThe simplest queries involve either semantic search or summarization.\n\n- **Semantic search**: A query about specific information in a document that matches the query terms and/or semantic intent. This is typically executed with simple vector retrieval (top-k). [Example of semantic search](../../understanding/putting_it_all_together/q_and_a/#semantic-search)\n- **Summarization**: condensing a large amount of data into a short summary relevant to your current question. [Example of summarization](../../understanding/putting_it_all_together/q_and_a/#summarization)\n\n\n\n## QA over Structured Data\nIf your data already exists in a SQL database, CSV file, or other structured format, LlamaIndex can query the data in these sources. This includes **text-to-SQL** (natural language to SQL operations) and also **text-to-Pandas** (natural language to Pandas operations).\n\n  - [Text-to-SQL Guide](../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb)\n  - [Text-to-Pandas Guide](../../examples/query_engine/pandas_query_engine.ipynb)\n\n## Advanced QA Topics\n\nAs you scale to more complex questions / more data, there are many techniques in LlamaIndex to help you with better query understanding, retrieval, and integration of data sources.\n\n- **Querying Complex Documents**: Oftentimes your document representation is complex - your PDF may have text, tables, charts, images, headers/footers, and more. LlamaIndex provides advanced indexing/retrieval integrated with LlamaParse, our proprietary document parser. [Full cookbooks here](https://github.com/run-llama/llama_parse/tree/main/examples).\n- **Combine multiple sources**: is some of your data in Slack, some in PDFs, some in unstructured text? LlamaIndex can combine queries across an arbitrary number of sources and combine them.\n    - [Example of combining multiple sources](../../understanding/putting_it_all_together/q_and_a.md#multi-document-queries)\n- **Route across multiple sources**: given multiple data sources, your application can first pick the best source and then \"route\" the question to that source.\n    - [Example of routing across multiple sources](../../understanding/putting_it_all_together/q_and_a.md#routing-over-heterogeneous-data)\n- **Multi-document queries**: some questions have partial answers in multiple data sources which need to be questioned separately before they can be combined\n    - [Example of multi-document queries](../../understanding/putting_it_all_together/q_and_a.md#multi-document-queries)\n    - [Building a multi-document agent over the LlamaIndex docs](../../examples/agent/multi_document_agents-v1.ipynb) - [Text to SQL](../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb)\n\n\n## Resources\n\nLlamaIndex has a lot of resources around QA / RAG. Here are some core resource guides to refer to.\n\n**I'm a RAG beginner and want to learn the basics**: Take a look at our [\"Learn\" series of guides](../../understanding/index.md).\n\n**I've built RAG, and now I want to optimize it**: Take a look at our [\"Advanced Topics\" Guides](../../optimizing/production_rag.md).\n\n**I want to learn all about a particular module**: Here are the core module guides to help build simple-to-advanced QA/RAG systems:\n\n- [Query Engines](../../module_guides/deploying/query_engine/index.md)\n- [Chat Engines](../../module_guides/deploying/chat_engines/index.md)\n- [Agents](../../module_guides/deploying/agents/index.md)\n\n\n## Further examples\n\nFor further examples of Q&A use cases, see our [Q&A section in Putting it All Together](../../understanding/putting_it_all_together/q_and_a.md).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ca9a81f-1f42-45d2-9a15-f2f2536cbff4": {"__data__": {"id_": "2ca9a81f-1f42-45d2-9a15-f2f2536cbff4", "embedding": null, "metadata": {"filename": "querying_csvs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6fc068612c1f529e3e2b29f2be50094b30ab13ff", "node_type": "4", "metadata": {"filename": "querying_csvs.md", "author": "LlamaIndex"}, "hash": "e04ee251a40859702edeae93ed17f31fd47181b100c5a954aa6b6141f1208c92", "class_name": "RelatedNodeInfo"}}, "text": "# Querying CSVs\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 21, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7ae54d0-a67c-439c-9f07-89ef97a7723e": {"__data__": {"id_": "c7ae54d0-a67c-439c-9f07-89ef97a7723e", "embedding": null, "metadata": {"filename": "tables_charts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b27999821cbc0546d25ec628a0a7a10f4d970b6", "node_type": "4", "metadata": {"filename": "tables_charts.md", "author": "LlamaIndex"}, "hash": "e3a64fab59fa168903b022c5930188d7091b5815297b77ce101ca168e849ffa3", "class_name": "RelatedNodeInfo"}}, "text": "# Parsing Tables and Charts\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 33, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cab292f-157d-4f90-87a0-a4717b7d1eda": {"__data__": {"id_": "1cab292f-157d-4f90-87a0-a4717b7d1eda", "embedding": null, "metadata": {"filename": "text_to_sql.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d021d3b23824ef25223b141405a6385e337a580", "node_type": "4", "metadata": {"filename": "text_to_sql.md", "author": "LlamaIndex"}, "hash": "ea84e71913f5f9f0757b508022917d604efbcab28f45772ca590b2082215e3c1", "class_name": "RelatedNodeInfo"}}, "text": "# Text to SQL\n\nTODO", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 19, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"b792e8d0-274f-475a-9516-aafc9754602a": {"doc_hash": "8bee70ed63433b8952ce392228f6ff652ebcf07fa4acfb4d7447b8b53048bb87", "ref_doc_id": "e0833904f0cb3e584c30ea17552ca65d29c8ec2b"}, "6ae4fbd2-958a-4f14-ab43-bab452bd5d99": {"doc_hash": "b88e454610fe546ac104671664a913a2e174e1ca630e4c3a4a1255b00d2a7f93", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "c7551b70-321a-4b67-9093-8f2cd66bcc46": {"doc_hash": "d02d19a42607f9408da6f3b8c72fe6c4b91e2aa72c46ff4a8f80106ebbbfae42", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "60928bce-84fc-4405-851f-03537938e3cf": {"doc_hash": "154ab53342770c23bd6629cd3a98594f24e920157c3c55c216aedec95fbf80e0", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "ecfaa01e-1fbb-44f8-8144-c98dc436c2fe": {"doc_hash": "caaa6e95837aeec882593094734f25fdb7eec04fa15965f7bfd763329574b49b", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "44739aaf-b113-4ceb-bd8b-5cbe5ca3c979": {"doc_hash": "d95ca99665dc400ff007a09c32fa88d42ecff43d46e6f19348eec18818626569", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "e2a9e57c-1b6b-452e-99ec-fe1c9420e635": {"doc_hash": "a976b89d97c450eafa0a802772aff182d298e28f5c96cde9da9d5216c6dc0e16", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "8b9deccf-7e51-4399-94c4-bdc360486765": {"doc_hash": "03acf8fc7cbaf5d8dce718d1dba2a143d2d5b5ca67c409018136afd03b37f7d6", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "608f9d5c-4558-4073-ba14-f392b4bac5c6": {"doc_hash": "cd6da39f111bac38f253cd07c47cd5c2d57a5766c5bb555c7bddea4ee0ddf11d", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "589e32ed-ca8a-4e99-989e-1cf14851e02e": {"doc_hash": "b1037f46d693357f7b44ca0ede0eeb23f9b9cc67ebf58e3840beb0f82e14483f", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "04a94e78-c521-4ba0-aa52-cb4545e39784": {"doc_hash": "371d58ab10754d3531d34b876c33e9b1ce7d17b00851c319c4ec74b41033d1a7", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "ce22dbf3-ab7e-4e89-8c40-25ab6446f0f4": {"doc_hash": "0296b1185938dcda2d68c574611ee3c125f5fb5d4c349c1d8612ed48c6498e4e", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "8a6735b3-217b-4935-97de-65b95e6ba33f": {"doc_hash": "00f5d4d40aa1d5d8224929b7727d4006e3245888fd38b4bb886ec2e0eb49c7ce", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "521bd1ef-cb12-4147-8df9-9e402cc342cd": {"doc_hash": "93015251e25371be8606125f6bf5c8510064a5bc07cf10d2d955af1354e95656", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "e6736c08-bd27-4d4c-b26f-f703ce2ed11b": {"doc_hash": "b8f4db610f8c1c25e501a1f439cd0f54fe953b936ff6bee7a1be5a3ff6f648c1", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "fe40c17c-7acc-4ed5-9847-47507a6203ee": {"doc_hash": "7856f5bfd8f61e2c6299f4111a57b4c026008fc7f262f18445359ef374105854", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "81398365-cc64-4ef7-8738-ab73077c371f": {"doc_hash": "302f8cf9ef05f85dc6b825713d58101bb42ba6ce71654a726131a92681330dad", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "a7ebd903-a483-4bb4-af32-c97f1b92d8c9": {"doc_hash": "18e7a2a1ff12145c77e89c3c8a88308ff648a43e297a7b3a33432d2312612018", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "ea03da5d-436e-4faf-bdf1-2347f2c90a72": {"doc_hash": "92984faa787b0f750b67eead8ce448186ebe5ef57ff669da412294e792ce841e", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "7715c403-8d93-49fd-8c16-c389ec67101c": {"doc_hash": "f5bc488c4e3c91a5145fda17f460b957f11bcf883ba7186aecc68ad14b469a9e", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "a624113c-d800-4688-a23f-449bf1b4eb08": {"doc_hash": "ce788247dfd81f7a7a880590354741932e7f76a0fd4c29734476536a79ff828b", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "c8539a10-0e5e-41d4-8730-0c5f31de2a2a": {"doc_hash": "bb871bae7337ee02da77f13d3bcda653e138ae1168195f0b7202f3f4fd65974a", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "09b3b9d6-140d-44b1-878e-bc27a3f7960e": {"doc_hash": "cac6c5629e7fc93b2322ca7d3860d5ce8e1c68ca5a84fe50f6d0426415d34dbe", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "944baf46-a119-49dd-b99e-856985851a41": {"doc_hash": "c01199b76890510c3ae44c6bd2f46aba8cbfb4543ab1e41897ca11fb0d035edd", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "06034f25-15ea-497d-95d6-cc94392e5dfc": {"doc_hash": "93e8aad734a62e9b0ed729b44d367ccd03c94bcc294c73f1a00e4c21ddd8cf6e", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "818fc19d-0d24-43e8-bab3-b0d64ba44c71": {"doc_hash": "96bf99d5b6afba13700bea91440d3b8bd990cfd013d7b63c4a20b90037d2f6dd", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "19e2764b-cc4e-4442-b19f-78d2ce6179ef": {"doc_hash": "c4665f54d9e9e8990b52b2c6a695431d1585a5ef416269049be656ad8bcb77c3", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "875c60f6-1092-4990-bc34-90063190e68b": {"doc_hash": "9e6a5a5571259f8646e63681558fd016932b55aba39990cfdc778b54bfccf4c1", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "e95cb2f3-e0d4-4cd2-9ae9-168e21d70207": {"doc_hash": "8341cb8f67ef30898fb7fe490eaeb98937c6d4e50882fc01555c182f752e42c0", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "d741cc39-c5d4-4b24-b9d1-ebd89740b8a3": {"doc_hash": "0625226bd723b00885e720fb1df8414f2dc8a9e60212b3c0b63c49302dace375", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "ee5e3059-c337-46db-bb7d-1c48a027ff2a": {"doc_hash": "e77427e08531c4e00771722dc26a7ba6e37f519d65c12be7cadae10b38d669c9", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "885c847a-6ad7-4d8b-8dc3-5f82718ecd23": {"doc_hash": "e9352297308f4344255d88c2f7493d8219fb349983a266e4def46883edb9bcf8", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "a48e8673-3a35-478b-8a2b-a06f593b7947": {"doc_hash": "7a7245438e032fa09552a3eb741ea0be72259c434d68afe32c04a49f05bf91e8", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "5f3b36ad-c843-478a-bb90-ffc96aec3687": {"doc_hash": "e6a97324eb29bf920e9cd8e1d1ff6503c765a8a82c90ee6a8463903bf5d881b7", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "bbf0e854-ef91-45ec-965d-23688e4af1d3": {"doc_hash": "1db3709252057e7b50f420ccc2ccf290cc59f95c81302d930b1912719285391a", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "554672da-ce4b-4d97-86b3-062d879d378a": {"doc_hash": "679bc6a7aa9a2243d976dd53f7c3cee3aefb3908214170bbdad7f77f928b0ee6", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "b255d14f-2ee5-4650-b02c-cbb6dd5c726b": {"doc_hash": "a4c5b9a34748d77463805f8a7f2201f7c03363faa9d13251e44166aa407ba7d2", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "0d85e59f-b606-4363-8536-4641286809af": {"doc_hash": "0deac1ab73853924697af78860305ca13e51d46624cd9e6c081475ca2fadcd88", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "16d0b0ff-ef0e-4b97-8de1-4f9d3cd5b94f": {"doc_hash": "b00534e4b3096ee0c792c96331c04deed43e32f2b2590db7764d048ab11bf67d", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "16f9bb34-958a-40f5-8873-009d3c2aec91": {"doc_hash": "3c8bebea72c1f7ff2cae2471948210c1c34609bf08925c4db4a8829a4acfce65", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "d74ca02f-8e69-4060-897a-d223cbb2f8ad": {"doc_hash": "abd0bb3fb6351e61561ef3b0e0101ba9aa77c16531452617a39c6206d5c13da1", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "283ed427-fcf5-4640-be7f-79c87068aad4": {"doc_hash": "4d8d273d2d132114d911e174afe47107e664583a34a6def25f75d553cf76e9ed", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "bfacf8d7-300a-4ecd-80c6-54a34f4b960b": {"doc_hash": "fc42ba96c74a1cd8edd0f2aec2d95e3b3139e0478c787daf8944cb47c59e9a34", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "e78faeaf-698b-4b8f-ad76-2167e81edbd6": {"doc_hash": "e51f94d12a70fe92e5b16922a37c616bc02904f40e4338d7c34341e8c80d2864", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "9ef35aa0-fe90-4724-b110-31aead47cd65": {"doc_hash": "87c0c0f24565504301f780b6a08c22ddea3022312092c020f44587ff3f46729c", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "d5f3f4e7-c37e-4f71-bf29-c093547e12dc": {"doc_hash": "8626a2b4171e7edbb5fb0e5e59616b1af7ad515c55201a5a1fc35d01d28fd47c", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "9694e8bd-4a25-4afa-bc22-3f2dd81b599a": {"doc_hash": "c6675dfda917ceaaef29036152179bef519550c73ab79567e038bf81592f71b5", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "64141801-9822-4204-93e5-5e3120f2724d": {"doc_hash": "0d57911399a9ce8a0f36a1f0ec046569444b20421dd59069a058e3e626d2dc91", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "baedf5c7-709a-45fc-a244-4bafbc97a1e5": {"doc_hash": "d8281a6aafc7d1d93e41e9d3fa4502c676f25fc7dec488dcc4c7585d2eb2ce40", "ref_doc_id": "a22d9c95b25b2d0c78caa0228ca771c3f30d8421"}, "a1c8e200-b34e-450d-ac57-0f6e502ea432": {"doc_hash": "cd3f09d0d2431da003f8109e3b2055c5b30396a85fd228179ce20ad3733a0036", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "854cc8dd-8283-458c-b0f5-ea525a501db6": {"doc_hash": "dc92ef77ef04c4684ab9678d2ae74f39dfb320cf0c94bbb525f31966c98d495a", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "6de201b1-8334-4afa-8b37-aa214a838d04": {"doc_hash": "f2f4311ec9e4851960681ecbcc991469e70165b5cc5f6e796c78e7506e7b984d", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "c7be4176-06cc-45b8-bebe-4c619f551900": {"doc_hash": "454f8e8cc3cf4ef1f9d3cf64cd163850a510c5303d4d444d59b4786f2a216c70", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "029e0587-a00e-4065-a6d8-a06346b333a7": {"doc_hash": "30ca567e80abf82d69e52474e17109377a64bd633ae3ccfde567e2984db695aa", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "52f42635-7b7b-4392-abf3-cc931a7fcd76": {"doc_hash": "ac7921eb53e953404e1e3db6d6556a413f4216164ed19afd2cfbae9a4bae4d5d", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "3b7e4be6-e3e3-4d68-93df-f307de14a25c": {"doc_hash": "26fc90a73a4673dee4dfc57a712cc7ee9d3ab2aded2f5a3f9f4f25399ab8fef0", "ref_doc_id": "988ecacad3cb4bb087af10226cb90ba28644358b"}, "2742e447-2626-41af-b45d-3710c119dccc": {"doc_hash": "8bee70ed63433b8952ce392228f6ff652ebcf07fa4acfb4d7447b8b53048bb87", "ref_doc_id": "e0833904f0cb3e584c30ea17552ca65d29c8ec2b"}, "e90053a0-d6d8-4d53-8647-d854c523a368": {"doc_hash": "dae7900c4c9114192504e513af2248941a4eb85f4e68157207e8090683b0e1a7", "ref_doc_id": "69ca05844fee838474d2f699701f5b17e635f74d"}, "88e74cb6-b027-4a57-92dd-e59bc6155cc5": {"doc_hash": "bd3edf74eac612b10cf866d00d446081e5f0dbb6a5dd45cb26d14958ec294d30", "ref_doc_id": "603373f629406ccc3e1b52c40fa0eb613eb35457"}, "74c21f5b-c2e1-4e28-8925-5b518d20052f": {"doc_hash": "1a3865e784e13e08f2355bb14fdee225228cf0f3b9911feb21255430c0b5d308", "ref_doc_id": "17ae72b0a22f88e84377b8a0ffb1ae56cd6ba12b"}, "68977cbe-09b8-4be1-bffd-66150b67d4bb": {"doc_hash": "e5da32b9822e5916efb5e2954f3fc2bfc6207aabac32610dcd8456f9956e81bb", "ref_doc_id": "f616528dd75e506a8a56ccfb0cfb05ae13c7f85b"}, "ba7952cd-0dda-4fa3-a9e6-a32deba4eb25": {"doc_hash": "9f6c83aad32737c78fc5d8026258b50547a1067ec099b7c32aede01ceeb57436", "ref_doc_id": "5d4a7ee83286a7f6372eb2170aca4ea1a5ed07d5"}, "7b6cbf9f-0880-4d3a-8248-3e42a3bd0159": {"doc_hash": "90edc17a9506b204bcd0cd58f07e69bfe967b1280b6415df385757af9c42b4b3", "ref_doc_id": "2f38e027b0cc23677cb881afa76a110b6475461a"}, "e1b0d932-082a-44e2-a23e-6174a76a253d": {"doc_hash": "911ecc0bdaa538993b1b8b0b65a40db88a850be2b6b03edc3f93d19c257e1cef", "ref_doc_id": "287aae35a75dc0952398524a3f68b45b27ce867b"}, "4f4a17ad-c422-49d9-92fe-7a4612024492": {"doc_hash": "5b44d66fe80f287579d13419cb57a2bf137041f6f084a60d3e1e2c66a56530b8", "ref_doc_id": "8a5ae2117aec4e482af006c69ef5ccdade10fdb2"}, "9a907c07-3a30-4e32-9f3a-cde247efa738": {"doc_hash": "ba5ba19ecf65670594ea9d993f16edae430df12d301a490245f314c2f799a61e", "ref_doc_id": "4af66f1c81f129a0c5969183122e82bc38e1894c"}, "7989c642-bdef-4462-80f5-f92dc3c69a35": {"doc_hash": "28657b51fd06fb696af630849e6c3bcdcf95ba4693945f2fb94680fdc05dea1a", "ref_doc_id": "a0cc9017a2af39284ef0b905c15f7491cb10dfd1"}, "ffe31ca3-208b-479c-af24-37269a933e4f": {"doc_hash": "af62d63c8a5e20eee060ee20481fcf9709227b2e08e2071275de4a1092981667", "ref_doc_id": "39dd143435c8abf079e0a2092f0b2ab6be3f80f2"}, "d19041e6-1840-40c1-815a-b572885325ee": {"doc_hash": "d31347841d03a33149592a8ffe6deeebcdeeab05ae191bd7df9b442fba089627", "ref_doc_id": "2646025f16cf79a5afbbfda01563ce0beee9f097"}, "e4781a09-f477-4e08-9821-5672418c7806": {"doc_hash": "15f43e88db5f4245790b26eb6489db658500953acd17ae02e6876c8e6b2bcf17", "ref_doc_id": "757f1e0deb93afae14ffaa7599289796dcad6840"}, "462dbdab-caad-43e7-a108-a68ec651e23f": {"doc_hash": "4acfaf8f7733fcb5a28524037c1bba3cc08b525c23084c5ffcd795d8948c701c", "ref_doc_id": "aab0d51281e9294d03fbdadc24e31f57e19e1c47"}, "e60145ca-a605-4cb8-a459-d0c468e08869": {"doc_hash": "42369e83785c15c6c6fde28bac5107325c055064cb3b86fe6db654f145a785a6", "ref_doc_id": "90cbae31b63e7e07f4bc552f8e6e536d789d260b"}, "ebd04fac-a813-40f5-be8b-98bc83615fc5": {"doc_hash": "8391704900feb063bd00b0a88e70ca04cc408ee5d7aba7e0b4d43e0fbf4c334c", "ref_doc_id": "42380e7880a85169f13f65af52b8cec8eb056ac8"}, "9f309e88-3c76-4169-a922-fe8a9df50868": {"doc_hash": "04bf783c2d6b3a7833ae968f44e9c8af1c7ebe9c48abf69ff03f3594583cdacf", "ref_doc_id": "0b53cf4d8ba360a2757bf01bb6c6e32c2c3ef67a"}, "96ce022a-68b4-4646-bd46-47f7d0800554": {"doc_hash": "615d18a8264aa0814e5cb009e510abcd298a7fd2045ad3b89ed617772cde70d3", "ref_doc_id": "2bf6d446f263898423a27c71e85d004d8fdee467"}, "52e68092-2c76-40ae-ab58-8f3e0f8dd877": {"doc_hash": "aaf9245a70441d8d48fb10bb1de78e109974e2d6ca0452666b0b5973741bd1f8", "ref_doc_id": "0190c850bf0c26acd86872d3abc6e39a542b5482"}, "c4bf58cc-7a88-45e8-99a3-1cec931c9ad9": {"doc_hash": "bc431c9ea4e774350b5f31ed319cfa6b23b608f7b2f9357c33357ebd1b5d89d6", "ref_doc_id": "cd9a6279365ace6475d0d451ff571c029152707e"}, "142d3fe2-8e36-4c6d-8ac8-0684e3b6f87b": {"doc_hash": "4f75d61ac70002c2169f4fde4dded79cbd570e204ffbc60bc6ad71b554f9371f", "ref_doc_id": "d7ddd153e8feeed45afb4f15a6bf9b80541dfc92"}, "4ed26d83-2824-4ab5-aa27-60210c0c009d": {"doc_hash": "0db2f9d846796b29c2c3d2422e1872ea7eef318ab58e19a58b5ba73e30758728", "ref_doc_id": "5d1069b68f636d9791598c1642fe6b7f865e7e2d"}, "3690a6e7-63fe-439b-96d5-6bf410ec4bb7": {"doc_hash": "2dd9b08c02e255fce9e8d55be500dd04ff15ac87b5a3d948aca7762de17b331a", "ref_doc_id": "0c4e3ececa5d957c3fffcb76f11a47af50304323"}, "bb3645d3-92cf-4ced-80a9-cecc368d87f1": {"doc_hash": "286068b1b8661a27d4376f025f062bf5c18f08c59647bdd94929b999c5d90403", "ref_doc_id": "542938dd7521d5b512d56345c5492c95a6aa7061"}, "9bb87352-fc99-4d40-ba92-e2f3d4e051c7": {"doc_hash": "64234729cfc6f4660e0c30da38177624b8f409b463e27fdce602c0839c691d48", "ref_doc_id": "ac22564adcb0a4a58e4a020d60c94599c6e60a0d"}, "d368abd4-8c23-419c-910c-b8337450a2e2": {"doc_hash": "3ebc809db4b4fe35eebd14cf9e8d6dd364393701cc4f999935c6cda9897bdf67", "ref_doc_id": "7048fa82244f1053af906675eee700b6c9880a86"}, "70f4b5ca-ce57-46eb-8b67-897a5a9c8d03": {"doc_hash": "99427ac11a7cf77f58d7f90630e05e43b0183acce44a3f5299b4c21d563b45f2", "ref_doc_id": "8ff746c7e9b22a37a0814146b898783132e9eff8"}, "403ea2a1-315a-4594-96f4-f817146847c1": {"doc_hash": "0b3b62c5b9bdce63ab11da6a31f817f6800e7fb913eda0faad0edf78eb22e03a", "ref_doc_id": "b2a39240b5d10ee437f77cc2d2c468c975c0df9d"}, "be4cdbce-7f62-4350-bfd3-5c448d85363c": {"doc_hash": "8e8ef49a9de167ac3ae38e587d1aa675d545a82576997a65f2196336358f028c", "ref_doc_id": "3fb7e6316c7c692ef1b578c5b851e9e7cea01695"}, "6a565ec5-7eee-4465-b328-54b788f5731b": {"doc_hash": "d142e7df471f50eb8da7f22682e22ca3e3a6605483c153ca5e145a67a283f2e0", "ref_doc_id": "d03a1780455f535a3e892e33a0817b27ff33ddd0"}, "8cd1c7b8-ed60-4223-842f-491fae4ee2ca": {"doc_hash": "6b8bdc3444d50e60fb1b7c2fa22c46c1377cb3398c29bdf5eee0aa32e29b0607", "ref_doc_id": "20dfb2c99844a7132271e6b5f98472602faa4e9c"}, "b40ca217-23fb-4cf5-a721-33a5c456c8a8": {"doc_hash": "9b61b139065a43fcca69881711019de0eae330aa29e6b79aa68369c2ff1c2e69", "ref_doc_id": "58fafe285f7f61df92909f18b6ea223f91247dde"}, "80510c5e-9623-4ff0-8dfa-6aed4bdf1e68": {"doc_hash": "0762b134ae96193498c8d9d27a72022f10d47ddcfb73a5244611e44bff1e0873", "ref_doc_id": "59b4215f02794470bc32c9e13b0e4c7c7b6fec64"}, "bc68fceb-51e2-4126-ac2e-03c34f694f58": {"doc_hash": "e47f129d4e795a0b39deafd564a56d6ac5feab3948b661c8d81ea5b50226d861", "ref_doc_id": "fd0e6cf50a32507bbfe5faf6d2567605f029fb07"}, "1142694f-6228-44b8-b05e-5df5b00cd7b5": {"doc_hash": "4f1ed5121a2b1246d816b40b34075006943eeae1006cefa26f8027f7eca876d9", "ref_doc_id": "deb879454f442ff04a98190d5767a7249d28c344"}, "05ed793d-cdea-4353-b1af-ca79d1727e16": {"doc_hash": "ad81fc92193468739d1dec7ec7873860c6891e624c337a2cc7c8bb49edfd5537", "ref_doc_id": "5c4169243c44062c6071b43428bb6a2f1f759625"}, "3ab53cca-fd7e-4833-a8d6-00f92101f1fd": {"doc_hash": "80ecbc72abaafebb330445bb6676bad8af82b2510d5d5e75e991516f4bd4f0da", "ref_doc_id": "e75627093e093dc1157afa7f5e2d8b4d93a4bf3f"}, "26b69072-2e3a-4b91-877d-35b3234ba613": {"doc_hash": "97dce92936de0f4a7af430e89ef66fd0f490a26de1215d23c34f7c4c222087bd", "ref_doc_id": "f95d2cd62e28e7916481e1c72688a302391b4830"}, "4e6798c6-f370-4aaa-8a61-b0e9073964ba": {"doc_hash": "d7e42b27ca85aaa0d1ab3654784e5ddab9f1eb4bb8559d53cccf17fc2e0f18b2", "ref_doc_id": "650939e22e20b56a4e25f03d7ee9ae9f70ec7843"}, "11e71d21-c88a-4dae-a096-0fc050374794": {"doc_hash": "482d4596ce5f882fb5e414c8aa1982f7d5d3da93ccb24ed6956ba5b21fa50e80", "ref_doc_id": "4e5f0679d046dce9c09e45a7265b9ae776c527ce"}, "52868fd2-eb3c-407d-83eb-fe99a8d6f238": {"doc_hash": "075d60f41fc0b10bef2f6663f1d3579b03284071db75f61ec840419bff283b8e", "ref_doc_id": "09edad536982ffb11596ff8668ece20d93dffc99"}, "2f1e1d32-7a9f-46fd-a1dc-75ec1aede38c": {"doc_hash": "f6a0d6c0ea8525d6f4b12eae7deba941645a615bc375fd87b7cdafa1ea618c85", "ref_doc_id": "43e9011ce30940c5c1e3a635d83bbc8cc87e4402"}, "04dcfcf5-90c0-418d-99f1-e78420157555": {"doc_hash": "caf6ca3055f5d86c0e1f0273c11fb64a4a7affcaebd2434a39fd70f7ef07cadd", "ref_doc_id": "46465555d31a9669feb53cc28d0ff1c748ebe69e"}, "0e371af7-7895-45a9-aa70-6c6d80e65938": {"doc_hash": "e77929626f49cd6331f3a9912541e156c749e5a5800f506524bd17d1f707d63f", "ref_doc_id": "879fbccee52df8f271f71a4c5ea48dda22638d40"}, "3f7e55f0-7a5a-493e-8c43-85103f33b9ba": {"doc_hash": "e0df075ca41db56ce279673b76e63083da3af50a3b2ead096b7603873b49c261", "ref_doc_id": "0ecd56ecfebe51234fbdc56933728e191581f585"}, "5b181ce6-b21e-49cf-8554-dca5e85f3537": {"doc_hash": "6baee93ea7da2674b70f79083d5f04c0b7e118dc7e8f22ee1b6a5746f4abe5f9", "ref_doc_id": "a67cec3bc1de08e112b0dda0746cd2e15be76228"}, "ed717d8a-582e-4e2d-b352-02e664ffca04": {"doc_hash": "087ec7fc44b67324dc3e7d769ae1e5b18f5323e55e92fac735903192a5ce6e70", "ref_doc_id": "6ed192170e17debba7cd23e5681b8faee03dfebf"}, "ee8a6591-03d6-4562-a760-c4502a7f809a": {"doc_hash": "458268f70f2af3100636791b24950ab137cd5f9fffad8c292950d200a5d7ec39", "ref_doc_id": "597bd6798f3a59cec74173593b0d668b4cfecd32"}, "4ce34e09-dcb7-4f78-8ed3-265cf316f9cd": {"doc_hash": "fafbd2f777a367cc44b4ae127bf8eb24209542621a7494ad055296863087bb22", "ref_doc_id": "26888df65a1c3ce297ef5ad574454781cec100c1"}, "53522978-ecc8-4da9-aaa2-3f15fdbafb2b": {"doc_hash": "15aae3b41c0c5d519d8460dedb386bdfc2e9d326b415b5cd6716060a6309f1dc", "ref_doc_id": "98c987f9abda163920bbcf93bef712338341e766"}, "d4926ef6-fa79-41da-a575-6c7a22b4991f": {"doc_hash": "88db0d947543df37495b8f67124d9520a3ce55448d0355e706c35ed9f285e083", "ref_doc_id": "d82ce271042dc3d97f67d77463cdac19558c6908"}, "5140af2a-8063-4fae-988d-17bd0b853008": {"doc_hash": "7ff26b1a5f3937a3a1b60c70bf7dbc964cf24f89a66401513e795345608aec74", "ref_doc_id": "d978ccdcc20b0d6cb39ed74fc2747e75c60f4346"}, "078f43e1-bef1-4bf7-bcbb-3075c37f28ea": {"doc_hash": "ae1bf1a06a338df45202af8dc53cb441e15a6833464bca93a376cb96fc6c695c", "ref_doc_id": "78c17615708ef4c3aa78b327ff01f25a1daa1974"}, "4db5f860-562c-4bd7-9df5-9073880a2958": {"doc_hash": "0db776a592f6cad4fb6dbcd7050da65ecdf76648c7737b748233d458598bc5df", "ref_doc_id": "4d6e265a30ed48986b89510d6ad1faa09d8bb6eb"}, "7338529f-8fea-4061-a39e-080f447be73d": {"doc_hash": "f2d00868fc623f4f3918f9f16f758443145899fcc1224eee4168ba120fc402bd", "ref_doc_id": "9a53fe6942f8027d3b5dfc76eaad3e5254fcfc4e"}, "dbc0d42e-f6f4-4057-96f4-eb0e3f6fbb96": {"doc_hash": "c70b8a2e80e95bc96367b7872ac34c2f8b505a00dbbe4f999c7177f5d9561c22", "ref_doc_id": "12e8722618cc239c739fb8c561e29084743e6996"}, "86e24547-9b19-43c4-9c43-691579dead5f": {"doc_hash": "fa3cde4697b0202898facbd82e6ca34dab278ed1056b07efa839458fa5b06a24", "ref_doc_id": "7ee9de231fbf091b59efc4b2b809806de3b72494"}, "9276ae1f-7304-406c-bac7-a966b97db906": {"doc_hash": "d66d0eaf1c8ad9a1f97accadf657d34f41e1867c5e162a0cbfeff45fc0f175a0", "ref_doc_id": "7c52e55b8f66941a7fa94eb7aac76922c8ecde3e"}, "e766e0ce-23ea-4b75-b3f9-06a742d4854a": {"doc_hash": "e594a9e6dbbcbbedaf07c383ebeb6d3d8906b7c9e4cd7e52c3ce3712ab9b1c14", "ref_doc_id": "35f749210c09fdb470716d46b794b5f45eb42284"}, "a155cb10-9e7c-4905-aa9b-bc762fb2c9c6": {"doc_hash": "7983dec172c141ba5de56ebe88a8b8a09d0d43db40148be59fd98bc91075d56e", "ref_doc_id": "cc6bcc9d27ec11896321ffe29054948f16833d5b"}, "013737e0-09b2-4744-9b7d-e8f6cc7f5080": {"doc_hash": "b7a62a63d8488e0a9f3ea3728cf0002d76ee01457279ab28447244c684d24527", "ref_doc_id": "5ea5e8752d599a6c3a16cc00317585cba0f09c1d"}, "9c2c87eb-b2ef-488a-a9f4-1ded1b9ed427": {"doc_hash": "0a241acccdce70998b103100576e536ec2bf42a12beb5bfeb7e78177654ca5ad", "ref_doc_id": "7fc32264593d3d829edf075fa8cb61ce0bfe1f02"}, "bd118f23-569f-46a2-a0d0-e77dd2484896": {"doc_hash": "1851e85dfb5bb7973fbf61e48980a51c7a541a711925175dbd928cc77b5648d4", "ref_doc_id": "8a24a208766627a9c8dda41d10a4c9b371893dbb"}, "1315267b-b38c-4a92-be10-1d82f11792f8": {"doc_hash": "d3b4094b2f8e4bd1972f00539935145b7e0b8576cd123e86e3251adc0d101e5c", "ref_doc_id": "f5f62b2305f5b3a1266816a748c340735ac6d6eb"}, "6c3961ea-568b-4668-84e7-9b058925a2cb": {"doc_hash": "54343492f5014b622f5b8472525e2aa958044439ca12782865dc94b91aa28473", "ref_doc_id": "4168de5426fced8f70c1599b129094c399edda1e"}, "cf0342d1-6e42-4b71-9e71-6f12b7dc8f69": {"doc_hash": "86f270c0baa3ec1aa76a7523a3a2a87363a0e892199b4951447abb27299d34ea", "ref_doc_id": "6cbd46cb8b1ea2a0e8bf46a763d92dd33093cf68"}, "96c90f72-dbb6-42b6-aefd-7d46d0eeb695": {"doc_hash": "1314f0b5d03fd919839d824f922d926d33500a02d2184af4d46cd04b92f7dc2e", "ref_doc_id": "1ea78a1d30232e796833bf40391bec307db04e99"}, "fcd9e1ee-baa0-45fb-b33d-8b91631ea281": {"doc_hash": "627c736befebae34c2d2137d6553b3b105d5a17d8f8767ade9a90ad03e89e579", "ref_doc_id": "60071a4c2cf6d03df19c2d78dd2ea311dc0078bd"}, "2c60fc95-7338-4a3a-9a16-46e133a5f6cf": {"doc_hash": "311c2d3a726c80c8c3c0251dfb8151558e59c1d127b0a619a2da9b11cc08c218", "ref_doc_id": "28bd274caed599637ddcb651116ece0586cc6e63"}, "26dd8d38-db48-4ab8-901c-05bc168d1ecc": {"doc_hash": "78d3e49ec05072fb880b52c0826307bbee7b7e05473aacf9841ce6e6edd5a2ba", "ref_doc_id": "6692b3046ff299377dc55595e44ab37a6ded22a6"}, "a7873598-4f78-4d25-b96d-99f7ae237790": {"doc_hash": "6e474ec5780f76957c234cf7e58338cd08c22985cd0925abf96e1158c35e9f40", "ref_doc_id": "8a2b769d8d75382122edfe502a0ba3ccb4ce4ca4"}, "ae8b9f92-6558-4e9a-967b-37580ca01af2": {"doc_hash": "8635bb0ba062576a8b6e87f9f48222563a7760e68c5a739a2e810099c585d88f", "ref_doc_id": "c7606e62ac5a13833186e84288ce5e225ab24888"}, "884450ad-1f0a-4cba-857e-6dc56fb1291b": {"doc_hash": "da2dc68e7ce1320baa10c159853e7848b50c4e1d38d9b8cfeacae2627f1e2241", "ref_doc_id": "3cc68deec351468387158cbefcb3f0e3b317d47b"}, "875f77c0-6b71-487b-b79e-454387e0c7e4": {"doc_hash": "fb6952a0ef9a99cc11dbbc15b3269c43aaf54ad91d451049ce11df589e82731e", "ref_doc_id": "f8e23654b4ebacb82d6e70bc56335dc6e1902a85"}, "72bfbc57-e17b-4c79-9e77-da3dfaae4f6b": {"doc_hash": "47a8ff9cfc8bdc4f370f4e214ac832428b73de32df2ba6b0a20b2d8e9ed9af6f", "ref_doc_id": "ab961b59ebc7d588aed8db130b6cdf93a5259d7b"}, "94ac891e-2db8-4302-bc04-7eb75f894077": {"doc_hash": "0a0f5ebddb74f3eb0a4f64dbf8293998d86825916ca5999de4efb1e05939b0ff", "ref_doc_id": "d6e27b2309465f8cfc56b77048050f6b39a7cc0a"}, "17bd13df-427f-487b-9b0a-0204a4699a7d": {"doc_hash": "74e1951ba84bcaa424f2626f40e472119f59d5f82a64b88a79bbf0b36b46328f", "ref_doc_id": "b388d315826c8c8c2c9ab1750ed30bc6742bb3bc"}, "769b2334-f764-49ce-9206-40bb7237613b": {"doc_hash": "c3293f1b4040a17a8f9d13dcb60cb3cf762b267f01a83f49ae4596c3ff09ad78", "ref_doc_id": "a0d3735a7baf29d05eae2f2990246d9223027e58"}, "46e4c544-2eba-4bf4-a930-b232b7eb04c7": {"doc_hash": "51b9b5751ac1e98a97998bf906a3368b36ceab4f92ee1d6e4a47d38803275329", "ref_doc_id": "15b91eec0647e7ed5d1f8c5e7907933f8cd1b6c8"}, "c7445092-5016-47f4-b47f-ffba1c08d7ea": {"doc_hash": "a72abf81231ab8b9d7b007ae2d180c9ec952e37f6b197efe1bc9783f1c9e163c", "ref_doc_id": "56a5d36345a1321b8e3cef5d39e7e6938f58a270"}, "f6c401e9-206e-477b-a598-ea41ac6040b3": {"doc_hash": "e828c5d87b13466c260ccd435b62fac5ef591f79c8ef59829ea909e766aba797", "ref_doc_id": "eb29bb1bbd40d934161e8b0c52c8fa725ee88eb9"}, "d64a7acc-fbae-4430-92f8-f67610e918f7": {"doc_hash": "b9c1e6b214740c4c9b0273741140297730f47c51c586a1ec7fe666a2d2143066", "ref_doc_id": "cbda5302f5b1e4202a5996618f34751b257540b7"}, "edf0c907-91f7-41c2-9f44-3634f750bdd0": {"doc_hash": "c54b78996fe564c35283494a63a6bd67d949716c618353e7f5f1073d47fb57b8", "ref_doc_id": "b8af2aa0e87dc4995c9c22439e5dfde16b7d8824"}, "5ebdb769-d84c-4808-a30e-8475fddd5a17": {"doc_hash": "9d278327f295562c2f722c5eaad89f1f783067a2adeeb6be94d27916b253138c", "ref_doc_id": "3d48c81cfc6c86e9ca9b3f03537ebdb56c3175d4"}, "865b05c5-740a-4f1b-8424-bed6689918c8": {"doc_hash": "8a80e123820eb424259b2374cdab059e57580f86131a65ca1177d13e69c4e45c", "ref_doc_id": "054370016c232d223a94b5ab0a735a835e0d07c5"}, "fb24c6ea-d8ef-4526-a3d6-e5b445010858": {"doc_hash": "e866084acc15ae7a88b8ad2a414afee6ee9fb07f8b36866138c1344f9777cf60", "ref_doc_id": "f46d3a172fe4021651bd442ee85d6e196864369b"}, "79f2908b-41ed-47e4-96c2-bbdbdf0697be": {"doc_hash": "99fa7dc6aec033b32be1b518bb597daf1fa1abca805e529ab34407143a5dfd15", "ref_doc_id": "e021feb3d795fed17742261332e3a0c6dc4b7ea0"}, "7558c0d6-f51c-48fb-a8cd-017257ac1a7a": {"doc_hash": "d78f6d57b349fc288705eabe249e69b450e2b646afabe6d48e81962c5f04d278", "ref_doc_id": "3e5fd6aaa813de68511947117ed3433e555bed76"}, "4208bd86-6ea0-4d9b-83be-6f21e8a104eb": {"doc_hash": "9310e19da36679cf9b39c0c2114575d27a55102ead2e3518a1d35a6aab720e02", "ref_doc_id": "a14267d569b4fd0e1b47ab6d1deffab320431983"}, "96804470-ffae-40d3-bed0-ded6bc7792fe": {"doc_hash": "28020956db52ec9b668a92f2a0b336b250663606a248a4079e378f08a85f7424", "ref_doc_id": "9e4378809ed87be6eb84130a8c8cebf2600aaa46"}, "01add33b-cbf1-436d-a912-61cbf7d37044": {"doc_hash": "baeed4db51583ca190e8286f7d64ae3af170ba6dae9cd1b74a388fe6754e5037", "ref_doc_id": "1a0d70287e1506ee9bd9846b76e5a7885d86cf99"}, "3147d8f6-f7eb-4ba1-a928-f61e9a0312f2": {"doc_hash": "8da09065e22612f51219be0b2409c18f4fe47f59e0fceb331fb4db1a6f7d4ccc", "ref_doc_id": "9ec1682e064b854d4c0530818d4a4cd8fa691bf0"}, "f3e8e12d-8bc1-46e4-8030-d250bcd51974": {"doc_hash": "75b428872af93ca04de94548404652838d80a531ce196841ff6d29c9255db2ce", "ref_doc_id": "2d284f695b264749117a7b11dcdab395f3e40431"}, "edbaf4f6-7fd4-4ee9-a9b5-42c7a878d05a": {"doc_hash": "752a1223103759f4bfad1b46c111bd96c84eee490353cbfd42cf9d6471a24344", "ref_doc_id": "08383b48b4feebb8a1f994773514ba990dfb50e2"}, "25fc1f5f-1f1e-478f-9f7e-05f2be85a8a7": {"doc_hash": "bbb92781e9a2e88e2b49db05d3e3f61a97ef0566fea5e33abd4ecc1d5e1dadfc", "ref_doc_id": "af7cce494818be3843e5fb4f2df22938900a98de"}, "e4a8e040-ec6b-47bb-985f-71ca95f8ce5b": {"doc_hash": "87b2db7cfd45ddc995251fbed14a7c04000ea9887daa1f31da9ad5786efbc963", "ref_doc_id": "43b2d8ff0c00fd536467985bee8cd3a73b666556"}, "ff840cae-decb-4741-8143-58405d021743": {"doc_hash": "8d37f6a8f50b10d17ea5ac1e5f650d3bd6e60faa1742da754ea4281412bf7cdd", "ref_doc_id": "43d74ded76ea7c3c9f5cb4b2915d487990f99525"}, "387d9672-baef-460d-8a64-b1b156a28348": {"doc_hash": "f5f8e39ed920d9543d1bc6abb1074909bfb9c570687143e93acc8df9d9a88a30", "ref_doc_id": "34dc85f0d49b0ad409469f5ef5ef54edd6e860fd"}, "13ec55b1-45c5-4678-a500-fe89c727f9c2": {"doc_hash": "323d5fa18c057e47a8a630314a1726e5dd346d5f6c4d2dab0fbba8a92ddb17c6", "ref_doc_id": "162e80ac7102b451126e00f218131e488dec80f3"}, "47fd8cf9-f5df-424e-ad19-f5064a3a0063": {"doc_hash": "36f770b09e6768e487537e35c1f357df11080142ca0b96239bb92f2254996ced", "ref_doc_id": "54ad5d2f7128fcc54caa013b0de0888311341722"}, "5357751f-4d11-4884-934e-873bfe452639": {"doc_hash": "b0da676d6c495cd37143db611cfe57c008977966d4e94173b11af385d1ba7947", "ref_doc_id": "a39fcd5f089d403851b69caacc1e5fc7ed47c264"}, "f16c3eed-248f-4691-99a5-9c0cfbe74768": {"doc_hash": "33073d8f7ba721660645783d3ce139dd92b84d4ed0a44ad9066bb01a491cfeb9", "ref_doc_id": "ed48bdd383dc09e2944a569eb0d4646c91fa59bc"}, "5bf50750-46ca-4ba4-a0b4-82adeed6d434": {"doc_hash": "22f986475503d2285b8493aefccf1be0f9061cc194d689280114412cbcf4f598", "ref_doc_id": "eae0cb598d1a7331a43eb36ec679ee34e3422146"}, "c335cae6-cc33-4f79-ba59-523a1b854b3a": {"doc_hash": "441a4d7ee3b8d909d1a18defc7e823f73d45df07559aea8f00e3b22f87d8f3bb", "ref_doc_id": "f4f3ffb9b7497c715ded13a91fd97216cf5324fb"}, "fc42c7f4-01d9-4d37-ba86-6a57ba73e074": {"doc_hash": "c3b78080167f2d4c941a67a7d8c32faa1401374ebfb74a883aec14871b1606bb", "ref_doc_id": "9af803ed3c3f35dc34b33870976455911eafddc1"}, "7f18cf8a-38ab-45e9-9aad-ec86f0324697": {"doc_hash": "87788620268d530e82515dc681b3438444bd0e4c32ee5924b85b5d699458710a", "ref_doc_id": "c32021044eea7b39eb2d9e3fba03d0308e187f9d"}, "0135998b-137c-479f-925a-d95d8f0a838f": {"doc_hash": "87a24054824170796f250411ba68aba768d627efc751c781ec550f343a2cdd66", "ref_doc_id": "6d23d3f649889ec210eb96989d83365b43b0551e"}, "89e7350c-0630-4383-9b66-95c37ee45daf": {"doc_hash": "4d59d12677b4b3f2c434e85e1be5fba31cc6c3a373cfd2e7db489efc413fb76c", "ref_doc_id": "683e8c50ebda5a9d8ad6b082e64fceebc3261f5e"}, "6b600fef-f262-4ca8-b435-674241e52075": {"doc_hash": "bd6d7bec5f5b6f8c316cbfa9e741eefb910df645d4bc6f6c8f0a97cd6eb2affa", "ref_doc_id": "83a19316c19a6246ac7be1ded127fd1603907be4"}, "1f4a2d54-95d0-4ccf-8a24-d439595ccccb": {"doc_hash": "14ca47cf632b8178680821a3f0fe1aac4e51c84f9e0ad53cea058bfbd3e345e1", "ref_doc_id": "2511eebb101dffbf1c87fdb0b73a582882d9a63c"}, "cc314758-d006-4dcd-b9d6-9f408c12e846": {"doc_hash": "c90160a50e7d0f33519a47fb42eba793edcac3bcde7fa34cdfa39b7b41bab8f3", "ref_doc_id": "7d45b19879802dd16a33b9dd9ef68e3a121109c8"}, "a81383d5-102c-48b6-9785-17640f0206da": {"doc_hash": "b68de93471ecfbf5a43b075067d37720e439aa2eef4d3cacb115c409e3a1a182", "ref_doc_id": "c1f0eff012ad405263922913b4070cd48b98fdb5"}, "9fa07f9a-5e8b-4ae0-8b09-a59c4d8840ad": {"doc_hash": "d45c0d9b3ab6ed86975d36b63bbecc7b6d1dd55a21394d2c2d1f8fb14ca22ba8", "ref_doc_id": "af381e6b3d6d2ee802f342437983d48ff8018002"}, "5f7a3799-1ff1-4baf-a1d8-f8a5c3bf86b7": {"doc_hash": "8444d41069e0f4561ed26bdb502d89f19959ef8db69869589d036bfc748c3ef4", "ref_doc_id": "cb6a6e230d166a9550f76454e95bd23bbc6d717e"}, "c722246a-a835-4500-bf18-c5b6d9b05edf": {"doc_hash": "950461026805c3da1c68c82db1abe87af29bb34ffdc0b02f22e705f0925d42a5", "ref_doc_id": "2b59c4dbecec2e420ac2a9f9a9510a02c6808f13"}, "015603ff-b935-4164-951f-4f34f504a01c": {"doc_hash": "6f23016de6bab9d4c5456217fd8057956148bc7c8aac4c167acc2202a2f85cc6", "ref_doc_id": "9678fe6b412c9f7d4839065feecc311c6535572c"}, "a9d6e56e-fd5c-4dd8-990e-a26c11e7330a": {"doc_hash": "1ce608dbaa09bedf5107c6e9c344d9201dce5f31ebff3fbe6b1bc3e4152d22c4", "ref_doc_id": "4c58f355afca9c4d92adbf06bc96472dcc51cc4a"}, "64702c81-29cf-45c5-a038-0c01dcf435df": {"doc_hash": "63068d2db69e7051ceb09b8838e17e8e3a4d1b4030a760f0916052559a5112aa", "ref_doc_id": "20d8ddd47f8b4eaca063d2777a6e3d929e27fa4c"}, "3ede26b2-8282-4353-8bd1-fda2ac1abf91": {"doc_hash": "a2fa8a7cd2d82d43c1847a481c71dbc4cf0e7d4f0d3214573e2e0671c6d09142", "ref_doc_id": "6ed6504d8d2fdf8ef54684c4a8f89aa942332b7b"}, "cde30a2a-91b4-4e6b-854b-8aad4f400cd3": {"doc_hash": "a116adb861dd6bea13138e78b39507ee37b90410b61643f73a58703544643274", "ref_doc_id": "a61305e895dc94bf0152d72eea1433bb572e0b86"}, "0b53e727-aeff-4b2e-b06f-e3597dbfba96": {"doc_hash": "aa1ebe5128f1b91f6fbbbc55e7eaf7967b602ab8ec6415e410eb1546fe45b67c", "ref_doc_id": "bb8ca7a10d21a99bfdcb66e44a54b5590ea7bb41"}, "d6377cdd-7455-42bc-9b16-f5ebb38a3240": {"doc_hash": "577bd8f056af97814151ccce364ec9e19d747aea75af2997693137d0ab119df0", "ref_doc_id": "4362e7f9dfdc56d3830bcc4e5367a97e60d74aed"}, "1fc025f3-9177-4077-a6c7-cbb755775973": {"doc_hash": "1164c931e57ebae46091eccfd76257315cd6aec1ea8cc05a9af17c23f7055d63", "ref_doc_id": "fe3717d7aec5fae08bd7e851cc20aae92ca64176"}, "dd6beb6c-ca5d-4a06-9718-49c193f15902": {"doc_hash": "109e37029ef9c318f6459cd1f769811dc6531c09ebd5c1a7a98e99e97ea521f4", "ref_doc_id": "13cd09784bfa61b79cb1f019da168bd1e44a8d14"}, "663ecd12-646e-4170-b4ac-52a2ef0860d1": {"doc_hash": "a90ba2bf9c54ec60ecd4a035822ad8e05ef93dabb69471f3ecfe7d5514aa35d6", "ref_doc_id": "84fe4dfe57f76f671e5e82ec192321cf34d88214"}, "6c427cf6-18a0-4f04-b628-3051bbf87814": {"doc_hash": "124cd0ee18635f2c1afd9339cf49e10d82d0a1e03000aaf29e33ccd4a8267ebe", "ref_doc_id": "ac29761e696a968494cc57d506078b74d4bf0ac3"}, "dcee966b-5097-47a9-b0f3-5553ccdd14e8": {"doc_hash": "561370bb72ed56a833477e65ab5cea8767bfae1ae119bdbe73ad289fa8738bfe", "ref_doc_id": "bd36c295e59a76c0d9c684d05a537100c7db265c"}, "b2c7effe-379b-4d09-a7e7-cdb93de1d638": {"doc_hash": "e359a5549b357328c5ee3a5ca0ebbc7e3707f94c7f3cb3f23aca1018521b84cc", "ref_doc_id": "60c45f68d54000d2c5fd26fe505ac09ecec66b5c"}, "22c94b06-5d07-4f57-bbd1-59eb0fe004e3": {"doc_hash": "6822e393638ce656544c0f14f90576b082543e48bc638d078d1185df46bd1467", "ref_doc_id": "40be432eecfe185b006b51294a8d122bfb6f7c40"}, "8dc600b2-ea33-41f6-9969-55f1e73725bb": {"doc_hash": "63204196b8bcfbdba6883eccc68e56e0fe296e94ffc9a8d7bd04572d3bc2650c", "ref_doc_id": "727474cb8d2c5790028569cc69e657261a108a3e"}, "48f67bf9-2f29-4346-8fc6-ca589c7af96b": {"doc_hash": "ee52712cbf278dba81cc4e068b874c3985a551ebfcedbf5e9202d9bdf8be5f58", "ref_doc_id": "e0606ea5f2f66d4530d53b92a948204d63be5191"}, "517aae2d-df08-4db3-93d0-5b4cb6b9e430": {"doc_hash": "430f84619e3d7f29fc5773b4adea1728fb31d8e69c5bfac379a04c2e6ab0de40", "ref_doc_id": "6bf2378d051a3e82f96a6766c5da3bbce6134336"}, "9ea8545b-90f6-4e49-bc3f-4b1995b8f506": {"doc_hash": "32264ad6677dab84f521e21eedf23e21be2f4d3e1d70edd0046ce9e0ad9a7ce0", "ref_doc_id": "80ba1182c081a6fe6940f9bbaaf2aca5b760404e"}, "0495e7c7-a396-425d-bb5e-e6d22c2a3663": {"doc_hash": "ec0fdc00532f983ec951599fa361bea3d6b896bbc4a11595d9b596fa6cde3c8b", "ref_doc_id": "f678f2c6095eee77cbbbfa375d64206c78d8c429"}, "863211e7-3760-4a12-8bdc-6707bb3853ae": {"doc_hash": "aff73b1741f54e1afcfaae9c7386904841da137a0b964a96a1d01d739ebdb15d", "ref_doc_id": "bfac5a69f019d4aa9f7b2a5bc4f158e3a4606a80"}, "0e588059-7b2c-49c7-9ef6-b2c8d4a48d44": {"doc_hash": "b03e72f020e3414b6380ba912bd9fb815a60ad28f64ef4bc655785cb841d018c", "ref_doc_id": "a33d292f6205ffcc5146e83258da344dda1f9979"}, "9fba9c1a-3f8d-4d7d-9f47-a292f53d4bfe": {"doc_hash": "7aef88971d37d9b5e119019e5d59e38de8ac93cad62fe7c87ab1d634b87973f4", "ref_doc_id": "9099216a7e9e4b4cff2ed38112fc536a7cbd9b94"}, "a633f210-fc68-4cc5-b377-def14456eee3": {"doc_hash": "9e1ea68f09f3191d9ae29fb37a51b47adbc78480c69b7f07276207f371eb74ce", "ref_doc_id": "067b1fbe41f1ee204ac838682d0eb3b863d81e10"}, "3a8f06a9-8d09-4135-8a6f-7000fea0b2c1": {"doc_hash": "6bb7758e96f2740aead06847004ff4970948e0debc897102b3626475718f9d0b", "ref_doc_id": "c419c3f753f6fd5a03cf5dddb504aefdef996253"}, "993e0164-e598-4137-b784-df6e0e466bc8": {"doc_hash": "11b4efcccdd7d5a92cbe889f0c1fc0fe6b8c9a9b28ebe34986026688219f9af8", "ref_doc_id": "46196e28cc71c6406f891b030e62d80ce99e78ad"}, "941417a8-8510-44a8-b3bb-f0c3679e4063": {"doc_hash": "925adcb78804e772241f8f5320fd4e66181382da8a98cf0a554da95446fe1a5f", "ref_doc_id": "08da8bdded2b73a1a4b731dc30454f3e26d5bb0f"}, "0105fac0-6144-4707-80fd-1a39856c7e8e": {"doc_hash": "eb6aca429cbf3cd6293e4a070207d35448c2b47b5f022194f777c656c5ba6e29", "ref_doc_id": "9007974d92c18f316e6d7ea38f76c2da7461efee"}, "69ca6e5e-c50d-4c21-8983-7d53e18a9fdb": {"doc_hash": "27abdaef379b1db0276802d7ce908d89820e677e9a56dcc78aef53d7943666ef", "ref_doc_id": "8091da51f2b03c30974f186babde0e8d1f62309e"}, "e3a75f41-494e-46d2-96dc-0ebc31122413": {"doc_hash": "be23497ea8dd8a6cbf953a945d23e52a11327e95c035c968ac3cc3598677c609", "ref_doc_id": "84eb198685c264eac4a9183f6906abb4b4a98d2f"}, "c24bb819-1750-4567-9490-8bf4cf63f437": {"doc_hash": "ca4847e2aee2a642108f7d740ffbd5affef71321009df81a8d6dd02943fcc710", "ref_doc_id": "3b2f298c2d1617c2a680f6dec501e9d20c41684f"}, "f4b433f4-3b52-4630-a576-a9366d08d3e7": {"doc_hash": "1a06fc8a79a0563ab72edd8ec2fe87dedef25404bf23041babab5bca488b7b3f", "ref_doc_id": "85e1eac31aa22b348b3aa13155257f6e4c82aa89"}, "efe53e2e-0c99-46b9-829a-796469662c89": {"doc_hash": "320191bbc7b93d70743af2625256bc6327f5ff80a23b846abb5223f4520e2e0b", "ref_doc_id": "a03d1953b7c015f646be41a483caeb0e103b4220"}, "fce12447-47a5-40e1-bc60-a8afb45a241b": {"doc_hash": "2f1f609a09f82c8d71e3d194cd037ef9b3656af036c411f21bce82010680a95f", "ref_doc_id": "33ee697f599b2805b5329823b7ba34a9306f4a2c"}, "22e8eba6-91a7-4bae-8ae7-a09ac3a7d9b6": {"doc_hash": "dbe9788fb81841d2906f20e8e10b5397b31fb18492671528e1b4fa4c1c376dfe", "ref_doc_id": "7c70aa23d57862aaf4bd7ddb3a866ca4cb8af63b"}, "479911d1-05eb-48a2-be02-c30a1b9cc837": {"doc_hash": "be7551c9e1df3ff714bd3a6c6ed8847b3290be495e44b70cec4646908002ddbe", "ref_doc_id": "9c714342127a6855cd46748d76458f77e6741782"}, "714a03b5-50dc-4a7c-a549-65db96cae3d5": {"doc_hash": "fc9f867da56725c11ee6dce29cc90ce4365bf78fb67ec34198aeb6acd9c03dfc", "ref_doc_id": "aec44b11d79026095c37b02c13d5ade2185dc5ee"}, "d455bcf6-f55b-484e-88ab-f0b1fcdbaaa7": {"doc_hash": "c2d620814c26c919e39ac4cfb613fbbadda3f705b5b6be5069d9026e9afec227", "ref_doc_id": "af2e07a58fa4e935374b3205f1f576c5d832719b"}, "5142deec-cc76-40fe-a90e-d34d90cbfb8a": {"doc_hash": "4ab12a19e3ae12a101fbb56cbd8a60024def10404702dc706a8931319fc280ce", "ref_doc_id": "bb43ece399d17d5eddd6503382136f699b9461bf"}, "bc92ec55-ef92-47a8-b64b-9e6852fe6e19": {"doc_hash": "1d3354eac4dfee92cda6c8b062e452a503b58eab3ec16f2e0d40d2d0c631fa5e", "ref_doc_id": "fa1ee2a2b37ce4b2e73d65e50158c8e9d339e622"}, "72a4b730-e316-4320-8de5-a8d5294c430c": {"doc_hash": "b2e1998bec10b269f05a6cc36e9f30bbede3ac1d5b1e113cae15a8be9efc3aa5", "ref_doc_id": "fde85ca0e7c587a432998a27657959609b1c5fa1"}, "e6888fa5-6c81-424b-a57c-e74ce5966f0c": {"doc_hash": "8676eeeb47799b4b2ec4e7a02c8c335e38746745a96d6cf0e52d1a9134004efd", "ref_doc_id": "1e290fe18a4d1db7a4d05c2bc4f79dccf23e7fad"}, "e56f6282-462a-4228-b1c5-2a44adfb946f": {"doc_hash": "b6197744ee813160274718f56f72640e552dfa8403728f3d10b0938990a6a314", "ref_doc_id": "8ac0ff38a07d18a59329c21f89b4f7b11b0941cc"}, "352c3bca-963b-4745-b690-51ac62783d37": {"doc_hash": "4b73b39e9fa9eeabb888d222f4cd68c99884b586a00c1de7733d6791d521ee99", "ref_doc_id": "92a0edc07916d81956005ba900ee646d86c34d56"}, "33b4fb12-7d0e-4f77-a122-e30a8f94f45d": {"doc_hash": "7d0b147d50b57f113a55b8f646954ea0c4a3c95b0e9e9a90b9740d100268bec6", "ref_doc_id": "bac99c24902a76dd2a1886038b3993f197ad537f"}, "23aa3911-c09a-46ad-8d63-615e28070d58": {"doc_hash": "f0eb1a9e0d9f2447492dddc768e2f61c6dcbc6af62e38e838040bf011ec6493d", "ref_doc_id": "ab19c6d419976b8adc3acf90571e36b1307149c6"}, "563f983a-ba41-4083-8aba-a9ee2b339c46": {"doc_hash": "544054a65f1a174e08e30526ce53a67f8d2c3d6cdd0b0b1cd2b8f1850c1c531d", "ref_doc_id": "0d95fa07ea28210ed8a2b7f855a2fa1201b3b726"}, "d96a1780-8361-45b9-ac99-a8b712a20bfb": {"doc_hash": "21fe4fd0518e5e82f3e5dbba7bbc0e49661e95b7da1de245f5543d462651ccaa", "ref_doc_id": "72158a169d729fecc167992fdeca176a685829db"}, "99b420b8-48e9-4874-b1e9-6a78432ae508": {"doc_hash": "aef0420e3ffef7c5b1b9581dcc74539d5a74568f8cc8a6edce707190839bf88b", "ref_doc_id": "961377d4aca7ea1d0274b49daceeaae10137dbdb"}, "cdfb7534-bf82-4c32-9e59-ed870b05178e": {"doc_hash": "11bc7f91c2c8eda09bf62e51cc2632d82583414a5afbaea40ab4ee1472b1d619", "ref_doc_id": "74ec8e3d5142748a58917e5fa625fccc93cab8e1"}, "e9c24c1c-10ee-404d-8d6a-048f5d76cef6": {"doc_hash": "b52fffbf679e746eaf87f5a8079fc31057be97fa8c4399fed37e6fe90105c633", "ref_doc_id": "0a9a83e64b89a73c86f2f0eb737eb5ccda93a8dd"}, "d41bcda5-e843-4111-a25f-6f36666b5082": {"doc_hash": "78c281d597c085d33efa7d6d224d705c79bccd7176eb35e4a385d2fd2516eec8", "ref_doc_id": "8d7dc7c977c1b86d5c7e7aaa12e348bd3826d9de"}, "a9f37820-8422-4414-b7bb-3f46e331ebf5": {"doc_hash": "69081e5e9b19cf5797693e9ef9c9ac1fe8790041f2b85678c1a96a548e9b1270", "ref_doc_id": "4d04873a942949ae2ddf345444a0e1e470827d06"}, "09cf7df7-e8d9-4545-88a2-4cf2b5ea6762": {"doc_hash": "9c05dccff8d9447b059c5cd0032022533ca73da94a7ce6d4240dfde0559579c8", "ref_doc_id": "86b9e5886e1f7db18e4d0088f116cde432c67bf3"}, "1ba6c9e6-a9cb-4ca1-b500-0dc1912d9866": {"doc_hash": "1243bc1b53e5cb85c6cd37718cac3795a34ac6d0b078dd252dc4d35330cd38c8", "ref_doc_id": "2a4d13bcd0a88f856d45915747867fac3162dd3b"}, "11e7178f-4809-4162-b1fa-a4617c8973ff": {"doc_hash": "b527decc2aabd5564bbe4e440e5e3950e4e8cb8ed90ea2b82926937d9e37b618", "ref_doc_id": "3cbf177abd2f536b86f88adfc1ebbcbf5867ef4c"}, "2d3dc81e-59d2-4d12-b8c2-dc5bed31a5bc": {"doc_hash": "85e849ff2e911d05aaab6ff5b85722d16376f261d33f06eee4fa3875777b98ff", "ref_doc_id": "83cc20a19d58e1fb839250caf678f884d418b133"}, "96b88442-0862-4781-b1a1-034bed9d5549": {"doc_hash": "e4753bbf137a5bd7b499847d6163e9720c924de55e81768cbbf3e39dce82a7ac", "ref_doc_id": "dd6b10e038ee5477eb07514c7f0d3124d04613ab"}, "dea87571-c91c-4cee-9593-cae94753cce9": {"doc_hash": "24170677eb0c7ae472cb517d4ab8f928d2096709a74da200ae447ced4e122770", "ref_doc_id": "7bf7d47d60f8e3ca542f394ec46075758a3cc7b0"}, "66d4a40d-93fe-44a3-837f-a7e7e4f5dd13": {"doc_hash": "4e18bb496bc1bc0c66d858d1bbc5628b13efaa627430210de85b964b90128ce6", "ref_doc_id": "ac8ac08abfbca7a02503de846750715a164f7c6c"}, "9baf0a05-12a0-4a67-8252-85938ad5445f": {"doc_hash": "a5b47b9a44242336ee1b3e13560ce2f2013bb7784ce2d735c47842b945b5d10c", "ref_doc_id": "2673ecfdc7d512afd9b61e328c5e4b9ca3b5497d"}, "78a4cb67-e5a0-49fe-a9c6-d31ba8c2f592": {"doc_hash": "e4b2657f8b2962168805c158ae3b806b4ed179fed0d27a7dca5708449a192c40", "ref_doc_id": "0038e82c7610d1aa0d22c4ca4322efa093f37b26"}, "7b2b6cd6-2d9d-45e8-aa86-6a2f623c7eeb": {"doc_hash": "1d43c9a2f970ed3a8da8d2097f57b1f12656acb7c0dd25c23da1131cee763250", "ref_doc_id": "43230982b1d7bbb45ab508fe3ffcae361c9d9bab"}, "a514c6e4-70e4-4c90-ba1a-fec248f27079": {"doc_hash": "08464a87ab80a422a0dc017d2dee4259c703948aab2ae7aa7d7fcb6ca9e0f495", "ref_doc_id": "132a3be5cd821cabd8083bbf35b25748ebdfe5af"}, "fad728c5-f084-48c6-a650-feb6f6fe8c8e": {"doc_hash": "f250262338ca5d93459893e3b6e2d74e926492b2b659cb78e871257c9ebdb6a7", "ref_doc_id": "0c14c627525dacf03d693369000d48797724d57b"}, "4c789ce4-5053-48a9-b928-f960d740a5dc": {"doc_hash": "8597c90642e3e12d792ad0c92df2de43153dc4e87eed08a68d4923fa206ad501", "ref_doc_id": "126e9a8727a132922a9e82c9fd564ca5d7a297f9"}, "83bfde80-25fd-408b-aed9-3c1f6d9e0abf": {"doc_hash": "e0466e0934056d30e35504bcfb29a48a51b743c6b29dbca61577bf901b91ee33", "ref_doc_id": "f677afd214b768a748dfa165f4159427afebb17c"}, "95615e5a-19c2-40e2-8dcc-2a306f97eff3": {"doc_hash": "ba9a2b30c8d0a2d92633fec5659aa267b343554131651f4e73f954539be131c5", "ref_doc_id": "6d8a321dd2c45b3da2e9b22615e0ba62ee463c51"}, "ebfce126-6a66-4521-93e7-d44b1cf8e72c": {"doc_hash": "2c50eab1f7e2bee8befe4fe61c5087edc438e238dcb46467e582caa6cfe86621", "ref_doc_id": "ca134577fd704889716fea8083359bec2079f0ad"}, "ef11c736-40be-4a43-91ba-ea7daf40966a": {"doc_hash": "77f241e1251b81b0c8eaf90faedf1934d9489a7a51db7035602e8487c783b1cb", "ref_doc_id": "2fcd64a416bf306ad31365273975fb4bfa7fa772"}, "80cab939-0992-4dc3-b6fe-056e46d23f4d": {"doc_hash": "973b50bf722597d511e7a3fb82b372cafafd627e4e8a82140166b59cda230963", "ref_doc_id": "62b0f1daf00496bca6c57f3a4e193800fea33e97"}, "8bb51e66-40fa-4283-bdff-48ebec9a9249": {"doc_hash": "99c3551482f7e3844a51d3d525848694d668cae7a0450297e1807ec22d529ce0", "ref_doc_id": "edc6639228a4e90854f701cdf6ed6422e2931cf2"}, "a23883cc-ac55-4a17-a5c8-883101b57751": {"doc_hash": "d2f1afdfb1651ab3f4743990b130db2959f08ab8171caf2f9c67cc74bd300ff2", "ref_doc_id": "1419f258f124640cc51b5aaa5754a0f6386e5a14"}, "6396bfb2-9e0e-453d-b449-51b86e6243de": {"doc_hash": "16456a8eed778f7b015fb933196751e5ee8cd8fe9456416f516163cf37740a9b", "ref_doc_id": "9c93fd2351851c49c51f7218ff4750026acff406"}, "de7812e3-a553-4e49-8675-903d900e128a": {"doc_hash": "f8aca4115dfcba0f846d3f7769378f415c64e66dbad1ff8ca8c799c2bf7318d2", "ref_doc_id": "a3a223bee990164c007bdcf1b70051858ce282da"}, "56cad37f-9c3c-49dd-84db-d77a4edd42b5": {"doc_hash": "4b910cac875449857691f94afbbab20463988ff1030a30423f95d6b17fbf12c9", "ref_doc_id": "afeb9ae9d3543bff4c154b584450c4be4f8e1564"}, "52b1c710-d85e-4689-ace0-b33b3b416155": {"doc_hash": "fa3cda34cf4c5181479160678632d7618754a39ad2da75cf8cdf80cec80c738a", "ref_doc_id": "19e3fd8bdd5d4821302a98d0b8a460a2adbb36b0"}, "33133590-6a40-4593-b23e-0c78ee0d5037": {"doc_hash": "23970f5137430217d98bf660646c2ab4da071a571391695e26166a3ac3d9823a", "ref_doc_id": "b69773c0c9b6ab783a52a49b48ec0962228319ea"}, "26cf4073-6d41-466e-a641-a326aded4216": {"doc_hash": "d1418ea69519c9b8cf5468a8736469fdb711f37585a2f62b1b2f9a956212f549", "ref_doc_id": "ea36317231ea6768aacbfd2a370830e34183cafd"}, "8da56310-a4c8-4b1b-b803-e7b677685dc8": {"doc_hash": "a6153b575f46c72cc1d0e8cc8883ad63e5f564b86d7094865b621f79a9e27137", "ref_doc_id": "4722b733cb5aa7afc09c222def98031a1d701d1f"}, "d549ae49-1b01-48e6-bf48-a6bf23c9d66f": {"doc_hash": "b812421e84b403494f7de507a1b2e66d6cc62fe198294efce3deeee4df8c2b39", "ref_doc_id": "be0e8188c363e3c62f0267893b30c0164cdb88fa"}, "529a7a27-ba42-44fc-86e4-8f02ea50ac45": {"doc_hash": "39c590ad72f1123ab94393290beb842e6e003102a478a902aea789173fae6989", "ref_doc_id": "7fe8271834efeb9712ab116ffa91a382c6260fda"}, "b380951f-10b7-4382-a68f-fb506295f819": {"doc_hash": "ba6f098bcf30d1e263ecb118247c65327f6ecff246e36e024b6ef66396149640", "ref_doc_id": "4018d1f8c35f1fa7f5b2494ee5f17b37dc986fca"}, "7c1e2698-cef1-4276-947a-fbb5f3c6211a": {"doc_hash": "2a9373e29aac7c54536f7867e1d14cb97945088bbfc7cb880d0b9b959fa9e17e", "ref_doc_id": "b7c5ec6375565e856ac2fd7b14868b539db6143e"}, "f1333a66-d22d-44b5-95ac-45d1b62433be": {"doc_hash": "865aa11af45b6d229add0633a294d848397cade7eb3e903389deebaffe6b0d7b", "ref_doc_id": "334b77b0d28559d24d715a578fc97ad081fcfb06"}, "8e7c0f14-629a-4b1b-a6ad-daa408e2ddf6": {"doc_hash": "fb2dd3ddc16471b5850cac31fb3265235e84e651b5ffbf8afa2297638f5f4217", "ref_doc_id": "de1559ae3f3990a07293820122f02cbed8684e1d"}, "ab44f382-394d-455b-a062-27e1e011ecc8": {"doc_hash": "77bbbde398e8f5ffc2e2de79435676dcb0a6e551b3d287a66f84fd667d5d72de", "ref_doc_id": "9e85b90c5a9c1db9f27cff6aac94a8077c8453d2"}, "d687452c-ed3e-4c48-bcb0-4061880e933f": {"doc_hash": "c28a16906ea4b5ca532db7062c67c9dceaa5f8c2ff54fdf4326fe0e60dc9e361", "ref_doc_id": "bb50c0ef256dcb735fcf1cc703156ca0530f4f9d"}, "9ab4f797-4b56-494f-b13a-a4f210cb6165": {"doc_hash": "e8daa323ee600b3abd7b5adbbf431c438533c89740bbc433d34d75bcc5955974", "ref_doc_id": "9dc27895b184fa9134083f8d6b3aad12a3da19eb"}, "96956a5e-3c88-41be-ae2f-0827d05cc577": {"doc_hash": "58da9df225582b9fe06819d9759f9ad2a2d02a135180ba36b5d4061377c91331", "ref_doc_id": "61718ea082b8d18c205059176989104d4132d4a7"}, "acb52e51-e830-4b0d-be1a-fadb7d01eb64": {"doc_hash": "2cd2eb1526c5b76fccdf6a0ba339efc7c026faee9566fbc92a24f0cbf6340c2e", "ref_doc_id": "5a70e3d2abb81c179f96cc0c1a6990328e0737e7"}, "0b3c67ba-3c50-404a-a6a2-4613b10ba823": {"doc_hash": "d48f2a2d4bc264c80d263afbb8ec8cf5c30948099e37fef46286726a92881198", "ref_doc_id": "4922bce0a754ff907229da3158d4430e674bbc02"}, "f7f1a673-14df-42e2-b711-5a95bd017761": {"doc_hash": "f9eb0cae09a2a6a4eb9c0e0105634b9a3a8eebabdd6a42c2ae1f1e5538913965", "ref_doc_id": "6d7edc82e3bc649b8becc4edb7f36a4d746e574c"}, "a6882ace-88dc-4357-9326-ca745ba8a349": {"doc_hash": "720964a72e9d611c9c342f039b42f447df8b49c4aeb112291ca1c07698ea2cbb", "ref_doc_id": "9a3c55fcc919c971743594e5c74d9c9f20f55d78"}, "1abc96da-fbf3-4dd8-82e5-95d330c75828": {"doc_hash": "bb2e0f79978c736a6b73cd951f3d28c4f3ac7d3573988d69f854f6ec50ca2340", "ref_doc_id": "9c138dc98f30ea564d0fba065622327cc7848175"}, "d72f0545-0715-4b5f-9d26-beb263362b33": {"doc_hash": "ccc7a12eb6fd4aece9e3ce584daa7592878298bdb9a5a0b6fc3a5e1aa8a47b32", "ref_doc_id": "6b7526a553973a61217093dceabbfcf6c982636f"}, "7a23d2c6-4272-4e0b-8d2c-3e6a3832fcec": {"doc_hash": "48821809664768a1a79afb3fcb1de451c4e8aa0e28650eb7efc2ea4600cda13b", "ref_doc_id": "63eb306018fe2bc1d75b8ec7b8f9d33aa6b93974"}, "6ece38c9-4435-4c53-b09e-afd840c5d4a4": {"doc_hash": "930f214641d714a07175d269c7ab22497a997356a56d592d90d22dd8ef450978", "ref_doc_id": "222cd70b2d034e786c6ba620a876fbf064bceca3"}, "f945ec71-fb43-4b7e-9a63-7cd08d5a0413": {"doc_hash": "d18a60c3df07a2925bb45170b8e0a28cd87d1d8b2093663eda66f17d01b5eb17", "ref_doc_id": "5245b851143d3e6d83b4e2cfdd31e63ba67fb480"}, "4b626086-2cb4-42f6-9a8d-a80cb7b1f8b3": {"doc_hash": "54f1334ce233b91fbae42c482af49d91b2587c8a48d535a7b871764448789074", "ref_doc_id": "11a977e9457d77fcc76cbcd4666ba8a6b9756b75"}, "854eb623-3ff2-4c46-9312-52fcecf7efeb": {"doc_hash": "dc53081f12c4bd702d5ebbd32dd87e7e78f4f29b028cc7cc1eb7eea906555561", "ref_doc_id": "359df9a40dd5f8cd94cdcb0927fed773170ba8ac"}, "f680187a-708e-4285-94f0-67312eb056e7": {"doc_hash": "109560b02e0fcfbfb94f6ca1b87ce0057a3f878079f7ab6ce92cb504fee14357", "ref_doc_id": "eddf2015da7c28e2a652cbee446c3714f89a28de"}, "10226bc0-4dd3-42df-b919-0a11931dc0d0": {"doc_hash": "8e6c92138047b2910e429491721ff740a251a91d5fc2f77c50b4c830c663d163", "ref_doc_id": "6dcb2bc004da48ba773d9ab5668e90a06d8e2a05"}, "1ac97b0f-7589-4b45-a0ef-be5bfe7080f0": {"doc_hash": "771ded0c6495bd3355245b9629971e53ed4606a375992fb2f220212f0eff7c1d", "ref_doc_id": "f8a8228790d2b60b2b24085c9c3aedb01d18015b"}, "79eeeadc-a72e-4a07-a111-ca14ba5499f8": {"doc_hash": "aa9dc2c36a11c74f227b02bb19ef871c5c3a4c0e6ebd8255b8cada31a6b99d71", "ref_doc_id": "242f3bd61a1e9d9f52c7cb84501c8a39cc99d9e6"}, "2d48b9ae-356f-41cf-8cad-fba0a5988bf6": {"doc_hash": "43ec3bd3d9ba45b3d87a3955e562e2cd6a10a1f283c60b7f0a348c9d75c11268", "ref_doc_id": "5dbf50ba0570f630b3103fd3794d9a40b193b07b"}, "dff749fb-6540-4f30-ab48-49511f4a781b": {"doc_hash": "40c019ec411a6f37ad1ba5b0417c824a53de8fe8ea90f8daa839b3bb4e6b4dfa", "ref_doc_id": "bcd78fda211589bb545008bef898bb179cd90802"}, "cf7f0be4-5698-4346-8ee1-e0dc9c695bc8": {"doc_hash": "a5bd8104875192ee59b8646dd38bb6632c8ee7f2b68744f312b6bf97caac15a7", "ref_doc_id": "a77fe2180cc9ed72c294f03c9ceefdf102ee0a9a"}, "434fd670-3021-4064-b036-ee9e21a84294": {"doc_hash": "602e43822fe63d27b03db6c5feb0db7999b5757573890f87bea490a53a092752", "ref_doc_id": "10c0ff46005471baf20527ed7a2139009e198dc8"}, "19858768-2da0-4160-818b-b36d9fb6dc42": {"doc_hash": "ffada87455a77bce51861a3bb9378a0c4e5b80f1a5f2d70932731c9a99e3513f", "ref_doc_id": "44da5da1974ed6d5e2e7ad01a11cc9c8c163bfd6"}, "0ba191da-578b-4d9f-a573-a0e8f1f73b58": {"doc_hash": "81947348512a77415ddbb54c1cfeceb25e5bb0ebb877114154fc0fe740708bf3", "ref_doc_id": "e596dcae04b94f1219f10ff01a0d4dda85d65c23"}, "b3f58b1e-cc4e-404c-bc92-1b3e483ebbee": {"doc_hash": "0a40c67d0321a2dfac51009206398243017c0f4f1bcff9d3bb4f5fa6d3119d8b", "ref_doc_id": "8d7b4d3cb217c7a4382bd05a5fc41e40443b6020"}, "1ce33de7-ef99-40f4-b726-71923d469612": {"doc_hash": "2bf68f40abcec2ea6d780942f6daba78c7df3db09f0bffecafddfbbaa73bd68e", "ref_doc_id": "2239a557df3d96794997289b383d0dff8ef4b6af"}, "77dfa1bf-83fc-443a-8803-ff2a142b8310": {"doc_hash": "cf9342765bebb591421fa784c792dd849fc64c7349e818b86d1e813f0102ec8d", "ref_doc_id": "8392d65c118bab5f1976a39c729fb3627aadc376"}, "42f4303f-6272-434c-a4d6-7599305f31ca": {"doc_hash": "94ef40c0b465bfaf5d13413f5730269638819b80e388e4b29f251e467a36691e", "ref_doc_id": "76fbd330da97b2236addaafabad0be6dd1e8cc85"}, "207e2ea5-a65e-4766-b41c-5efe333cc87a": {"doc_hash": "9b25f96ba6de4592e9c0651101da6503bf3c6cf8e0e22475df13455b91521cb9", "ref_doc_id": "355e093146539a8d2d9b9f88d141cc9affd56567"}, "c72beb0f-2e03-4628-90ad-1a95f19ecad3": {"doc_hash": "1935276f6818ed612a66077d1d94788c175a8d0d69ecacd592d50d1bc61409b2", "ref_doc_id": "c156a9e31df96226957ee03924deafa6da448422"}, "43ba3f65-d918-4f7f-bac8-26f669e60632": {"doc_hash": "fc11ac9faa494d5297f3726af35e5a38c090843a2e56e17a2c56e37d6b8469f9", "ref_doc_id": "5df6de2e4a9ceff5af92c252c4552e9aa64643e1"}, "842b7a21-4728-481d-bda6-7ea689f6f6ee": {"doc_hash": "1105362f8baed8f9b54ebcf9af260afef403c531e941345b5367a72a8a81a05e", "ref_doc_id": "ec5ce22e00e61d767e1213f66aa5f5c025ed2f26"}, "bf39f0c8-4ed8-4f35-b13b-92f63a98cc6a": {"doc_hash": "9ad95d253d4a25d28da2105d93b47b9e3c04f4f6a899a668807ca4a6eb5a42ef", "ref_doc_id": "49c5354ccc2221360119cf6af52e2404006d88c3"}, "81028195-73b9-42e9-9859-cc3e9d601190": {"doc_hash": "73a2e1c66b1f44c0c546fa543307b8d29d7b962dd54e16ddd478a0cc6e343daf", "ref_doc_id": "f8a632ee6fba14c0c09a70d073a08cae0745e0a8"}, "c1ac3472-45f0-46ad-8045-4ad4b771bb5f": {"doc_hash": "f91c941be524f8d1ecf1c73f1007d790f59eb47bbfb343fb9c177b9b893d1148", "ref_doc_id": "b3a9daca9d86a97cad8ae3d854de485ec09cf905"}, "5d8258df-9b9c-46b8-a9c0-339c046bb167": {"doc_hash": "fd0144185f866fa230c7ef60d7ff4ef36bd838c7f759b7ca523d42cca50ec76e", "ref_doc_id": "3f8c92242b57cc153978707c0b98e1a65679b77e"}, "b410e342-d907-4cb5-9718-9360a3d01879": {"doc_hash": "e1f181730cb6372be7a6a849dc5ebb9902994a115d82ed8639e4fd0bb75f3ff8", "ref_doc_id": "6ef43db627cea1e9df3dd819c118aaa09f76f209"}, "65c701ec-ba3d-4b7c-a622-aeb114ef1769": {"doc_hash": "3c3f374600c3dad63fbaccfa225fe17287e77359d1f8c9372657e0da5e49e382", "ref_doc_id": "577b2976c1d90adffec2c7d29d78a30f39aebc1c"}, "660cf76f-34e8-4f4c-8333-35c0b41ee94a": {"doc_hash": "35582c9a8902015c2f60fab9999449a665525bd4a14cf859d10db35503e115b8", "ref_doc_id": "3a20b99202e671b94db9daa35250050a58bdf410"}, "a9ce7903-25fc-470d-8aac-2c1ab52337b6": {"doc_hash": "eec8f41b51a9515ba73e622248d727a619b3baaa3549103760d48b9e58c03733", "ref_doc_id": "dd1d8d64a8aa84a17dc9c72e9ea12907f4d02ca5"}, "00b12f47-16f9-49fb-884d-99778331154d": {"doc_hash": "a2506ffdd5ea6d20310df63e70a39c8be7ab135fecb23713903b9c79e98653a9", "ref_doc_id": "6822a18ab128390ac1943fea5c83cfdb1a93fdc2"}, "07a78963-2d62-46b7-b931-66314b7eee61": {"doc_hash": "bd399b672030117d9d93edef2b1e90b166d63a442c62eb6b46bbaa75fc37caa9", "ref_doc_id": "d4eb8fd2dfddd56081a971e4f40951f45954eb6e"}, "4fd42729-bc86-4de0-ae90-491a69d9b6e1": {"doc_hash": "efe864c42561482b838a87db89f8ea4d4bf323d4a5a2549a04b9902bb038e857", "ref_doc_id": "ebd2bebcf4088e27b7f081df6567d34a835fa938"}, "4296ad33-6fb2-4c25-9bfc-fe19a2fac19a": {"doc_hash": "987e62e5a576430a5a8842ae5d6bb362dafa84de683e138136fb03503a6f9a08", "ref_doc_id": "34787b868dd9bd5d50f5101598e0dd67e99d8395"}, "b431b915-8cab-40da-a8cd-78a3328ca66a": {"doc_hash": "7d2305aa9ed689901eb62a3e21a7238a6b1b8bff4d690fd9f09fa1a309b03623", "ref_doc_id": "fcec0101e25a088294492ab195fd2bddc134b497"}, "76fee423-40c9-42fc-9335-8901ce2c7245": {"doc_hash": "1778f29707af29f14daf38624b78b5341c2ec1a9e59f26a7b4dcfac43384a7b2", "ref_doc_id": "6776675e95ff5c94832961ec1c75e684ece65c27"}, "1e865aac-551e-4c37-adb3-26be34e616b3": {"doc_hash": "52c0879d6eb2aa10203e141f4978bc64d3e4e9bcec80755b42c02d98cf9a44b8", "ref_doc_id": "c6d1ff5356cd05a25657927ef40774300bd77d15"}, "3cfcde5e-0ac5-4c7f-a99f-56a528adc113": {"doc_hash": "c6abd48f126040ad1a6285fdbf3f3213367623a8e113a6629683587372ec6296", "ref_doc_id": "41f52a108e151f281202c316417139ebcc5e2c44"}, "9027c120-8b25-45e2-9115-0e75bef8c779": {"doc_hash": "f5581c915ff06ce139d3344a26a47d8863476de534417f8787373a04b8b4e92d", "ref_doc_id": "374269e8fc062345b8f37106fd7a5ffdb0301795"}, "bf508a54-a7dc-479e-9805-6a89ab731400": {"doc_hash": "c9bc247e3f493adb4f4ffa2684e20ade6d2d0bec03d5fada244c9f6454349c54", "ref_doc_id": "7f9aa415323798473d6ef254f47ee596484744d7"}, "e81ae16a-c957-41f4-864a-9c9e680858e0": {"doc_hash": "f8b70913ec73a037e3f72d1170a7607f2be7a74437e9ed7510e51988f88a0ecc", "ref_doc_id": "f8a45d4832262f95f27af6892508e9b71326f556"}, "994d9882-b73a-431b-b61b-8eda6c8ab49c": {"doc_hash": "a497687d6ff7e28f69d259826e59fcd3c384700d68712af17e85707b404f12cb", "ref_doc_id": "102c575e7127dd460ae30ac75569a83842c89046"}, "52ba0fe4-28a4-4a2d-b1f6-105618944b9f": {"doc_hash": "c4dace872a96bd375f79abb16d46351dffd913ff999894fbe7ba438a437d065b", "ref_doc_id": "9f4ae18efb5a648f5fdeb9d4346aa28fef15baf3"}, "3d8c5adc-f88f-4392-8358-5487e4141bda": {"doc_hash": "18672dc007db1aa5a41effc9d0f56739dfbd1efe5466f44323288119db03ad48", "ref_doc_id": "2a71761f9031375e936cd14314061d98a6115cf7"}, "bb5dd8f5-75ac-494d-a23d-ac616771095d": {"doc_hash": "1b65e817a29a1c543417f54792512dff1f8be5a2990fe65124bb942418cb5d20", "ref_doc_id": "1ffa1d6b392b65a4a5407254ed369a3dbcd7caa0"}, "d3a14fbe-49ed-429d-9914-8dc48b9e9612": {"doc_hash": "a6c0491d3db3fc63826d3c3ad4acc25329b17a568539fef10b4f99a4e3a54f8d", "ref_doc_id": "b2e1759e9623871b0328cab4e8c48bee0d8cfff4"}, "82f82075-4388-41ba-8a12-032da77aa173": {"doc_hash": "6447db792eb600f6dcc8fe01ece8ce6ec07a23685d39d70f228581043b0eadc9", "ref_doc_id": "5f2819b90f26c11e3dfacecb6d9efb9e0889ab50"}, "de871bb0-e8da-4713-bbf5-fe63a11b7e1b": {"doc_hash": "27ca61210e2be3d9dbadd9c64f8b3b8ae3ad176e85189806a389c417c636e442", "ref_doc_id": "c7e5dc0d40cb46cacc6d7cbb3e9f72f57721c33d"}, "d89ca5ae-bf3f-4a40-bd24-3c0cc5ea3808": {"doc_hash": "9e3f3eea46bb46378eca1a6dce58ac6b2b0466af8c711f64e90fd7e35d15dbec", "ref_doc_id": "e7abb81699d52c5ae12ee43d557aa3f36c393d07"}, "ad25ef65-2732-4e55-a908-2f9cd27274f0": {"doc_hash": "64a3f099ec06cd7d6cbc1547ece05a9ed8ac8fb8e1700b4e48a58491964c6092", "ref_doc_id": "a4d9d2d8cce00a7f749ee4913a6305f6caac60a9"}, "11a99d8f-6a8e-4b68-bfcb-56888a6b711b": {"doc_hash": "15972db34c3d865f3142ceea3e2198a37a92a1e1dacc2903a0724ec5d3333a1d", "ref_doc_id": "99cc50418ecc429fe30d364dd6e7ce6fbf295116"}, "13b61079-ff1c-4634-aa4d-3a764ef5185e": {"doc_hash": "5c9dffbf8c44a862632bb003d90c5d439dd9994eceeec00996951ab3ce5dbde4", "ref_doc_id": "dd705e54bdb157be4215350464bd92cf5f64c710"}, "b1dcf25f-c16f-4212-b46e-e78c16f6eefc": {"doc_hash": "2e1917bda598d04e321d6611783342bc49d13757b50a5afb7dce4d0683303436", "ref_doc_id": "3ba372d831c3f7316ddd28bc5936d3778e61ac8b"}, "21096fd4-ab2a-4d12-9c48-af7c065abf0c": {"doc_hash": "2a3da3c25897fdef7b106522f5484893e745a24997a04ddf436c71d9b1af7517", "ref_doc_id": "483d1723e8905308f49f012eb62d0387f4df2546"}, "3ba783bc-59d9-43d9-8164-730b31035e75": {"doc_hash": "239450f07da2ec242aa36bcc73202b9450ce4779515dcbab1337ffaa83d35b7b", "ref_doc_id": "53923b68c11fc48b3fbce0a1e9de069ca90625a7"}, "55ac3ab4-57d8-478f-b8fc-cf0d8de3bbcc": {"doc_hash": "71c903b497aedc06cf60e937034b5dc120201c9e0007a7903f8b50dac4ab31fb", "ref_doc_id": "a146be0fad1c7a822210eb25eb8b0cd174bd0097"}, "e7c8cabe-649e-49ce-a5c8-771faf3954f5": {"doc_hash": "6b16ed9193f795d9213d88943b7d5683a62878945ec1dae43d3ed42661f4c247", "ref_doc_id": "897252ff5f80ec87becbf86401829f3aa03e86b5"}, "28f2fa26-91b5-45d0-be41-698c5f9bf8f0": {"doc_hash": "936cf51127354ca1f9424a392c778a87b33309b5c1892fa8a422f60753ef8193", "ref_doc_id": "191d68103a91b70d758c4931b225a89aae60177f"}, "09b8a0f4-1d4e-4c75-b239-36afeeb50a48": {"doc_hash": "29b9f20af3d8127f42e239493add7ccc97358907ab4aed73bd20d7abd7a3cbc5", "ref_doc_id": "ebeffed79f54516898b8a0a2ecdcac4053030f79"}, "78e15a8a-58eb-4b8d-a4b5-eb6da0ff70a7": {"doc_hash": "752ad070eb35085b18dfbb768e7bbb6c173bf9d7f0ca9abd773ba325c42ecd75", "ref_doc_id": "5ece95c86fffa759a334a12d87c2c6e03bdff1ff"}, "d7f3246b-8a2d-45fb-8a2d-47731b25b226": {"doc_hash": "458d7d9662e4c930b8d31ce6bf69e441ed60bdb8f8e418d9c4eba0e71e664cd2", "ref_doc_id": "d84407d9f70b6ad3926aadeea5e775cb62337070"}, "a02ac731-b8be-47bf-acb6-03efb06da6e2": {"doc_hash": "a225949bbb3514ed4a3454b801e4554d6ee32d288ac55c6306ac9e20ddf6d51a", "ref_doc_id": "bd4e6420a2433e453af8e20fbfe573ced6a2c7ca"}, "961f8966-03d6-4add-8d92-42b11f21eb25": {"doc_hash": "7ec1f239227a4a1a28ec61d0be14cc932cbf22c5aada3469b254d3f332c6b815", "ref_doc_id": "35b966ee8ae82cf730ff1c8212d9b01adf75abb3"}, "1f96294b-d0ba-4ccd-8fec-8ecd7931b506": {"doc_hash": "44efb8845d1e3f063876eee629b510005241e1f8ba011d03e070c4db5c59f744", "ref_doc_id": "6366801e42c7d1f97045b3b65a43e33729708399"}, "72b061f4-af16-4bd8-a0cc-98dc94a4fe64": {"doc_hash": "c5aa196c6f18997a25e7760c02fbc0a7cec128577ffaf2729ccce7a2d1adc73d", "ref_doc_id": "5cc4569ec83408df687be7cc36a6e6afa66f3539"}, "fcc6a650-5c23-4291-860c-2e6719c7a830": {"doc_hash": "9222fcd22260d85e17303bf5c476eeb01bebcf1cee640d721804b72b794bbab4", "ref_doc_id": "43980882c896dd55a8628295e0b9bd5ba046e65a"}, "97e7dc10-26ab-4ea6-b4ff-598ce541a17f": {"doc_hash": "0455da5802484923719dd037d2604d0a122f9058c9f141b602d7bd20da856240", "ref_doc_id": "1fbef21a5d5e2e3be7c51ea575ef215cd73227c5"}, "cae5264d-4b27-4982-afe4-97be32c2a1b1": {"doc_hash": "36a953ad3a9c10716d6716767fdf1872580ef8cb5185f441438e59d4d062b137", "ref_doc_id": "caaa991886f6640c21314422751369aee412bbcc"}, "33879e2f-860b-4fc1-a611-8b045c079a32": {"doc_hash": "d9b5dbf9227c6bec3887383f5c780799bf38bcc2ba496054ad804dbd804b5f12", "ref_doc_id": "48e1a7f49735b33c854551199a96842656f24bf0"}, "a1f40bbc-120c-4b8c-b171-c4953f948074": {"doc_hash": "bbf534c8a5c21dca2fc7d75c439ed2d45c8853e29f29592c45e41f6b177e5b50", "ref_doc_id": "0ca0f9cc168dbc4800872c6d40639d52d5ac4dbe"}, "fe788021-75d0-492d-935a-658cb87c5801": {"doc_hash": "0c329263030fb916709cc6ea7ab91814c2dbe5b4e3ffca55ba6ade469c55bac2", "ref_doc_id": "2da0a60c399c630ca77822b0896e2934efee23a3"}, "006b8f92-92e1-4c7d-9e48-9ad9eacd22ad": {"doc_hash": "6e81fef9d102e952e967bae60ff9ed2911e800f886dae8442d5d8994923d13bf", "ref_doc_id": "6a5ab91077ba8095195a3eb9e5d397d956e9bc30"}, "451cb770-8928-412d-b5ee-d9d792847c9f": {"doc_hash": "f577a8b1e30d0252b65d0110267a369ce38ff79b8775cc6f10897a7cc4fb2284", "ref_doc_id": "2b5d9ffe10fbca2582c979bfba9ea241ab139e71"}, "bf05d549-fc57-4378-a790-987897f5d55c": {"doc_hash": "ffb511dcd9ec037d665a2fabf3c9213cf2ceea6d7ad8c6f8c84e37069471a4d0", "ref_doc_id": "505fe0393c3587376191355d48e5797d255ea0f0"}, "692437fc-cd0f-4054-a2aa-5ec6be96af0e": {"doc_hash": "3ee6c02f9ad7c01ebd11b4e9be108fdee146c36a50c3e0612f543a2bcaa3d279", "ref_doc_id": "92db1008e836731e78a5ff2b84098429cc73373e"}, "b473115d-ad39-40c4-b9fe-688ac3142da8": {"doc_hash": "62986acd95291a57dfada0f425c9be8db9570eea09d19563889abea6cb62306a", "ref_doc_id": "0626c02421ca26ff8dafd9bca93b3c517ce7ebbc"}, "0b5ee20d-8f91-467b-a613-4f8145d6c9be": {"doc_hash": "09905bb20e8e308ab98222e7dd8c77d38b1c10c65d6046604b1bd58331135ecc", "ref_doc_id": "5d0968c1800e3b7e35971711ff8838bd5e5fca8d"}, "a76fb8ba-842a-47ca-893d-b2683093500c": {"doc_hash": "456fd715d1888ca58fae618131591986dcee1b8781be12cb95f04d89d8682e1a", "ref_doc_id": "317767d77c9e7e5a8f99823e43382b8f34b8cb54"}, "267db816-8b3e-4192-95ab-bb646179b69e": {"doc_hash": "bcc02e79256db5a721b392678aaabd4743188f344c1f863f6415ac20dfbb7be4", "ref_doc_id": "393009be341e9c0bbfb59c829898b97dcf94c556"}, "765614f0-6342-4e11-98dc-75302d0a23fe": {"doc_hash": "ba584abd88228368b5949eb0ef233bc8b5c080fdb62058123a2e14ad1caaf23d", "ref_doc_id": "fe3f7f42272dcbf94f5d1e68a84a4e0e6c373c1c"}, "a53a3d6e-bb7c-40de-ae0a-c636aee95fad": {"doc_hash": "17995c0fb3ece02c5838a2908e09f6ba70848f327efb99ebd643cee5801772c8", "ref_doc_id": "dfcf1ec300035f6570667eba8b3abc78fdee32cb"}, "c83a0ed9-4ce8-4cd9-9ff0-defb25ada350": {"doc_hash": "559a6c446bf068fbf9b56f1301ad0afe053baffb6502da08bbc2a78a6129dd2e", "ref_doc_id": "9c0fd9cbc734220f7844159e2fee6d6628697570"}, "f907ce16-4df2-4d56-8d73-0588d713b382": {"doc_hash": "e3d4f9e88af1548eab67112b4cdf46be4a09e737ca4291a30af84e14877ff4d4", "ref_doc_id": "639c67c3ab0789ab4fc4e2e0a98d9a4041924d64"}, "bab2c2e3-b398-4405-a1c1-6137d61c9cf8": {"doc_hash": "215d51e02ec118abfc41088d3792a383eb727cc7542c93d8f308591ec0e41bbb", "ref_doc_id": "0d3e44c8e90b949827671f8c6e4a2cbb7ab4b6e2"}, "f2548b2e-d510-41e8-a3b4-a7677d9c60ff": {"doc_hash": "5026392b27da20bb9b68244f8d8d59022fb90a8f5da81e7bba7040398e126986", "ref_doc_id": "c3f1227c67fab6106568d52eef84f3ae17c3213c"}, "ae34db84-f248-41db-b017-b6303d9bf975": {"doc_hash": "538e30bea8ca6708b23d17175b312632a94b506e5032384b054eaa0c16bf8cbe", "ref_doc_id": "95e5145a57017cc88d33a739d9f0b4f465ecde47"}, "5ab6d12a-7b9b-456d-bf2b-8e042f07d346": {"doc_hash": "beaa4a5852e9ac767ad0d4a3bdb38f94b7520ff6846d876cb5790b333d3132f3", "ref_doc_id": "0ca6558db565f17fad49e07717d3fbd7e2139f93"}, "7a3d301f-d7ad-42a8-ad28-24f7f62f2876": {"doc_hash": "7ec22c6aade86c545d815de051f480872d45f082c3dfdb01e3d95752fe11f9ee", "ref_doc_id": "30bc917e3f3f08b7c782248291f4a353cbecc0bf"}, "07fc7114-38cf-448e-9cfd-d6223d959c99": {"doc_hash": "869c7e2c9ebd92b47bd3a2e050dced821d3e5fc73ef5399b2e04933ed84138b7", "ref_doc_id": "38ff0adb180d9f5f605839f3bec5c4c1b22cecfc"}, "304d9cb9-aa5a-4116-ae94-361bf6bcac37": {"doc_hash": "27faed6470f5f86a0ceb4ae395293f0513e3e24f048c32273fd012a6b1399b12", "ref_doc_id": "a8c2cac0dccbdc3b584f7b1e29bc30107f1a3710"}, "0230e0ba-8103-4241-b8fb-61bb853b8d83": {"doc_hash": "d24946c5b2368caad0c83f6c0a89dc64a67cef2fda2102e6338e62bc343856bc", "ref_doc_id": "6819b3bd28808e56ec8ee4a46896530247e22509"}, "f009f7b5-d927-4bc3-86cc-c155b5403ea7": {"doc_hash": "1f4a6688b3d90b1e743dd3fb3b1920c6a0b801ed0eb67a2b9b840ce55243b04b", "ref_doc_id": "90548f00f95c74fce851a8cbdf848bf8a70e367f"}, "79a8f70a-c18c-4db6-a1b8-6de92127c12c": {"doc_hash": "467c5c2bdc9cf0c35eda87bbfce95681824cff90da9549bc99bfd4af6421cf74", "ref_doc_id": "396de306fb36a381af379e5db478bafc88b9de75"}, "dac17d90-b4a2-46f3-8af8-8c57bda3b7ad": {"doc_hash": "6aa5d0293cc0c40d6dea3aebba09f6e40ecc6b21153a46d8c304344a81a3c5da", "ref_doc_id": "41161917f7bf81be1efce7133c5a39261a992b02"}, "62a10a11-7116-4394-b48f-bfda3780eb62": {"doc_hash": "cea721530214c2c20735e4466823b9d96ae7a304f86c04c90a759520e5a43f2c", "ref_doc_id": "ac0c6c932ceb663de686e2bfe6a085b09787c252"}, "e793cbf2-5df7-4b52-ae61-93525c823cf7": {"doc_hash": "9385e831c6904506f2b93ca83b3ee1b5bb55cd0b9b1b567d21afdf011f20a8d2", "ref_doc_id": "096dff23fc3d7557391a9aa53d656f5571b30194"}, "275cae37-8ee4-418b-965f-bab2faa7e611": {"doc_hash": "ff21560ea52cebb6b57e49d6882bf794fbe0f0485b8c222467b9693a2cbb1582", "ref_doc_id": "2fafdec5d9459a0179b06d42b14398a8a74d4dc6"}, "6a1f1ec9-6237-4b92-af67-7832588e4268": {"doc_hash": "7d7ef0240e8b2e49bb61dd37ea2f4cb44d224bc36c4b2dd9017dc8b94a57d75b", "ref_doc_id": "d4315961ec405da54ef65d95bb4458e691bf90ea"}, "0ae1eed6-b91f-4cef-a471-5ec0c775342b": {"doc_hash": "0fbc0578603ccca24771d316ecdbf1cb2aef58be04a0eae6b1e337b48855ecb0", "ref_doc_id": "5b1669f1c6eea89badbdf01b0460cbef0c37946b"}, "2e0ce863-780f-47a7-842a-3d5a48a762b6": {"doc_hash": "60d546559d22b97bb453388d73e787d9f37c30b60ca69f049cf01eec86dfd641", "ref_doc_id": "5c5c4603b86f39be53a727ec34543d6a40e2f54b"}, "16941e8f-e5ea-4f4b-8e22-62abda57c0a3": {"doc_hash": "dfac7ce99e6b908a74e0df6ae6f2a3a5b7e81b23bf28074b901c29782674acdb", "ref_doc_id": "1f28d351e804a5c6becab81f09aa50f5a39ac30d"}, "d15ef93e-093d-4711-b9de-18e8b060bb39": {"doc_hash": "1f0cfe91d695e236004a631d5a0b78b5da1b42ff16239cd3678a81efe76279b4", "ref_doc_id": "e0987f232209a6c1b5d0496e877848efd3b84631"}, "a1c92599-a62f-47cf-bab1-195e90e37112": {"doc_hash": "a711f07e8a206cbadab610be01840961aa795fff7b94905e97ef15a3a2d91541", "ref_doc_id": "e2e7f1f832f93a12fedb36f3583a5072bdcb4166"}, "664a460e-579d-45a8-8c02-f6934e2e088d": {"doc_hash": "4220c06a4c2168b5950486c32b81b73a0d0d6c5b20a5fd45f2b774768c811813", "ref_doc_id": "a2809da0746fbee9aa15b379e0246f83ff324458"}, "bfbe5124-247a-44c0-8b53-994a90217100": {"doc_hash": "de157c33fd029938b6b5c40e79d7e9aa71f893bfd20d654103c787fc3fb4c125", "ref_doc_id": "3758a15fdd4303efb67249f78ca11675f390e915"}, "7da63f46-124c-4647-a330-3797d9d79508": {"doc_hash": "7e462e40f06c1532805743fc71e18f2471aa268a975d7fbb50be1ea70cbd8235", "ref_doc_id": "67d7504d32178700091ac0a81a081d038fb60146"}, "31931501-8218-42b4-9c80-af75e6aaabfb": {"doc_hash": "f199b14ab1607f15b628858edd8a60d5f353d8ba50659ff95e425cfc677fcf4d", "ref_doc_id": "3d11274cc985bb890af3e3fdf8bf7eb05b85346a"}, "f6af0580-621e-4b86-bfac-f391f76cff44": {"doc_hash": "dfa43551773608c678515407f217e4d0dd947fedfeb6b57942023ec462666246", "ref_doc_id": "1aae1c50883e9114c85ea1dc91f90015e17d49a6"}, "c744f41a-ff4e-4cfa-b470-52e44f542b13": {"doc_hash": "5459d2e919378d0c95efad9310479da71525bbda180a3efdce768ca9099fa0e6", "ref_doc_id": "c02591f3c6c772d32e396f9b377dd5fd9fc29af5"}, "60387b50-0b6d-4c4f-9dab-5a3c41d90218": {"doc_hash": "7e10d2ed964e568b06a56ca8220be04923e9c7895502a2fff8cd770e0d0b4856", "ref_doc_id": "cff89b6afdc50d1beca0abdeaa8efd803c498557"}, "7df224f9-8a9f-411b-82ed-53014aaa8d2a": {"doc_hash": "1ed42838dff0bd6da56a5e0be5b7c5324652dfdf77e429c7679fefe8b9212407", "ref_doc_id": "233cf8942bf7115ba614f20861b2a976e408335a"}, "f7915051-023b-461a-9dc8-46023991a991": {"doc_hash": "96166b0b778db080977ed673f0b7d7f1a7b6fc6480ea10c09dc4ad428c5b5921", "ref_doc_id": "f511b9ddf67a6125bbedcfa829825980f73e834c"}, "c75ee29e-efb6-4be3-99a3-409f6a7194f4": {"doc_hash": "1f1df4245dbc751e33b9b12f773dc40b3e1703e9db7bfd429ff443500ea2b8ef", "ref_doc_id": "7afbbb4a6aba7af099b56895b45d02dc26c30a0d"}, "63bb391c-035b-4810-9249-adb329d4997a": {"doc_hash": "4f130b97e05608d4948ceec1e338c016e9d1281dd935ed6782cc9b0165caec5e", "ref_doc_id": "4b002c919739200212d11beca3c43a08d99d38cd"}, "d6f03d5f-490c-48d0-b654-dde3743cb3e5": {"doc_hash": "df85384c17b8a5cbedae749fa4408eced9b3a6685a3dc79938fa0396a253e748", "ref_doc_id": "4b3470d23cb1925a902db320efc9ce2d1e2b1bf1"}, "f0e722ac-f530-428d-8810-010481a2d120": {"doc_hash": "4606443e69249389c21188f455c79f0f7402d12d291fca10c66850f2787e6e4c", "ref_doc_id": "d8eb4d213ff1031a254b1e61037194b87ae8f230"}, "76b6ef14-0035-45c1-bc27-66be9771fd08": {"doc_hash": "8c3cbfb3ee5169c7c71ccbcb43988169531c91130e3b2b0067851b4e32ef2a92", "ref_doc_id": "4ce5ebc5d202901163ba11456068515bc34513e8"}, "7d20a048-3bb2-49b6-99db-a815d180c8d5": {"doc_hash": "b4968065ea7917bae331720c3e4d900b1fde86b61779f89fb987bde0ee906c5f", "ref_doc_id": "1469cb0a57f85b992238492309457ba8516133cb"}, "659cb26f-1727-4ce5-a6f7-5be747dd4a20": {"doc_hash": "65765c23e5613ce4eadd8d90778e45a771d5a977c6c9b0f01b9aa2721211353e", "ref_doc_id": "3506582a83f3ef3444a8aa9b01518d31efacdf40"}, "016e0094-7c3f-4edd-8738-435a8b46c7d0": {"doc_hash": "fc4e4bc2dbcc4a812b5613a413d0ec9a61258cdff212452a19ae631d42214fcc", "ref_doc_id": "c9a7e92c697e3eef575351a643d0b1f0cca4496f"}, "2ba9d7d0-8580-46c5-ab9a-3bc794167e58": {"doc_hash": "7380f274f4d52c56a06ca474245c4d2b2650d60e89b2fce0fabea0a2eacfb359", "ref_doc_id": "28822ec547c0d60887cd4bb270dc2340bd248dd0"}, "85c43f39-f3d7-42c1-b867-eab9db40f156": {"doc_hash": "1a5fef4813f51b421815d693bd8a12e38b38d638ce964ef3f5e8dfcb3608fafd", "ref_doc_id": "41843a9d39987c7b7d5276f021d4ca3bf8e492a5"}, "94d94985-50de-4244-9a00-97a7e4b6497a": {"doc_hash": "c1e3089eacf850a52a1af73fda9a237aa5b943ea123633e2b35c8c92451e2701", "ref_doc_id": "e23da97edf8d96d827ce489d54c7d2620d630b08"}, "c33e30c4-09cf-4fc1-9ee8-ab0edff53569": {"doc_hash": "9af1f61f23dad0a5e88f1605f52f76e58ae0ecf2bc1952712cda8475c7fd73cc", "ref_doc_id": "d36ceaf20d1642bdd253c3dfb0d3640d5ea45735"}, "a41a1e09-d9e2-44fe-8614-00099bbeea96": {"doc_hash": "f08b88d3143624928f11b8a028bef62ae90540b4e080c449544b306bed73692f", "ref_doc_id": "ebf3f775909cbd58cb433d3f3a0cd88bfd2068d9"}, "b9065d79-572d-4382-be99-d1d35cc2201b": {"doc_hash": "b1cf1304995fb1d395b26c69c2008719ff527088bd0b3ed53a14e50138114ae9", "ref_doc_id": "ece1e7b851ab4126a11ac2803f215101664111dd"}, "272ae961-e2ac-4469-8e15-1d697dad1cf9": {"doc_hash": "95c9981782317d74b3fcf10862fcaa2342494761659839cf0a09899419911734", "ref_doc_id": "ee9fbeafb433e6bdeaec2ef98e03798f79d9c08a"}, "53ee1945-333d-4843-a3eb-a5a8d0b7c4cb": {"doc_hash": "f7de0488cd7499260bb4aa3290533bb7a524d894c78df5e6d0b5e93a2004ae50", "ref_doc_id": "b99b574d83074753a9736ccf866d23821139d13a"}, "653382e2-67b2-42f7-8c16-f48c2f68a337": {"doc_hash": "fe2e94078a304318a542ab8370ad331646c2bc6dcd3f5bce349356d55a99d9b5", "ref_doc_id": "dde90532c7b6dbde48e037b3eef12ac2c224451b"}, "ddc7cc3e-1486-4b10-9db7-fde345e53d57": {"doc_hash": "2511cd9290d29f0720aa9562e2f3de30abce4dfebab7a5a95094b786c42c96d4", "ref_doc_id": "5f0371a3f9adf27ed86e5242f8cc934d4de4095d"}, "af6e710a-ce7c-48b9-bf1f-1dcdbc57d268": {"doc_hash": "9093b1fa621d62f00ed6a6ba4cb419f656ebfa04c878f5b45766913f92ce6fa2", "ref_doc_id": "4ba253eff24b7f8bca06da7da3e98d8e965296ef"}, "e8ee322c-9e03-4eb5-9c78-10262d6fc473": {"doc_hash": "a0de30a377f15a100fd4feb527c6710494f9e8a2856086395bc0d62c584335ab", "ref_doc_id": "a4f4ebf888e72a6133ebdf9e370a17ccceeac080"}, "90d9c522-f255-4700-87de-334dbaa8708c": {"doc_hash": "e44347feff6b25cb2c90a9ad0ac48dc856e5880c5f5ae192c15ed6466c01a43b", "ref_doc_id": "1f8385d02e134bb2edfa5f250c07e8124fad644b"}, "3c7eb89a-1ac9-4118-8f24-53bd7ed42d2c": {"doc_hash": "cc0296e389583d133d8199cbeda9ae304a5a5640e4663ca70582919e9e49bd4e", "ref_doc_id": "00bc83d779827dc0ccf7504d08b95007ec09e22f"}, "3aa1d73c-517a-44c2-98d1-b5dc06752c66": {"doc_hash": "dfbda6ba46e51b5b60b99f70def77bc0490907151aa870ecdd4565b90eca32a9", "ref_doc_id": "623bd8e5d53920a5d151cd4cd85cf2c1a2baf1e8"}, "84b63c72-e310-4271-98cd-ac36c19db6dd": {"doc_hash": "0389740f4e51c186aad1da03e9d934f0a7931ee1ea8f29e9292e127f6995fcd0", "ref_doc_id": "34c9740e3973eefd944dac92c32747a265ad3798"}, "9e63e9c7-697e-4e35-87f4-dd6d00f5c4b4": {"doc_hash": "a02d56ef8832889797e9ed97593a02a9370d5a79220d4a1f35c2af0f594238fc", "ref_doc_id": "8bee7cde3d96c38985640d695144c72f19ebd3fa"}, "d5d3d634-83fc-4ff7-85ce-a0157c97cf31": {"doc_hash": "6aede92abac7f64d968f4702d8100c12f317cbbe11ba7333a75e132475564be3", "ref_doc_id": "3111f2357803513407a8e89d48670f4b7df56907"}, "41ae5866-59ac-45c7-bb65-7ec85a82356b": {"doc_hash": "532e81c2c0f061d4cdf146584e233ce1f1b227e8c98b81de076d0bb12821685e", "ref_doc_id": "4e44d859d84869444c03dbd2cef1d63cbf9c43d4"}, "c090e9dc-4aaa-4f08-a8b8-612db0c27cd8": {"doc_hash": "ba7d1ca28f6388c4ab358fa393cfc6cc0b44f2644aaa66d41847861f2858e047", "ref_doc_id": "463c9401f53097c4b4b188c031506ea053dc6301"}, "3a73b319-361c-4e14-9a27-a7bee4dc84d0": {"doc_hash": "49df5226ce3cd6e8c45021a26647d7f754f35a4fc0cd45cf16ed2a8871743b32", "ref_doc_id": "c26f152ab67d2707afe697c4679a064ab66556e6"}, "6723f58d-f951-4a2b-bb2a-60c513660031": {"doc_hash": "1f46a8d23151d71701c4b6f39a4628a64f76bb78ab71781779f0c3a90040d76a", "ref_doc_id": "0809d75107799494fb8c16972d51f970de76ad30"}, "c439405e-e16e-4d61-ba97-e7feafb57390": {"doc_hash": "42c63985a8c13fd57bbdfcfb5e75f61809d3a67038714e1b54836cef6da94c3f", "ref_doc_id": "42199ba9aba0cca4a2a26542cb765283597105ed"}, "002c8a0c-48d0-4fac-a87f-9cb0794f0dfa": {"doc_hash": "093e05e9f97a591202a73605c94e99ed73aab0cf50141ddea48a0de16e127af2", "ref_doc_id": "85e8b5587b6f31f9b448727975a1e3ff36833238"}, "9c61c742-2d76-4c8d-9c3d-299de43a5ef9": {"doc_hash": "ea8487e7d8ae824ef9bd73a46a70a3f36c85d660c2acc967e50f07bef7c84c18", "ref_doc_id": "40dbe0cc13f191c976775c15adf916913fcaacf8"}, "c47dc2db-ecc6-4ba6-b218-10d8882f7108": {"doc_hash": "a0017814751f5c45b5d17ec8a151ec3238d882022cab7b4076c25cc640f6a2bf", "ref_doc_id": "00bfc9d550dc8385671f78f6db81e62b3870dafc"}, "5acf6bc9-9f72-4937-843d-5f676e268df8": {"doc_hash": "735234862ddeff1c3917ae574a8dcc2bf9ad1ecaaa4255febefec7f746cd0374", "ref_doc_id": "6f218445a84916cda4ab84d2501af92ef2624fcd"}, "f114bf48-435b-46ba-99f3-0172fa3a043b": {"doc_hash": "94b761b0cec8e2f75ed9a2de5b76ddbe2139ecb7dd32239842396baa4c7cb771", "ref_doc_id": "d1a2a171beb24aac21c623d9fa3607ea8b292681"}, "72e55be4-ff14-46b1-86a2-40c762ac08c5": {"doc_hash": "db327e96cea696027059eca9db03a6b976d1ba4daf89a4fb7cc0841a600bedfb", "ref_doc_id": "8c4b9b3d5709cbd66b0d1e93f5a51c1aa281c41d"}, "e907f270-0da5-4deb-b910-c156368a2211": {"doc_hash": "2613d22a2101513bcaab3c47a350e98eda4b88dc805413adc3551ddec1dea5df", "ref_doc_id": "970cafdcfa65c29c86e8a91628282045b764f1d7"}, "29083378-e353-4980-bd49-b957fd91d837": {"doc_hash": "bb886b01f85d5c07395281792e6accd30fc6e4f6f856b049fb4e2501e9b655b4", "ref_doc_id": "b93b7ef6f84fb060e458c7721cf967c6fe28a2cc"}, "abb1cebb-532d-4000-b868-8563f6d9435f": {"doc_hash": "588a94261e8dcafe27af0613a2a13328a71b62159dd6bcd019d05caf7d6a4bdf", "ref_doc_id": "bb386af040fd28f68ab307e651d57a91ca9b18a0"}, "d07278b6-16e7-4214-a27d-44a2f889f0b2": {"doc_hash": "84eef05a162612b19e27f0e50fc371170b4eb3487cd256797e5b91fa1336fd29", "ref_doc_id": "27d4f2754510f1ad3306d9c8469add6501eceeb6"}, "3b94bf0f-0dd7-4086-afdd-aa4a618d0e84": {"doc_hash": "2d4a1111a66f1aebda7c03c46407d924d0de58801d69b819904018819d43fa93", "ref_doc_id": "a06423a8e321339a51d190da81eb3b294a449648"}, "44930d26-079d-4b07-a272-c132e4094f08": {"doc_hash": "72679dcee3685c612568a1615a46a758aee66b4587e66f9e7a8d10b969681c23", "ref_doc_id": "b967bc936db6ab9d3c0f3e6f2869bbb27b0ef57c"}, "7c48762a-3619-47e1-8226-e7c04ff637b4": {"doc_hash": "353b71403d6f255b1892160561d7c3baabebeb53210dc0e037be52eb2b3505f4", "ref_doc_id": "eceae1a86e8aa3288cc1a43800c351434f59b415"}, "21f1f5a4-513c-4bf4-aeff-046d068e00bf": {"doc_hash": "ce993838e2ee551ef521d48d65f179b9df9b98608c7201ee951a2cb055a2106c", "ref_doc_id": "6a99bbe6aca306dee217cee196ac8d2a27d0194c"}, "a11074fa-eb19-437f-8b48-72709f9ff8f2": {"doc_hash": "4b28cb2fcde9f06428c881b187fd6c4ee5ee4b8ac62019d99d5c22b2371da22e", "ref_doc_id": "a7aedc2af5dccf5e23d70755a09601a8f3b5d7a9"}, "61446d34-5524-4d32-8dea-6623a4f8ef5f": {"doc_hash": "b6e988e6788334835bb9b3a924208c84a95c4d81008faf80fc91deff1e617ded", "ref_doc_id": "1c026d42ccdda4de172bd6724ba1ab8a6e9126b5"}, "e2c4037a-4d11-4b22-9e58-21648b51f6dd": {"doc_hash": "e20f7a79ea21cfd84dcab90a476109304a45d8d94d4e91d768a55356ea9deb5e", "ref_doc_id": "34339929e65ed6d22a4daf2f208094c329b5e136"}, "be9861f1-e458-4a92-8aca-d131b796f45c": {"doc_hash": "af614f1bb0d7913e0e32d4e86025bef2516c44847e1c1722bcca9f1f69a7395a", "ref_doc_id": "8df8dffc9f42e7841f6780d58b35c5aeb8399227"}, "369f8b4b-1d27-41be-910f-80713dfe4837": {"doc_hash": "da45d765a03193eceeec334ae189afb2f2deb13989f2e36aae035de526ecf8b5", "ref_doc_id": "416a1c1201b91f1a634947ecf07be73d4b894bc9"}, "d35002ca-f2fb-48d7-973a-55a83190fc6f": {"doc_hash": "f6976b36b8cfde7a1ea5fd958b4a45e689ee61db910d45b30b2f8c00e52c41b0", "ref_doc_id": "8cc32ade63f68f6128d28564aae817b8c8211e4e"}, "2bbf90e8-b1a3-41b1-af00-a3a2f713c975": {"doc_hash": "e94f0a52a65ace16bd3c55e39221efa19dac33de3babac27dceff55e9caa87ef", "ref_doc_id": "3f982a315aced401f13bba71aad1b8ed7d0bd5b9"}, "a28cec98-aa6d-42ef-91e1-20052fc19e4c": {"doc_hash": "f3eeec98b9be77c8d236b8c8e7c3b2b880bec95201393e8c80e76ad2ce0dd4db", "ref_doc_id": "933712e1d0b9dbaf6149d80991fb7b5dd3b476a6"}, "95d37e09-d156-4253-9ccd-a7db0eb3d477": {"doc_hash": "3a582b3e2d8d17b65cb1f9f0979cdb7807d70d52f0027968b984ffa1d37264fe", "ref_doc_id": "2545f995aecf6fcf4928aee098f8afe13b54888d"}, "174c791b-1dbc-49c4-89a5-0aa66bed6ca5": {"doc_hash": "47efbcb765263b50267e7ee0832306fd7daa7f1f867f3fc29689d02a60863289", "ref_doc_id": "538c2be6d3571518441d74d0ae2fad71ed7ab499"}, "e5d50b55-3a0a-4de0-ab90-bd9e6f56e2c2": {"doc_hash": "9dcfc98ac4a1df843616359d6a5293043766844de933c3a5255dce451a59b05c", "ref_doc_id": "8e7a9a3fa93ba4f04c3efc76b6d53de8330b159f"}, "640ad0c3-708b-4ac0-a90c-41a76ad5ad60": {"doc_hash": "0b47ea018a6b8165e53f8a060c3f9bc1ffeca1559dfe0e24a6415374b74c6ec4", "ref_doc_id": "35babd83be1a7c82971424b2c56c07a686cb265f"}, "ab894481-83c6-47a8-bed0-2ccd98a5564e": {"doc_hash": "d25692856c1f33e9b5e303718d41a7da89d2bd6a636c74b72fb8262678cfb36c", "ref_doc_id": "9f8a67c04dec932bd7f9bf2e0865d7c9cace065a"}, "f7ce1ff6-02f0-4e89-9c02-3dd8d1127cfe": {"doc_hash": "bf0cd3d0aafd37dce2d6fa0fc8a5535c075719f6645d55ec634f1338d23bbca4", "ref_doc_id": "dc6ad63a8fdb1acf9ca5633a804c96bb185871cd"}, "c0784f31-ad9f-408e-be2c-8b865d59ec71": {"doc_hash": "0ddacd016457d162b6df46bd2d5c0400bc070cab07f04f0e420cea4212856039", "ref_doc_id": "1f97f4831dd726aa459b6ad3e4918ad5b235bc74"}, "b5894634-07ca-4b94-aa9f-4c1d2656409e": {"doc_hash": "13abb9d56b9d76705acff0f802e64fcb592dd57e43e572846b5f44b14dac1e12", "ref_doc_id": "4219aacd5917410ada7578371c2438fc75044ae7"}, "d6522a31-8585-4fdc-bc26-10f905e99217": {"doc_hash": "6b2e8f9cd76ffec4635cd23343152292fae4226c1d3799809236a239c213d1fb", "ref_doc_id": "9b0f086bcb57c5d72780c8577ad9cb4cca0aa91b"}, "a53ba005-6004-4e98-bb39-09e445a041a0": {"doc_hash": "4defdad2feca35dcbfb03b06b7d22e87f1fc573f9c85d253321ed5ab0e7bad4b", "ref_doc_id": "73931cf84544f3d0ad1e93e5c2ded7b3fea84d35"}, "183e17e5-4445-464b-9ee9-eb86ba7b7849": {"doc_hash": "b3a1580ab5cb0a770a51b1c37accf84e687f8d51d9db651a43af19ccb3c3b6c7", "ref_doc_id": "d30b811d03d45da9374a82e644f8bf366128c8be"}, "67876a23-b885-4ad6-bd8d-d7266d9aaf85": {"doc_hash": "02fdebf05f6a064fc91a3939b56b47c662685e1f62f356b1e235ecc070efa945", "ref_doc_id": "c87419376264fe68e6822fc06605da7dfd799a2f"}, "487d1587-42e8-4de9-abc5-b1b63fe3b023": {"doc_hash": "7ef4976a817be8f071581777e27daa394747066febe31e7029febac85e02be3a", "ref_doc_id": "08b0614909cd1811603a919ec2a8e032aa1d6e26"}, "32597eac-33c1-40f0-8032-1d98be800345": {"doc_hash": "9bf31f89e5ac65c31f6743a2e05fd850f08a861c71bba5ca8adc4695a45ee3e1", "ref_doc_id": "48d0f51a27058ddbe8230a955175d2d6354077b2"}, "cfdc0185-83f1-43b4-a9b3-33781bc68ae1": {"doc_hash": "f0720aa567293578832e43b3c142423ae127e6b5c842b3ae422cd9a0ef2ebc86", "ref_doc_id": "eb9d97ae9b9f049307b05ae99655a52f8bf5d21b"}, "0ff5aae7-1b9c-43bf-bcfb-e5913152153f": {"doc_hash": "855953e7a511c25d3ed45a68e91dd2b106f82f28fc77d1d08a375b227c180fcb", "ref_doc_id": "a795673c5a71f4f4b4d089cf08871330b34cbeb5"}, "ecdd2a2d-801d-4588-8f56-59979a5ad5ef": {"doc_hash": "d8bb6fc5dec0f3d856e2d9a9838017b56e224607b5150cf6090e8e11fc07340e", "ref_doc_id": "8228cdd00f4751c0846e191773965c648940be90"}, "13594da6-17d7-4bc7-9bfd-076888d5b71a": {"doc_hash": "72c645a7b438ee911a1af2f584130b41fb816e79352379fe704738e9512bb783", "ref_doc_id": "9e725e989a7664f0830c50a86b13829393b2065e"}, "d40925ec-542b-4eac-bac6-5d0306603efc": {"doc_hash": "3fa35ac81d23665205fcfa4f1f762a08f71059231b1f89a619f2d02a4a30c96b", "ref_doc_id": "9438e50dbe92161cf658710ab70d72ac14791148"}, "b8af8e66-d204-41ad-bb4e-a0e4076dd3c1": {"doc_hash": "daac86d575d5b33dbdacba8f4af39d8506b4f7a99b721b608edc44a774cbec07", "ref_doc_id": "65e229d914eaaeafee90b74aae91502f96f48eb5"}, "b9aad2c6-2128-4b6c-9e6e-f5a15dade429": {"doc_hash": "ccbe218fc6732ea30fc455d620f32b06ce25b453d626f6f6fd61ced0c4ea646e", "ref_doc_id": "55d139eb9f2b47278624b8779915f2bda2dc6292"}, "ed4ff34e-306c-4cd7-bdef-efda21b31137": {"doc_hash": "389b01306debb4e50087ad2ac6509bdb5a5d794af025df3ddfb55c31a6dee148", "ref_doc_id": "968e69eea53249b40a963414dafaf630bf731fb4"}, "d7b3f4b3-d591-40bc-bce4-c1ea9c352404": {"doc_hash": "16cdbfd52a2a1023f766c8c754843b99112fac7123b34c56d07ea044de287d18", "ref_doc_id": "a3f581c0e175c5d94f3d3190860e2dc718d8b4e7"}, "be2481e3-940e-41f0-a3d1-f509a8d6874b": {"doc_hash": "3b5c793862a1be9c71730540cc4a5e6ab5116dde7447f49333c029ecf051ea45", "ref_doc_id": "40ec2cdb9e53bc33ccf7d0b5713ce71c5f0a1ab2"}, "bef5a0fb-8b49-48d3-819e-a3974fd701e7": {"doc_hash": "8738f7ac7d5ee9b0e5f2558037266295cfa3e36695c3507af8b69db50f460909", "ref_doc_id": "4ef2eba5f69e07e083e3bede7802ddd7dd82c3e7"}, "9b15527d-2f7f-4588-a98f-54f3d83fdbd1": {"doc_hash": "096758c00e0bdd2e96e826230063ff44f8627df686ea4173f102c252f40ff0dc", "ref_doc_id": "5c961591ca4316d537c54ae366b105188209f93a"}, "b46c8ca4-db84-4131-aa7e-262eb17a1072": {"doc_hash": "09d7c3aea10fd9b26989aed84a4d8e34b119cb5ad2c37df8eb8ed0c296cd2993", "ref_doc_id": "5de35fdc046688c1ecc4dc6844bdbfff2f7dd0ed"}, "86f9a39e-e126-4ae6-a7ca-f5f637b48ad1": {"doc_hash": "5190e7062f6528e3f995736c856fce477b2143b1fac89ea659951c0aaab2f9c1", "ref_doc_id": "7bfde8aaf3caadc721be5257d4353637b6a925e5"}, "884d5999-3bd9-41bf-8ee6-62a669ba60d7": {"doc_hash": "80f34ae973e4e03b3069ab2cce13cc5158cc555f1c0f06ea92a0b5cc50b51f7c", "ref_doc_id": "95f9ac48e5ab26a68d0419e7e512eb9e56f116bd"}, "61b9f278-0538-41da-8720-44e039e3c008": {"doc_hash": "89c9b5a21bc49593b3e854c4c4b5c8b2c47a61991f6e5e2c2089aa56de228a42", "ref_doc_id": "d3b2ab411b4b812a9e117b7408bf55e2e5498abd"}, "99424056-e37c-4744-8c5c-c31c994d1234": {"doc_hash": "1da7ffe6ac063b27ca94b8359faf285ff79f7e1c1b2c6a50633c96176596268b", "ref_doc_id": "28cca30fc708b5aa6dd3ef2352afbd6a66f4ab94"}, "0a543bf0-d4d5-41d0-a8e9-2932339f948c": {"doc_hash": "29019aaf3a52f103d867f541a6fea6e33aa6f88d9c14e91cb9831987d5d223ea", "ref_doc_id": "0edf0980ee6fbadb26c2da2d97d01712898ffe3b"}, "22d163a0-74a7-4438-b7be-9cf478136169": {"doc_hash": "c779d58dd35c5a7ae315d90a987ff4638f00da425283e5eb76754587dd85cbe9", "ref_doc_id": "708c39b8c2d11d81d2feaab86b27262bc9451cc7"}, "6aaad9ed-b8fe-422d-8042-914ebd9057c6": {"doc_hash": "2a31412108141c29b1d8051f6f13bc24b3d81103748027966dad06feef023e60", "ref_doc_id": "9deb167eca77eed0f121233a821264fc4b5dc9ef"}, "6235c7f8-ef2d-44c4-8ace-691cccd24c17": {"doc_hash": "5dd285bd9e05c4de7a731f88bde7572d2c62bcc4ee939e141e43b810e6dcc834", "ref_doc_id": "111d9451854da0e93fa67400322cd3b49660e216"}, "e8186450-cbbe-4aa7-80ea-301eda1f331d": {"doc_hash": "5e7f058295636fa2ffb18c425c5ad14d6642a36fcdf5e3e1bf3ec6386fc483ac", "ref_doc_id": "5703a1f04551ed71420c36a24482d9056274a71b"}, "52371d23-bf75-41c4-b8bc-323624cfe161": {"doc_hash": "8fc2ba199a00d89dc8b35dbd779a060be826c207c649646015467e506f1058f8", "ref_doc_id": "e65818ed0ceaf046428028d829514d5c444ea6e4"}, "11690fe9-013a-48b6-99f5-808b88573a71": {"doc_hash": "030a89e050ac0b4ef0a5c9fb0152332e58531f173ac55e6b2888ebea5afda9aa", "ref_doc_id": "20d04acf6ee1912b7349417483d003f68c9669d6"}, "df76d07d-ebe8-45e1-8a26-023ca1f300ab": {"doc_hash": "02ec89bd49db932086465b5b24060e5daeb0d3ed95ead9011c20e23206c80267", "ref_doc_id": "af7601407b5960fe3c30a3f924fed10d5f9b0026"}, "7bda83c5-87e2-4603-a775-6abe3b931f7b": {"doc_hash": "18f6ea4f68e538002634770776d517003a8cdff7245c5a9b245db3f3c6e50d38", "ref_doc_id": "3fda42eaa673ff97b26a1e7a0b8f165665a24875"}, "ebc91640-fb5c-4f3d-b624-8ac75c0f8a2d": {"doc_hash": "1bccbd11a82a7711466ed729609af8b448a9c970334c3663a86abb54687f4ee0", "ref_doc_id": "56678f5674fcdb8e57d41ea4cde22589cca58404"}, "f7cbea75-8cae-4445-a055-d6f763b86a22": {"doc_hash": "ca2d38c007114a908a653e5b7ac78b9c35e66dcafc41ec44400cd3066bd25d27", "ref_doc_id": "a35b120c851acd7e9ddf9a21638a9d9420db70b5"}, "3ee98647-bb51-4cae-ae27-efb6a9e9831c": {"doc_hash": "c724e2e8e798565a8bb37e1feac98a3c45c8a6d1c682a7215235fa62c56843a7", "ref_doc_id": "1b23649d2c29d5758bec9ec05d75035849e5d857"}, "10b1f377-3be6-45aa-a356-356ea64f4460": {"doc_hash": "0c09e5c16fc690e3027f19f11e20227be432979bade56f259e727a9995eed8e9", "ref_doc_id": "5f1cf93672363b8d2ca62c2c44249419dc4cd41b"}, "0b0b843d-557c-4c9b-9b10-cca4f7960128": {"doc_hash": "1cd96c8196c0bf263f043b5b8a1a92f9bb6affe8c2544c1fcf8978c111873db8", "ref_doc_id": "073c0d1e0d48079d6a431f59105f9fe0692c129f"}, "8916022c-2a20-4b7d-b036-ad836c93ebe6": {"doc_hash": "b42119ebd9fa589c616422114695f26729b54ab003159e5ea12e2200e0ee1180", "ref_doc_id": "d1f3b9613f634c5a7ca44acba9580820e4bf6384"}, "1f7b80ac-8520-4bd9-89e7-e3fb55036895": {"doc_hash": "8f4dfe0145b70ab4d9e62ee03654f81f4b2c0cf813eb2ba2f87fc6689d2b7841", "ref_doc_id": "fe649b280ad28ebaa911b5f19160c46aab707892"}, "b63572d9-8359-43b2-be38-6ab88b7191da": {"doc_hash": "df9f9f2d53d668ab4646d63f497fe1950f0197cccec4139bebb14ba6c47c573b", "ref_doc_id": "585f57f1747011659453ae04fc61bbeaba9a9e86"}, "1667c08e-07f6-453d-80e1-d5eea464cb36": {"doc_hash": "ae447121de441fe162085bb00e22e108d76291eb29fc0f21cf4c2df93fc68f7f", "ref_doc_id": "93a0fd585f76e379efa0f711177fec7ba0441a88"}, "863c1935-d225-4bc5-a98b-50fe1e4f720c": {"doc_hash": "5adc6ab0fa214b8be6fe5521f8a887c37e17453af8678b14da6b0c5794ec6106", "ref_doc_id": "af0889ad3ceef79b60a37d986f7ff5cd9812ac9a"}, "52da75af-87f0-4706-8b92-afd4dfcb4982": {"doc_hash": "9fe35a72fe3cf7ae31109cab9c09e14a9a440bc7072422dac46a8572a1bbb84f", "ref_doc_id": "c3b28830cb8554237936a9c82b51a2450739512a"}, "933b7fbe-5397-4e88-a7a7-57be1d0ff893": {"doc_hash": "2e0fab3b27c06efed05a53a96630f62712b234b43596a7edda4bbad4d09383fe", "ref_doc_id": "b71e34d84d9579104d0cd97051ee7e2f76ba99e5"}, "40ea3c87-7fec-4e8c-9465-5ef58c5c76ba": {"doc_hash": "fc1ca82fe71fcba740131e03129a1f72047de729bae4ba965da71ee6b2aa17be", "ref_doc_id": "2942924bc485616364cf0689e9bef63f49a7fa02"}, "2fbd5341-ce71-4292-a4af-84451e7941e8": {"doc_hash": "ae12959eebf1075d35a643dbb3b508b545d59294dd1425833bc1054d24342803", "ref_doc_id": "6494818b45eba0391bd0945864bf46d4bfb6463b"}, "4815a023-2ee0-4ab4-b80a-a354c6e34af9": {"doc_hash": "6e34573ef07dcac9b267cc09d08fdebfd94fa23ea2dffda958be66455aae6f88", "ref_doc_id": "bf705843796aa56369cc4e3113d93ad33b8d3baf"}, "51ea4b04-2c80-442f-ac5d-424dab6b0f2e": {"doc_hash": "e3a9f844022f8ba1448b6d37a451dc78a712af04873a910726910ac3d7596bcd", "ref_doc_id": "6864421bb639bb70c83e141fe8861f57da90663e"}, "680d0299-32bf-4976-ab70-3287c207a9e6": {"doc_hash": "a6dff1e5b421c868c238a6ea892f2548938f77afcb360f457151d0472af854dd", "ref_doc_id": "cd5fc89e05a6085337a5db2aafc7418e763e0e2a"}, "c0223e76-6475-494c-8f3e-620aeb17ff14": {"doc_hash": "0b12aaebc172c81d6a08e57f4b59fb22ecc8a5bdf2f08f9b887d24ecf47943b5", "ref_doc_id": "99591f15e438aec1cdb1969b8d299caf6af56f6d"}, "76991441-812d-4103-8e53-032f96bd247d": {"doc_hash": "8a129a27f0e068db7b23dd797da773c10f3c8eb4c7c5f245aa062030a056837f", "ref_doc_id": "470674882f0425673e271285fc98f788f314cb82"}, "4fb223d6-9fd1-4d11-8815-9037c6853926": {"doc_hash": "0a4a29a5fb90d3d08730915a9e41ae034fda04d0e677edf238ff80d6e74752ef", "ref_doc_id": "17dc6f68884f3c5fd83d2bf86d0886b008a2b120"}, "395917b7-c3e6-47b7-8aef-48af468f417e": {"doc_hash": "79900b99b0773662de3d2d73f274ad8670dca8aa1edbb4174648747d9c0c8bfc", "ref_doc_id": "a3c793394089374271bf4d22d350d63345c0c709"}, "247c10ff-3366-457e-b13a-d2aeba4b74c2": {"doc_hash": "6183807e16e1cbaecbbda5226fbde55650cee9bf91ee088dac4bd4fd4fb3653d", "ref_doc_id": "85deed719c2bf7221170c5ca440ee7745ae62e0f"}, "13d310f7-c6a1-4920-8357-31232f639912": {"doc_hash": "c8bc3cdc82a9bba378b57f776a1f27cc73f3e76b67319191c5ec593698a4e8cf", "ref_doc_id": "1466a864334e90a545eb87c8b9b89b20700bb634"}, "294ca400-783e-4c95-bd2b-dbfb0ee8f8f3": {"doc_hash": "ba97aee176cf6faea169273ccfb63ea24bad25d90247ed08aa6ccec0ed5731ff", "ref_doc_id": "eea962e0a1fc0db744a84a85d1f805688dea56c0"}, "cb59511c-ad04-4909-8a08-7a45e3a4664a": {"doc_hash": "d0639c3cb013c3a48067fc3adba969bf251bdd36f10611bca16e2d797b2b0d27", "ref_doc_id": "6e120cdb8ede6f4c518289399e9434407d6d08a5"}, "9b578903-836e-4b6c-b0ab-64e858fa5b46": {"doc_hash": "e02b6f35e704b898e24f40288e21c8a32a7192fff6860a48bc940a5793bb8232", "ref_doc_id": "93c5abc5d6f40ca768eff15129ab1dc28b1a8ddc"}, "819b4f68-56de-41ea-8ec7-ade5f09997ee": {"doc_hash": "3e6368b4d70a9b1bb1fd3849c57d4241109dadb498cdf2b2645d213382cb9659", "ref_doc_id": "f61df34cc7535506442d9020dfce6f4a9c3769a8"}, "2c98edb3-39e5-4882-9d98-c34b2e325dbf": {"doc_hash": "c0cd2683eb5194be28eec4865333f2f6844c0ba0bdd382ecf34e239f6b3d0e3d", "ref_doc_id": "812a79b09c3be2e63b7b03b2a5a2579e4fe872db"}, "90a79192-b11e-4b07-9f5d-1a50bcc3038e": {"doc_hash": "d37f11173fc7733199ed4c9419a667caa86d8d00f8963867c2d10e13a8c1e7d2", "ref_doc_id": "a355ac6f950a5c186a92d10be74f16773eb0aa45"}, "9d74f94d-b1d7-4bfd-8f74-5770dc5e9ede": {"doc_hash": "6b50bb665a86f21de4af3f2f421d093df90f676d8c97dd4670e22763e09ae273", "ref_doc_id": "35d731eb08fd9cfb35c2d282f7afc4c86484e26d"}, "36104a2e-48a8-4407-b123-ca749ae44aec": {"doc_hash": "c01091f7c4035727465366e0b7fbc30b8bb5e840d6481861fe281e318cec808f", "ref_doc_id": "9670c55b69450b1cc1077e085201371e803f7c56"}, "4be2015e-690b-4ca8-95ac-a3d0a352f2ac": {"doc_hash": "c554550cc983bcc21ee108290d999e4f7c8af6b18875b1785b590d937b5b7f34", "ref_doc_id": "0bd37e6c3c91515ae46dc5f7dada9bb8bf073255"}, "a0bc7711-05a1-4613-a43d-f4ec927a83fd": {"doc_hash": "57be0378138f073a436c59c933733a0eb2558e083c649613dac5176ab2304e68", "ref_doc_id": "146b90c79821009d243072444742da5904f7ddf8"}, "749a830a-e84c-4a98-8dc2-f82e5571d2da": {"doc_hash": "b0aa5ff350c009f5d9e1aeef502570867272475a7340d21dc7620dea10124a33", "ref_doc_id": "2f5e47a62b2956b5d7761868ca27bc304b607ba3"}, "a3d54bd6-d359-4104-a8bd-fe3e42a786fc": {"doc_hash": "ad12a673593654cf6e43383972331e6daaafc25a79daf5124b59148eedf66f4a", "ref_doc_id": "9a153a76d1a863a089b008119426634326c33670"}, "39a1b6e0-ad30-4160-951b-b09da91b5082": {"doc_hash": "b8d19d558310c11a5877fcd94666e21c8613bef9c1336a8e3b1938b137bb20a4", "ref_doc_id": "88c4aa106ca92abd7ddd89bda068f25b232dbfdd"}, "4a17a333-3d96-4861-87c2-a51fbf4ebb5f": {"doc_hash": "5dae4341416cd0f63c0a1f1ff34d410361f26ee6804b3aa6df23c422d01b17df", "ref_doc_id": "1f4f18f9ff3683c22d2099c48117953600d2804c"}, "715cf458-1d34-4915-8516-578cc98dc090": {"doc_hash": "a6e01992ea1d4fa00914eb355ca4a047b289f6fc538678d99853ec0e53eba1a6", "ref_doc_id": "a266ef18e03fc2400a105c0ecc6f8b9e5a50748a"}, "9d5321cc-c330-4e4b-be5c-caac05445ed8": {"doc_hash": "f7943d5b851a7a5fd995a5ec2571c1130381b32ed3295364c36f7ab8a2a751ed", "ref_doc_id": "4011658728dca3169d4e10c488ee9964de3fdfeb"}, "3441db2a-45e2-436e-a2a1-030b9a1286c9": {"doc_hash": "318d3eff11824c809813f9c666a682c8b5384d158b631917b93bca2a0cd9e5c5", "ref_doc_id": "8e9eb28eddac9513bf7441b93ff4331ace44d5bc"}, "63d5548f-bf77-4a2b-9d6f-b8ba5d17568c": {"doc_hash": "45423a395a8a28c35028f7a4be5606668341946e7dc980fd9d53df228a198f2b", "ref_doc_id": "ca7dfb98a4b2ec8a5a431eb03399c25aea19b3a4"}, "7646ee70-3251-4179-a1e5-32b14dcd8bda": {"doc_hash": "ce02ba1d4e8e74f74230583c7101dc716e96b5fadf40fbf34b5d2e01397c63c5", "ref_doc_id": "7491957cafb40efc1f27ec60fd37677de4fb47f6"}, "c7868ea1-95ca-421b-a690-d375c5755ed4": {"doc_hash": "d80915ca0e4e51bfee326c6c5823fd36bbbdaf276049f9d4bd7fea612525e213", "ref_doc_id": "693177d604653663d10afccad72362b93a6dec8e"}, "51e5e9f6-04d7-4a64-94b5-3fc66466d4cd": {"doc_hash": "be797dd996a6345b8616b8af2ef311f6a4fd09960f50e6676a4ebe6134cd07b9", "ref_doc_id": "40a7fa6a6cacaedf05d04a7f5ea68a71527b044f"}, "e77fd714-c5d5-40fb-b45a-bde01b978701": {"doc_hash": "91bd5bacd762ff1e09a3dc1ad7795ccb44a451a3b1204b2cb65519e19641f54c", "ref_doc_id": "d2d46e9f195d857f3f9da7e417300e9b8ec72490"}, "f228ee35-ac64-45bd-8dc1-27d942fcde82": {"doc_hash": "6960969c7fe0a6a93d964b37d45c14889b6e02a78634ffca4310ac1011f1db84", "ref_doc_id": "4e2e4c253d0d9d75df540cf0a1748e6a6a1dbaa4"}, "9c6f0da1-15ea-4462-9f6d-7f2a0a4b2c71": {"doc_hash": "608d6fd920621774de41d7ae618e771875c4d31cc78fde8e1d3556bcb16a4be4", "ref_doc_id": "f82a29aee5662e7f900228428dfb3fe176e67bb2"}, "2122aa37-814f-4fca-a5a9-549a22d8d2f6": {"doc_hash": "c32346e779384c4a0cc8ab9f08623f27d1c6aad153799e80b96b6cc7af9f289f", "ref_doc_id": "5affbb64582c3dc9d8a2d395a77786107939a1c9"}, "9977898f-d7e3-4781-a007-0c23e2db0af4": {"doc_hash": "8998b44648adfea0788e0b74fd59666413d23b0f5d0134fa22c03c358fe15833", "ref_doc_id": "4bd28dc0d85d77931eec041337df2e7cf9e97113"}, "b70d1870-8033-413b-bcab-4f1c37d25906": {"doc_hash": "650cb23bde90487be40f618c40f81a0af99ab00a30a6da845876d1a2138a1b97", "ref_doc_id": "2b36b4029bec0d03c0b330100ef29a6f1caa7847"}, "46a07493-1694-4638-8771-9f32206884ac": {"doc_hash": "738e7a50b1855c8cc6b9847d9030e7706a08e7b09141632c7ede27a7d1f359ab", "ref_doc_id": "0a6ca314971e60a1936935700957fda0bf89e806"}, "06ddef22-f3be-448a-bbd4-50cd633bba0a": {"doc_hash": "7313b552ed23cf7e74e5cfa7d7215d62c3665f23f6202a642e17a6933434111d", "ref_doc_id": "8624f784c10d51511f4a718960f23c092e6052d3"}, "ebea690f-e5d9-4ea0-897b-cbcfbc3de85a": {"doc_hash": "b24d7690027dac8cfe8de393d6f3642e85b9983637a213124105aeb7610306ae", "ref_doc_id": "3ebf340c0b9bc8b8306258e61c9c170e5462ca86"}, "ecb51072-0d60-4723-a948-00a280931726": {"doc_hash": "baaa3ecee8eadaa88280b14348299f54146c2849746ba315e5543069807adc29", "ref_doc_id": "c6c43da91f698137800ce66f5830b2d0f9a9367c"}, "cd9a6192-ec7f-4b35-9cc7-c42aa8a12a90": {"doc_hash": "b9b7ba3f44770d775c075ffc9a1a07699c6d3e1a33024c2f3f33c646842ba30d", "ref_doc_id": "3eb70c5518ee54be5c2f78b8b5c860c3b0343d19"}, "61edeba7-76fb-40c9-b104-650fcfa85abf": {"doc_hash": "7aacd5bcb35de7d1f32ef2a7e593071311bb4fddba6cd8e55a4fb3c7e5a15b4c", "ref_doc_id": "2c7fd19c8a42353e861e3afbf09fc6bf2b2f8c42"}, "9cd617e1-8362-4c93-a752-e5f58540ced7": {"doc_hash": "b0f8fa4700fc4fe38719820496b3b8f443731b878c601208bc6a64f0f875dd80", "ref_doc_id": "4e3d7754b23c84aaaf95ad6226cf6fd752005977"}, "9b865f83-856c-4fc1-b1ac-05d6d8736d62": {"doc_hash": "68b7066d762a680d103b900135b39632bc7dcb9a57e33af03f3d545d5dd79e5b", "ref_doc_id": "fae51af416a3b67c5de7b7d7bf94a4cf728160b2"}, "2078d050-a8b6-46fc-8a50-c93cd60aa67f": {"doc_hash": "3c7170a21737a11d7960ebf9fb3f331337aef7d1bbdbce37661644e5b4dc7131", "ref_doc_id": "833436604a77440b19eb68f0c374727630692910"}, "2cb9aa63-03ab-468f-8ef2-4f668631d72a": {"doc_hash": "58a42977f743bc1c19e04a399e663740c8a923e461c278ef613777215d1aa708", "ref_doc_id": "910cd15c371d6413fd00b0c63396920a5c4ff81b"}, "68d34a40-00e5-4cd3-b151-57c0b93858ed": {"doc_hash": "9bf31f89e5ac65c31f6743a2e05fd850f08a861c71bba5ca8adc4695a45ee3e1", "ref_doc_id": "48d0f51a27058ddbe8230a955175d2d6354077b2"}, "f24e4521-2a60-482d-926e-5c7452925323": {"doc_hash": "7b225d887563b512ac3db99d41a55f0a65f840b9262f4ace7460321225a838a9", "ref_doc_id": "de64246483ae8c6df49eb973eef177f136d36c05"}, "7a0df90f-47aa-4e83-9bf1-ab30d799fd1c": {"doc_hash": "317084bb575893f00f714783eabb02ed0723712e1e11afd5c6c2ccf8fd24287c", "ref_doc_id": "93036c25eea447b615e6a9d0da5bd3ed72f5312b"}, "002b4dd4-2881-4a6d-adc5-cae1031197e9": {"doc_hash": "0362318d6d3464159b2af620cd9cba06473bc264ca4e2562efc354b621bef426", "ref_doc_id": "e37c240e3711d44c3deef8958510693c6ebc1d9b"}, "81ac9299-1c7d-4a94-9637-d38099e7ec59": {"doc_hash": "a607452af5a3a704ae2f391c58c819a1a9328f6c1b90a1201292136e5e01f796", "ref_doc_id": "022e849b38330614d62f493fd9d2674a1367b60d"}, "e885a90c-656e-4917-aa42-2e3d919f1603": {"doc_hash": "62214a072477399e79c9944c379bc73b6a840c666d55aee20b7c4c1a3415b34f", "ref_doc_id": "740507ae894011157ea7cbd6c215ce87e40e44b5"}, "38921181-cfed-4044-9a86-9edc979ae941": {"doc_hash": "10e0d5010e67531d60114a1acc2239404419bb31d7a4090fc3603b9821193e8b", "ref_doc_id": "85c66da61db9e08f0b92e444d4ec4d7d036d3679"}, "39d6aea4-9204-417d-b9ee-53be2faba1bc": {"doc_hash": "5fe0217eb614227bb0e4488cfe424ebe912fa10c3fbb82f8f74749be1e876062", "ref_doc_id": "408bc9de18eb9c1f945d66cb68772ecc2ad786b9"}, "25900ba9-4290-495a-af61-ed9887c017c1": {"doc_hash": "c624c705d0713a36b3c615a073ff1f3dabd37f58d94a53ce512bbeaca5f7ecf3", "ref_doc_id": "914e511e523bf748d78568d41cbca3dfd69613e7"}, "daac4a1b-2828-4b15-95ba-dd3a4cd12e87": {"doc_hash": "e1be7ab354f4b5cb8886d7a5904affc39aaa5e9a119633b944360c51036a0069", "ref_doc_id": "31e933c95b0e910c41fd5fcabba64cc17bebfdd9"}, "8afb54b7-8c71-490d-9a76-8ff8a67c3306": {"doc_hash": "40590d06561b521281dccd1c94573db5742951212fe9cc552c997551a7bf9955", "ref_doc_id": "1b0db256d641b3fe2b03ef491f9e4efbc7ca2cfb"}, "dd6f30f0-b0c2-48f9-898b-02046761f37f": {"doc_hash": "5ac1b7ad969e5959b542b23b0ccfbba037ea0a847d403c905d394ee9bab7deea", "ref_doc_id": "0280cc1899d33902c0cd5c9d4b2faa1c58624ba0"}, "fbe401e7-09e5-46a7-a193-0db90f1cdaa8": {"doc_hash": "c36425900a2bc5da091f9a3e829058c864239b4972a3ecb328669007387173f7", "ref_doc_id": "5b3029dd9921cc46763915be9e4e890297a07c51"}, "8dbbb499-a357-4f65-b825-d1301c2385af": {"doc_hash": "249470df3b1241bca4a86f02dd1b45b491efcb620ff06e3b32e75a98cede809b", "ref_doc_id": "734d6fce64418d0bfa0f062909198ee98aef37fd"}, "bfde5c17-fda0-45fb-a940-dd55b072b24c": {"doc_hash": "f4f77f0e3d9f4785be6f9d457076e7025fcbf0391aa518371a96d06b0d37906b", "ref_doc_id": "586cc46d5c8f7b1f8b69c72c27f21f4e7db849b6"}, "e1566b61-3f98-4ca9-936a-8d8da644649b": {"doc_hash": "1b9f9edd0f9ab012f770ebb21793b9939e0e4a31028ceba88664e78d3eba621e", "ref_doc_id": "9d5f8ec36d72cab49b56a8dc8df806a92af1a72e"}, "a312fd72-0774-4933-9e3b-486571f34d04": {"doc_hash": "f2df1b29f2feeebfa682b2ab330ff6341fa8bf5195d4a5cc4084510c4f67965f", "ref_doc_id": "89fc49ef34fb7491d7c81318f97efe260beda8f1"}, "9a4dcee6-6760-497f-a98e-28287bb021cd": {"doc_hash": "fcff9cc5cc19dcb52bcf58aa15943bb4e1356625e5250073dd19771ee98341bb", "ref_doc_id": "357809ebfb6641a4be757a9f4a6e3d1f644a7839"}, "f4fe2c9c-979c-4e80-9655-abbd2fab1efc": {"doc_hash": "638dd3c440135a5509cc19f901b70fd50ba879f75c20b15978b1927af0cfd5cc", "ref_doc_id": "890be0da4b482be0ea867018dafd66c08ec2d0bd"}, "8fb5fd44-a5d1-4bb0-a702-a228aef2353f": {"doc_hash": "95db67f5104ec109efe5b2003d465485a14f85117d8616cf07b446a9869cc56c", "ref_doc_id": "5d8ade8f37253419b9ba2c7b3ac738dbc098a3b1"}, "6e5d82df-9016-41c5-bea7-40ea92a16537": {"doc_hash": "c8fd3508df84ea7529d38f5bb1ffcaaa87683729a1cd642eb4d9b96447b81314", "ref_doc_id": "26752d4865eaef7fa41439e9aaf99317dd556a1a"}, "5d8adfd0-51d5-4d4d-9539-337e89876d9b": {"doc_hash": "3269c5639fc9d86c8bc7f6ef3295bafb49ac8fe4b5f6f45ee2cb09f9d3b0e207", "ref_doc_id": "db20c905e943a6d0170a5b66992865814602a6b6"}, "b5a31247-0f5a-4c81-af50-7295fd2cc50c": {"doc_hash": "22493a39bef234039093a073f7c69a721eccd1f7924e351c8c6cec93e79fcc97", "ref_doc_id": "b89eea42594b57e31d87954a2339bf21676a4291"}, "78864713-c31d-4b5c-a75f-2e35224836fe": {"doc_hash": "2eea10cd35183677c149b52d311f1a52d30cf7cbd84b8ccb8970757e1fd99bcf", "ref_doc_id": "5b7f2acf072cea75f33acb439f9a25b7dfeed062"}, "c1c3fc12-d7a8-440e-9720-178022885e6d": {"doc_hash": "30f77c295d16cea0c3afc76cb7489ac0f534ac59899102b3eb0c4fe2c430fc79", "ref_doc_id": "8b7096a2c2ead5ae7d3a4f0478433226e54d2c16"}, "929deb35-19cc-4fa3-afd0-9623c4cccac5": {"doc_hash": "76d8ee5aed00820a6ee8c2a1d8352f776c466d0bfdb0c0a3884b8423f6d16a1b", "ref_doc_id": "30452628318def79ad377ea4cca17da1a3877b9c"}, "16839a38-f5eb-4b32-bac8-15a918740a1c": {"doc_hash": "72bcd50dc377380d7ec61019c616ceb0d6abd272f6ee0fc3f05111b4f9223dcf", "ref_doc_id": "031e51db9756578edb7f513446e090197f061c80"}, "6d755120-f481-407c-9a43-1c5948b941bc": {"doc_hash": "30fe0e51ffafedb290c880040de7a765d5dcaa618a200fc8467b46e041c6d2ee", "ref_doc_id": "61d8bf6faeea49ad098334c547ede763dbdf3069"}, "4a997189-0663-4709-8666-787982f28fe4": {"doc_hash": "6f97626e6fffe13d3a2927d8995d68c202d266a307d7be4d378a5b21dd1c4f80", "ref_doc_id": "fbd39b1fcd7d617c9be60de9ef4a1584e2a48dc4"}, "2cc0992a-76e7-4f44-b308-08b0ef886f0c": {"doc_hash": "e7d9c20af3af4fdd291e2dbfb3e27517a63eaecede28766f1a7757dd430d8346", "ref_doc_id": "56318c43b0a532c109df5990766409115b864d41"}, "02043eb4-0375-4ce6-b095-a49f4649a197": {"doc_hash": "b3af6267875dd43169b0079f30de187c291076a25204107f3985e4e981c37a7e", "ref_doc_id": "46dd9dbac596eb7265dd0ff431dda3cefc2cef7c"}, "ea04c740-5b74-40cb-a2f8-284358e3907b": {"doc_hash": "8b252fe66e2f2057c241a195a1d47059ce4d74965d7fa40add67019adb751f5e", "ref_doc_id": "3f9ae643520832a6886ca0b483bf6d24f6205aae"}, "b24eef26-dee3-4cf0-bf36-cbfd8eb0f7fe": {"doc_hash": "91532f58e684384b607e61b75e7568563a00bda487c0e50b7814f9a939b17bd8", "ref_doc_id": "bc46978ee1045df6d5d89fc3f55617970ee7a3ff"}, "3126cd00-c833-40b8-b3e0-f6f66cb76234": {"doc_hash": "a5c65d9fb6fcd057e626f3221d6ed592ab5ea304ba7c89d241562164a05dc68c", "ref_doc_id": "b8ae1db630dc30634d8404eb8c1365bafb99716c"}, "3c4d91f7-891e-4730-a246-bede2c6a2f4c": {"doc_hash": "2d5651d0feeb5c9a8e9397e91f951d3066fa24ebd2cf8b02235f313a677a3540", "ref_doc_id": "e038987c90f829c6fc21ec1f541abebd926d0686"}, "d880d95f-78f1-4ebe-8b8b-1635255841d0": {"doc_hash": "2bc2484e9cfff2b593126b82835e61cc7fdf1f1c7014ebaa1006e34fd14d3d3a", "ref_doc_id": "1fb219e1b1af75202a166d2e0ebac17a2f3d6739"}, "22debcf5-b223-4b78-86d6-14259a6b1328": {"doc_hash": "591305f800b2451bf874640d7cf98e93f6e78c4103bce5282792982d3072c204", "ref_doc_id": "cbda070e4bba3c7b07f6886c357b2c4d0e721b22"}, "3a9aa7bb-69ed-48a3-a5cc-b949ce753bbd": {"doc_hash": "96ae6417e7fcde2ab98ea5e4326073f2f8cdfd06ddb7434608c69a0e3f1ff0dc", "ref_doc_id": "a7ee67e48f8c0c95618dc39221f5a27a34ec829f"}, "c1ad5dc5-8922-4552-8f85-8eac211f1896": {"doc_hash": "fb455a02fc082f73ad7fbac96a67cda99181afa07f2992b29c8d745786dd9dac", "ref_doc_id": "75481ec81e17040fe2ad0d03bd5d3dc928687a07"}, "ca9d25ae-725a-4949-a514-bcc16d700e0d": {"doc_hash": "c12cf7efcd5b74a97945de5f6ea8aa3ae43a80fef4914ff46d10c5a60def5b45", "ref_doc_id": "c7c763148fe6a90ad34d1628b4197d9432dea81c"}, "f7446438-ebca-48cc-86e7-bc9b927518df": {"doc_hash": "61f81d34ab0a53fb447faba5ee418b4e60934460b40525743ed826e81173d47a", "ref_doc_id": "886d77cd229608205776525bb114a6444ead4aef"}, "f08ea7b2-4ad2-4065-8e7d-bb04336b1785": {"doc_hash": "6101d8559aa688780c811f59a05ebf5770307574c12c75ab5c4dc431518ca620", "ref_doc_id": "d86bdd5b6e515127e5cbf105b1aa8f4ee0dc659f"}, "74fbe73b-b9d7-4cd4-9615-3eba7820d8db": {"doc_hash": "8762a4e485753fd23fa7b4fab47b4a0a7b27225a245f9068dbc515fcf2de3b53", "ref_doc_id": "12af139e1de3404bef178badec24959cc81d6419"}, "378f5d30-e333-4928-be19-0d3c93abc2f4": {"doc_hash": "83ea744a8bb1f307e3f0556c21c7e6dcb5c2750bb7d477ec9485b2a22996db79", "ref_doc_id": "99a1a37a467216a08a0cb98e469faa8e2cbfb01a"}, "d6176453-1f33-4dad-b714-8bd3cd08379d": {"doc_hash": "cee2a7d99967792dd6115d1d06624e582cc52878be2210b94468924d57347ff2", "ref_doc_id": "a8a7fd8d00ed3497217062af2fe4618463558839"}, "a4b2530a-2bfb-4fa9-b962-bc1ca017ff01": {"doc_hash": "6d4e52d7a591d8675b93d2d8a700ead4b060d941f6e24225c2afa50d37ecc2a9", "ref_doc_id": "66ed75cc0f93c38031e833ac9b63b8736eb8c317"}, "76b83612-9b9d-48c9-9bc4-3c0f4442dafe": {"doc_hash": "cbc36eea8299cc9ce78b9bec8bd4f4b8ee67b2196689ed42863362063b2dc524", "ref_doc_id": "e83205e9973e1635664456e9b37a316ff55571d9"}, "d58b485d-d61d-4301-a18a-8ea259cf2a70": {"doc_hash": "ddf31736873bee8df906d33b9715813b71354923fa7847e7500d0d9216161112", "ref_doc_id": "e9d2c6fcb1f11f8a89234ed887632f0a625821e3"}, "b41a84d8-69dd-4efb-b5cb-3cea6e71ef09": {"doc_hash": "234ea9c7564f4c86aed92c31452f0c7eabc1af3581eb3c6e512803e136ffa3b8", "ref_doc_id": "e80b6aed37a70227ddf23fcce6724c295f2703e6"}, "fb7de2be-19d9-458a-a1af-bc6e056459b8": {"doc_hash": "84ca2a8a90463de62e9c8f5891ac1f64711fff7736fbbd4c26a2a8508dc76bd3", "ref_doc_id": "c07fb19215997328f6329fb50d4884f384a25ace"}, "57a5e2e5-688e-4b9e-9a59-89abc195fefd": {"doc_hash": "966ee421df857bab928d28919bc191d70e4734fb81a62e7b39fdb8b605691202", "ref_doc_id": "ec4480f32b0bc57c291ec034219105a53f1134ad"}, "c4290f30-c836-4472-8e09-cfca109c1624": {"doc_hash": "891ceb9f60d6b8dd7d518712dfd803da69bf10c036fda87cff9f39713b883ba4", "ref_doc_id": "1da13086404288c96ea3cbd4f305f0c44e63d837"}, "8f3ea72e-34d6-428f-af3c-df8c3c46cae8": {"doc_hash": "1adb936baec4bfacda4e0b462a89e9ce78f3a1cd6dd1290d48467c8989c89fa0", "ref_doc_id": "dcbe227332a9b52d9dc36db4704cc284a4c3ca74"}, "77ff3670-5771-4768-9894-01cdb058a559": {"doc_hash": "547a27e6b879155971b0b4bd3c9ddb84e39e5d98d520bbb6990745b4ee3b3002", "ref_doc_id": "f63a55d6923cfe8aeabe86ec140570a0ad82d2cb"}, "c4f80904-63c6-4c9b-bf2f-4fd5f3f72e07": {"doc_hash": "1fb3e2fbb6b02927f2f4838b5c634b2f75b3227dbe344057c0879ece7799969c", "ref_doc_id": "1b8b541953a0f33ec63398f62b2a70f703a1d428"}, "ba0e5786-cf32-4cda-ad15-484f03a297c1": {"doc_hash": "e335e5a4be9701bace33eed3e40e5bb65f3bb12b8b2cc1e2acd7b802a3aaefcc", "ref_doc_id": "a69de484874bbab04cba05602a2a554cea3cc7a5"}, "85822257-d65d-47bf-ad8b-78ae0827028f": {"doc_hash": "f80e0045864434e8e0890c6a984dbc430a898189025e09e89693aacbe0a23e27", "ref_doc_id": "2a4c3bc095b3bb31b754fa53904d845d0d0fd60f"}, "27a6b5a2-d57c-4b86-a3bf-9cfe48d26b35": {"doc_hash": "94a2c47c9ade4d2b071d1c86f1e80a5d209c803b82326d68d2ce3446dbcd5656", "ref_doc_id": "c0544a715d0d5b6af091dddc84b2df11ad1a8cf7"}, "4042e84d-9eb5-46b8-9bc3-29152f924a23": {"doc_hash": "e3a68dceadeef6aca97c7600c733985dc6978e5bed2ff37d4f291a266f9aa2c1", "ref_doc_id": "0dea41f8e24efed91986d3032c78842346b2378d"}, "9dbc23f2-ab86-4442-9039-460b0b9a6324": {"doc_hash": "939eba6acecb429438562515a8e58cd861b40a611c4d66e933b0657f05682a95", "ref_doc_id": "d98fdf3dad795fff9e2e214644bb22ecc5b34364"}, "43f0da14-6f32-4312-8f63-6b9939717239": {"doc_hash": "497c3d1ca2609b2c19c916008a3dde374081c622cf279a507507562df5695b06", "ref_doc_id": "e8040efda72877bd805ef7d3a73a0780948350c6"}, "cf3a1721-66b1-4f82-82e8-bc8c825e9b74": {"doc_hash": "305613a52c971a792a6e720557b26450e8440d83d034943f159345c2663e13f9", "ref_doc_id": "2eb6050b73d017c151d38f50f463322537859eed"}, "aa39c52a-cb73-4da5-ad92-a49aabb6680d": {"doc_hash": "f7574ef63bab4b8c4b1bd0b46df443d36d7aa892682232bf4d251bac3856d75d", "ref_doc_id": "407afaa3eb4b69e39c60127c639b9b372b391e29"}, "64805582-a164-4b41-97c4-5890e1d0c311": {"doc_hash": "fd8161dfd3b8527deb658f098962c4e0ccd3c339eb6407e16a9255794fb1ece1", "ref_doc_id": "663b4c0e043b116c3777b158246345ca89b77a05"}, "0d7fe055-404a-4bda-8464-53142d7d526d": {"doc_hash": "f5464039ec94f1235f38c812fe6a82b9a012a63a89fa7bc0318f383b8182603a", "ref_doc_id": "a97a5f79f3afc397269781615d15197ddca96952"}, "fd813165-dba1-4d50-97fd-1d3cba7f14ff": {"doc_hash": "b0eebd44a8165c0f4b1f6e72e7dccf150d75cffb39f6480bd1fa018122f77d4d", "ref_doc_id": "10acecc248c980944d096d2ec407e5455d6d8497"}, "dddc7395-e6e7-4e62-83a7-0f453160505c": {"doc_hash": "c85d8d0690bc4fab66b75172ca469c4c66b954d4453cc196b47e52aa2422ffd3", "ref_doc_id": "5e2855ae06a12108ec939943dff84ad094a50ae0"}, "484513f4-5da2-4769-b17b-986593a52707": {"doc_hash": "a52a2cff4fb3749ef82ecedcdf0fdd533d9c8018b197d9927598729012e00da2", "ref_doc_id": "f5594011622b0bd3c18889073299e9376e67cc24"}, "1ad7a89b-994a-4afa-9a6b-0a1d4ac0d5c1": {"doc_hash": "9c8e1263b4b21e5c45973c69a2d25587865e4739d17d71456cf30de6cc6cfb72", "ref_doc_id": "7e6c3bf9596f03184005614b05008628bc7f7ff8"}, "4e37b9d6-5600-4793-b58d-e0c2502dbab1": {"doc_hash": "e34db53b04f06ed149e73685495cf964dd1f8f4617632180d587b8e3583431ae", "ref_doc_id": "1f9732dc505c9a49a933d20dacf000be167648b4"}, "127a515b-4631-41de-8396-e623f308e22a": {"doc_hash": "9853ba4ba1a5affa6d49dea372d64f2545b955b96d3a16923b421c47855781b0", "ref_doc_id": "4a306ee8aeac99c9023623e5ed8bf7576e40c832"}, "b9c66e8c-4f8b-4736-9abe-61923a1bb331": {"doc_hash": "d9c4cc8e003f0022c23c9de614005b397c40b70fae24758f5e17d9ede71f435c", "ref_doc_id": "b300c65a0e069a78d9e6cce53f15e86651fc94f9"}, "d209f22f-2f1e-4a25-81e0-08b993f97d73": {"doc_hash": "e0eca844848f2a6b5f8417d48423f21f1423388ed77ca4dec7c9156cc35927a4", "ref_doc_id": "5aae897ec710accfb4ce1785058c8ce90e51f12c"}, "c899d932-d5f2-4053-a7fc-29389606d912": {"doc_hash": "d1f0fd16b9b81d72ec9425f79cc1f86c4f30f461fc02494c5c00606859709ccd", "ref_doc_id": "10279885bbd31a672544267253b384908ca09331"}, "b1acd3ec-7877-4d2b-bf2e-09aabb025820": {"doc_hash": "e6ae3a07837ae8855a7dd535c89b0db086e9460eb49ab944cb0aabaa1cfcf741", "ref_doc_id": "cc79b814af4864c553cfdf575c1bb6725bf7cfa8"}, "ac3ddaf0-bad6-4be7-ade5-0493d244ed54": {"doc_hash": "0a7fdef6b26bb0c53e90a872c1e95064ffd18f54ddf1ce59a22e90b89fa88907", "ref_doc_id": "e896c313b100dc5b23ffee9a80ff61430bf1008a"}, "8af98b70-df77-4035-959d-35aa28457212": {"doc_hash": "cdf2e0ed842146e723789f2daa29fde60cddcb533095e9d9b8af33e8dedc4a33", "ref_doc_id": "c5e26fd088717b776c077c9a26ae44752bd7a7b3"}, "30c97429-409b-4a99-a5cb-d3f911d02969": {"doc_hash": "8a8ea7051f224627886ae4174589b331318f8414364b48140a5cfadfcc3367f9", "ref_doc_id": "c214862b81646beae9750041b319d2e7d840da07"}, "ed2dc0cd-c08b-4b4a-8772-0fed8e74664e": {"doc_hash": "22d29713793d447f95398ae8785a3d5bf24761a8d00cc9f1b89687c928bc1b53", "ref_doc_id": "371e0170c4fe3af69f4976ab7d79b5a8ec035128"}, "35258239-163d-464c-8cd7-6ccd9afd3581": {"doc_hash": "a4ea5d1c055b9e961d4b4724ea0092474408c0034119c47767a861df8a982c28", "ref_doc_id": "76d26756aea7f5970b9df86db5cd9a2d3dd44df1"}, "28ed0b4e-6ea5-496c-b20a-67392f6490ad": {"doc_hash": "334fc181fd6377c59c9d566c1143771930de1d4af109adf8ae71443712ec01e7", "ref_doc_id": "0c55ba01c3e62081cfa69a01a20958c447e33f12"}, "a30e75bc-6821-4b97-b7a9-1fbead056471": {"doc_hash": "3c14e5c49c66390facd3ac1caae98d5abbd385abeb5c7993d06cdeae40ee3339", "ref_doc_id": "0d49e91b9eb4f8dda620bd048f1bdb80aaabeb95"}, "09231805-90de-4b0b-87d5-0bb27982db95": {"doc_hash": "47620b2f056f3c56c0b7c538195fec888cb266d5537dd70cfd19dc0b6c82ab41", "ref_doc_id": "670d8ac915cfdefef43c356afcba29adeb56d0a3"}, "ff1b4e8f-78ed-4787-886e-ae7fbf3514cf": {"doc_hash": "a0bfb358ee8f3151152ffc860a2227d641c05ccd091da26888b5439cb2dfe676", "ref_doc_id": "4996bd48dfe4e17bafd7fff506641920c39f0efc"}, "97c0c400-3127-4d0f-a40e-85fbfdf7e04a": {"doc_hash": "68477a59b260db8bfe9a3dee26448f8ffacdf87dc9085502df8ea7db06f314db", "ref_doc_id": "02494a2d5e625e26c55cdb7e32b9a61d63cfddc7"}, "99af2be7-3cf4-4ac1-9e97-77265242b4a6": {"doc_hash": "2173804c985a4f4eb92af93bcac3d2b491f883fecec67f20b299ce5c59c1f335", "ref_doc_id": "b7b8e3eee3a696326daeb646aa0c48368bfcdcff"}, "59ce7ca1-00be-4ee3-8b95-833d7512d844": {"doc_hash": "b45e42ae34dd7d3d8f1e4d149ab5c7a895422c161b7f0c68b3c69e147c775356", "ref_doc_id": "607d0cfcf290d6cc1be543431d31abdf16c3b351"}, "4231905d-0e05-43a0-9653-93172c089305": {"doc_hash": "31aefe72b3d45d9d6948214e141addc07844a6d1b25276ba39628360d44acbf9", "ref_doc_id": "c1feb56dd0624fd7712e5c3d9dfdbb04488bf861"}, "bb483ef3-3bda-47aa-8376-27878613187f": {"doc_hash": "3f19ee5aef803c7b8e2944ce29252fedb411856e3c7bfd85a355c6cb0909fcaa", "ref_doc_id": "49ca31e540c7071afb3356e611410e178463ff92"}, "194bdd24-6d15-4379-9d5c-b64bc4a27fae": {"doc_hash": "bda5f302f9bfc67c0a9112170e43fdaaf78b12f200fff8ee5d6f3f8b48b0873f", "ref_doc_id": "b68e2c3d86b8f398e85e23caaafedb1dd9002315"}, "9c5ac026-e90c-42e0-9ea6-bad8f2771f33": {"doc_hash": "a55ecd6d62fbdcde308555dc3af64e1e9cb6ab64b400171a63ba29bb0a8bb4f0", "ref_doc_id": "cfe822056fcba34b0701800668bb2eb19a7488ce"}, "1b125ebb-320b-4ac0-8d40-d45fe3832c0a": {"doc_hash": "803fa70d653ad41a81da4667f310997ce51454e6b678ca586e88f5ba3b8bdbfb", "ref_doc_id": "12906a65f9a48692c714a887bb32d057e7e05259"}, "0caaa87c-90f0-4303-8755-e9e235464353": {"doc_hash": "2847c387cc249d20bfaa0ef73a5c2b7e990c8f01ed9e7b429f1387e44a8c4ee2", "ref_doc_id": "f6e0160d9595777106da177d7f67f7320713653a"}, "a09193c4-d601-46c7-9629-98fb2f3da823": {"doc_hash": "4000ab2dbc7efe0c592523be5ee274c15e04831c0a0782df033bea357e906ef8", "ref_doc_id": "a19d228e43df1d13ae4600bc942cd8d65dc807f7"}, "b6059110-c9aa-4dee-a4f7-030268cc61eb": {"doc_hash": "436f310c96253b7e2825473323f34275827b584f7515d05603229d3dcdce2d10", "ref_doc_id": "8f01f3f46df5440243807f85939d147b0a030d6f"}, "558faf48-0604-48cf-a0b9-4fb2058e12d7": {"doc_hash": "56141245d97b89172177881a324a8e60bcae114bf51d9b11a7d7e44da1b69b89", "ref_doc_id": "0957d156917f8cbba31d6ad54ca9442b165ad55b"}, "768d520a-6033-487e-b820-878c6fc0e2c8": {"doc_hash": "041b97ef7057edde6ee8c18e00343904916bc1e48ac68f27abcaf132cd4ab9f1", "ref_doc_id": "8be441642f4cb39603b91d41b37953e2d4d2f35f"}, "75856f2e-176a-4c1f-9b1c-adaa178b6a5b": {"doc_hash": "be2a5db99657b6029de6061b6a7abb40aced95e9b5c5561cdc4e8880b17af368", "ref_doc_id": "3c725b57f5fd2d75a3ea881c5684802e8f01b095"}, "33a7c7f5-ef53-40ba-8a15-a266b16190b9": {"doc_hash": "f676c07164971d4e0eb0015d2aea2a5df6fcc4dd3954218d366d9e62e668279b", "ref_doc_id": "6688e22be048a39850c496f2b862f5393ae9307b"}, "6c2039c8-7e87-4f44-900a-3943e23cae22": {"doc_hash": "db78818e0a108b8972d15c67b7d8d9cd6b4a298d31def8beaa43b10e1701713e", "ref_doc_id": "0627cf498dca4bf5ffa730dcf7794114a796079a"}, "806004c5-6aff-418e-8ffa-c5ab86443309": {"doc_hash": "49974878421a7e79f7b1d67402999bba6c2dc33d4592178376031b3e320fa172", "ref_doc_id": "787d85fe604d459eb8d701822189317931a7351c"}, "21a064b2-5451-4da2-9b64-68fca1e19d86": {"doc_hash": "ff20c615f070ce33a582fed5a515cff7ea54331f9f180409bc187af6136698d0", "ref_doc_id": "d60e2e025dd8b3e2934fec4dc9454493a40de692"}, "36f05abc-4090-4610-80c9-78853b6fb116": {"doc_hash": "446f5c9f13ec4e45967f0e3011b9e045a756554bd2c52e9f6674c8f7e5d3d1ab", "ref_doc_id": "248d183b455a8698b2f33e7a761aa4d265c598d6"}, "375d233a-3249-4a1c-a0d4-2ed8a455328d": {"doc_hash": "f2a2461ae44d455cf108377588cae50b75b766089a2906565c7995c476ad3339", "ref_doc_id": "b5882e3c415a9bd436f9938faf7dbfa219a87165"}, "831d6c7d-2cd5-4688-b99d-bcfeba52d8a7": {"doc_hash": "a68432e0e9ee53be7cffdd085172b9061cd42a7880050dad4f07e4ab5ef82c50", "ref_doc_id": "63efb122574d65025688e1a907992915dca4b7a3"}, "512aa655-a0c1-466a-a6ac-17a4bcfcd7e2": {"doc_hash": "4fbc2c33d3c70a735d6554d272343f5047f7275e5b25c5b3bb4bfd656b161fe7", "ref_doc_id": "de08126f1174e7be0e25e1aa7f925b6c667a9450"}, "9c88ce24-694a-4d42-a170-87dfa7066b67": {"doc_hash": "f89f05fec34fdd109dab5b9bf3de7089c1ad4788ce5b55c7e460f334cb765936", "ref_doc_id": "c4b4e1574388385e3ac26e03a09e6cf7f83c2d8d"}, "b84ac2d9-3aaf-45d7-81f2-195683198b2a": {"doc_hash": "93bcecaafb8c92ce95ffa9deb71223687d41a04554f9798cd18620e82acd62f1", "ref_doc_id": "fb4785a2b2c175d5a08789e35f0f15bb271b385d"}, "94bfb0ec-ead7-4a53-9a35-9d17eb294488": {"doc_hash": "998a561a8c6f7a1cb47f3bb3b118086ac2fa2b9e7a810fd929ca49ec42762ef5", "ref_doc_id": "b841797df11b7f8afeeaa248503cceb137258f43"}, "3e273b43-8d3f-46f7-9aff-0a09a2b654b4": {"doc_hash": "f4dc611c99b2c70baab1b97a2f26f932a5edc73ed51cf33d4e0cef5bbaeba288", "ref_doc_id": "7ddcca0436cd2835a84cbbdf8f60afa019d12d39"}, "8c72cbea-dbee-4208-b66a-5f075a7cfb8e": {"doc_hash": "062eafa354644e224d806d5bc960a671968308c1effb45a4ecd3e83fabe28ee0", "ref_doc_id": "5a90fc658257dd07a5d033e0b8aa6d67c59f534b"}, "793c8ed3-14e3-4585-b7b7-15003595deea": {"doc_hash": "ee383a90859fa402ab0fb499d86e318f7e4933f896684d4d5b57d65d125845e2", "ref_doc_id": "be0c036c100e39b8be69830850c0650299627b2e"}, "010f397c-af09-474e-a8f8-04df085eaae6": {"doc_hash": "6fbc4bfd82ecceaa7ec1dc1cc279594da1922428a26dc28650a32cdb226a77c1", "ref_doc_id": "9a6bd621bfd10e80c467c4f51742663d4db4f1b4"}, "c9faec05-e494-4e0e-b16d-d8e1fc973848": {"doc_hash": "877ec709f3db5c3ca2d21c141d47f1b7f421a26d66615ab8f847711a13a5e59e", "ref_doc_id": "40d22577b82504e8a7a79f2243f186ae3e80b8f3"}, "c277181a-1787-48de-b9fd-74b5850d48e4": {"doc_hash": "db35c485339f0cf7d34c740606f122afc320ed57dec96497ef568e7bc4ee6670", "ref_doc_id": "58c4d9506b715838d23c4080080e0287d0cbd4f2"}, "b69d7541-17a3-4d7f-82bf-aa048bc36222": {"doc_hash": "df7f758b0fbfef160f6bf473a1d98b134338e33307114a2cc951c0a53b41fd89", "ref_doc_id": "9d26b914dab720218475d97931dbbd716a46f6b4"}, "5aa74481-7ebd-457b-95e8-018977e5f191": {"doc_hash": "962eb73f56687dddfdbf1aaf2a45257ddba47ddc9a24e414ae3eccb5f10b9f7e", "ref_doc_id": "a2013198a78917e0ec6b303d6641d7cff6d026f5"}, "80d7d37b-d124-473a-a790-b215b9ba30fb": {"doc_hash": "77cfca12cd009226778b49e493918af82e6173c498edc2957c473116132a15fd", "ref_doc_id": "687787325c1484d6202c85754c9b28261432d8b5"}, "4e637ebb-628a-49d9-8edf-5dbfc26215d2": {"doc_hash": "774563ee270af56780efa0cd164c8ba9d0664f4ce5a9daf69543a977c6faf45c", "ref_doc_id": "ca0f6e260c988c093738f5ca5c5e50d2211f5301"}, "226f32a7-2db5-4f0e-9ac4-f94ceb3435b4": {"doc_hash": "67a8cdd75655fa97aa7f55c62d29be79651e67e87b25e344f540d2543212353d", "ref_doc_id": "8333c9678e05f23879715e9ee3ef0b5eda62ce17"}, "14b092f1-14c6-4a2a-8518-b3014cb01c7d": {"doc_hash": "2d5c98edb83e194b7ff29ed5591880d42eecaebe17dffed1c7ce6e93bb65d279", "ref_doc_id": "b9652e5f424891c692db6c172ca4e6d7dda6d697"}, "11eea536-1df7-4f48-9767-b3f5b44c7c4f": {"doc_hash": "c09babca582c116d2d7f6d0f7b776234210073e0c037e1c34ee83382cf8231e3", "ref_doc_id": "cc2a34a3601b85df46b81895ae1dad043b8e116a"}, "319b29c8-77b4-4dd5-9b32-78403002b385": {"doc_hash": "0586f83749dcae877d2c45b2d10bc51bd658cfee1011b707ef063bfa348bbbea", "ref_doc_id": "18c6d661141a680cb3f296e36c68176d31347dd2"}, "7096f605-b37d-4d46-ba65-8ff293460899": {"doc_hash": "c3b9cdba0a510118dea43eb0e827d3f00c793ad9e38a1ec3e4dd397bc9a57941", "ref_doc_id": "c0eb48bb8ddbb48be83773877275d9c7412f52b4"}, "86043378-1a60-4cdc-845c-97d47a44645a": {"doc_hash": "1c25912996cc434c5c9c9859cfce4d27599c03f46f972df00fc64e756574f3bd", "ref_doc_id": "a1acd7a0db42fdd0b1fb85e19766a17f8c884cf5"}, "bfa72da0-5c5e-4351-a7b1-e206464c5413": {"doc_hash": "f49bfa8aa9318b1766c9b796cfb94fae3d26915bdd6470027c3b06aedd0f0736", "ref_doc_id": "e3208fc943b3ea51088f4d3e785abec789f43b84"}, "37b28f43-6dbc-470f-9406-c2f1345a3210": {"doc_hash": "b6ec5f1422a276324013a816868f0a0dc7194a9ce4d0b312f95047d8eee08ba0", "ref_doc_id": "439d94ba03ab39cbb75c59ffa30edecf4cb193f8"}, "75ca0cb7-2297-4fc4-ae29-25180bef664e": {"doc_hash": "11d090ab59d6409887ab51383c7c437b634abfdf329660b0436d16fa9fd54228", "ref_doc_id": "1db993e7176bbdf18529febf41278e148c0972a5"}, "a9f4c44f-a239-416a-b664-c474b4011708": {"doc_hash": "c33710f4073d0a926baeb3ec649474434e74099ef404e6c3b460ce381fe26e37", "ref_doc_id": "d340a58d8ba5237037a3ee74e87a2f8ca8f7a00f"}, "67cbe419-72b2-4a98-99c0-64200f904c13": {"doc_hash": "8da3c68b5e9d2bf58b5ffd4e431751d348c4125da6d44129a1c52e00cf456b9f", "ref_doc_id": "76a756c480190a3eaef63bbf41e265fc24e35103"}, "8282ce5d-9667-4409-832a-46d88a9b6388": {"doc_hash": "624c91f4ac01cf431797fe1f23a47a6a7e7b8268579597f952c2862b1fc12b7f", "ref_doc_id": "808aeb5f89693308bec1e838703ae74cad8e261c"}, "24544fd9-7b81-4b7c-ad90-2c2fea80a0a7": {"doc_hash": "6f04f395e060456937c3da93521f23f6e528163a2d98a34100d79ef68476edbb", "ref_doc_id": "95060b04c5679701bcfd61ab2207cd94877949af"}, "1120ddac-d410-4378-8d16-f6bf3906a0ca": {"doc_hash": "544a39588fbbea9f0483a1419b003cd8ba7076aa639d01e6521867aa8ef10f15", "ref_doc_id": "86516421e1454770de5799541478fcf17790116d"}, "7a595a12-802e-4576-8cdd-926152f2d439": {"doc_hash": "500ac516138a48a4d6c6159aef44aa922875c597f301af6e78444d6133cefd4e", "ref_doc_id": "07170183cfb921c22440563582c084d7d5e6dd77"}, "070f3a8a-072e-4dc9-aa0a-bc2a42292380": {"doc_hash": "6c110b1291e9af2afd764aba3a9d9b28cc591088a5e3f17745fbb17f69327813", "ref_doc_id": "28806b8812ef1b644dc04237edf9d138b5d4ba20"}, "55ff537d-0ee7-49ed-a9f2-dfae42ea4593": {"doc_hash": "760c2a60a3566addca180fa9fc081bfa15a7e94fbdc63ef407138ac52bf59f9a", "ref_doc_id": "c2b05684b9db920793a0dc780850eb5d00e8e6f4"}, "0bdf83bf-eb4c-4bf3-8b18-74701839ba23": {"doc_hash": "3da5944ec739876512ed0f161177d9145f41619ad7d09a6e9b6db72acf475671", "ref_doc_id": "0ad4a194f26680b8c109f5f9ff8c4375d3f561ba"}, "77dabd47-c47f-4b37-a88c-4535d34a9953": {"doc_hash": "30976edd7f7ae3c9ed3770259f3e078f15345aba1486fa8db4c04cd32c28397e", "ref_doc_id": "4027451bcf03e4e37ec1cb57328d4268f575ff71"}, "caf74100-f71f-40cd-98e6-5bcbac6525f1": {"doc_hash": "fd3eeb257791d299c6a2b1c4dc26d3fc44708a788ff86d4ebf88982e902dc653", "ref_doc_id": "803443512a8d0db713ab54ce30daf529c1a5065e"}, "7a1a42ff-56dc-4a6d-b92b-dfb7791d824a": {"doc_hash": "a1b8a753b633151d913fcf23333dad495f2bcbf6be6bc29274f6c9ec905cb7b7", "ref_doc_id": "8a6e63b8cf7dfeaeb6915069e417ff32562e9d00"}, "fe93cfd2-fe63-4b92-b996-d1f4acbc5fde": {"doc_hash": "bba0877c36c3998e972d810f608be28ccd95caeb9b7adcb0a133b89b5f5f6d4f", "ref_doc_id": "1ef6af0468ba5c83ad8616fd047305b4213227c2"}, "e14b8ee2-b91b-4858-8953-1c0041f4cdc8": {"doc_hash": "af6b3700d9f432faf2a9b8ac6da9931bd12db20a6680e8dc703db8729ecb83fd", "ref_doc_id": "506b92e5e454cd2d09b924f620dc9b22f89a9e3e"}, "b9566af8-6bbf-446f-8267-546fa702b12d": {"doc_hash": "d54f9aadd6074c42ba460a9c884a38ffbca34354a7c2519a2718a5e0e2dd1c11", "ref_doc_id": "f5f1f3cfd2e443207c801c5131ffafbf775309ba"}, "8654ce25-2581-4174-8f49-e998e0de68c0": {"doc_hash": "57673d0233260aec9c0d7807456951db56c633305495bd1317bd7e22ace6d081", "ref_doc_id": "7df120054cd53f159a9196a9132aa7b1c5c66eeb"}, "631ad8ee-72f7-4dbe-b01c-4f3bf4b50cc6": {"doc_hash": "28435da6eea92e16d44a4a0772948c2d236d165c9dd8f9e30c4745360b2038d4", "ref_doc_id": "2f87712504009f42009fdbc7a2363b6e5419dc7e"}, "f1bf0c83-d82d-48d7-acbc-ecc74fc23fc9": {"doc_hash": "70e8c5486a6977e58189cff12cf12e1a3fd930721daadf6b39076db79dfbbaf9", "ref_doc_id": "3788d660894b98e477bb25c8f4e9ce36d4adab91"}, "8841b879-49b3-41d5-92ac-cd7735ddee83": {"doc_hash": "d7160f1993fe69879c8d46926bf64acdf69b9bf80de72da00ec3cabcd9ba4887", "ref_doc_id": "3172e0bc5c79c2938ae36e2dc7a3858f7c08d23c"}, "232f6b0b-662e-4255-a6e3-c92253dc6097": {"doc_hash": "3a6f6ce396f83425063d8da45db9b5c9bf5fef5dc432cb662b8a6da1c38f436f", "ref_doc_id": "a2bddfc7e89764b1d4ee5676c9283ba0fe28afc2"}, "c663bfaf-9c97-46b2-bde1-7ce5c16d04be": {"doc_hash": "21041e01c147c94878fd241d68f4e81af1cd3b840cfd62b78bcfd311f43fa352", "ref_doc_id": "61e97f04829c81c62ff5872a3b457bac48185766"}, "9ed42e14-59b8-446b-ab8e-be1da4641135": {"doc_hash": "44e8c0b1cfb38a607e3608495b63315d4b341bb911bbd26a42e574e783327b57", "ref_doc_id": "4648d8f9b42fdc6d4e673a96f2315a0ccb274592"}, "2f268372-ead7-410d-8302-be5ff5e8977d": {"doc_hash": "750332118264a0aa1202bd5d670d49d9781bc0955038d11ef09a0cdc8cbbf18c", "ref_doc_id": "7d68ffa95fc716fe5f74d71a3e9a8175f5dc7927"}, "1996aa83-1ac4-405f-9342-07ad989996ba": {"doc_hash": "f90fa5fff0aca562d7b1267a1f6ca7f08c44e6f32c490d82ac0174409ae1ec2e", "ref_doc_id": "7b0eca2344dbf2f17da0cab04601db9f32d8882b"}, "4a9e948d-6200-4ea2-aec9-9230e46e45f2": {"doc_hash": "daf72446e0ba65f1be6d05cc578ce8236fdf1a618fc2673e36bd882572471881", "ref_doc_id": "cd90659e92e62339e45e814e4dfe6aa8cc1eb171"}, "13ad0d37-77d5-49ca-9d4f-0aaf6ae3a615": {"doc_hash": "aa309b7f1f581ef074cc25996cfe738a6ae9875a85bccac728572b9273e23976", "ref_doc_id": "605595d2f0969942898003c5b8e44000a29540bb"}, "11e787e0-62e8-4511-8557-ff5e2b569398": {"doc_hash": "ddc8b9aded9f8f63542ccbc9bd857c0442958118ed4e7d1c6dd2175b6da2897a", "ref_doc_id": "554d048e98a3ca0268668636b9c59006464b6d81"}, "473fe75f-af2e-414e-9dab-b645bc359f13": {"doc_hash": "7e26ff1b797325bfbcc9c741f7cca345c4ab4d0605970f36f84b403a5ace1a46", "ref_doc_id": "26b57e6987b8353385d4a5b17747660ee8a108f0"}, "d0121c8c-949c-4e63-804d-62f8bdb99bd4": {"doc_hash": "3917f0dfa8998e82f54456758aeb40ad6bad14ecfa64bf37b1ec51b44a2fcb7a", "ref_doc_id": "1e4b44ec6341dd51d58750aeafc652125870f289"}, "53d457a5-5104-4276-ada5-07ff7220f2e1": {"doc_hash": "890d0ca5960a264221e8939794279284445dd88a2490fabefd806d27950dec25", "ref_doc_id": "b79974b6688f3972cf1cf3729a6442f735961484"}, "1728e364-cbcd-42f6-bb07-da14d606cf00": {"doc_hash": "cf5d2076caf7fb03a70287c3a9f202f324ae70462abd8def728c2368581cef3f", "ref_doc_id": "eab5be5bb35894a3bbf43ea70fa2fb127b5b06fc"}, "af8f08bb-5153-4964-aed7-82cb33682a4e": {"doc_hash": "11a5cda28e7a14f5fdb77a1d6110c7778e72e07a3f3fa4138f104ddbf49d5762", "ref_doc_id": "f29d4590fb52378bce5e54c338eec94fd7ab1d32"}, "153f1623-ea76-4890-a5a3-e108b9777ba2": {"doc_hash": "7bf2c5b15b671b4a007b538f66deba32cb00d0c18a52a976f7bef4a2531b852a", "ref_doc_id": "46568006c34d31a56e87db66b658249b4c228b83"}, "64c354d2-0bfe-4592-af71-1cc7ca1eb177": {"doc_hash": "5e657f9189133b20788916c71cda41feaae6df80abda34583c6e530fc19261ab", "ref_doc_id": "92e937765c2976bba8a932c68b3888f7c90757fa"}, "4354adc7-ba37-4b59-b14c-339e7c04d34f": {"doc_hash": "a7a6bbf6e5c90eda89ac8ca9f44be1b18d76feae6bd082f6d626a116433c6e1d", "ref_doc_id": "1b097cf514f5e3880a331c1e2cc04a12965e68b0"}, "26019d6e-4307-4954-9029-1a6e5c9f31ea": {"doc_hash": "e7e9a1703d2f1f78668d83c63a90ea930f77d08ec1e241b0bd061e0194dfb512", "ref_doc_id": "365a4700e223b622add53e6f2027ff39675894e7"}, "f767aae3-538e-47d4-ab53-ad493925e9b5": {"doc_hash": "47e36b8988bf7ce2eddb90564e4273c75f0b66daf81636e0ec834e6bd6734485", "ref_doc_id": "aa4dbadcd55c85e01c59e5436fc6cdf8fe47f068"}, "deb48aef-b743-455f-b406-f1b108dc9cdb": {"doc_hash": "7a80b48aee358fde5a3ba8600c4a71e709b1305c5b50653af64ed4e49d8b421b", "ref_doc_id": "7e320c4588c63c842c5aa47a3102599daf7826f1"}, "dce786da-1673-42f9-9588-d1b4b101e2b4": {"doc_hash": "aa5205b05a4a7136d35e6d783f920ac785a0f3162578c45a13361a5805ae3da0", "ref_doc_id": "dacd27281e38a51183d1afc83ab5888030f354b0"}, "6072ae69-8089-42d0-93e2-51ceb9919424": {"doc_hash": "5508c9ae153d5f067c535be1494ed161c1c9a69e32a53241b12e985ffa47cbc1", "ref_doc_id": "e4931c57e26726934cc7ae7e8824d8a1dd2b4c26"}, "96753138-f47b-4849-8860-ca567be9017c": {"doc_hash": "dd5265a4862b37a42b578e9e934198b8133dfe3c5da82952b43d27a3692c49b5", "ref_doc_id": "71b6f40101c4f42bbf4109f7f67e247463d452d0"}, "be740b43-06f2-43b2-8cc7-bdc7faeb3ef5": {"doc_hash": "2cc958cae32105280c4af9a4f27b91642bb01f9c9dc720d7d6ba02fba187ce8b", "ref_doc_id": "8f091bb33b841ae45b35a0760e114bf1a89cd1e6"}, "61896bf3-20a9-4827-8a55-eed0ac4804ec": {"doc_hash": "51e4537b89ff3c6464ffed45ffe503b17350b96e495598ec64b8d5df0ca34e03", "ref_doc_id": "d63e36d36fdec2f24b4e612b49e0acd3124bae93"}, "9d562cd8-accc-4bd3-a49d-d4df85dff903": {"doc_hash": "e0ac94c556c9ccd32e1a6f85a69b678624a360760b3268b93379c6d904a491f3", "ref_doc_id": "bc03ae558d691e71ded8ce6be00440f0e0cc82ea"}, "ebb4cc0e-a293-448d-9ca7-dd78f36c11ec": {"doc_hash": "0821b42d88c0c04e76adf650b358c52590eec3c2ab5207581105a039035e96cc", "ref_doc_id": "2fd22331dfbb5aedcb66650611a212328d9f30b8"}, "5d73a4bb-6564-4640-9f72-e1b93cb697ce": {"doc_hash": "31d6d204d66f261ee8cadbcb6351d829d1755520d85d4a0a6c7beb1b636de96e", "ref_doc_id": "1f9be5197348b83a091314dfdbe2b0ea17783fd1"}, "2c6d08ce-85e3-4a32-a582-b77d98fee3e1": {"doc_hash": "ca4bcea29479546bee44d1f742fb14fd8f114ee1c29363a5e1243a1ea922c93d", "ref_doc_id": "fd572c3108df403ede670e66a9fe1dfe77105ccb"}, "6cfbd38c-09b7-48ee-9998-3c748045b144": {"doc_hash": "11796eda421e24b74282ffdfacb8d865c86bc0f1d70587faac26da55c689bd3a", "ref_doc_id": "542bf85fb706637da54ddea9e8f5a25f7cf8d73a"}, "5345a1ef-26be-4e9e-9732-f5df28501c18": {"doc_hash": "b38191bdf0f6de0d900517c22af8fc165b002a7eb2cb4ff3e2f3f2e705bf5952", "ref_doc_id": "1727c5dcf1859ade43c55464441430347ec33a10"}, "23188879-d580-4d78-ac38-662f1fef9267": {"doc_hash": "ab4af5957260b6fb8cdccfc29e490095228886b63aaa98bc5dc7495171713531", "ref_doc_id": "ba6220c4d1135e18526b108ac742322b1014217d"}, "2956c786-241c-48b9-9db1-3043e1b5943c": {"doc_hash": "a7777fbc8f775bf6d7af6cadfc36bdce847fdd5253cd57e7c9c074f3cd9feb55", "ref_doc_id": "be9a7e4fefe33933328aef0d5cf5dfcf2b4dffda"}, "0f9265d3-13f6-4cad-90f4-ee5c76789d09": {"doc_hash": "fd25253e47d0d7dba5549b92f9f3c4f12849c0295a3ad74979e69fd5541b652c", "ref_doc_id": "ffcdba15338dfc47c9283667b90c8f66a5c9b8a9"}, "76c91302-7080-434b-9d4d-d62a08560487": {"doc_hash": "7080dadd3ba34416faceb9cd3710f1ec78ecb3b2c0e1dc5f5554462d74f64af9", "ref_doc_id": "2b4a3fc749f4d048c2ea6204c892209ebea3eb98"}, "922b9b4d-8561-4fda-acb3-861af6969d67": {"doc_hash": "ffe5857d49433c6f81d75f3993c43e32956d419639ff664ec84f4c21478c1a60", "ref_doc_id": "8e2f71de0f7d7a9a07ea5e648d7452e048734181"}, "3390ca43-b087-48cf-a2b3-cbb2e37ae46b": {"doc_hash": "6773fb192cc41e4f8860b36bf180775834cb3fb6f0d32fd5ed99810f62fc8e3a", "ref_doc_id": "3fb4214c7ed2b451df9586d4583e5165db32258c"}, "15c7affd-7942-47d9-9295-45a239eaaeb1": {"doc_hash": "b6338b4445243ef81ffb6b6b878d87064e2a7e4eb35c04e2d978a7cfa32cc11d", "ref_doc_id": "c1a080368716b8002a0030cddc06c896fb148aae"}, "cf142090-7adf-4ccf-8a13-c18990c4cc04": {"doc_hash": "b39e6322b7a1492ecc78fddfa38fe1b5db5d2d04e3297620a4d31e7dc25d29e0", "ref_doc_id": "9293f065b3de98115a89c139d3cd943c59689e71"}, "ace5b223-3c3f-492d-b05a-f8f1dee1d297": {"doc_hash": "1959f1f04b6b2f8fb7bfc5d3abfcb2c4c665d47c15eac18d324672c473c3ab33", "ref_doc_id": "db9f58b998e05a0ae5b4bbd1f1e71943bd1302be"}, "008aff95-6183-4df3-9e87-e2ce758d8627": {"doc_hash": "2b055a245bced1ccc5866c02a153421767fa42906ae3e4baea2000ba5f974e5e", "ref_doc_id": "64466a160145bf1459ca20e9229319793f4a4b4d"}, "8bb63174-ff12-4157-85d2-3ca28772639f": {"doc_hash": "d334526eefa56fc07c3855c40d879cd4da045bd63410de685c04972b435164b7", "ref_doc_id": "d3f3fea719d34c164af774dac1057541e9664843"}, "1996a62f-99ac-406a-b16e-15efcae0611f": {"doc_hash": "22e78c30c3fa7f41f924f47d74a86b1162809cfe7952338136a810cabe1fd6bb", "ref_doc_id": "355baa269b5f919f13276d6e1e850850e8a6acb8"}, "e8577d57-b362-4376-a01f-6c553296fb56": {"doc_hash": "c5804237e137d31158680058d83d3bfac6d8b8d89215af88240e4d995c13768b", "ref_doc_id": "e3fa46ba542a11f7e57d6be7b26f163b5dca796c"}, "f7b597a8-a824-42f4-98e7-f2a7fc1e0658": {"doc_hash": "0f50881048a7d2cda5659f81578f28b0ee2f72ee4071e377a8e033f55e573b39", "ref_doc_id": "709c7ef9e4dd22946d2fb1d95828bbb1bb963e64"}, "055e416e-1f0f-4559-afce-b30035ecd35b": {"doc_hash": "9d2d331822dc0da1b865f2e2f069d17d5c4b7969addd98e8c33696faa762f9df", "ref_doc_id": "07504dcad16ce033453209dae761505727fcd499"}, "b28e076f-6e66-4d50-9882-5db330c1c922": {"doc_hash": "85b3bcac6744def37d1f2380f3c56e42e38e35387332b37c7c537ebb55aca76d", "ref_doc_id": "5d48014c22505992c57c236c0a706f823adf94aa"}, "c2cbba05-82ba-4def-86a5-341404c0981a": {"doc_hash": "17c34fa34ade081fe9c70ecba359a7425bcf58aacb8388917d1721915b743a66", "ref_doc_id": "b4aa40d5e2d4d5142e21e83d58a1ca1d78255f8f"}, "0792ec0b-7959-441f-9dc4-736923e194ae": {"doc_hash": "f3b4ba2e255e66a8014ff448957c86ced70872e5363a77394177f03973f7e261", "ref_doc_id": "185d816feff18846072b1907c15972a1882847cf"}, "0b137793-b111-47e5-826c-c607a0bf5fad": {"doc_hash": "f547fd5df06bb9e3a88fc3e0427f06f52d0e437a182132f6d14065d4a6bd0df8", "ref_doc_id": "116e4227ef84b8c40d1b29ae0741163409e2e7bb"}, "ac89a504-3734-4ef4-8604-8a4a1448b954": {"doc_hash": "aba0aac51984f5ab77c76b65b52afedd568e38463bc767bf201d4cb6cb162536", "ref_doc_id": "6d854217c50954078a94f89a6e00e9314c689130"}, "b2f5b22f-8a2a-43ba-bc5d-7ee6f9d6e1c9": {"doc_hash": "de574596e8300cf62dfd22fcb66ab9e440fda7066973b4c4746667d7d52e1a1e", "ref_doc_id": "b7d3b6f27c74f7f395a2673bcf18dc12795608db"}, "8589cb46-714d-4cd6-9d9c-f2724f2318a7": {"doc_hash": "5ec166dd3a17ad4d06a6648af2346b3eb37a0cb06b7f198e79272322ccf10ee1", "ref_doc_id": "2034149b2f552493715eee753be92de74c900e6b"}, "aa58bebe-d887-4832-92a5-18e1ea195165": {"doc_hash": "7945e798c064c8ef6274645ee0f5b955fa13c757ec3f4653c5ea1df25d8dba6d", "ref_doc_id": "4317cb97eb1283f0b9a9f29b93bcf92af4e5036a"}, "4e372d18-974f-43f1-aa9f-a14938f381b5": {"doc_hash": "59bddf8ce551d6f3604a70507a73236d91a614ae561556a187358bcde59744f6", "ref_doc_id": "cc284055768ce117814504803eed3b6f274f792d"}, "2c8303b5-7b84-4353-9493-b6a1a4f05525": {"doc_hash": "fed9229d7c457d1353244508577a89b8bf8ff9ef05b9aa6b10c856cd9534ce9a", "ref_doc_id": "a8e9cfa2d224b5ff0c47542732693b077077076c"}, "6ad931c6-3a7c-4b35-ba4d-b0be8ae7d975": {"doc_hash": "8b4627df9cc93c8616fa94d7d70162343c75de46c47ee9cfdb7267af2601c919", "ref_doc_id": "46f7195d9d60fd3c04da10c193bd535e3a765603"}, "d177ff11-7f57-47e4-90a8-8fa778bc6374": {"doc_hash": "11178e5631a674287603ca3dab6b6f47dc9d2804780aaff1bcc69c4151db87bb", "ref_doc_id": "6ef82f7852fe9bde8dc29e657aeba9183e0206c5"}, "c438d13d-beef-4565-bce9-0d99dfbfa37d": {"doc_hash": "81cb789eed68179368b01ccb318e2ef5f635c6598c0d9b438137c3f28c1275b7", "ref_doc_id": "9fd2b81cf0ad3f5097092f074b46c83b480c51ff"}, "938632da-d05f-4c8e-82e0-36918dbe9996": {"doc_hash": "fd34716588b5cd544f35ee150b3477e0fc701e9dadcd96f844e4c6779a408dbd", "ref_doc_id": "abdccd6c70178d88056d5c3f95f5e682c01552d2"}, "0fc305b6-71f9-476a-8f45-aca460aed43f": {"doc_hash": "5024db1da240750c6fde5056265d537a123baedd778a43eb0102e2d6154dec6e", "ref_doc_id": "71ab9b855e9fe249152b08e3a31bbfa2d94f2ef4"}, "debaa7e3-422d-4d07-9edd-755e731bfdea": {"doc_hash": "9e13884ba651af2b0dedfedc303e989fca94f0a2b6c3e25203ad9bbe5d98e893", "ref_doc_id": "333555e6837acfc985ee503aff3738b02e235a83"}, "c683e8f5-8c15-4e71-8511-410674b7f919": {"doc_hash": "788c0f59a9143b4f08cfcf8384267e52dd988c023f11263946bd0bef926b7278", "ref_doc_id": "0e7bd1f42fb9f323c6aaf5b7b0cb850e085edd39"}, "fd30374a-e820-4eb9-9167-fd480e30924d": {"doc_hash": "d0656f7cdd66d366f8f34fd6f8922fcc9fb191d5d854daee6c1a48e6937ce322", "ref_doc_id": "322464bc0ed19531e31cc13022d897b39ac35590"}, "55b573e0-3f17-4b46-834e-cd7edf3604cb": {"doc_hash": "30f88d935c3372dcac60d16d3146bf7ed6f656221f1c04c7022750a9d7890560", "ref_doc_id": "9f4491da62a9777a1a33c7ce82d443ab3e6647ff"}, "09a5b943-1426-4988-89c5-7c2e4090c038": {"doc_hash": "7492187a5c5fb49b725fd88295c950e11631a0a80aa49ecaf90d60aaea238873", "ref_doc_id": "e0d6b2562242f80ba727b940ac329a95deca78b2"}, "592f800c-b972-4cdf-9133-4af9227a713b": {"doc_hash": "968b106aa2ea1940a3965c71635c18d0a6c3a56ff1effae4db0a6d40b6eb39c6", "ref_doc_id": "78435b150a662a66178e45f86288bda056ec9c0f"}, "f5760e3b-f2f5-4bf4-a8c0-a5025df82f9d": {"doc_hash": "968b106aa2ea1940a3965c71635c18d0a6c3a56ff1effae4db0a6d40b6eb39c6", "ref_doc_id": "78435b150a662a66178e45f86288bda056ec9c0f"}, "c59e0268-97f2-4726-b376-5e65cf68c40b": {"doc_hash": "5efb4fbfda5eb9e838f4519d30aad76d458dc4a519846b4c17184bb7fef286ee", "ref_doc_id": "de6f92bf433429fd8b435bb1722fdb02989e28a9"}, "c9f6b55b-7a90-45b7-bf75-4583a2fa1124": {"doc_hash": "120ff9205e44efdf14da799ab750781eeab8910b986a3ad35bacadaae256b628", "ref_doc_id": "3de3f185942a3ac31c1cfe13013090ad661047dc"}, "a105becf-315d-4b1f-81d8-a38ff6255bc6": {"doc_hash": "f38ed674ab83458227c990b61c5187b32f9f493d92f6c64ad82688f174e9f4e1", "ref_doc_id": "cdabc731c10cbc98d779251698cb8708675bac6a"}, "5758ba7e-24e0-4aea-bf1a-cbef907491b9": {"doc_hash": "905be0c1c9eca247969279a31e31686ea04c2e866b50543f48e2167b0411132e", "ref_doc_id": "db09301091bb16ac12c1b067d7d40b6bad9cdd58"}, "f7657528-a354-45c4-ac97-61baa19f3158": {"doc_hash": "8e2454d7a3c950d255b8fb3510b2b9bf208da153bbb7a5328017a8d1c9c236af", "ref_doc_id": "523be4b94a0d269e9a6cd3d6da72901750e0f104"}, "3940a82e-afd8-4901-ac8a-1e3945ac8c80": {"doc_hash": "250d5eb8903695603cff123b653fd44a0a3670795009869f879fb465b61a192d", "ref_doc_id": "f325adc1c1e0eda52891815d6ec3b9a952e76e97"}, "34878d76-7c56-4264-8b52-7b4fc1a7598f": {"doc_hash": "be71eb1bf5abc885830992c1c989aee4b46d1980d72796b206b744edeb474e98", "ref_doc_id": "477869049c2a71e82df47e8a89bf134a5beac7ac"}, "9e65927b-ef8a-4f66-8867-49e0a51c20e7": {"doc_hash": "f8671530e1bccc841de4f1c0fbba458b9cc7ddaeb7cc7012ae26fb60fc482eb0", "ref_doc_id": "d6ea9f728754d2956e82560cbd9108bf82389c30"}, "afaa2348-da83-4c89-86cb-bd8aad8781d5": {"doc_hash": "34cfc484fbdeb88d5458bde23b35f9881320f3a3eff092c81f33995da1182dec", "ref_doc_id": "79c5528e23c27c710fa527deb8f06b1f6523bb0c"}, "58f02a33-44f3-48e2-b746-b051cc4bda79": {"doc_hash": "6b39cc0d1dd9d8543a15d7b876279bf5fcf8192b2ce7a5b7012133bcb4e4e0af", "ref_doc_id": "3cba2508ffe2fa6002ad3ce3cdddfb09d82d57dc"}, "aa0302f7-f023-4246-8726-817e374aa6b3": {"doc_hash": "d347bbd88bab69fbe193a7f2f36278f4f1b18ffbe58f4e8c6eba3055d7339024", "ref_doc_id": "ab67ca65cd70b806625d9e2ca7b36affaae86440"}, "445ce08e-85dc-4214-8d57-704488610acf": {"doc_hash": "57822f05b4d99a3572323585b4c9dd6b2ec8652dbdbd223a1509f5d7c3ee8b99", "ref_doc_id": "5236ea3806a6f74a0f39a8f8f5e6d81738863a1f"}, "ce779173-1b03-4fe6-a456-655532dc0897": {"doc_hash": "b002ebc8f520ed862bdf3c694c7002aed97ec48de2ef0c35d5bd16bf1f2a3d37", "ref_doc_id": "9289f3e605d97cf186122f19631d36af404a4716"}, "a8699425-f21a-4ad9-89f5-4fe4732afcf9": {"doc_hash": "cca71f5c02dc03e6bab23e4dfb0032262c8392c81feccecee8e7a9ed1216dad1", "ref_doc_id": "53c3b8875410015ffd75bc751ef3aab7db685e16"}, "d8977ccd-79c7-4522-b554-73bf1df21832": {"doc_hash": "0b08aeec915ee84c0e62c17cf2f1ffda60f44edafd8f401d3b2e8c605fd738f5", "ref_doc_id": "9a70b8dbca4493642a51af7ab7dfba62be404c6a"}, "1407522f-1913-4615-b6a5-a60c3339aef0": {"doc_hash": "ac1adee6c9cedfc3a267fa38b6b9640015e869d51c894a51d56eaf6c25cab9e1", "ref_doc_id": "7415d36f570f9b2733da56309398b61852dff424"}, "baffdb5b-10a8-433b-ab32-722caf695fc4": {"doc_hash": "b5c94c2d3090978cd08f7bfabcbb8a608179c24267b63991bdb4d6e3277ec67e", "ref_doc_id": "c0b157307b096ffca8d28c528806272969883a9c"}, "b3d7e1e3-edbf-4fee-be3b-5e771d7a799d": {"doc_hash": "6a04cf4dd7e85f76be3f60c584f4fc4ddb3073f99eef6ff6bb1183aaae21a776", "ref_doc_id": "61b233f691ee83fd88e7f7199de862d110f01985"}, "23b06cd4-b1bc-4eaa-a8d1-f699060821c6": {"doc_hash": "ec913739c44f663075be5baaf280cf2f88b742f531b6a8c2cc77a76bb063b055", "ref_doc_id": "8ebff447280ac4fe02791afad8900922d7d1c9a2"}, "d75ea97a-5bd0-4e3a-974c-d56342fc8162": {"doc_hash": "d24c8ab5fdf0a60381ed3af26f35dd2f67570a31037499005d9bde489519dca2", "ref_doc_id": "a53735a48d5c8a415373181e7e97b83e2982ffe6"}, "2471f739-df7b-4d34-bf72-57bee1305c11": {"doc_hash": "1364af8113db44939a87d0a97bbfecf4b1a98d62128ef4820e649c6d704ce33d", "ref_doc_id": "625b12aadf4db855f603ab991d12ed43dd7dff19"}, "dd501d63-f773-47f8-b9c0-b7f2518fb8d6": {"doc_hash": "fae1e1d58aeeb02084bd509ac26d12e2a78d906b9085ca1b14a22117552b34f8", "ref_doc_id": "5e7f93f9bb78878e74f9a5599bc9486ddd378819"}, "020aea8b-1da4-43d3-bbc7-3c934e1b8b3e": {"doc_hash": "d75d6b01c0456179df28f98b346b3cf997184d89b4d2371fc558ddcbaf8e8427", "ref_doc_id": "ff91e1a6eb875f799bb32c8fa58eaa54d04fe6e1"}, "945e1359-4010-4118-b2ab-850389660ab5": {"doc_hash": "bbe7d49a07969400046513857267620bc0eda98337bd199a2170e31e5c55ea0a", "ref_doc_id": "71b6ef509a2ab7c2175c22180fe1900b60f5a0f6"}, "292179f6-10e8-444c-9f94-39e89c3c0b22": {"doc_hash": "d30031872fdf2267a8229d5a416f0351cffe42734edc57957ea81547947d2205", "ref_doc_id": "c76904866ae5040ec1441258d3543405e3b54537"}, "51de17b2-66f0-46c5-80dd-e4d6320f87bc": {"doc_hash": "791d4ec2fa22e2b597a3b6be70b5f3fa95ad3cfc5b9e381d1e4c4e23e0eec9e7", "ref_doc_id": "5a72cb856d4806600a20aebfe2269318aaf606d2"}, "d43b1aa6-4a86-44ca-9ef8-a4aa1acade23": {"doc_hash": "0c6fe0cc500ecac37290d7a374e7dea61012258ab256e383bb4b76624a55f0b2", "ref_doc_id": "816ae0aacc09beb27107f1a46864e8bf70945e7b"}, "9e8bda47-2ea0-458f-9cd3-d0a05b9d569c": {"doc_hash": "416f60a3535a71bbdf95917ba93465ff6a040859346e12d3325f1f406b103975", "ref_doc_id": "816ae0aacc09beb27107f1a46864e8bf70945e7b"}, "ef08b63d-1ff5-4281-81dc-e05059f0da88": {"doc_hash": "c8e07d1b9661196fd8b3fb2ddab751d0af6ae3dfd28da7ae7efd9c18be1e83ce", "ref_doc_id": "9ee13d2045ec53fe785be09e8395f3e5a7aa31bd"}, "85fa35a9-6d8c-4a8b-b6b5-2101e643907a": {"doc_hash": "f92460a33c34e48337650602b888e8be9f6c1669961a10d06245d67757a728cb", "ref_doc_id": "59b15025d2314a7d91b792d39f42fec3ad8d9928"}, "ab2a1282-0a49-4fa2-ac77-4d914dfc5244": {"doc_hash": "2f8cbe8a5f3796894388078530a710f677e34aee8c95d9aa2b419909fb9e8ae2", "ref_doc_id": "59b15025d2314a7d91b792d39f42fec3ad8d9928"}, "52efb7ed-147f-4041-b745-4298216dc14a": {"doc_hash": "975d18d38d7a04b35a1ae9aeec454a3cd73ae1271b28acd24e729b2538dc4306", "ref_doc_id": "5404ce4620cc5b83879a69b31999055595dfcecb"}, "7c3f7c90-ae57-4a94-93e8-67ed3cde8a2f": {"doc_hash": "4a8d186595b636bc737b149d0af9fb653866d9a6170ae346a2d7a50e910dd8ed", "ref_doc_id": "51fd3ad759e55dda2ab917c0de14aa3a85241481"}, "7eec17f9-6748-4cde-bb55-0ef1096cb92c": {"doc_hash": "cc27fd81514c04db137f62c11e370365bd21a4521a22caf87aa311cd055e785f", "ref_doc_id": "10715a649a423561966a0e92d4b8b73dc8f759fb"}, "068fdf37-cffb-4ec8-aa88-804d885d395d": {"doc_hash": "b48d3f5e9ef055c74180c81d8d24a14c615c455e903b15acf7c5e0abb6435875", "ref_doc_id": "ff51ec2aaec9e9b17d6bd2eeccf865480c1ef8e6"}, "d1db8893-6947-4ad1-8efe-868d4db5a704": {"doc_hash": "0b27fc5f4c54529cf5d5c9172fc16573ec6200a8e90961384dbffa2263f42f09", "ref_doc_id": "7abc011a97485e5f645b91b8f5dc07b71b8fa8ef"}, "69f14098-8699-43d2-b987-20acbb011b0a": {"doc_hash": "f7ce77b77868db74752165f8b14940f78d9c743bfca06614e23986100781f5a3", "ref_doc_id": "7abc011a97485e5f645b91b8f5dc07b71b8fa8ef"}, "8b3ac817-4e6a-4d82-9174-143c408338df": {"doc_hash": "c81650951ec32a91bedf6b45370abdf1da517f659a3a82d0b47e7b7b8f6c59de", "ref_doc_id": "453fd7e59647e91d21ed790a04b0abb9438daf1f"}, "374a059b-5a51-4cd4-b599-4f0be4d84033": {"doc_hash": "14d18e1394f63ab37ea841e7da7fb0f3b65738ab3397e0d0008db0d4ca5fc60c", "ref_doc_id": "453fd7e59647e91d21ed790a04b0abb9438daf1f"}, "9d8e296b-b279-4ba6-8b45-6a6817bfbffa": {"doc_hash": "5c276fdd5a4e46c3d1285a1b2a46cc958e63c0bdf4f549c9893fb6f2b79af513", "ref_doc_id": "453fd7e59647e91d21ed790a04b0abb9438daf1f"}, "871f9e51-41b7-4c13-b5b0-58cd55c8bae0": {"doc_hash": "ffb9f6388fd4ae22c693d512efc07f0ab014c8c1b83a45a155579bf7eb735b82", "ref_doc_id": "6e0236302a716f81414e0841e20dbcf67e7a4510"}, "9ae40079-a15c-4620-944f-afdcc08cc6d5": {"doc_hash": "c7c832a6bb3fe37f5c9634ae2e0cf1e774d5ec150cc547b8e0a6437530926859", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "04c517d5-22f8-40e9-9628-0df073b446d8": {"doc_hash": "d2f8cefcb4c8e1d1ed871c812415931b0e51d1e021b3f78154e91aca1ef031ca", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "90a3483c-3681-4dee-adb8-14fd194c0a9a": {"doc_hash": "3328a53314504bec8f1006a78bb91e24482b36054fc58ddb864c2f981571b95e", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "a451b0e3-04d3-4f66-afc7-6680ea984f2d": {"doc_hash": "93a608f419ac7b48570eef62d47ab14263344b2e2bea2e63a3c45621860aa64f", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "12ce2c8c-66b0-41d1-88c6-e33210581848": {"doc_hash": "bd3f029c269908a7929840cf9ddf6f734f4c55f983efb1a494f73ae4c1f9cc55", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "1778b502-7e0a-4491-bbe0-28ea5af9b3cd": {"doc_hash": "dd11e3500f11a4c7d86a9fcf9d6e3b58de3e185499b4e3c89833501ade7d8212", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "6700c1d6-15f8-400e-8fd9-0fefa8f2ca28": {"doc_hash": "ca826804c6820eb8d533d45f0cffe90d7d85d29ef2117e18c10884fd6caf6434", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "bddee492-c1fe-4af1-891e-67b228788bf4": {"doc_hash": "2b163e805f9e62f681df9481c351d9c5fb83a86eb9a174bcf80efdf0bd29f6ab", "ref_doc_id": "8aa9273228ccd9b32167d9f4fc72091f96ad92d0"}, "762e4b18-2a15-4e06-bab1-26aaccd04d16": {"doc_hash": "db45c090ecb5c41a31d935234cdb10171a5217c56e090abea3af3ebc2faa805a", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "f4abe42c-d54d-4d45-9c97-78c827dd66e8": {"doc_hash": "394fe715b9e3e6b5503643740fe2f20c4285b7cd09d8c219c41ceb9e88292fb2", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "bbe143fa-1682-477b-be13-dc548e826e27": {"doc_hash": "21cfecbaf540bbcbc2a19bed3211c55396f82d3fb04fd2e2ea31d25c1aeed735", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "aa459331-8fdc-4afe-8d6d-6f9ad154e68c": {"doc_hash": "b613e82427bdb716cccf024a669e00727d589975f449aa374a543dd41bdc3f46", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "821b4839-9952-4d71-9a18-f49a07ba3a8f": {"doc_hash": "626fb1406334048dbb529f5ce7be0862e5941fba72213d0cf2b9226eb00ee080", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "b024718d-2f71-4a2e-86e5-3e9747c17c70": {"doc_hash": "0bfc71d1ab60b38ba54334709ef9967f3683fe74d834321df9028988d0572c31", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "9caa6f7d-2d68-4650-a0ed-9c6697363e25": {"doc_hash": "a8a782f0a5552092f04ffd24782991502d3f2f8fc894cb73d5c4a666173f8bae", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "974be74c-dd52-4dcb-9a6d-31370839ae67": {"doc_hash": "ad716cb77254dd6bd8db99071e9dcb309479a68f1c0a8ab13a8f2e088a344f14", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "749aab62-5a39-4801-8622-7ba1658e44ef": {"doc_hash": "be76106430f0946b06347aed95c658c4673f0d7f03aedc248d8ab6284f63b76e", "ref_doc_id": "33e48c1eebf15a1de8645abdba0d9d3e14b52354"}, "2d35da44-b194-4e02-9d90-d84a6afde63e": {"doc_hash": "c48a43a2b4abc670d341c1bf6ace97dcae06d31856b59a3b3b4e03cf21799857", "ref_doc_id": "916ff78cec40c2c67492af2e717ed2276683ec41"}, "cfcd6113-628a-412b-8e39-164aa737814f": {"doc_hash": "3f8e1a660a1b4bf7dcb364eb2440942d96ae80292d083f9dc78e923d8319bb70", "ref_doc_id": "d18e0fa6c6ef302a627263bbeaa9304cf00cf783"}, "075c2f7b-2ecd-40fb-9026-c9e034f2d1db": {"doc_hash": "f23c86a404c1b7cb2db4daaf092fe6fb8e4d56ee8ea61f40fb1bc5904b852ecf", "ref_doc_id": "bc10f69934e71f412af3a179f180659770cce0d0"}, "47463a30-136c-4f56-906d-2d95f521adfb": {"doc_hash": "0d35c7288b389cb3532295b41f6e2a4af1beb12c09b0d1067116d47fd8d92a27", "ref_doc_id": "bc10f69934e71f412af3a179f180659770cce0d0"}, "c227ed5f-8fec-4610-835f-184a4c091c6a": {"doc_hash": "e50e2390e1528c686549f113c6e933976e250d3232bd4e01756b991514deb13f", "ref_doc_id": "96e9abaceb5401652823a676eeea7ddd55e2f8dd"}, "2ae9acf8-04d0-48e5-a543-b6ab5a16f6de": {"doc_hash": "6f8f9c56a36555408b08a29f3763b5e6b24d83accfc706e8cbc50c6dddc6541c", "ref_doc_id": "58672402edf2a60067ad81e382e144ca2a4384e8"}, "6ff7170d-66ee-44b6-9589-2d0edbe0ef55": {"doc_hash": "ac51095866e3ab81ce0f310bb3604d44b968580879eb87d788a862eaae67fe38", "ref_doc_id": "58672402edf2a60067ad81e382e144ca2a4384e8"}, "1035c692-0298-480f-85b1-992d1d6e11f1": {"doc_hash": "b5cb93912cb9a9e957dedf1288f53d29e1cdff99e3dcc095263a722077bb85f1", "ref_doc_id": "cf80744237deefb64f56086b326ce0580b43033c"}, "85afe527-52eb-42ab-b52d-318713e0cb30": {"doc_hash": "764d9fbd05cc0c13913fcb48b48ddc86ed706e514a3c6ddad99be594c257d944", "ref_doc_id": "6a47db45817a3ffd872c3924d6d0506c02130060"}, "7c950f0c-b34b-41bc-bbce-054a3ff0c6da": {"doc_hash": "b0b1bd92b5c0e736196c125c5cd05346fa9127269983252ee6b6a9212bd451c4", "ref_doc_id": "c18c27023b15f1ba0a55f687ddb894da962ad0c6"}, "f89b8060-14c9-4920-b261-3564b01b807d": {"doc_hash": "ea8e14a89929fc5f47d7a9302bec64d627c917506a9a315ded7ea4568d9c6010", "ref_doc_id": "05be9f4d914e8dd038fb31eee61f1c41fd15afa8"}, "5832e624-06c2-450a-aa8d-b35a4ae589a8": {"doc_hash": "c095ca94f5686aad2bde00a0230271b1cb947060bed980945f24ef04bb5ac230", "ref_doc_id": "bb99e8ea9919822e6be72f75bb353e047c2fc65d"}, "7698bcfb-b51c-4088-acfe-f5f7094415c9": {"doc_hash": "c137395e4705fdb54555a344e9707b157e2723b4abf390ff78dd730fca890927", "ref_doc_id": "096a588190ce7458a2ee39bba893a36a105cddef"}, "85566be3-83f8-4b51-be47-7f0390262b33": {"doc_hash": "0dfb1712f7ab225c35b7a7c973ded6118dd098b7daa7e4efb06be25eea7d50c9", "ref_doc_id": "e806aa9811b226443b89d58d38d935b5870c416c"}, "296105a5-d1ef-49a5-bc74-4257ce966caa": {"doc_hash": "b2a24d9a246fa6bababcc99000aec6990a057cfcc94a6b5fcb7a919e10a4a356", "ref_doc_id": "9e8fec25473a3a8acbf31c22c0724e70656b79eb"}, "f966ec6f-bad4-48fb-bdfb-1044c2ad8c91": {"doc_hash": "e137d210389283e9b29be15fbdd845f197887e52eba577d1ff55ed171429b375", "ref_doc_id": "9e8fec25473a3a8acbf31c22c0724e70656b79eb"}, "bdcec2eb-6d10-48ab-9710-cce8e567f72d": {"doc_hash": "7ff1527bc2f846aae8f642bf5b5043771ea83f404060e066bbe69c3e5308f57e", "ref_doc_id": "9e8fec25473a3a8acbf31c22c0724e70656b79eb"}, "d3ba5492-f10e-4161-9f52-419f51fed945": {"doc_hash": "81bbfd2acfb3727cc1e099151b36275174d20b6abb80eecd239c633d5d3a9f40", "ref_doc_id": "fc96944a4a6324fae65221bbb5ae98f94eab2d29"}, "c168c344-3d07-4216-879f-559c38f1dc68": {"doc_hash": "d0c9d26c7609ad7003b4ffcb5ed5b8188539c84d80d88bcddb2452a7016f50f0", "ref_doc_id": "ea415d37e1d324bb37943c19ed2e911f8b6575a0"}, "3e5d43b6-bd31-4fcd-9989-59c90b07994f": {"doc_hash": "b1694f0185b6681149433e2727be54c122b34a91314e9da9a50e17a689e8a2af", "ref_doc_id": "ea415d37e1d324bb37943c19ed2e911f8b6575a0"}, "afa98542-ed95-4d19-899f-02ba8b72d152": {"doc_hash": "3b28922a4dc88ac9e78fdfe3864859d904516cd4b5fcddd5cf5749b944205aa8", "ref_doc_id": "10936e3d4d5965b1d10371f938d4ec5412780f5f"}, "e7ce5151-bfee-492e-93aa-1b706fe3d763": {"doc_hash": "f4d8cb02bed3f0de79e65a6ba09548c11bc3c8b96e7f24ac8f34497bfd2b51b1", "ref_doc_id": "3df7f73a5d0b0323962df3503372039615edce86"}, "1b22d224-b093-4a7b-a833-c189f1c95769": {"doc_hash": "a9794db989762986a0cc052fdf3a43449b61aab71d56b36028e9ae764f361994", "ref_doc_id": "8a5bcc880bf595840eb43742d69c694c7caa9ea6"}, "faa587c0-1bff-4ac2-90a7-5ffe096eb1a3": {"doc_hash": "08fbb06ac8b21a5c83e08e0b6ceb52b4350909157c0e8845a7505ddd018d38bd", "ref_doc_id": "97d01c63bebf308fa89525d6bc23a465ff5fe10b"}, "87f906b3-263d-4145-b0fd-7b06da6db63e": {"doc_hash": "758d4d5e30220f66c5754c057f07b5c3480feefa26a7c4f7b08da21978ca2bc8", "ref_doc_id": "ffc2242219ed84c2611720b75a4dc898f8cf9137"}, "d68c4121-62b3-45af-b381-024dddb2a546": {"doc_hash": "5ee6a6c7dee82d5238f576226766d1bad740977aa03bcea086a945aaec1bd401", "ref_doc_id": "830b99379ad5a29329aa2d7c01205eb59cb03b07"}, "6c2efcbb-f5b8-43fe-b397-0b9dbcb5431e": {"doc_hash": "74d419955f0dc79d243fe4cd53470865593e1206be1bdf5b67e08131b0dab3bc", "ref_doc_id": "830b99379ad5a29329aa2d7c01205eb59cb03b07"}, "f5043496-0a82-472c-8a40-d5078c3b8f93": {"doc_hash": "bf479c206dd87308e2815bcd14fad6947bb40d84b05df4efd1af33cede101b50", "ref_doc_id": "7f783826a679fe51abb4c0d66fdc46c9c020e279"}, "1d8ab80f-bb36-40b8-8ae0-8e8d774a27f7": {"doc_hash": "711f6ab27c5b693c16613c48e70cb1dbbe4d3a00130646ea9f199cce1f88822a", "ref_doc_id": "7f783826a679fe51abb4c0d66fdc46c9c020e279"}, "ca2dfc41-c648-406d-9a97-de4a51b4a9bb": {"doc_hash": "0328621443011858807f29c8d244dec5363d3703b8bfa638831633c0cc96999d", "ref_doc_id": "7f783826a679fe51abb4c0d66fdc46c9c020e279"}, "71c9be82-2fc3-4242-a4ad-a384caa76eca": {"doc_hash": "582d1a6fc0ad12d2b317fcee87ead437e6b66a7bd295db71b717fd9ebd52b6bd", "ref_doc_id": "90a29a791ea806e4f8165ea892ad3cab04964e80"}, "adb51f20-8ed7-441e-9a2e-c11fe2c94ded": {"doc_hash": "42627f2e2c71560cdb2ca86040b9559e163ff14f58387bc4fbb24716892a1f0c", "ref_doc_id": "5d514bf223b78bb6aec5f04d07eb8539aa351a2b"}, "7c6a06c3-5afb-46f0-99cf-abdf8c23675e": {"doc_hash": "cf971be7683381fa7388c2927424d480fa4197a320de777f0dd20572d3dca074", "ref_doc_id": "654c85513f7e2ebf22f882ab7584dbad7e414e4d"}, "8c14e1b4-3b02-43c8-8318-59b091d5891d": {"doc_hash": "508fde66ef9dcb1eeeff9bce9a7de4307272c5872b1d9dfb4fc142c403882705", "ref_doc_id": "baad5c9fc1e138f10e9032dd52d6701b6afafc23"}, "e40b6f0b-66c6-4b35-a706-97d76da18af9": {"doc_hash": "ffb79ac15d5a73a70bbcd7f5cb1db7a5f906803c41c23874708708cf3c5c29d9", "ref_doc_id": "e1780c4afaa8a6255c1434ec43e9acc22bfc7074"}, "f78eba9b-4c52-4562-b58e-c3e5c3077c71": {"doc_hash": "f90712418bfb61fc460f06e8336bb1dce5c9f461cfa69a846e4cdf08708f9023", "ref_doc_id": "b37a463d35c14412c1cc3c083db5849fe3546f17"}, "d7f1df1b-6799-424c-ac1d-01fa8a9c4be4": {"doc_hash": "d1cb039f74c2fb5b2f65126c5707f59b6e7c8a430b35e372f7e677a018495a03", "ref_doc_id": "f1ea31222f5293c15f9309b5e8fdc7b60fceb400"}, "3ceb3a0f-5076-47bb-a58c-cb4ee72e3a6a": {"doc_hash": "09bbb211815796e6be292074ccbfca39a6350b605ddbc68649e64fd608bfbd5d", "ref_doc_id": "e33362e052dbd20d533a14930fd8fb6fd8f0a1d6"}, "d2603283-c8ba-4411-b89d-b1ad5ab873a7": {"doc_hash": "1b2abea06b604402b021c3667f0ec30488da62ebd6ceeb8dc09414c455412f21", "ref_doc_id": "26d2a4ea4bb24eebf07cad04779b578d822643cc"}, "275b0e72-b556-4a60-9f1f-04b37ad2d886": {"doc_hash": "1e5a69f491f1704b13ef5dc8295c766596373c6dbef8f3628a709138aca00284", "ref_doc_id": "69070b34fa86205567406e2c0eaadbb58f27ff4f"}, "923f5321-9efc-46a2-839c-a29566c04bfb": {"doc_hash": "f896ca2ddde1765c68f4ba7876bac3b27d8a8fbd8a859ae2a308b943651828d9", "ref_doc_id": "48d75b05a3a9951a67b52b423e6527fb07921ebb"}, "d5541ebb-c5c0-479d-9c3b-06ee8b534a26": {"doc_hash": "fe35dff4e7b3d042a3604b44444b39363bbee46706ed548641401f050717ae18", "ref_doc_id": "cca36f8bb178dbcf9ade3e622a08968f6562ba4e"}, "7817930b-212b-49bd-908a-731e14618654": {"doc_hash": "140e9298d8874f9886f7edc42615e997ce53bd881ddc553e7a2097c280b38c26", "ref_doc_id": "cca36f8bb178dbcf9ade3e622a08968f6562ba4e"}, "a86b4e56-4314-47da-98a8-d2631b6addc9": {"doc_hash": "245062cd57f8553be14dcbec1c9a98389c0d456a5446bc96cb6d7b16a08e0010", "ref_doc_id": "b9fda169d2e74ed48369492a5eda601273460ff7"}, "502c2412-a41f-40d9-95ae-6b8b2cffd446": {"doc_hash": "5d091f867050758bd962f49c98e5d7b5d2e238bfe7b0a49f35fcab7382b6ace9", "ref_doc_id": "f881cab5d82a49f2ed10efd47d7828cfe9a8ab52"}, "51f15e10-99a6-462d-91dd-416c3ea15636": {"doc_hash": "b85700874c07e6dda3d207b70bee7689e08ebcee76894d5c911ce3bb19bd53e5", "ref_doc_id": "f8586922541acf4113bdcfe51f28d0ebad05f715"}, "a4b4cd52-be2a-4dbc-b30a-1aef6cd2bfb2": {"doc_hash": "237230344763137d24efd63f0f851aaaf52992e4db704939dc0303ffe0959966", "ref_doc_id": "f8586922541acf4113bdcfe51f28d0ebad05f715"}, "83a5d336-db48-47fc-a5e6-bd51d457928e": {"doc_hash": "951313980a80890a79ba4292d5cfde1908a43d9ee31f0f014b3258d8148e68e5", "ref_doc_id": "ffae42c9393a37ada2fbf9ed7d462e36ecd838e8"}, "ae7ff0f9-b680-4325-acf3-2a70ac99752a": {"doc_hash": "d4b975970be97d346f40235c4935e74aa896f6d0e551273796f145885bd213e8", "ref_doc_id": "be87b382526ef0e021562a7d760349640eb423ab"}, "7196dcb5-366b-4075-86cd-d4c9e3f528c1": {"doc_hash": "a323137b1bfc7ddd0734f4244a337d121c66bd83501e59dd9c264105238a48e7", "ref_doc_id": "97b753800ec87dcb4bdb5b0e49cd4c95d7e8b038"}, "8ba3e436-d5db-4ebb-804c-1b87b9c8b4a1": {"doc_hash": "0f7727bf07217d7e74450332fce17e6615f8936bb64c72ca6c3c2e928ad9cc4c", "ref_doc_id": "97b753800ec87dcb4bdb5b0e49cd4c95d7e8b038"}, "88b1db63-99cf-451a-bdd4-82497d2adb58": {"doc_hash": "8440ace1dd195616ef9ec2816be823241bb975a9b0d666811c256d7b259da83b", "ref_doc_id": "1e48eb7420437477cadae15b0255a6661888fd83"}, "a10a7420-9c69-4454-aa62-1eac19023ab2": {"doc_hash": "07af60dc337b755707b308ed99ea12c3f7c48cf81f77137af9df0944bb7221df", "ref_doc_id": "20c7bc0cd799f988a578e8ddf58de75afe4604ee"}, "109027ef-a4c6-427b-985a-d6eb2873aff7": {"doc_hash": "ae1885f0df24b421e79f351c5169d0a8937e959ae94ec26f777a6a8c92422b47", "ref_doc_id": "966b2643797e3d9a354326d911abf292c21f2eb5"}, "01a5c1a0-9148-4228-97b0-15885528aa9f": {"doc_hash": "23b4fce508f2a1f711add0f4d8e062dd2341823de2a26138dc241f01eb96206e", "ref_doc_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885"}, "99644ce3-417d-41a3-8512-fe058705dbfc": {"doc_hash": "97d9388c33e13906792d947b318d7e22261200f8b0d5309134bff343be71e52b", "ref_doc_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885"}, "d555cd51-f1f7-4fc0-b79e-b5f1b4fabc0e": {"doc_hash": "94fb6e9ac1ad5d3d566fe920bb17d1a5f056e8c07f7a7fc40107ec198165bb4f", "ref_doc_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885"}, "a1c0ae13-4b34-4b35-8aa6-cd9007b59c22": {"doc_hash": "937eb1cea863950544a61db371c7d7e3c134ef164a0bdb0b08df2d3a7af37d34", "ref_doc_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885"}, "256e9a35-9af5-4466-87b7-18b9f7a13201": {"doc_hash": "26f37c9948f7d54aa9642412a11455dcedc0c964d81688a5462be81906abf9c3", "ref_doc_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885"}, "42cfa1ee-ce02-450e-bdd0-453468e0c07d": {"doc_hash": "7a2580a401ef4aa33becd9cf5143deec39b73df4d0f21951c7eaccabf7280a54", "ref_doc_id": "505edb5b7bfd91a42b0ada3720ba24599dd9c885"}, "d1e2dcfa-a758-4dd2-97d8-3830c9bcdbcb": {"doc_hash": "5936921130fdc2df07ebacd79cb73886018a2f1a13c3f7f98cdca7f9a4c4036c", "ref_doc_id": "003fdbf2cfd080fcda93238c19ad70a942de95db"}, "bb9aa5f6-ebab-49c9-95b9-36f24a564c2f": {"doc_hash": "ff4204879f711b5fa975abd023b85a0b4cd175eed862bf0a2f03d59dc46c492e", "ref_doc_id": "f9f07e858ff7f7ed3f676c006fc3295319ecd6d2"}, "cac9ece9-1f14-4866-986b-31a55cf7bb5a": {"doc_hash": "2617c938c689fca41c866ef86c4edd19a1639b2653fa7ddb4628a4e6ea66051e", "ref_doc_id": "f0d2076e49dc2ca51f0382a9ad9bbc12d1b196cc"}, "12468260-8cef-435f-907a-04367701d5ea": {"doc_hash": "720e017e19e3961e89e792b89adec489b73acd98861ac27b9b45836362254393", "ref_doc_id": "f0d2076e49dc2ca51f0382a9ad9bbc12d1b196cc"}, "8d598c8a-2401-4a87-81fa-d1b6d168e09f": {"doc_hash": "6423e9808296c0c181b480c220454be363964ce8bcdd9d476f7e59345e9b748c", "ref_doc_id": "db1628c3f26ab1e24771a7bb9a9fded18ba0db60"}, "41bb98cb-73cd-4afe-9dd7-5cba47e857dd": {"doc_hash": "042ed9699f47d08a7cbd7301f39e6a39804ee45202503140073b1fbde378bfbe", "ref_doc_id": "cb93eb0420a35bd32139dc025311c9f02731162a"}, "8bbc4f80-32ef-4f92-b8f3-b625a7bcfff9": {"doc_hash": "1861200ce1c95503fdceb1773e502c14b964d7607c15eabeedcb374c919242f4", "ref_doc_id": "a4e2b156a173ab2c193789762a34d7c4f2c1045b"}, "ddefba23-0586-490c-912a-5ec6c11cb97e": {"doc_hash": "f8882e2f9050d33a638de2b7800b83ff0123129f78061426a4f4faabb6006aa9", "ref_doc_id": "febe1a6dc3ae70f1ccc84d9ab73b9b0d4d41f4d1"}, "55db4779-fe1f-4228-86ca-40db50c7cdc7": {"doc_hash": "d7d8e631779a4244ad731e0366f7394fd78fcffc9909909bee752955e9af8583", "ref_doc_id": "b9a2cde29bc150fa880d1870c64523de9e3959eb"}, "8d49d386-97a3-497b-b97d-6930b747b955": {"doc_hash": "0ae1b92258cef1c2bbf76a2146b0bc08bc36b3035357bdc5b41afaf4fa3c366f", "ref_doc_id": "5aea5486d84d09f270c793a59536240261f8810a"}, "3e213b74-6511-4a04-8d2b-ffbeda15e6bf": {"doc_hash": "167dad4f1072df9e1303fa4e6eb3cd95c309afaf8e398972ec682ed609f76d0b", "ref_doc_id": "5aea5486d84d09f270c793a59536240261f8810a"}, "9630c7e9-f254-418a-929d-e34957752420": {"doc_hash": "e109d2c76d3066c34addc66f05b426cb74c7c7077b032fa7d28d6857fd19a26d", "ref_doc_id": "a6651b33b6853f1c4f0987cb8490f884551e448c"}, "0f571887-a1a9-418a-b82a-26bd8f9a6198": {"doc_hash": "e18d761699e7a1c8fd6d247071c0361f2201cbe723b262457097f5b827f67e61", "ref_doc_id": "b9d2857886388f91254d810017f749b7a953526b"}, "abc8f1f6-6097-4a59-9262-8c0251dd4b54": {"doc_hash": "9b6ab32aa36f8b9e42cf4037b85ab39d1eb56348aa2750ba0dcd15336e89afbb", "ref_doc_id": "3c59e954477b96d4c8ef57891fc813cf4a770611"}, "de3fa269-76eb-4504-81aa-01c4d11e8623": {"doc_hash": "e893188b68ef11466cf277c4356e478592c47e749e7e09a860a97c8fc11e591c", "ref_doc_id": "a7b4bc4956dc73a6179c2c38c289a7c578bd82f4"}, "312097a2-60ff-4c06-a683-def9dde59b9b": {"doc_hash": "98f320b5b5e286e31885d6c8e4ec878cfb66d57baf5cc3d6f7441aa231447b19", "ref_doc_id": "a7b4bc4956dc73a6179c2c38c289a7c578bd82f4"}, "b4b13daa-4c47-4309-bb1b-025cfda1a80d": {"doc_hash": "91180b31af1cf4bff0c579ed77dcff8051268e03ab61981ebe79de72d93e3015", "ref_doc_id": "945556c52ede7e60e504797cf3f6fd8ca72ea488"}, "9e0e9601-4c6e-4b9b-943d-8792acada127": {"doc_hash": "e8414e58cb82d18b85e2078a2e28c37b3c2339248f03b56531b707a0ce3beb80", "ref_doc_id": "fdacaa57234ac784192ea6e96219f5a450e733e6"}, "b929670a-dd74-4230-a697-36b7945b3a77": {"doc_hash": "2cf1a2b10c0b6392492d537470206baed0ae629c81f26cc031ce866774c66cdb", "ref_doc_id": "16a2e41dfd88d8ce6c6c34540bcba1a965173514"}, "3cbb4201-a375-4ba0-98ba-57c95fcc1e85": {"doc_hash": "84dcbc5bcea3d130d33d6cc77248eb6312a7aeb76482bb4a48a4195718888f62", "ref_doc_id": "16a2e41dfd88d8ce6c6c34540bcba1a965173514"}, "d81c3e60-6c45-4fd7-839d-ba7b6e0e4076": {"doc_hash": "ff5d359289e63bf54717dbc9c1eb001639f049937b709930f2580392e488dc45", "ref_doc_id": "dce108780a52d09a89f0720320a5dc820fa9887b"}, "cfd62baa-6d4a-4f20-b1aa-58c2a3375864": {"doc_hash": "2e43e990c8b8ae5576a55b6a77d5036789f3510490ec3c2ee0601f2de19af681", "ref_doc_id": "dce108780a52d09a89f0720320a5dc820fa9887b"}, "c202918d-9b5f-4f43-80ea-e1abeb5c9609": {"doc_hash": "5a54749400a564ea9769ae836bbb1a9bf26650cec603eb36aa49a5e93fe0c6ba", "ref_doc_id": "3eaef138dde935320ea0ea0d9062eed782ee2d91"}, "ef3eb03d-7622-4fc2-8248-b2dffea3a2cb": {"doc_hash": "ff1b891f727814a7e0c63dfe0988037cc6554e5cb5316627ddbf4ee6964f2a98", "ref_doc_id": "3eaef138dde935320ea0ea0d9062eed782ee2d91"}, "e45c70e1-2d02-4c14-9d03-814dbd76623f": {"doc_hash": "2a8bf0065d974fd66b086640023d1f26c079c6091e31511ea0a2b02f8f12799b", "ref_doc_id": "3eaef138dde935320ea0ea0d9062eed782ee2d91"}, "f9b3f4dd-d922-439c-a8fc-be1c6639103c": {"doc_hash": "0642d55daec0bfba419bb743d455604c7d3ffe256a9bc6b0f8287ebad83fa95b", "ref_doc_id": "2b1cd9fc723729fb62e156324143f99c75482674"}, "14cfdac9-4e3e-4918-8ff0-27c4a9fb256d": {"doc_hash": "79da4afbb382d030169eab850b6fa2f438b919b1ce0d48dde5d3c11b749d397c", "ref_doc_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987"}, "9edfdc8b-14a2-4ef4-b70d-6d3e3ad8d220": {"doc_hash": "1d48f47872aded279b37542a55a8fca7307efd7046790d58966fd06836071bed", "ref_doc_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987"}, "896cc3dc-06ad-4ca1-a99c-6311ec30e707": {"doc_hash": "5ba52d8f09e8a36a73167ddd0475599ce5ad9d04aea874c2c3ae292ae03b291f", "ref_doc_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987"}, "9baea55c-b96f-4b84-89a3-82ec48aaa859": {"doc_hash": "83c4b9dd80c31adce1b0f87bdefd28327f391d532324ced0b55975fe604971f2", "ref_doc_id": "33b3f4b3aed48c0516c4e215faac5ca842d35987"}, "d2102222-0cd7-4c3b-9434-4cded109a41e": {"doc_hash": "093f40a73638c27ea8f06587067eee50a899a160d8ed58c75cfc801b668b2acc", "ref_doc_id": "59eb08939c32fd4818c84ce9d7e7590dc52305d2"}, "dc3d7afd-ca11-404d-a784-2dab32b61dcd": {"doc_hash": "3f161c08cdd8cce9e3a6190ef28ba786b3e03aeafe446a3a507e59ef95f2ac07", "ref_doc_id": "6a95eac841f49acc114f9ae658bbfcb61f581af5"}, "3c33f1e3-3346-4155-8fda-b07b7f09346b": {"doc_hash": "1b04bc8f1a7be52927047d9891e04b90098fe73906b9864cd6b9ea30d3a4476a", "ref_doc_id": "6a95eac841f49acc114f9ae658bbfcb61f581af5"}, "dfcc0107-f2e4-45a7-bf3a-7e1c2bff1b30": {"doc_hash": "60cf76ad3a9c77ab65930572f1546cc3e1386a72ccee30d2d07ac53d32bf1e0d", "ref_doc_id": "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b"}, "59af9214-fe9f-44c1-b909-4693911612b3": {"doc_hash": "042bd7e01d81b286753c599478cebcbdc54a7b24e68ae6d294d81090a4e546f8", "ref_doc_id": "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b"}, "511386b0-0f77-488b-af76-53e45eb182fd": {"doc_hash": "373bcc51bc18ee6381c7bfd05c0f25e963cbe43f7f130aaaf884dc95713b3ee6", "ref_doc_id": "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b"}, "5d68da8e-8a9c-425f-a5fd-8d111fe93921": {"doc_hash": "f5b6dd169edbb78130ccbf01d30b634a7143b48a08b643ec06333516ed5dbff7", "ref_doc_id": "7a2118da9168600fea22ccefe226ebe714f2e6c2"}, "88f8da1f-8bdb-4d3e-bcbc-70d3c0ffccc8": {"doc_hash": "779cdfdd57a7d321e392254b6ef9a257094b24df058c872f50ce30f647896c5e", "ref_doc_id": "dc2edf5349b16d42d5547db8da4d674ea3f15254"}, "e2059480-fe0e-41b6-8653-0cb34fa5f4c5": {"doc_hash": "faf3bde253acabbf637770fd97a9ba42d3c3cde47896a921ee3f1798588995ea", "ref_doc_id": "dc2edf5349b16d42d5547db8da4d674ea3f15254"}, "de8feeec-0063-4926-8d98-6720c89974b2": {"doc_hash": "0cbd9ac46061105a63cfd95517e5d6f4523c4ae7c61de3fa23425c992cc74c97", "ref_doc_id": "dc2edf5349b16d42d5547db8da4d674ea3f15254"}, "40eed48a-468c-45e6-8754-b488fbe02c9d": {"doc_hash": "ac61630fc8502d331e21df026927c2c79ee63e1e1af5ffb985fbc9d6912819d0", "ref_doc_id": "e0a9fc269f28231be4b85de9c8bb64f6dd0efd59"}, "b47abb54-6d58-4d05-9566-73353805deff": {"doc_hash": "8fe9514d372961fb7ef0037c63ebb88e3ed927ae317faab935a7fabb15582a59", "ref_doc_id": "0f9f0fda622658f6cda0735aa0c2948a60c5f9a5"}, "1dd800ac-82c6-4883-8ffb-1de7a0b4dbe5": {"doc_hash": "9da09286ff94029c58d7095cf9a0324b893c2152a895c4f167f47939a581a1bc", "ref_doc_id": "0f9f0fda622658f6cda0735aa0c2948a60c5f9a5"}, "77ae39e2-9b9b-42c2-afce-463f8733d32e": {"doc_hash": "311f68ab52a126a1acb0f05be4515699ff2478cbc39ef2c54894b1390e697ad6", "ref_doc_id": "46f4b004e019779b647ea31563438eea98a10cb0"}, "7422c027-9d12-4fc9-98da-dc047a5d19c6": {"doc_hash": "ae0e73edd062729fff84dd88d52ba236c0debd64cce6ed557e4326100d0e8ce3", "ref_doc_id": "32ed2ba753bd1b4012ea41e7ff2024775759e300"}, "c94b2ee2-472b-4b75-b5b6-a867e5153535": {"doc_hash": "2f9658e23985e3bcbe1ec53a614e00291c74d28c787537704f502c38d9eb0b0c", "ref_doc_id": "adeda652217e9ca8b70ffc662c540458dfb35011"}, "f69e6f63-7b75-4617-9f89-8046c04b91a7": {"doc_hash": "5d2dae53c38bda037c7d1684403663f0572c828917f94c11951ce3d3ea75bffd", "ref_doc_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7"}, "9881c9a2-7cd4-4693-adb4-f1893ffd6583": {"doc_hash": "65daede1e35e3e24674fa1d9ed7209c3733665b60dfdb2b83b7530ad7ad48d2a", "ref_doc_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7"}, "271b31dd-0780-4d59-8afd-1564ec8174ab": {"doc_hash": "998ecb1e902f236b7eb36dc4959c43d52375a5094ac0aecca8a39fbd70cc5207", "ref_doc_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7"}, "6c2d0c50-c887-4f6c-adb1-c3ee46b634e5": {"doc_hash": "d6453f6ac23463ce6a193387fc267a2c72348989239026d5861bdc4a3a1e44c0", "ref_doc_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7"}, "9ea3e6dc-2ce2-402c-8900-85244acfa8dd": {"doc_hash": "5fb0931ec8d9f299ac6b4dc8d85b17b1bfc96e77c4c26dc1ce513d4393d53103", "ref_doc_id": "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7"}, "0b5966ce-1ee7-4440-810a-d8405e500f90": {"doc_hash": "be458e5528ce60782968aab1b22f9257c4abd2bbe9e491ae395c2ca4451ac491", "ref_doc_id": "087f92b3f99e3b6f0bd593c8d761c192e801b497"}, "5a43cce5-d1d7-4b5c-a87a-2a69a8753985": {"doc_hash": "d87c1b1b805fcc961a9af106fb898a6dafde9cac03e3ece55963f159dbb43518", "ref_doc_id": "087f92b3f99e3b6f0bd593c8d761c192e801b497"}, "440c4567-07d1-49dc-a3b0-b00ba048f8a9": {"doc_hash": "9661cb440c1511fc90656a1e3a74367070f490e79624588027a6fc4c820f2faa", "ref_doc_id": "087f92b3f99e3b6f0bd593c8d761c192e801b497"}, "ad32417d-ceea-4b77-acf5-32bc4d9510b4": {"doc_hash": "17efad7e785aba719b3170e7128d37a34e314c3a81d99dc13111754ecfebe24f", "ref_doc_id": "90c457c0f61b534d3528b3768c98c033adf65c76"}, "88fdeaf9-65be-4a04-943d-5ced50737a90": {"doc_hash": "5c4760282f5d0fddb78cc1fadf61a6f53d1543e50f3fff38163cf412bf631c2c", "ref_doc_id": "7775a580b5808b1e16dcd3b82eca11a1cdaacaf1"}, "32b89b34-f75d-4e98-9f3b-b0d9ee92200a": {"doc_hash": "6c75a466c6a73a74f6aa53e7a9acfde1123416ac6e51e529a5caa6e71c7eb365", "ref_doc_id": "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c"}, "001aa0a5-bfb5-4ae2-8b97-26e646350e3e": {"doc_hash": "929320f9b48de7c66728592597b4b0ea9f98f2a6bb6ced36a1075a8bac497fa5", "ref_doc_id": "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c"}, "56e75dc0-8c34-4684-8dc8-3a794abe9f5b": {"doc_hash": "182424609d58b5016764ec663b2fd256a009e0ad5de41121766b3a6135a7defa", "ref_doc_id": "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c"}, "dd2c94fe-cb58-459d-9d5f-23d93c47e5bb": {"doc_hash": "22900ad652b1dbb9f4b5762cff82ed98b2c40c1b0715ccbf449dcd8b13768325", "ref_doc_id": "6271897bdcd950cdf7bce0f307d1caf7b7bb03fd"}, "7344e602-b6a4-41d4-b350-9597cf183478": {"doc_hash": "ee9a4cda32cefb5c22e013e178b6e1826268ebfa88392209e05997d45f0527f4", "ref_doc_id": "9b1ee63acc0a41105f5f20f3a969f3b7a7ba7f0a"}, "e42fe406-f404-4183-b6d8-7298a465577c": {"doc_hash": "4b078de3f8bed09186fed86569ab9157d4d5b7f436a2b75557a6a850c0412d1a", "ref_doc_id": "4c0ab1e45f97c43b64548c664b4cf93fefd63c94"}, "e5ba981b-78f0-43c9-9d0d-a39343ee3110": {"doc_hash": "31e572a6fd23f402ac21797f4e278a18f7337e26a0e68bebf29862aa8445d4b0", "ref_doc_id": "bd2aac0f6299e5f3d78d6adc03033b85752cea84"}, "abb3f398-33ae-461f-bc6d-25976e8f0c1e": {"doc_hash": "333c2cdf1d68b077146bc144fdcce37f1a0b81a6336492ebf84d38aad5d56c04", "ref_doc_id": "bd2aac0f6299e5f3d78d6adc03033b85752cea84"}, "8ceb340a-2197-4aa2-9042-1de2c606f12a": {"doc_hash": "2ceb0a73b2d27433c026d2f7b4779220af5a1a29e3a6ac09cc80cc23ae815f44", "ref_doc_id": "6bde276b52e12284b88a00264f708a23e045215e"}, "017ba432-8661-462e-ba0c-cc9858f88a91": {"doc_hash": "c91a3ebb489796ca4670257d90a2138f677fc7edf7fcd1b372a06e5535597595", "ref_doc_id": "6bde276b52e12284b88a00264f708a23e045215e"}, "005dc515-1fd8-429c-9dc6-3c5ead7bd887": {"doc_hash": "dcd9276a3736add105cb9f57f94d15d74d310df9a7bc0400f8b09643541f6d97", "ref_doc_id": "6bde276b52e12284b88a00264f708a23e045215e"}, "95fd65dd-eb21-4164-bd0a-d9c6622f2041": {"doc_hash": "f3084384ff89e24860ad89c759f0aa6c643e0c99ac185abf1e141394f1a7a216", "ref_doc_id": "72bca5ffc14ac583cabd4f18a30c37965743159b"}, "30f35635-6c5a-42f1-91fb-1115acaa47c9": {"doc_hash": "cc3e9d6fe8208b923e67504daa424d4d8e7c77525ab773ecad92af5547cfa957", "ref_doc_id": "6f4002b19f77e12dc75b5fcb4338ee6624d5f0e0"}, "4d7add49-b129-435e-834d-7b315f696c35": {"doc_hash": "63e4f0786724701eb393d45d08362783a6e502d1921c8c97645b6d468f6f7bb3", "ref_doc_id": "fd5a0040fe0758d794e9feaf60baa92ff4a4460e"}, "56d9a115-a1cc-4148-911d-c3be4426c228": {"doc_hash": "6444bb9f1afc1704fd023a165c8b0a09a2fee34e7e7fe9509eedd1f230852397", "ref_doc_id": "dace94e70011355e27cec93dee718c3e229afed4"}, "0a81b3d3-33df-4bf6-8fdc-1969fb80c46f": {"doc_hash": "177946a1ee3208ddf41f911ccb5d4b2fb223f6ba335a5cac69fcdcadee3def02", "ref_doc_id": "d9cff212012951393eb8ceb0352e73260c4eb3d8"}, "ffdf68d7-4a10-409a-adbb-76892606a43e": {"doc_hash": "a03559b115b2b25cb2a2b15c7b13fdbad5cbc77147c57b98ffaff11f35f352ed", "ref_doc_id": "d9cff212012951393eb8ceb0352e73260c4eb3d8"}, "2ffd4a76-91d4-424a-b913-520a0720f543": {"doc_hash": "bb5ba438a331624d99704d335005066bd361126483d4906fa5c31e3e7193fe18", "ref_doc_id": "d9cff212012951393eb8ceb0352e73260c4eb3d8"}, "3e10328c-b630-4d43-9868-a89922b86e7e": {"doc_hash": "2cfe3068ac5b660d56d925355900322d9d20cfb3021081eb772454a3e062cbe4", "ref_doc_id": "f63d7cc27bc4c0726384f55108bfb73209ec8365"}, "74a0d9ad-2451-4f8e-8e01-99d386398b2a": {"doc_hash": "2bfa5151474e11f7eecfaacdad725f45c98819bf849273870aaed5cfb9917ad2", "ref_doc_id": "5048b1edf1ee1994496d53784f5453ba788c67c1"}, "03fc6590-bec3-4500-932b-31404888b9b2": {"doc_hash": "21e4d325395bef2294da2a31db3c942478dd68371815c9bd2a41a7e5838bcfbb", "ref_doc_id": "341efd4f1f6ce99a44b92bd8908e2e815a54ce3e"}, "208d18a1-e92e-42b3-b3b2-8d093ecb6f66": {"doc_hash": "dad9dcdfb1581f7296e088d2bb3212fe34e326d55ea79bcdb2dabc86640bd147", "ref_doc_id": "4c305db40c58f76897f40776d7794d0eafa9cd2e"}, "a95e80af-b095-48e1-90c3-20a6f5f28076": {"doc_hash": "fa9e6a44e0c161d88f3b3e794b3ad1a639ed840275c8412944da2f469698f279", "ref_doc_id": "9e3c493bbf5db7cd81286fbc35667bf8ecb5988f"}, "cb4c93c5-d350-4ade-aa32-11424891af8c": {"doc_hash": "59d5ca26154d5c79d9d6c62dacd32f5bd990da742092aebac918892477ca4ce3", "ref_doc_id": "6057fd181fd26a8d792742d392167abf078e3126"}, "dba5f0fd-3e3b-48b6-8ace-799a80257088": {"doc_hash": "ae0aa767aa29b7906923450d5e5eb0c50aff93b3c93da45cfe3ba86bca6e80a0", "ref_doc_id": "6057fd181fd26a8d792742d392167abf078e3126"}, "2d328b13-658c-4a29-b294-2a5819cd9cc8": {"doc_hash": "f28c48182586bada4bccf57974e99428135d2f7e7c809cea82943dc97231448e", "ref_doc_id": "0b0825184a1ecb80c940fa0aa1e55f17f4cacd01"}, "45910725-7ea8-44e7-9703-a2a9057dd277": {"doc_hash": "629f8a9a72444f5852a0a081a239116346b345f22799f01b894b07dcbfa1545d", "ref_doc_id": "001f4ae9ca75b1982018cbfef5fe8d28a9dc1f71"}, "cdaf8922-814b-4224-907b-7a6515843a79": {"doc_hash": "7b1f1b356f06949c4f4881a8e7365e8a38afdedbb49e641694f5b009da006734", "ref_doc_id": "f5951f73f265447ea5618ce68a03a45c2c764d99"}, "5a7448de-a0da-46a1-8c19-c069c3e8dcb2": {"doc_hash": "d7e59959c0346df6672bc265d9daae246eabb56326592958db1b21edac7cd2fe", "ref_doc_id": "2eb4ab9b0553400c4a180c36963d238e6f8d58b3"}, "bf5521ff-45b4-4de9-baa1-0d65c9446e0c": {"doc_hash": "6fd3a3ecdef0e9a3fee2a4780cf419aca3959c23657833bfcaf26e26e7e7506a", "ref_doc_id": "d2795506280f8e5a95edaa8833718e344f1432da"}, "ef18adb9-327b-46bc-b35e-c66fcfcff91a": {"doc_hash": "c25ae8c8d44c5c696c7f2b35b28fad5149044a1a07ed84258cba792ed434ed81", "ref_doc_id": "c7056de4a07291707fa62602421e326f4d6ae3a8"}, "c4f7b06a-19df-4c55-8ab4-7ac2bd3c906d": {"doc_hash": "7a3d84dfecf4f2967fcd26dc9d211950cac46a942da0114772a96d6574aa2490", "ref_doc_id": "c7056de4a07291707fa62602421e326f4d6ae3a8"}, "4efaff5c-1f5a-4192-9486-1bdbdcd5e215": {"doc_hash": "ecb7f7b7c309071a9b45384340a4306d84fc5bdc7da9545542aa4f0dbc28cae6", "ref_doc_id": "c7056de4a07291707fa62602421e326f4d6ae3a8"}, "227f4bbe-a402-49c4-b50c-19feef22cb65": {"doc_hash": "56e99bf30703b3d5228d7b00bc74400f3bfe24919ecaf0e6db842b769a931c9f", "ref_doc_id": "f7e81ae86f06048d8cbebf43eb91533f569a167d"}, "286b3fc0-cf3c-48fa-a178-b9c49fe5a126": {"doc_hash": "1c16c6c81b2a78d5280afe7f75c88adc27ccfcd91efb7ef44e2a4286a9c7324b", "ref_doc_id": "dab4ed48ae8ad35ad4ff9e8366b381a8214fbeb5"}, "567a245f-5513-4645-83d5-15ff1ccd23eb": {"doc_hash": "987d9baafbf372875ebea01a84a89786578cb4dbf26f2ac014482a627064322f", "ref_doc_id": "53cb322c58610c30a3904b7f7bcd79366c6aa029"}, "6e2ce25d-1928-4ce8-8aa7-e83df2d8a391": {"doc_hash": "53e24a407ed3d9af690bcd011518673ee7bec6b2140afb0a1d05079fc465c456", "ref_doc_id": "2bd3b4b59e759befd2a4bb027a80d643993a0059"}, "7442e7f0-c24f-4e3e-9898-951810821ab6": {"doc_hash": "3087e8e70cf137533e597d18fe0dbe8725d60e037fa9c0bfc1ac94d1690e578a", "ref_doc_id": "6898fbb0aafcc638d46e1b15b1bb424ca2867931"}, "707e0313-5c55-4df1-beed-54860b948737": {"doc_hash": "af7e34bf9e52d629a7f80f2bb73025815f29213ba06f6f960435ca485bead260", "ref_doc_id": "dea1bbdbdbc70740b749ce5983610afd9334c497"}, "f5fa9e28-7781-4985-aaa5-c1eaa146de78": {"doc_hash": "b4686cb48eb9bf63991ddcb9fd53ce1e0fc52acc9291e1c34fe6a03870f98829", "ref_doc_id": "3e7abbbb610c607fa279635a2db89b63e86b9186"}, "aa0c69e5-815b-4534-8f91-d150133becf7": {"doc_hash": "07ca824758e3855ecd3f595e4c8a9ac9e672dd0d3f6a9a2c082f0488c009237c", "ref_doc_id": "3e7abbbb610c607fa279635a2db89b63e86b9186"}, "0499f9d9-a1cc-4303-903e-524c91532663": {"doc_hash": "3c741952821d2f24b881121f06c9095a1e8d20d6566d75568567b1fb4f1fff1d", "ref_doc_id": "f041c5d9d10959ba897d229a632c1cd8e0c6991c"}, "fc024ccc-c066-4912-8fbf-c2c9f0945243": {"doc_hash": "f82200da98819dc40b1f40f7e8233011e5ef0dcac65d3bf567f1bff6952f3b13", "ref_doc_id": "972306828914dfa639c87d3c1a964daa88fca55e"}, "21c4b589-5fbb-479c-92c2-3715a68125b3": {"doc_hash": "829ad37278e75c4ba26f26b17b72f1ec87e2e370c1bc5860af5359f7f5abe9b2", "ref_doc_id": "c435a0cbf93fc8d2a21024417ba7efefbf636d40"}, "ee03346f-d784-4531-b1a3-d6c994dbd7f1": {"doc_hash": "052000edf4c710740647167d6659da42d4481239988cf1ae49a6c9dda9ff73f6", "ref_doc_id": "eb057e089363ffddbbda67e822923a7d173ca225"}, "c9800936-57a0-4a2e-a038-fab77a504e3d": {"doc_hash": "1b894f2bc33bcdf7dd0461fd5d433a699a9a05338fa43dc493b3253cf363e8aa", "ref_doc_id": "4dcc118424e9f2e4f070197ca13909f86a7471ae"}, "40cd32cf-499a-4950-8c89-e1c8249ee062": {"doc_hash": "80813a8b3488d24441c7df78870c53f5766d0310a499d8c0c861225731468086", "ref_doc_id": "96c6062b9c4e89910918ec78aa542f3a707d3996"}, "1e697071-27d8-401e-ac2e-9e00dd24303f": {"doc_hash": "720db87f6f59eb51834449d7e9dd7b502e2d313e1fa2986f7225f1897e31b576", "ref_doc_id": "96c6062b9c4e89910918ec78aa542f3a707d3996"}, "f9c95829-907b-4e36-8a72-7eea66638902": {"doc_hash": "b1170fbf791988c0b7373172551810b161f35cfbd8d1de4ae20366f5d5930f12", "ref_doc_id": "96c6062b9c4e89910918ec78aa542f3a707d3996"}, "76f70d8d-828a-4342-b948-4e982571e87c": {"doc_hash": "6f8441c22378ea59a1f35107d52953aa9c61bd7afbace023d3ee6e4ad1342814", "ref_doc_id": "ab61d80aa6d929518257d345d5c406ceb6fdb6c5"}, "75ca9196-00d2-432e-84dc-4b7014baaa36": {"doc_hash": "a537886f7256e5ff6f78422da6ed9007266b48a5d16e5c192e838d0874885c80", "ref_doc_id": "ab61d80aa6d929518257d345d5c406ceb6fdb6c5"}, "45c94491-734f-47d5-8f41-6014f931b4b3": {"doc_hash": "bdf44c2ebcb6196b992696f1d03eaf5024b6743c4e6e02f43939a3b4feb4ebbe", "ref_doc_id": "976a4a7632a2f9e7ba16cd956f7dbc3b76f67b25"}, "3e54c64d-d182-4632-b174-e929896d7d58": {"doc_hash": "40f6736fd8b29e5f2f0cc7128784822e66125b1ad4410539ea257964b31c0f5b", "ref_doc_id": "b4c73cf633f2552255eb077668754504eae8a1ac"}, "9d594115-9d6f-4831-9134-b60bf087739a": {"doc_hash": "3f0066c07522551b869e129c69ae413876a65538b76edbe401840eed7df9b08d", "ref_doc_id": "b4c73cf633f2552255eb077668754504eae8a1ac"}, "a30ebdb5-6c98-4f36-ac2a-bb40d622f5ae": {"doc_hash": "b0777d895b39183cd2b09297271fa47123b609ed8967623f4b74e0a7ddbee378", "ref_doc_id": "9a87dd1768aa384a4a3009d61c7ebfb0e6435889"}, "41ea0afc-4f7d-473d-b1a3-205993586bd7": {"doc_hash": "8e40f9baf2f3a805c8f830e6d4f2404d38e09ae28bd19ccd7224f8f5b82cb702", "ref_doc_id": "4b2d6332c6226c8a1ef662e0a627c426d8b2c0ee"}, "78c646e0-f29b-45e3-97ed-f0001c59ba9b": {"doc_hash": "a2db9e4ed9441f85584311ba2a59f7c7186c230fb9c045d2009e990341771a1b", "ref_doc_id": "bdfbeb720c9784aac369b677c942a5120965a285"}, "bdfb9cbf-ba3e-491e-a172-028b1ee06f2e": {"doc_hash": "71062e510c504794293dccd31b3958e0bf30f5f9e1730ed3c0e36b70d017126a", "ref_doc_id": "06a667d42dac0856ec12be4bff99b2143826bd2f"}, "e790141d-9464-4384-a282-0810c1e86e2f": {"doc_hash": "5179de6ac1c203aebbb7a6b392d49d313d663e6f1c24035e6848af87720ccb63", "ref_doc_id": "48eeef24eaa808ab07adea1985c470b3591c81f1"}, "7bd7db2a-3554-4ecb-883f-6afd072cd25a": {"doc_hash": "cdcfb50bc4b16e75ca8a939e34293a473525a0a05199aa9ef9a1bce38b13f9a8", "ref_doc_id": "e0b064d3453212b574464941c25b04b017509200"}, "3b3d00d1-2498-4933-827f-faf5a9f48cdf": {"doc_hash": "34b35473db5884109b47f61305d0c7269698f21ef71d6ee17ac96d4f190a9eb2", "ref_doc_id": "774bb04cb1c31b33aa85b57be0c5ae7a8ae18e77"}, "7d8b94bf-6cb0-454f-a75e-f4dfb5a6d2d8": {"doc_hash": "889af59e5968dfdc7da7781fc08978c88b09c8d51bfdc5bb6eaa2a72099e0e8a", "ref_doc_id": "2b22daad3974506b6a282e44c164634cec28a52e"}, "61188a5f-28ec-4b41-86f4-a46e41d33106": {"doc_hash": "97de5e3b767a53c5b4f54ff970ea74c7f72f10a80064556ac9622c02fb2e9611", "ref_doc_id": "2de1b4ec5dfa08e732d8f4473e58d35c93dbc3e7"}, "1c10ea44-83b4-44be-9bf9-79ff1cf414c2": {"doc_hash": "d5df0bb058057613fe48f88458509e128a6217f3f1e596c6c48eec2f161d2078", "ref_doc_id": "2e02b57a874d53519207d53f185ae648374b1f85"}, "57f16fd3-792e-4f7c-ae44-c5933f25f59d": {"doc_hash": "ec5cfa7721a5142d599eb84992a83a801b9fce0c39bab73c6a17dd4712368623", "ref_doc_id": "2c4a022fa95ff87a103eb867d929db544435c103"}, "3530afe5-a11c-47c6-b590-1cc33ab36cb0": {"doc_hash": "b9b138bc35101eb2a85f55f98c9980a4cec7fade1689ca958cca45ad70db9dcc", "ref_doc_id": "bd0b8ccc8d3b9c8780fcc91930ab64f9a69834e7"}, "c3e48f27-b046-472a-a792-390a2efeecf8": {"doc_hash": "5c89339cd81921c7e96aeda8ba68d9ac5d3afba36c5b7eebf426bd7c5f63a063", "ref_doc_id": "c0a21f7c3d8d525947d227d1d691e47fb9f4502e"}, "7cd9fb45-8eac-4efa-80df-93577844949c": {"doc_hash": "6df0b544b7ec3c97d9b3cb6ac354e92564125176e8163d318796ccb4b6f7f977", "ref_doc_id": "c0a21f7c3d8d525947d227d1d691e47fb9f4502e"}, "6eb34d70-5ecb-45ae-b991-0d3680d18164": {"doc_hash": "a5251e957d765821e442c4aee4c180969b49c8a3d2b22c971fb844ee640bc42b", "ref_doc_id": "729325bb7f1845950d83b12f6e9fc8dfb284ab7b"}, "4a9f3d9a-cb31-4f4e-9701-8f574e686bdf": {"doc_hash": "f28cd4602ca57255a59d21664d905f4fab4f029a8b746e4fe3764e76e37523fa", "ref_doc_id": "729325bb7f1845950d83b12f6e9fc8dfb284ab7b"}, "347c8135-ffbe-4ca8-b8ff-f9bbff31eb6a": {"doc_hash": "8f4b46f6bdad3be107600b9684698ca2ee2249de0a88b6cf6e5dafc3d012d399", "ref_doc_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded"}, "f3a10881-f73a-478d-9993-9cc186892259": {"doc_hash": "cdd9ee504b28cb7523bfaf2c8f77f40c224761dc73da0e10596fe283033014bb", "ref_doc_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded"}, "bbc4eba6-cb60-416b-9d7e-e1afbe8512ef": {"doc_hash": "92274e6c2bee3e2a305e38465ea72c4206cb4787b0ffbfde9ac1a1cd7020c160", "ref_doc_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded"}, "bd914d53-d2ce-4a99-b12e-cd80392ec556": {"doc_hash": "c0d527ce44ad521e1c98da3d46e38fd73ff0c6cff33506aba6b630946406f79b", "ref_doc_id": "39df3e5ba0f608dea58a9f0a2eeb641e34506ded"}, "422a6791-d891-4b2f-8466-cd9d8136c974": {"doc_hash": "126e0a9e37293840e6708116e7871b7884f52c101ddf2dcc6b14a17f51e1b9b8", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "65aa08ac-e70b-4514-ab0e-a78f87004eba": {"doc_hash": "755fdafe43dec70cd38a4cc2a0fdbbf8bc9c8b93294c4a07886a35b0696a6013", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "71886cb0-dbe9-41fd-8281-2dab901dd1c2": {"doc_hash": "dec9c5b5f81e4c486b20f7084ea07fd3452038a252150e787d098e2c01c5d773", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "2425e5b1-624d-4255-a2aa-cbee41ebd9f1": {"doc_hash": "0ceddf68ae5ae825403af34fb2e3d19d18d09801f6dfc25f775bcd0fbe45c953", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "33a4ab75-08f5-4b2b-958e-dec9292dcf95": {"doc_hash": "4809c5ccdc894e55826ab250265ef58d13cc1ee208f94cccca1a77fec9e374da", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "bf7f510d-543d-4b4a-8494-3ec40abf0c94": {"doc_hash": "6ce51aedb7720ceb30f7869d0466573df05c9a1cc944dc0ba61a62fb07bfa10c", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "f50c83d1-e9b4-4654-b695-286cd79d7597": {"doc_hash": "283a55b000a154f9841bbc59d9cf74b275d02d8b91747945955672e4215704ba", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "817c954f-e6bd-407e-a8b5-543d7e2bddd8": {"doc_hash": "8d01d2a1704c5e82ebb0a69c8948e05fd3a6a0663ad7933409ebf078b42350d3", "ref_doc_id": "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d"}, "0a74955e-4ee7-4173-85c9-ef2b803c265e": {"doc_hash": "950953f1c6148762df1e22b67e721d6b98a1bd9cd4cab6c640514cbe9ff783c9", "ref_doc_id": "e77465268f6d2163e2e83ad1735a1dfda606c08e"}, "10dbf4b5-7391-4676-82b3-558a725f7666": {"doc_hash": "90803f096c17192e80487abcb8bf854c6896d785888ca29714edb7db7ea2b355", "ref_doc_id": "fdf22c259939ad9077de8acb6503b790e2775207"}, "fbb33c2b-debf-4108-869a-ebddf5e92b0a": {"doc_hash": "806db92afccf94e5c2db4ed895c091535a139300f6db1aa70d438b13d094b7a2", "ref_doc_id": "fdf22c259939ad9077de8acb6503b790e2775207"}, "1dfb79e8-a648-4f51-ba8d-66776f20d2f4": {"doc_hash": "053318339869c914b6a54756da5ec44303a5840c1b896d323ddf483bdc815e5c", "ref_doc_id": "fdf22c259939ad9077de8acb6503b790e2775207"}, "ebef20a2-ce27-487b-893f-605bc94bd753": {"doc_hash": "55833acbf2da5ce2bb3e7d4dd553bb970ce4e8a5a1d6c8a2a98de77871390697", "ref_doc_id": "fdf22c259939ad9077de8acb6503b790e2775207"}, "bab0d9af-e377-40e1-bebf-2bf0a0ff9766": {"doc_hash": "c019904df9b80594afe99bbdfd2d157160089f24bc207c6a03dff2d59144fb1f", "ref_doc_id": "fdf22c259939ad9077de8acb6503b790e2775207"}, "91115a6a-6c16-4f21-9873-969b6468bb8b": {"doc_hash": "4f1006cee3a330473710c751535d793cc30d5ce6e43f9bc79b41ef9e5b53daa5", "ref_doc_id": "27b13e6441729713e8f50968e11ef423af665228"}, "8fcbccc4-1ac1-4ed9-bb9a-25017ad1ab19": {"doc_hash": "25ff51ecb94542dbbcf5a4c03b55b49edd9e61a97b8036253754db962c357d2e", "ref_doc_id": "643a6616d589550cad7e3db6883dcbea8df513de"}, "ddcbf9d6-45c0-47ee-a6d5-728ae4a82244": {"doc_hash": "aa5dbaedb913ee8742611c68b6b3084d36768d2a2f94936eaf2e0bcbbd89fc34", "ref_doc_id": "643a6616d589550cad7e3db6883dcbea8df513de"}, "409d0cba-7e87-4ae9-a286-83aa1cf69abc": {"doc_hash": "58d17e104c9716f8a84cf731d8949c0739033a10f696efe7f2bed6d6723d45d7", "ref_doc_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a"}, "f96ed7e3-7f7f-4859-a2e0-82b36621c1f7": {"doc_hash": "df9d5f9b8c345bac326d3631f0a3d8d9b5d26c4c9ca2472d7386ca07291662c0", "ref_doc_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a"}, "cb56746a-c810-4001-97e2-a985b07c1fdd": {"doc_hash": "c12eda279d5833503c3c413d793a346eba3a2a88ab46a4b48c4aa6bbaf35e2ac", "ref_doc_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a"}, "58eacbce-2989-4e1f-9581-bf0328b1d5b4": {"doc_hash": "9dbab604c08221ec9adbd2d874009d89ac3c6c5218d3c2a39081ff8c796aca75", "ref_doc_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a"}, "ec5f532d-fbfd-4be4-a8c8-48dc9cf14170": {"doc_hash": "813c2614ac9144596d01ce5e164590c1369d3a4617cb1797bfbfe2616546f12e", "ref_doc_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a"}, "71bb5190-fd8f-4cdc-9530-39076b17a983": {"doc_hash": "d8658d3225a31b6009abdbc01ecdadc7a88c10d435a927874847091c3e57b3ed", "ref_doc_id": "2a92eb05a01438b1b5d90fe2baab978f918d124a"}, "fea0e53a-241e-4fc0-a2a6-ab1932e636a0": {"doc_hash": "1a83c445ee17e6b6fc6f33d2d81be5be10dae4cccd0be1676b0575b1178bd52b", "ref_doc_id": "7befaf646f70f91314c80d8097a8d148fe1015a1"}, "c9b30e7b-2038-4bb2-966a-ec443e335c35": {"doc_hash": "8ae6e54818b3f4dbdea00cb97ed8c125104c631a90ae2361286e4de142474e64", "ref_doc_id": "7befaf646f70f91314c80d8097a8d148fe1015a1"}, "f606be89-b4a0-42bb-bb34-d9c433cb21aa": {"doc_hash": "226da76f6c24f0f659b05e964c6292b54b749118fddccee7a0c81069f21d1f48", "ref_doc_id": "5fb0bbdb8aa45dde964666f84cfa8fea01418b6c"}, "305cd500-0951-4741-9bf9-0928cb7d0e97": {"doc_hash": "4f3230e83e2c38c3a30259262911fce34c3947aa4d099d0a2e643a5e9fd4e61b", "ref_doc_id": "5fb0bbdb8aa45dde964666f84cfa8fea01418b6c"}, "e27520ab-6cc1-4379-a9e3-810e27aa43d4": {"doc_hash": "31f31f937bd5c47719806ca8a81175b63c5722f8ea44dbe218b9df614c1f1a8c", "ref_doc_id": "06dc6f52bf6a6823f17a22e22673134108d60673"}, "66e4ab5e-7c95-4c3c-b965-86bc1092396f": {"doc_hash": "3f94324295b6d3909c3982fb1a975e10d4f720b05f14bb8f2d7f04c5653e58c9", "ref_doc_id": "6c969568f721b247d9defebb6ae03a5c5d8ff92a"}, "7c876828-10e0-45b5-b7de-28cb190564b2": {"doc_hash": "f3dcd2faef331a60afedc78ca3d846cecd28c9bf52cd07e7930e6cfcc58142b3", "ref_doc_id": "6c969568f721b247d9defebb6ae03a5c5d8ff92a"}, "a85e69aa-3f37-46b8-8218-a3bfa369ffb2": {"doc_hash": "6bb1068a78e09326f86633873b14fb12d6b50904ffb1a7dca68fd32d826e07c4", "ref_doc_id": "df7eace4860b2d8c98f8868c2f6841e161548b4e"}, "0208844a-7048-4b01-ad10-7f2163d2f9a9": {"doc_hash": "a0a462cda3e0813beab94edc7ea39199338225b214159995f34ea84bcd6cca30", "ref_doc_id": "e766bd372640230aa294aea2c7fd035c4e0d195c"}, "8326b09a-f7d7-4f3f-a4c0-5297e98e487f": {"doc_hash": "10ca9647ccbfd0d383d03ab98b933d74549f3ab0d6173bc1278865778fd4e508", "ref_doc_id": "7a4d3488f00d7f794a7bffb539aa3d1148a98bd2"}, "cdba2273-d567-4e1d-843e-03b10bbf2b38": {"doc_hash": "22d1b113c77c43738f91992651e4bf08bdd1ec1e94937f30a6aa99886ea1955e", "ref_doc_id": "cb3e042eec51f285bd52d38b82135d4bd18d0e5a"}, "adf8d937-1604-4169-8ad3-ee24d77d21c0": {"doc_hash": "43533c42fe3150214048d1991768974651c883cb6bc70a56de72006c5d8aced5", "ref_doc_id": "a4d7ec9b218757f8f4d3e28151a50802643dab81"}, "34944126-3440-49c7-9ca9-b5dba48a79e4": {"doc_hash": "7d95c22b353921f8bdcb08bb636ca1aed4119cc5c374eeb041a59fae74ade81c", "ref_doc_id": "65964fb76fdbb78a66531039d7973ef96d047da0"}, "99887808-c943-4eb2-85ab-f170ecb3e83c": {"doc_hash": "67a1a6d4bceb3ea54c02c53ed4e332e1169d7f0ce35f16ffa681573878df661b", "ref_doc_id": "b0653601b8b1944540c09fd36e4d3733b2c61375"}, "7fd7ad37-3c45-4cca-ab65-69bb95ec1d60": {"doc_hash": "ec43d7b25767f3ef79fab298e47a05440447a127206a8b090bdac9d0ffc29a3a", "ref_doc_id": "eda87abe70cf3fbe7f79f8366385815870f30363"}, "ef0b6a9e-9ce0-4028-aec5-0d13a854eb69": {"doc_hash": "9ee058e26f21ec798978b1aa6c371d277feec0f60768d987659dbd5632060a4e", "ref_doc_id": "eda87abe70cf3fbe7f79f8366385815870f30363"}, "9346abed-ce4a-4c13-b768-fd32a4f0e171": {"doc_hash": "81b99149266c1fecb6b0a525465333dac87f7ca70aed1137f35d5275010f1619", "ref_doc_id": "eda87abe70cf3fbe7f79f8366385815870f30363"}, "b29b78e6-2217-42be-99c7-fa1ce32b36cf": {"doc_hash": "df225925a33f2e42ddfdeaf0942f55d62120ba9c1216e2ae628862599d4978f9", "ref_doc_id": "35c676db7a821c2b5816ef06d9205d88ac3df846"}, "dc56db05-0ebd-4f44-abcd-c7edeabd0966": {"doc_hash": "886200ba6b33c44bb3c44a3e38b84f4a4b69540ba42e98d37d7ab20a0b49871f", "ref_doc_id": "4e2796b6f98168047d342c043567c094f98c95a3"}, "0e47d823-d3f0-4e85-b927-0cd89c68ae02": {"doc_hash": "bf424902a1b860a3aa1b6623af419f1f9f806c992112ad1492a93373d5512fbb", "ref_doc_id": "e8c82613334827ad40a42c0a42a6a985ac09ce2b"}, "331ce9e5-8087-4cc4-9f94-5fbe5999b419": {"doc_hash": "ecf5f8391785c42685350e53a32502f0332ba5ce3d94065ee2a5e5a5a5dbbde3", "ref_doc_id": "9390547c52f8e0469706addf6e38ed7870144bdd"}, "619b2dc5-eaa9-4ed0-8cbe-886ec0901d52": {"doc_hash": "3b6d9d4495858f8a06fbdc68d86cc95ccd7c03af98990df5dc088808d65c6286", "ref_doc_id": "759650789e1100aad523f9b1b24b557e3b201696"}, "2ca9a81f-1f42-45d2-9a15-f2f2536cbff4": {"doc_hash": "81b8082b89fc693752aecd7abca3949715625b16d7c1e9fc779a187a6a07e954", "ref_doc_id": "6fc068612c1f529e3e2b29f2be50094b30ab13ff"}, "c7ae54d0-a67c-439c-9f07-89ef97a7723e": {"doc_hash": "3ddab2adb074d6346345ebac92fe694b013ef8b1d7a818dd898b36cd5233bf56", "ref_doc_id": "2b27999821cbc0546d25ec628a0a7a10f4d970b6"}, "1cab292f-157d-4f90-87a0-a4717b7d1eda": {"doc_hash": "a0f60e391d77d987c1454f82883357c8ae896f05a4845f311cfb6b6e5337f8dc", "ref_doc_id": "7d021d3b23824ef25223b141405a6385e337a580"}}, "docstore/ref_doc_info": {"e0833904f0cb3e584c30ea17552ca65d29c8ec2b": {"node_ids": ["b792e8d0-274f-475a-9516-aafc9754602a", "2742e447-2626-41af-b45d-3710c119dccc"], "metadata": {"filename": "DOCS_README.md", "author": "LlamaIndex"}}, "a22d9c95b25b2d0c78caa0228ca771c3f30d8421": {"node_ids": ["6ae4fbd2-958a-4f14-ab43-bab452bd5d99", "c7551b70-321a-4b67-9093-8f2cd66bcc46", "60928bce-84fc-4405-851f-03537938e3cf", "ecfaa01e-1fbb-44f8-8144-c98dc436c2fe", "44739aaf-b113-4ceb-bd8b-5cbe5ca3c979", "e2a9e57c-1b6b-452e-99ec-fe1c9420e635", "8b9deccf-7e51-4399-94c4-bdc360486765", "608f9d5c-4558-4073-ba14-f392b4bac5c6", "589e32ed-ca8a-4e99-989e-1cf14851e02e", "04a94e78-c521-4ba0-aa52-cb4545e39784", "ce22dbf3-ab7e-4e89-8c40-25ab6446f0f4", "8a6735b3-217b-4935-97de-65b95e6ba33f", "521bd1ef-cb12-4147-8df9-9e402cc342cd", "e6736c08-bd27-4d4c-b26f-f703ce2ed11b", "fe40c17c-7acc-4ed5-9847-47507a6203ee", "81398365-cc64-4ef7-8738-ab73077c371f", "a7ebd903-a483-4bb4-af32-c97f1b92d8c9", "ea03da5d-436e-4faf-bdf1-2347f2c90a72", "7715c403-8d93-49fd-8c16-c389ec67101c", "a624113c-d800-4688-a23f-449bf1b4eb08", "c8539a10-0e5e-41d4-8730-0c5f31de2a2a", "09b3b9d6-140d-44b1-878e-bc27a3f7960e", "944baf46-a119-49dd-b99e-856985851a41", "06034f25-15ea-497d-95d6-cc94392e5dfc", "818fc19d-0d24-43e8-bab3-b0d64ba44c71", "19e2764b-cc4e-4442-b19f-78d2ce6179ef", "875c60f6-1092-4990-bc34-90063190e68b", "e95cb2f3-e0d4-4cd2-9ae9-168e21d70207", "d741cc39-c5d4-4b24-b9d1-ebd89740b8a3", "ee5e3059-c337-46db-bb7d-1c48a027ff2a", "885c847a-6ad7-4d8b-8dc3-5f82718ecd23", "a48e8673-3a35-478b-8a2b-a06f593b7947", "5f3b36ad-c843-478a-bb90-ffc96aec3687", "bbf0e854-ef91-45ec-965d-23688e4af1d3", "554672da-ce4b-4d97-86b3-062d879d378a", "b255d14f-2ee5-4650-b02c-cbb6dd5c726b", "0d85e59f-b606-4363-8536-4641286809af", "16d0b0ff-ef0e-4b97-8de1-4f9d3cd5b94f", "16f9bb34-958a-40f5-8873-009d3c2aec91", "d74ca02f-8e69-4060-897a-d223cbb2f8ad", "283ed427-fcf5-4640-be7f-79c87068aad4", "bfacf8d7-300a-4ecd-80c6-54a34f4b960b", "e78faeaf-698b-4b8f-ad76-2167e81edbd6", "9ef35aa0-fe90-4724-b110-31aead47cd65", "d5f3f4e7-c37e-4f71-bf29-c093547e12dc", "9694e8bd-4a25-4afa-bc22-3f2dd81b599a", "64141801-9822-4204-93e5-5e3120f2724d", "baedf5c7-709a-45fc-a244-4bafbc97a1e5"], "metadata": {"filename": "CHANGELOG.md", "author": "LlamaIndex"}}, "988ecacad3cb4bb087af10226cb90ba28644358b": {"node_ids": ["a1c8e200-b34e-450d-ac57-0f6e502ea432", "854cc8dd-8283-458c-b0f5-ea525a501db6", "6de201b1-8334-4afa-8b37-aa214a838d04", "c7be4176-06cc-45b8-bebe-4c619f551900", "029e0587-a00e-4065-a6d8-a06346b333a7", "52f42635-7b7b-4392-abf3-cc931a7fcd76", "3b7e4be6-e3e3-4d68-93df-f307de14a25c"], "metadata": {"filename": "CONTRIBUTING.md", "author": "LlamaIndex"}}, "69ca05844fee838474d2f699701f5b17e635f74d": {"node_ids": ["e90053a0-d6d8-4d53-8647-d854c523a368"], "metadata": {"filename": "coa.md", "author": "LlamaIndex"}}, "603373f629406ccc3e1b52c40fa0eb613eb35457": {"node_ids": ["88e74cb6-b027-4a57-92dd-e59bc6155cc5"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "17ae72b0a22f88e84377b8a0ffb1ae56cd6ba12b": {"node_ids": ["74c21f5b-c2e1-4e28-8925-5b518d20052f"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "f616528dd75e506a8a56ccfb0cfb05ae13c7f85b": {"node_ids": ["68977cbe-09b8-4be1-bffd-66150b67d4bb"], "metadata": {"filename": "introspective.md", "author": "LlamaIndex"}}, "5d4a7ee83286a7f6372eb2170aca4ea1a5ed07d5": {"node_ids": ["ba7952cd-0dda-4fa3-a9e6-a32deba4eb25"], "metadata": {"filename": "lats.md", "author": "LlamaIndex"}}, "2f38e027b0cc23677cb881afa76a110b6475461a": {"node_ids": ["7b6cbf9f-0880-4d3a-8248-3e42a3bd0159"], "metadata": {"filename": "llm_compiler.md", "author": "LlamaIndex"}}, "287aae35a75dc0952398524a3f68b45b27ce867b": {"node_ids": ["e1b0d932-082a-44e2-a23e-6174a76a253d"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "8a5ae2117aec4e482af006c69ef5ccdade10fdb2": {"node_ids": ["4f4a17ad-c422-49d9-92fe-7a4612024492"], "metadata": {"filename": "openai_legacy.md", "author": "LlamaIndex"}}, "4af66f1c81f129a0c5969183122e82bc38e1894c": {"node_ids": ["9a907c07-3a30-4e32-9f3a-cde247efa738"], "metadata": {"filename": "react.md", "author": "LlamaIndex"}}, "a0cc9017a2af39284ef0b905c15f7491cb10dfd1": {"node_ids": ["7989c642-bdef-4462-80f5-f92dc3c69a35"], "metadata": {"filename": "agentops.md", "author": "LlamaIndex"}}, "39dd143435c8abf079e0a2092f0b2ab6be3f80f2": {"node_ids": ["ffe31ca3-208b-479c-af24-37269a933e4f"], "metadata": {"filename": "aim.md", "author": "LlamaIndex"}}, "2646025f16cf79a5afbbfda01563ce0beee9f097": {"node_ids": ["d19041e6-1840-40c1-815a-b572885325ee"], "metadata": {"filename": "argilla.md", "author": "LlamaIndex"}}, "757f1e0deb93afae14ffaa7599289796dcad6840": {"node_ids": ["e4781a09-f477-4e08-9821-5672418c7806"], "metadata": {"filename": "arize_phoenix.md", "author": "LlamaIndex"}}, "aab0d51281e9294d03fbdadc24e31f57e19e1c47": {"node_ids": ["462dbdab-caad-43e7-a108-a68ec651e23f"], "metadata": {"filename": "deepeval.md", "author": "LlamaIndex"}}, "90cbae31b63e7e07f4bc552f8e6e536d789d260b": {"node_ids": ["e60145ca-a605-4cb8-a459-d0c468e08869"], "metadata": {"filename": "honeyhive.md", "author": "LlamaIndex"}}, "42380e7880a85169f13f65af52b8cec8eb056ac8": {"node_ids": ["ebd04fac-a813-40f5-be8b-98bc83615fc5"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "0b53cf4d8ba360a2757bf01bb6c6e32c2c3ef67a": {"node_ids": ["9f309e88-3c76-4169-a922-fe8a9df50868"], "metadata": {"filename": "langfuse.md", "author": "LlamaIndex"}}, "2bf6d446f263898423a27c71e85d004d8fdee467": {"node_ids": ["96ce022a-68b4-4646-bd46-47f7d0800554"], "metadata": {"filename": "llama_debug.md", "author": "LlamaIndex"}}, "0190c850bf0c26acd86872d3abc6e39a542b5482": {"node_ids": ["52e68092-2c76-40ae-ab58-8f3e0f8dd877"], "metadata": {"filename": "openinference.md", "author": "LlamaIndex"}}, "cd9a6279365ace6475d0d451ff571c029152707e": {"node_ids": ["c4bf58cc-7a88-45e8-99a3-1cec931c9ad9"], "metadata": {"filename": "promptlayer.md", "author": "LlamaIndex"}}, "d7ddd153e8feeed45afb4f15a6bf9b80541dfc92": {"node_ids": ["142d3fe2-8e36-4c6d-8ac8-0684e3b6f87b"], "metadata": {"filename": "token_counter.md", "author": "LlamaIndex"}}, "5d1069b68f636d9791598c1642fe6b7f865e7e2d": {"node_ids": ["4ed26d83-2824-4ab5-aa27-60210c0c009d"], "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}}, "0c4e3ececa5d957c3fffcb76f11a47af50304323": {"node_ids": ["3690a6e7-63fe-439b-96d5-6bf410ec4bb7"], "metadata": {"filename": "wandb.md", "author": "LlamaIndex"}}, "542938dd7521d5b512d56345c5492c95a6aa7061": {"node_ids": ["bb3645d3-92cf-4ced-80a9-cecc368d87f1"], "metadata": {"filename": "condense_plus_context.md", "author": "LlamaIndex"}}, "ac22564adcb0a4a58e4a020d60c94599c6e60a0d": {"node_ids": ["9bb87352-fc99-4d40-ba92-e2f3d4e051c7"], "metadata": {"filename": "condense_question.md", "author": "LlamaIndex"}}, "7048fa82244f1053af906675eee700b6c9880a86": {"node_ids": ["d368abd4-8c23-419c-910c-b8337450a2e2"], "metadata": {"filename": "context.md", "author": "LlamaIndex"}}, "8ff746c7e9b22a37a0814146b898783132e9eff8": {"node_ids": ["70f4b5ca-ce57-46eb-8b67-897a5a9c8d03"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "b2a39240b5d10ee437f77cc2d2c468c975c0df9d": {"node_ids": ["403ea2a1-315a-4594-96f4-f817146847c1"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "3fb7e6316c7c692ef1b578c5b851e9e7cea01695": {"node_ids": ["be4cdbce-7f62-4350-bfd3-5c448d85363c"], "metadata": {"filename": "adapter.md", "author": "LlamaIndex"}}, "d03a1780455f535a3e892e33a0817b27ff33ddd0": {"node_ids": ["6a565ec5-7eee-4465-b328-54b788f5731b"], "metadata": {"filename": "alephalpha.md", "author": "LlamaIndex"}}, "20dfb2c99844a7132271e6b5f98472602faa4e9c": {"node_ids": ["8cd1c7b8-ed60-4223-842f-491fae4ee2ca"], "metadata": {"filename": "anyscale.md", "author": "LlamaIndex"}}, "58fafe285f7f61df92909f18b6ea223f91247dde": {"node_ids": ["b40ca217-23fb-4cf5-a721-33a5c456c8a8"], "metadata": {"filename": "azure_inference.md", "author": "LlamaIndex"}}, "59b4215f02794470bc32c9e13b0e4c7c7b6fec64": {"node_ids": ["80510c5e-9623-4ff0-8dfa-6aed4bdf1e68"], "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}}, "fd0e6cf50a32507bbfe5faf6d2567605f029fb07": {"node_ids": ["bc68fceb-51e2-4126-ac2e-03c34f694f58"], "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}}, "deb879454f442ff04a98190d5767a7249d28c344": {"node_ids": ["1142694f-6228-44b8-b05e-5df5b00cd7b5"], "metadata": {"filename": "clarifai.md", "author": "LlamaIndex"}}, "5c4169243c44062c6071b43428bb6a2f1f759625": {"node_ids": ["05ed793d-cdea-4353-b1af-ca79d1727e16"], "metadata": {"filename": "clip.md", "author": "LlamaIndex"}}, "e75627093e093dc1157afa7f5e2d8b4d93a4bf3f": {"node_ids": ["3ab53cca-fd7e-4833-a8d6-00f92101f1fd"], "metadata": {"filename": "cloudflare_workersai.md", "author": "LlamaIndex"}}, "f95d2cd62e28e7916481e1c72688a302391b4830": {"node_ids": ["26b69072-2e3a-4b91-877d-35b3234ba613"], "metadata": {"filename": "cohere.md", "author": "LlamaIndex"}}, "650939e22e20b56a4e25f03d7ee9ae9f70ec7843": {"node_ids": ["4e6798c6-f370-4aaa-8a61-b0e9073964ba"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "4e5f0679d046dce9c09e45a7265b9ae776c527ce": {"node_ids": ["11e71d21-c88a-4dae-a096-0fc050374794"], "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}}, "09edad536982ffb11596ff8668ece20d93dffc99": {"node_ids": ["52868fd2-eb3c-407d-83eb-fe99a8d6f238"], "metadata": {"filename": "deepinfra.md", "author": "LlamaIndex"}}, "43e9011ce30940c5c1e3a635d83bbc8cc87e4402": {"node_ids": ["2f1e1d32-7a9f-46fd-a1dc-75ec1aede38c"], "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}}, "46465555d31a9669feb53cc28d0ff1c748ebe69e": {"node_ids": ["04dcfcf5-90c0-418d-99f1-e78420157555"], "metadata": {"filename": "fastembed.md", "author": "LlamaIndex"}}, "879fbccee52df8f271f71a4c5ea48dda22638d40": {"node_ids": ["0e371af7-7895-45a9-aa70-6c6d80e65938"], "metadata": {"filename": "fireworks.md", "author": "LlamaIndex"}}, "0ecd56ecfebe51234fbdc56933728e191581f585": {"node_ids": ["3f7e55f0-7a5a-493e-8c43-85103f33b9ba"], "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}}, "a67cec3bc1de08e112b0dda0746cd2e15be76228": {"node_ids": ["5b181ce6-b21e-49cf-8554-dca5e85f3537"], "metadata": {"filename": "google.md", "author": "LlamaIndex"}}, "6ed192170e17debba7cd23e5681b8faee03dfebf": {"node_ids": ["ed717d8a-582e-4e2d-b352-02e664ffca04"], "metadata": {"filename": "gradient.md", "author": "LlamaIndex"}}, "597bd6798f3a59cec74173593b0d668b4cfecd32": {"node_ids": ["ee8a6591-03d6-4562-a760-c4502a7f809a"], "metadata": {"filename": "huggingface.md", "author": "LlamaIndex"}}, "26888df65a1c3ce297ef5ad574454781cec100c1": {"node_ids": ["4ce34e09-dcb7-4f78-8ed3-265cf316f9cd"], "metadata": {"filename": "huggingface_api.md", "author": "LlamaIndex"}}, "98c987f9abda163920bbcf93bef712338341e766": {"node_ids": ["53522978-ecc8-4da9-aaa2-3f15fdbafb2b"], "metadata": {"filename": "huggingface_itrex.md", "author": "LlamaIndex"}}, "d82ce271042dc3d97f67d77463cdac19558c6908": {"node_ids": ["d4926ef6-fa79-41da-a575-6c7a22b4991f"], "metadata": {"filename": "huggingface_openvino.md", "author": "LlamaIndex"}}, "d978ccdcc20b0d6cb39ed74fc2747e75c60f4346": {"node_ids": ["5140af2a-8063-4fae-988d-17bd0b853008"], "metadata": {"filename": "huggingface_optimum.md", "author": "LlamaIndex"}}, "78c17615708ef4c3aa78b327ff01f25a1daa1974": {"node_ids": ["078f43e1-bef1-4bf7-bcbb-3075c37f28ea"], "metadata": {"filename": "huggingface_optimum_intel.md", "author": "LlamaIndex"}}, "4d6e265a30ed48986b89510d6ad1faa09d8bb6eb": {"node_ids": ["4db5f860-562c-4bd7-9df5-9073880a2958"], "metadata": {"filename": "ibm.md", "author": "LlamaIndex"}}, "9a53fe6942f8027d3b5dfc76eaad3e5254fcfc4e": {"node_ids": ["7338529f-8fea-4061-a39e-080f447be73d"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "12e8722618cc239c739fb8c561e29084743e6996": {"node_ids": ["dbc0d42e-f6f4-4057-96f4-eb0e3f6fbb96"], "metadata": {"filename": "instructor.md", "author": "LlamaIndex"}}, "7ee9de231fbf091b59efc4b2b809806de3b72494": {"node_ids": ["86e24547-9b19-43c4-9c43-691579dead5f"], "metadata": {"filename": "ipex_llm.md", "author": "LlamaIndex"}}, "7c52e55b8f66941a7fa94eb7aac76922c8ecde3e": {"node_ids": ["9276ae1f-7304-406c-bac7-a966b97db906"], "metadata": {"filename": "jinaai.md", "author": "LlamaIndex"}}, "35f749210c09fdb470716d46b794b5f45eb42284": {"node_ids": ["e766e0ce-23ea-4b75-b3f9-06a742d4854a"], "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}}, "cc6bcc9d27ec11896321ffe29054948f16833d5b": {"node_ids": ["a155cb10-9e7c-4905-aa9b-bc762fb2c9c6"], "metadata": {"filename": "litellm.md", "author": "LlamaIndex"}}, "5ea5e8752d599a6c3a16cc00317585cba0f09c1d": {"node_ids": ["013737e0-09b2-4744-9b7d-e8f6cc7f5080"], "metadata": {"filename": "llamafile.md", "author": "LlamaIndex"}}, "7fc32264593d3d829edf075fa8cb61ce0bfe1f02": {"node_ids": ["9c2c87eb-b2ef-488a-a9f4-1ded1b9ed427"], "metadata": {"filename": "llm_rails.md", "author": "LlamaIndex"}}, "8a24a208766627a9c8dda41d10a4c9b371893dbb": {"node_ids": ["bd118f23-569f-46a2-a0d0-e77dd2484896"], "metadata": {"filename": "mistralai.md", "author": "LlamaIndex"}}, "f5f62b2305f5b3a1266816a748c340735ac6d6eb": {"node_ids": ["1315267b-b38c-4a92-be10-1d82f11792f8"], "metadata": {"filename": "mixedbreadai.md", "author": "LlamaIndex"}}, "4168de5426fced8f70c1599b129094c399edda1e": {"node_ids": ["6c3961ea-568b-4668-84e7-9b058925a2cb"], "metadata": {"filename": "nomic.md", "author": "LlamaIndex"}}, "6cbd46cb8b1ea2a0e8bf46a763d92dd33093cf68": {"node_ids": ["cf0342d1-6e42-4b71-9e71-6f12b7dc8f69"], "metadata": {"filename": "nvidia.md", "author": "LlamaIndex"}}, "1ea78a1d30232e796833bf40391bec307db04e99": {"node_ids": ["96c90f72-dbb6-42b6-aefd-7d46d0eeb695"], "metadata": {"filename": "oci_genai.md", "author": "LlamaIndex"}}, "60071a4c2cf6d03df19c2d78dd2ea311dc0078bd": {"node_ids": ["fcd9e1ee-baa0-45fb-b33d-8b91631ea281"], "metadata": {"filename": "octoai.md", "author": "LlamaIndex"}}, "28bd274caed599637ddcb651116ece0586cc6e63": {"node_ids": ["2c60fc95-7338-4a3a-9a16-46e133a5f6cf"], "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}}, "6692b3046ff299377dc55595e44ab37a6ded22a6": {"node_ids": ["26dd8d38-db48-4ab8-901c-05bc168d1ecc"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "8a2b769d8d75382122edfe502a0ba3ccb4ce4ca4": {"node_ids": ["a7873598-4f78-4d25-b96d-99f7ae237790"], "metadata": {"filename": "premai.md", "author": "LlamaIndex"}}, "c7606e62ac5a13833186e84288ce5e225ab24888": {"node_ids": ["ae8b9f92-6558-4e9a-967b-37580ca01af2"], "metadata": {"filename": "sagemaker_endpoint.md", "author": "LlamaIndex"}}, "3cc68deec351468387158cbefcb3f0e3b317d47b": {"node_ids": ["884450ad-1f0a-4cba-857e-6dc56fb1291b"], "metadata": {"filename": "text_embeddings_inference.md", "author": "LlamaIndex"}}, "f8e23654b4ebacb82d6e70bc56335dc6e1902a85": {"node_ids": ["875f77c0-6b71-487b-b79e-454387e0c7e4"], "metadata": {"filename": "together.md", "author": "LlamaIndex"}}, "ab961b59ebc7d588aed8db130b6cdf93a5259d7b": {"node_ids": ["72bfbc57-e17b-4c79-9e77-da3dfaae4f6b"], "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}}, "d6e27b2309465f8cfc56b77048050f6b39a7cc0a": {"node_ids": ["94ac891e-2db8-4302-bc04-7eb75f894077"], "metadata": {"filename": "vertex.md", "author": "LlamaIndex"}}, "b388d315826c8c8c2c9ab1750ed30bc6742bb3bc": {"node_ids": ["17bd13df-427f-487b-9b0a-0204a4699a7d"], "metadata": {"filename": "voyageai.md", "author": "LlamaIndex"}}, "a0d3735a7baf29d05eae2f2990246d9223027e58": {"node_ids": ["769b2334-f764-49ce-9206-40bb7237613b"], "metadata": {"filename": "yandexgpt.md", "author": "LlamaIndex"}}, "15b91eec0647e7ed5d1f8c5e7907933f8cd1b6c8": {"node_ids": ["46e4c544-2eba-4bf4-a930-b232b7eb04c7"], "metadata": {"filename": "answer_relevancy.md", "author": "LlamaIndex"}}, "56a5d36345a1321b8e3cef5d39e7e6938f58a270": {"node_ids": ["c7445092-5016-47f4-b47f-ffba1c08d7ea"], "metadata": {"filename": "context_relevancy.md", "author": "LlamaIndex"}}, "eb29bb1bbd40d934161e8b0c52c8fa725ee88eb9": {"node_ids": ["f6c401e9-206e-477b-a598-ea41ac6040b3"], "metadata": {"filename": "correctness.md", "author": "LlamaIndex"}}, "cbda5302f5b1e4202a5996618f34751b257540b7": {"node_ids": ["d64a7acc-fbae-4430-92f8-f67610e918f7"], "metadata": {"filename": "dataset_generation.md", "author": "LlamaIndex"}}, "b8af2aa0e87dc4995c9c22439e5dfde16b7d8824": {"node_ids": ["edf0c907-91f7-41c2-9f44-3634f750bdd0"], "metadata": {"filename": "faithfullness.md", "author": "LlamaIndex"}}, "3d48c81cfc6c86e9ca9b3f03537ebdb56c3175d4": {"node_ids": ["5ebdb769-d84c-4808-a30e-8475fddd5a17"], "metadata": {"filename": "guideline.md", "author": "LlamaIndex"}}, "054370016c232d223a94b5ab0a735a835e0d07c5": {"node_ids": ["865b05c5-740a-4f1b-8424-bed6689918c8"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "f46d3a172fe4021651bd442ee85d6e196864369b": {"node_ids": ["fb24c6ea-d8ef-4526-a3d6-e5b445010858"], "metadata": {"filename": "metrics.md", "author": "LlamaIndex"}}, "e021feb3d795fed17742261332e3a0c6dc4b7ea0": {"node_ids": ["79f2908b-41ed-47e4-96c2-bbdbdf0697be"], "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}}, "3e5fd6aaa813de68511947117ed3433e555bed76": {"node_ids": ["7558c0d6-f51c-48fb-a8cd-017257ac1a7a"], "metadata": {"filename": "pairwise_comparison.md", "author": "LlamaIndex"}}, "a14267d569b4fd0e1b47ab6d1deffab320431983": {"node_ids": ["4208bd86-6ea0-4d9b-83be-6f21e8a104eb"], "metadata": {"filename": "query_response.md", "author": "LlamaIndex"}}, "9e4378809ed87be6eb84130a8c8cebf2600aaa46": {"node_ids": ["96804470-ffae-40d3-bed0-ded6bc7792fe"], "metadata": {"filename": "response.md", "author": "LlamaIndex"}}, "1a0d70287e1506ee9bd9846b76e5a7885d86cf99": {"node_ids": ["01add33b-cbf1-436d-a912-61cbf7d37044"], "metadata": {"filename": "retrieval.md", "author": "LlamaIndex"}}, "9ec1682e064b854d4c0530818d4a4cd8fa691bf0": {"node_ids": ["3147d8f6-f7eb-4ba1-a928-f61e9a0312f2"], "metadata": {"filename": "semantic_similarity.md", "author": "LlamaIndex"}}, "2d284f695b264749117a7b11dcdab395f3e40431": {"node_ids": ["f3e8e12d-8bc1-46e4-8030-d250bcd51974"], "metadata": {"filename": "tonic_validate.md", "author": "LlamaIndex"}}, "08383b48b4feebb8a1f994773514ba990dfb50e2": {"node_ids": ["edbaf4f6-7fd4-4ee9-a9b5-42c7a878d05a"], "metadata": {"filename": "entity.md", "author": "LlamaIndex"}}, "af7cce494818be3843e5fb4f2df22938900a98de": {"node_ids": ["25fc1f5f-1f1e-478f-9f7e-05f2be85a8a7"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "43b2d8ff0c00fd536467985bee8cd3a73b666556": {"node_ids": ["e4a8e040-ec6b-47bb-985f-71ca95f8ce5b"], "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}}, "43d74ded76ea7c3c9f5cb4b2915d487990f99525": {"node_ids": ["ff840cae-decb-4741-8143-58405d021743"], "metadata": {"filename": "marvin.md", "author": "LlamaIndex"}}, "34dc85f0d49b0ad409469f5ef5ef54edd6e860fd": {"node_ids": ["387d9672-baef-460d-8a64-b1b156a28348"], "metadata": {"filename": "pydantic.md", "author": "LlamaIndex"}}, "162e80ac7102b451126e00f218131e488dec80f3": {"node_ids": ["13ec55b1-45c5-4678-a500-fe89c727f9c2"], "metadata": {"filename": "question.md", "author": "LlamaIndex"}}, "54ad5d2f7128fcc54caa013b0de0888311341722": {"node_ids": ["47fd8cf9-f5df-424e-ad19-f5064a3a0063"], "metadata": {"filename": "summary.md", "author": "LlamaIndex"}}, "a39fcd5f089d403851b69caacc1e5fc7ed47c264": {"node_ids": ["5357751f-4d11-4884-934e-873bfe452639"], "metadata": {"filename": "title.md", "author": "LlamaIndex"}}, "ed48bdd383dc09e2944a569eb0d4646c91fa59bc": {"node_ids": ["f16c3eed-248f-4691-99a5-9c0cfbe74768"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "eae0cb598d1a7331a43eb36ec679ee34e3422146": {"node_ids": ["5bf50750-46ca-4ba4-a0b4-82adeed6d434"], "metadata": {"filename": "colbert.md", "author": "LlamaIndex"}}, "f4f3ffb9b7497c715ded13a91fd97216cf5324fb": {"node_ids": ["c335cae6-cc33-4f79-ba59-523a1b854b3a"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "9af803ed3c3f35dc34b33870976455911eafddc1": {"node_ids": ["fc42c7f4-01d9-4d37-ba86-6a57ba73e074"], "metadata": {"filename": "document_summary.md", "author": "LlamaIndex"}}, "c32021044eea7b39eb2d9e3fba03d0308e187f9d": {"node_ids": ["7f18cf8a-38ab-45e9-9aad-ec86f0324697"], "metadata": {"filename": "google.md", "author": "LlamaIndex"}}, "6d23d3f649889ec210eb96989d83365b43b0551e": {"node_ids": ["0135998b-137c-479f-925a-d95d8f0a838f"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "683e8c50ebda5a9d8ad6b082e64fceebc3261f5e": {"node_ids": ["89e7350c-0630-4383-9b66-95c37ee45daf"], "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}}, "83a19316c19a6246ac7be1ded127fd1603907be4": {"node_ids": ["6b600fef-f262-4ca8-b435-674241e52075"], "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}}, "2511eebb101dffbf1c87fdb0b73a582882d9a63c": {"node_ids": ["1f4a2d54-95d0-4ccf-8a24-d439595ccccb"], "metadata": {"filename": "llama_cloud.md", "author": "LlamaIndex"}}, "7d45b19879802dd16a33b9dd9ef68e3a121109c8": {"node_ids": ["cc314758-d006-4dcd-b9d6-9f408c12e846"], "metadata": {"filename": "postgresml.md", "author": "LlamaIndex"}}, "c1f0eff012ad405263922913b4070cd48b98fdb5": {"node_ids": ["a81383d5-102c-48b6-9785-17640f0206da"], "metadata": {"filename": "property_graph.md", "author": "LlamaIndex"}}, "af381e6b3d6d2ee802f342437983d48ff8018002": {"node_ids": ["9fa07f9a-5e8b-4ae0-8b09-a59c4d8840ad"], "metadata": {"filename": "summary.md", "author": "LlamaIndex"}}, "cb6a6e230d166a9550f76454e95bd23bbc6d717e": {"node_ids": ["5f7a3799-1ff1-4baf-a1d8-f8a5c3bf86b7"], "metadata": {"filename": "tree.md", "author": "LlamaIndex"}}, "2b59c4dbecec2e420ac2a9f9a9510a02c6808f13": {"node_ids": ["c722246a-a835-4500-bf18-c5b6d9b05edf"], "metadata": {"filename": "vectara.md", "author": "LlamaIndex"}}, "9678fe6b412c9f7d4839065feecc311c6535572c": {"node_ids": ["015603ff-b935-4164-951f-4f34f504a01c"], "metadata": {"filename": "vector.md", "author": "LlamaIndex"}}, "4c58f355afca9c4d92adbf06bc96472dcc51cc4a": {"node_ids": ["a9d6e56e-fd5c-4dd8-990e-a26c11e7330a"], "metadata": {"filename": "vertexai.md", "author": "LlamaIndex"}}, "20d8ddd47f8b4eaca063d2777a6e3d929e27fa4c": {"node_ids": ["64702c81-29cf-45c5-a038-0c01dcf435df"], "metadata": {"filename": "zilliz.md", "author": "LlamaIndex"}}, "6ed6504d8d2fdf8ef54684c4a8f89aa942332b7b": {"node_ids": ["3ede26b2-8282-4353-8bd1-fda2ac1abf91"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "a61305e895dc94bf0152d72eea1433bb572e0b86": {"node_ids": ["cde30a2a-91b4-4e6b-854b-8aad4f400cd3"], "metadata": {"filename": "event_handlers.md", "author": "LlamaIndex"}}, "bb8ca7a10d21a99bfdcb66e44a54b5590ea7bb41": {"node_ids": ["0b53e727-aeff-4b2e-b06f-e3597dbfba96"], "metadata": {"filename": "event_types.md", "author": "LlamaIndex"}}, "4362e7f9dfdc56d3830bcc4e5367a97e60d74aed": {"node_ids": ["d6377cdd-7455-42bc-9b16-f5ebb38a3240"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "fe3717d7aec5fae08bd7e851cc20aae92ca64176": {"node_ids": ["1fc025f3-9177-4077-a6c7-cbb755775973"], "metadata": {"filename": "span_handlers.md", "author": "LlamaIndex"}}, "13cd09784bfa61b79cb1f019da168bd1e44a8d14": {"node_ids": ["dd6beb6c-ca5d-4a06-9718-49c193f15902"], "metadata": {"filename": "span_types.md", "author": "LlamaIndex"}}, "84fe4dfe57f76f671e5e82ec192321cf34d88214": {"node_ids": ["663ecd12-646e-4170-b4ac-52a2ef0860d1"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "ac29761e696a968494cc57d506078b74d4bf0ac3": {"node_ids": ["6c427cf6-18a0-4f04-b628-3051bbf87814"], "metadata": {"filename": "ai21.md", "author": "LlamaIndex"}}, "bd36c295e59a76c0d9c684d05a537100c7db265c": {"node_ids": ["dcee966b-5097-47a9-b0f3-5553ccdd14e8"], "metadata": {"filename": "alephalpha.md", "author": "LlamaIndex"}}, "60c45f68d54000d2c5fd26fe505ac09ecec66b5c": {"node_ids": ["b2c7effe-379b-4d09-a7e7-cdb93de1d638"], "metadata": {"filename": "anthropic.md", "author": "LlamaIndex"}}, "40be432eecfe185b006b51294a8d122bfb6f7c40": {"node_ids": ["22c94b06-5d07-4f57-bbd1-59eb0fe004e3"], "metadata": {"filename": "anyscale.md", "author": "LlamaIndex"}}, "727474cb8d2c5790028569cc69e657261a108a3e": {"node_ids": ["8dc600b2-ea33-41f6-9969-55f1e73725bb"], "metadata": {"filename": "azure_inference.md", "author": "LlamaIndex"}}, "e0606ea5f2f66d4530d53b92a948204d63be5191": {"node_ids": ["48f67bf9-2f29-4346-8fc6-ca589c7af96b"], "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}}, "6bf2378d051a3e82f96a6766c5da3bbce6134336": {"node_ids": ["517aae2d-df08-4db3-93d0-5b4cb6b9e430"], "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}}, "80ba1182c081a6fe6940f9bbaaf2aca5b760404e": {"node_ids": ["9ea8545b-90f6-4e49-bc3f-4b1995b8f506"], "metadata": {"filename": "bedrock_converse.md", "author": "LlamaIndex"}}, "f678f2c6095eee77cbbbfa375d64206c78d8c429": {"node_ids": ["0495e7c7-a396-425d-bb5e-e6d22c2a3663"], "metadata": {"filename": "clarifai.md", "author": "LlamaIndex"}}, "bfac5a69f019d4aa9f7b2a5bc4f158e3a4606a80": {"node_ids": ["863211e7-3760-4a12-8bdc-6707bb3853ae"], "metadata": {"filename": "cleanlab.md", "author": "LlamaIndex"}}, "a33d292f6205ffcc5146e83258da344dda1f9979": {"node_ids": ["0e588059-7b2c-49c7-9ef6-b2c8d4a48d44"], "metadata": {"filename": "cohere.md", "author": "LlamaIndex"}}, "9099216a7e9e4b4cff2ed38112fc536a7cbd9b94": {"node_ids": ["9fba9c1a-3f8d-4d7d-9f47-a292f53d4bfe"], "metadata": {"filename": "custom_llm.md", "author": "LlamaIndex"}}, "067b1fbe41f1ee204ac838682d0eb3b863d81e10": {"node_ids": ["a633f210-fc68-4cc5-b377-def14456eee3"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "c419c3f753f6fd5a03cf5dddb504aefdef996253": {"node_ids": ["3a8f06a9-8d09-4135-8a6f-7000fea0b2c1"], "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}}, "46196e28cc71c6406f891b030e62d80ce99e78ad": {"node_ids": ["993e0164-e598-4137-b784-df6e0e466bc8"], "metadata": {"filename": "deepinfra.md", "author": "LlamaIndex"}}, "08da8bdded2b73a1a4b731dc30454f3e26d5bb0f": {"node_ids": ["941417a8-8510-44a8-b3bb-f0c3679e4063"], "metadata": {"filename": "everlyai.md", "author": "LlamaIndex"}}, "9007974d92c18f316e6d7ea38f76c2da7461efee": {"node_ids": ["0105fac0-6144-4707-80fd-1a39856c7e8e"], "metadata": {"filename": "fireworks.md", "author": "LlamaIndex"}}, "8091da51f2b03c30974f186babde0e8d1f62309e": {"node_ids": ["69ca6e5e-c50d-4c21-8983-7d53e18a9fdb"], "metadata": {"filename": "friendli.md", "author": "LlamaIndex"}}, "84eb198685c264eac4a9183f6906abb4b4a98d2f": {"node_ids": ["e3a75f41-494e-46d2-96dc-0ebc31122413"], "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}}, "3b2f298c2d1617c2a680f6dec501e9d20c41684f": {"node_ids": ["c24bb819-1750-4567-9490-8bf4cf63f437"], "metadata": {"filename": "gradient.md", "author": "LlamaIndex"}}, "85e1eac31aa22b348b3aa13155257f6e4c82aa89": {"node_ids": ["f4b433f4-3b52-4630-a576-a9366d08d3e7"], "metadata": {"filename": "groq.md", "author": "LlamaIndex"}}, "a03d1953b7c015f646be41a483caeb0e103b4220": {"node_ids": ["efe53e2e-0c99-46b9-829a-796469662c89"], "metadata": {"filename": "huggingface.md", "author": "LlamaIndex"}}, "33ee697f599b2805b5329823b7ba34a9306f4a2c": {"node_ids": ["fce12447-47a5-40e1-bc60-a8afb45a241b"], "metadata": {"filename": "huggingface_api.md", "author": "LlamaIndex"}}, "7c70aa23d57862aaf4bd7ddb3a866ca4cb8af63b": {"node_ids": ["22e8eba6-91a7-4bae-8ae7-a09ac3a7d9b6"], "metadata": {"filename": "ibm.md", "author": "LlamaIndex"}}, "9c714342127a6855cd46748d76458f77e6741782": {"node_ids": ["479911d1-05eb-48a2-be02-c30a1b9cc837"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "aec44b11d79026095c37b02c13d5ade2185dc5ee": {"node_ids": ["714a03b5-50dc-4a7c-a549-65db96cae3d5"], "metadata": {"filename": "ipex_llm.md", "author": "LlamaIndex"}}, "af2e07a58fa4e935374b3205f1f576c5d832719b": {"node_ids": ["d455bcf6-f55b-484e-88ab-f0b1fcdbaaa7"], "metadata": {"filename": "konko.md", "author": "LlamaIndex"}}, "bb43ece399d17d5eddd6503382136f699b9461bf": {"node_ids": ["5142deec-cc76-40fe-a90e-d34d90cbfb8a"], "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}}, "fa1ee2a2b37ce4b2e73d65e50158c8e9d339e622": {"node_ids": ["bc92ec55-ef92-47a8-b64b-9e6852fe6e19"], "metadata": {"filename": "litellm.md", "author": "LlamaIndex"}}, "fde85ca0e7c587a432998a27657959609b1c5fa1": {"node_ids": ["72a4b730-e316-4320-8de5-a8d5294c430c"], "metadata": {"filename": "llama_api.md", "author": "LlamaIndex"}}, "1e290fe18a4d1db7a4d05c2bc4f79dccf23e7fad": {"node_ids": ["e6888fa5-6c81-424b-a57c-e74ce5966f0c"], "metadata": {"filename": "llama_cpp.md", "author": "LlamaIndex"}}, "8ac0ff38a07d18a59329c21f89b4f7b11b0941cc": {"node_ids": ["e56f6282-462a-4228-b1c5-2a44adfb946f"], "metadata": {"filename": "llamafile.md", "author": "LlamaIndex"}}, "92a0edc07916d81956005ba900ee646d86c34d56": {"node_ids": ["352c3bca-963b-4745-b690-51ac62783d37"], "metadata": {"filename": "lmstudio.md", "author": "LlamaIndex"}}, "bac99c24902a76dd2a1886038b3993f197ad537f": {"node_ids": ["33b4fb12-7d0e-4f77-a122-e30a8f94f45d"], "metadata": {"filename": "localai.md", "author": "LlamaIndex"}}, "ab19c6d419976b8adc3acf90571e36b1307149c6": {"node_ids": ["23aa3911-c09a-46ad-8d63-615e28070d58"], "metadata": {"filename": "maritalk.md", "author": "LlamaIndex"}}, "0d95fa07ea28210ed8a2b7f855a2fa1201b3b726": {"node_ids": ["563f983a-ba41-4083-8aba-a9ee2b339c46"], "metadata": {"filename": "mistral_rs.md", "author": "LlamaIndex"}}, "72158a169d729fecc167992fdeca176a685829db": {"node_ids": ["d96a1780-8361-45b9-ac99-a8b712a20bfb"], "metadata": {"filename": "mistralai.md", "author": "LlamaIndex"}}, "961377d4aca7ea1d0274b49daceeaae10137dbdb": {"node_ids": ["99b420b8-48e9-4874-b1e9-6a78432ae508"], "metadata": {"filename": "mlx.md", "author": "LlamaIndex"}}, "74ec8e3d5142748a58917e5fa625fccc93cab8e1": {"node_ids": ["cdfb7534-bf82-4c32-9e59-ed870b05178e"], "metadata": {"filename": "modelscope.md", "author": "LlamaIndex"}}, "0a9a83e64b89a73c86f2f0eb737eb5ccda93a8dd": {"node_ids": ["e9c24c1c-10ee-404d-8d6a-048f5d76cef6"], "metadata": {"filename": "monsterapi.md", "author": "LlamaIndex"}}, "8d7dc7c977c1b86d5c7e7aaa12e348bd3826d9de": {"node_ids": ["d41bcda5-e843-4111-a25f-6f36666b5082"], "metadata": {"filename": "mymagic.md", "author": "LlamaIndex"}}, "4d04873a942949ae2ddf345444a0e1e470827d06": {"node_ids": ["a9f37820-8422-4414-b7bb-3f46e331ebf5"], "metadata": {"filename": "neutrino.md", "author": "LlamaIndex"}}, "86b9e5886e1f7db18e4d0088f116cde432c67bf3": {"node_ids": ["09cf7df7-e8d9-4545-88a2-4cf2b5ea6762"], "metadata": {"filename": "nvidia.md", "author": "LlamaIndex"}}, "2a4d13bcd0a88f856d45915747867fac3162dd3b": {"node_ids": ["1ba6c9e6-a9cb-4ca1-b500-0dc1912d9866"], "metadata": {"filename": "nvidia_tensorrt.md", "author": "LlamaIndex"}}, "3cbf177abd2f536b86f88adfc1ebbcbf5867ef4c": {"node_ids": ["11e7178f-4809-4162-b1fa-a4617c8973ff"], "metadata": {"filename": "nvidia_triton.md", "author": "LlamaIndex"}}, "83cc20a19d58e1fb839250caf678f884d418b133": {"node_ids": ["2d3dc81e-59d2-4d12-b8c2-dc5bed31a5bc"], "metadata": {"filename": "oci_genai.md", "author": "LlamaIndex"}}, "dd6b10e038ee5477eb07514c7f0d3124d04613ab": {"node_ids": ["96b88442-0862-4781-b1a1-034bed9d5549"], "metadata": {"filename": "octoai.md", "author": "LlamaIndex"}}, "7bf7d47d60f8e3ca542f394ec46075758a3cc7b0": {"node_ids": ["dea87571-c91c-4cee-9593-cae94753cce9"], "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}}, "ac8ac08abfbca7a02503de846750715a164f7c6c": {"node_ids": ["66d4a40d-93fe-44a3-837f-a7e7e4f5dd13"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "2673ecfdc7d512afd9b61e328c5e4b9ca3b5497d": {"node_ids": ["9baf0a05-12a0-4a67-8252-85938ad5445f"], "metadata": {"filename": "openai_like.md", "author": "LlamaIndex"}}, "0038e82c7610d1aa0d22c4ca4322efa093f37b26": {"node_ids": ["78a4cb67-e5a0-49fe-a9c6-d31ba8c2f592"], "metadata": {"filename": "openllm.md", "author": "LlamaIndex"}}, "43230982b1d7bbb45ab508fe3ffcae361c9d9bab": {"node_ids": ["7b2b6cd6-2d9d-45e8-aa86-6a2f623c7eeb"], "metadata": {"filename": "openrouter.md", "author": "LlamaIndex"}}, "132a3be5cd821cabd8083bbf35b25748ebdfe5af": {"node_ids": ["a514c6e4-70e4-4c90-ba1a-fec248f27079"], "metadata": {"filename": "openvino.md", "author": "LlamaIndex"}}, "0c14c627525dacf03d693369000d48797724d57b": {"node_ids": ["fad728c5-f084-48c6-a650-feb6f6fe8c8e"], "metadata": {"filename": "optimum_intel.md", "author": "LlamaIndex"}}, "126e9a8727a132922a9e82c9fd564ca5d7a297f9": {"node_ids": ["4c789ce4-5053-48a9-b928-f960d740a5dc"], "metadata": {"filename": "palm.md", "author": "LlamaIndex"}}, "f677afd214b768a748dfa165f4159427afebb17c": {"node_ids": ["83bfde80-25fd-408b-aed9-3c1f6d9e0abf"], "metadata": {"filename": "perplexity.md", "author": "LlamaIndex"}}, "6d8a321dd2c45b3da2e9b22615e0ba62ee463c51": {"node_ids": ["95615e5a-19c2-40e2-8dcc-2a306f97eff3"], "metadata": {"filename": "portkey.md", "author": "LlamaIndex"}}, "ca134577fd704889716fea8083359bec2079f0ad": {"node_ids": ["ebfce126-6a66-4521-93e7-d44b1cf8e72c"], "metadata": {"filename": "predibase.md", "author": "LlamaIndex"}}, "2fcd64a416bf306ad31365273975fb4bfa7fa772": {"node_ids": ["ef11c736-40be-4a43-91ba-ea7daf40966a"], "metadata": {"filename": "premai.md", "author": "LlamaIndex"}}, "62b0f1daf00496bca6c57f3a4e193800fea33e97": {"node_ids": ["80cab939-0992-4dc3-b6fe-056e46d23f4d"], "metadata": {"filename": "qianfan.md", "author": "LlamaIndex"}}, "edc6639228a4e90854f701cdf6ed6422e2931cf2": {"node_ids": ["8bb51e66-40fa-4283-bdff-48ebec9a9249"], "metadata": {"filename": "replicate.md", "author": "LlamaIndex"}}, "1419f258f124640cc51b5aaa5754a0f6386e5a14": {"node_ids": ["a23883cc-ac55-4a17-a5c8-883101b57751"], "metadata": {"filename": "rungpt.md", "author": "LlamaIndex"}}, "9c93fd2351851c49c51f7218ff4750026acff406": {"node_ids": ["6396bfb2-9e0e-453d-b449-51b86e6243de"], "metadata": {"filename": "sagemaker_endpoint.md", "author": "LlamaIndex"}}, "a3a223bee990164c007bdcf1b70051858ce282da": {"node_ids": ["de7812e3-a553-4e49-8675-903d900e128a"], "metadata": {"filename": "solar.md", "author": "LlamaIndex"}}, "afeb9ae9d3543bff4c154b584450c4be4f8e1564": {"node_ids": ["56cad37f-9c3c-49dd-84db-d77a4edd42b5"], "metadata": {"filename": "text_generation_inference.md", "author": "LlamaIndex"}}, "19e3fd8bdd5d4821302a98d0b8a460a2adbb36b0": {"node_ids": ["52b1c710-d85e-4689-ace0-b33b3b416155"], "metadata": {"filename": "together.md", "author": "LlamaIndex"}}, "b69773c0c9b6ab783a52a49b48ec0962228319ea": {"node_ids": ["33133590-6a40-4593-b23e-0c78ee0d5037"], "metadata": {"filename": "unify.md", "author": "LlamaIndex"}}, "ea36317231ea6768aacbfd2a370830e34183cafd": {"node_ids": ["26cf4073-6d41-466e-a641-a326aded4216"], "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}}, "4722b733cb5aa7afc09c222def98031a1d701d1f": {"node_ids": ["8da56310-a4c8-4b1b-b803-e7b677685dc8"], "metadata": {"filename": "vertex.md", "author": "LlamaIndex"}}, "be0e8188c363e3c62f0267893b30c0164cdb88fa": {"node_ids": ["d549ae49-1b01-48e6-bf48-a6bf23c9d66f"], "metadata": {"filename": "vllm.md", "author": "LlamaIndex"}}, "7fe8271834efeb9712ab116ffa91a382c6260fda": {"node_ids": ["529a7a27-ba42-44fc-86e4-8f02ea50ac45"], "metadata": {"filename": "xinference.md", "author": "LlamaIndex"}}, "4018d1f8c35f1fa7f5b2494ee5f17b37dc986fca": {"node_ids": ["b380951f-10b7-4382-a68f-fb506295f819"], "metadata": {"filename": "yi.md", "author": "LlamaIndex"}}, "b7c5ec6375565e856ac2fd7b14868b539db6143e": {"node_ids": ["7c1e2698-cef1-4276-947a-fbb5f3c6211a"], "metadata": {"filename": "you.md", "author": "LlamaIndex"}}, "334b77b0d28559d24d715a578fc97ad081fcfb06": {"node_ids": ["f1333a66-d22d-44b5-95ac-45d1b62433be"], "metadata": {"filename": "chat_memory_buffer.md", "author": "LlamaIndex"}}, "de1559ae3f3990a07293820122f02cbed8684e1d": {"node_ids": ["8e7c0f14-629a-4b1b-a6ad-daa408e2ddf6"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "9e85b90c5a9c1db9f27cff6aac94a8077c8453d2": {"node_ids": ["ab44f382-394d-455b-a062-27e1e011ecc8"], "metadata": {"filename": "simple_composable_memory.md", "author": "LlamaIndex"}}, "bb50c0ef256dcb735fcf1cc703156ca0530f4f9d": {"node_ids": ["d687452c-ed3e-4c48-bcb0-4061880e933f"], "metadata": {"filename": "vector_memory.md", "author": "LlamaIndex"}}, "9dc27895b184fa9134083f8d6b3aad12a3da19eb": {"node_ids": ["9ab4f797-4b56-494f-b13a-a4f210cb6165"], "metadata": {"filename": "anthropic.md", "author": "LlamaIndex"}}, "61718ea082b8d18c205059176989104d4132d4a7": {"node_ids": ["96956a5e-3c88-41be-ae2f-0827d05cc577"], "metadata": {"filename": "azure_openai.md", "author": "LlamaIndex"}}, "5a70e3d2abb81c179f96cc0c1a6990328e0737e7": {"node_ids": ["acb52e51-e830-4b0d-be1a-fadb7d01eb64"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "4922bce0a754ff907229da3158d4430e674bbc02": {"node_ids": ["0b3c67ba-3c50-404a-a6a2-4613b10ba823"], "metadata": {"filename": "gemini.md", "author": "LlamaIndex"}}, "6d7edc82e3bc649b8becc4edb7f36a4d746e574c": {"node_ids": ["f7f1a673-14df-42e2-b711-5a95bd017761"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "9a3c55fcc919c971743594e5c74d9c9f20f55d78": {"node_ids": ["a6882ace-88dc-4357-9326-ca745ba8a349"], "metadata": {"filename": "ollama.md", "author": "LlamaIndex"}}, "9c138dc98f30ea564d0fba065622327cc7848175": {"node_ids": ["1abc96da-fbf3-4dd8-82e5-95d330c75828"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "6b7526a553973a61217093dceabbfcf6c982636f": {"node_ids": ["d72f0545-0715-4b5f-9d26-beb263362b33"], "metadata": {"filename": "replicate.md", "author": "LlamaIndex"}}, "63eb306018fe2bc1d75b8ec7b8f9d33aa6b93974": {"node_ids": ["7a23d2c6-4272-4e0b-8d2c-3e6a3832fcec"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "222cd70b2d034e786c6ba620a876fbf064bceca3": {"node_ids": ["6ece38c9-4435-4c53-b09e-afd840c5d4a4"], "metadata": {"filename": "code.md", "author": "LlamaIndex"}}, "5245b851143d3e6d83b4e2cfdd31e63ba67fb480": {"node_ids": ["f945ec71-fb43-4b7e-9a63-7cd08d5a0413"], "metadata": {"filename": "hierarchical.md", "author": "LlamaIndex"}}, "11a977e9457d77fcc76cbcd4666ba8a6b9756b75": {"node_ids": ["4b626086-2cb4-42f6-9a8d-a80cb7b1f8b3"], "metadata": {"filename": "html.md", "author": "LlamaIndex"}}, "359df9a40dd5f8cd94cdcb0927fed773170ba8ac": {"node_ids": ["854eb623-3ff2-4c46-9312-52fcecf7efeb"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "eddf2015da7c28e2a652cbee446c3714f89a28de": {"node_ids": ["f680187a-708e-4285-94f0-67312eb056e7"], "metadata": {"filename": "json.md", "author": "LlamaIndex"}}, "6dcb2bc004da48ba773d9ab5668e90a06d8e2a05": {"node_ids": ["10226bc0-4dd3-42df-b919-0a11931dc0d0"], "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}}, "f8a8228790d2b60b2b24085c9c3aedb01d18015b": {"node_ids": ["1ac97b0f-7589-4b45-a0ef-be5bfe7080f0"], "metadata": {"filename": "markdown.md", "author": "LlamaIndex"}}, "242f3bd61a1e9d9f52c7cb84501c8a39cc99d9e6": {"node_ids": ["79eeeadc-a72e-4a07-a111-ca14ba5499f8"], "metadata": {"filename": "markdown_element.md", "author": "LlamaIndex"}}, "5dbf50ba0570f630b3103fd3794d9a40b193b07b": {"node_ids": ["2d48b9ae-356f-41cf-8cad-fba0a5988bf6"], "metadata": {"filename": "semantic_splitter.md", "author": "LlamaIndex"}}, "bcd78fda211589bb545008bef898bb179cd90802": {"node_ids": ["dff749fb-6540-4f30-ab48-49511f4a781b"], "metadata": {"filename": "sentence_splitter.md", "author": "LlamaIndex"}}, "a77fe2180cc9ed72c294f03c9ceefdf102ee0a9a": {"node_ids": ["cf7f0be4-5698-4346-8ee1-e0dc9c695bc8"], "metadata": {"filename": "sentence_window.md", "author": "LlamaIndex"}}, "10c0ff46005471baf20527ed7a2139009e198dc8": {"node_ids": ["434fd670-3021-4064-b036-ee9e21a84294"], "metadata": {"filename": "token_text_splitter.md", "author": "LlamaIndex"}}, "44da5da1974ed6d5e2e7ad01a11cc9c8c163bfd6": {"node_ids": ["19858768-2da0-4160-818b-b36d9fb6dc42"], "metadata": {"filename": "unstructured_element.md", "author": "LlamaIndex"}}, "e596dcae04b94f1219f10ff01a0d4dda85d65c23": {"node_ids": ["0ba191da-578b-4d9f-a573-a0e8f1f73b58"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "8d7b4d3cb217c7a4382bd05a5fc41e40443b6020": {"node_ids": ["b3f58b1e-cc4e-404c-bc92-1b3e483ebbee"], "metadata": {"filename": "guardrails.md", "author": "LlamaIndex"}}, "2239a557df3d96794997289b383d0dff8ef4b6af": {"node_ids": ["1ce33de7-ef99-40f4-b726-71923d469612"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "8392d65c118bab5f1976a39c729fb3627aadc376": {"node_ids": ["77dfa1bf-83fc-443a-8803-ff2a142b8310"], "metadata": {"filename": "langchain.md", "author": "LlamaIndex"}}, "76fbd330da97b2236addaafabad0be6dd1e8cc85": {"node_ids": ["42f4303f-6272-434c-a4d6-7599305f31ca"], "metadata": {"filename": "pydantic.md", "author": "LlamaIndex"}}, "355e093146539a8d2d9b9f88d141cc9affd56567": {"node_ids": ["207e2ea5-a65e-4766-b41c-5efe333cc87a"], "metadata": {"filename": "selection.md", "author": "LlamaIndex"}}, "c156a9e31df96226957ee03924deafa6da448422": {"node_ids": ["c72beb0f-2e03-4628-90ad-1a95f19ecad3"], "metadata": {"filename": "agent_search_retriever.md", "author": "LlamaIndex"}}, "5df6de2e4a9ceff5af92c252c4552e9aa64643e1": {"node_ids": ["43ba3f65-d918-4f7f-bac8-26f669e60632"], "metadata": {"filename": "agents_coa.md", "author": "LlamaIndex"}}, "ec5ce22e00e61d767e1213f66aa5f5c025ed2f26": {"node_ids": ["842b7a21-4728-481d-bda6-7ea689f6f6ee"], "metadata": {"filename": "agents_lats.md", "author": "LlamaIndex"}}, "49c5354ccc2221360119cf6af52e2404006d88c3": {"node_ids": ["bf39f0c8-4ed8-4f35-b13b-92f63a98cc6a"], "metadata": {"filename": "agents_llm_compiler.md", "author": "LlamaIndex"}}, "f8a632ee6fba14c0c09a70d073a08cae0745e0a8": {"node_ids": ["81028195-73b9-42e9-9859-cc3e9d601190"], "metadata": {"filename": "amazon_product_extraction.md", "author": "LlamaIndex"}}, "b3a9daca9d86a97cad8ae3d854de485ec09cf905": {"node_ids": ["c1ac3472-45f0-46ad-8045-4ad4b771bb5f"], "metadata": {"filename": "arize_phoenix_query_engine.md", "author": "LlamaIndex"}}, "3f8c92242b57cc153978707c0b98e1a65679b77e": {"node_ids": ["5d8258df-9b9c-46b8-a9c0-339c046bb167"], "metadata": {"filename": "auto_merging_retriever.md", "author": "LlamaIndex"}}, "6ef43db627cea1e9df3dd819c118aaa09f76f209": {"node_ids": ["b410e342-d907-4cb5-9718-9360a3d01879"], "metadata": {"filename": "chroma_autoretrieval.md", "author": "LlamaIndex"}}, "577b2976c1d90adffec2c7d29d78a30f39aebc1c": {"node_ids": ["65c701ec-ba3d-4b7c-a622-aeb114ef1769"], "metadata": {"filename": "code_hierarchy.md", "author": "LlamaIndex"}}, "3a20b99202e671b94db9daa35250050a58bdf410": {"node_ids": ["660cf76f-34e8-4f4c-8333-35c0b41ee94a"], "metadata": {"filename": "cogniswitch_agent.md", "author": "LlamaIndex"}}, "dd1d8d64a8aa84a17dc9c72e9ea12907f4d02ca5": {"node_ids": ["a9ce7903-25fc-470d-8aac-2c1ab52337b6"], "metadata": {"filename": "cohere_citation_chat.md", "author": "LlamaIndex"}}, "6822a18ab128390ac1943fea5c83cfdb1a93fdc2": {"node_ids": ["00b12f47-16f9-49fb-884d-99778331154d"], "metadata": {"filename": "corrective_rag.md", "author": "LlamaIndex"}}, "d4eb8fd2dfddd56081a971e4f40951f45954eb6e": {"node_ids": ["07a78963-2d62-46b7-b931-66314b7eee61"], "metadata": {"filename": "deeplake_deepmemory_retriever.md", "author": "LlamaIndex"}}, "ebd2bebcf4088e27b7f081df6567d34a835fa938": {"node_ids": ["4fd42729-bc86-4de0-ae90-491a69d9b6e1"], "metadata": {"filename": "deeplake_multimodal_retrieval.md", "author": "LlamaIndex"}}, "34787b868dd9bd5d50f5101598e0dd67e99d8395": {"node_ids": ["4296ad33-6fb2-4c25-9bfc-fe19a2fac19a"], "metadata": {"filename": "dense_x_retrieval.md", "author": "LlamaIndex"}}, "fcec0101e25a088294492ab195fd2bddc134b497": {"node_ids": ["b431b915-8cab-40da-a8cd-78a3328ca66a"], "metadata": {"filename": "diff_private_simple_dataset.md", "author": "LlamaIndex"}}, "6776675e95ff5c94832961ec1c75e684ece65c27": {"node_ids": ["76fee423-40c9-42fc-9335-8901ce2c7245"], "metadata": {"filename": "docugami_kg_rag.md", "author": "LlamaIndex"}}, "c6d1ff5356cd05a25657927ef40774300bd77d15": {"node_ids": ["1e865aac-551e-4c37-adb3-26be34e616b3"], "metadata": {"filename": "evaluator_benchmarker.md", "author": "LlamaIndex"}}, "41f52a108e151f281202c316417139ebcc5e2c44": {"node_ids": ["3cfcde5e-0ac5-4c7f-a99f-56a528adc113"], "metadata": {"filename": "finchat.md", "author": "LlamaIndex"}}, "374269e8fc062345b8f37106fd7a5ffdb0301795": {"node_ids": ["9027c120-8b25-45e2-9115-0e75bef8c779"], "metadata": {"filename": "fusion_retriever.md", "author": "LlamaIndex"}}, "7f9aa415323798473d6ef254f47ee596484744d7": {"node_ids": ["bf508a54-a7dc-479e-9805-6a89ab731400"], "metadata": {"filename": "fuzzy_citation.md", "author": "LlamaIndex"}}, "f8a45d4832262f95f27af6892508e9b71326f556": {"node_ids": ["e81ae16a-c957-41f4-864a-9c9e680858e0"], "metadata": {"filename": "gmail_openai_agent.md", "author": "LlamaIndex"}}, "102c575e7127dd460ae30ac75569a83842c89046": {"node_ids": ["994d9882-b73a-431b-b61b-8eda6c8ab49c"], "metadata": {"filename": "gradio_agent_chat.md", "author": "LlamaIndex"}}, "9f4ae18efb5a648f5fdeb9d4346aa28fef15baf3": {"node_ids": ["52ba0fe4-28a4-4a2d-b1f6-105618944b9f"], "metadata": {"filename": "gradio_react_agent_chatbot.md", "author": "LlamaIndex"}}, "2a71761f9031375e936cd14314061d98a6115cf7": {"node_ids": ["3d8c5adc-f88f-4392-8358-5487e4141bda"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "1ffa1d6b392b65a4a5407254ed369a3dbcd7caa0": {"node_ids": ["bb5dd8f5-75ac-494d-a23d-ac616771095d"], "metadata": {"filename": "infer_retrieve_rerank.md", "author": "LlamaIndex"}}, "b2e1759e9623871b0328cab4e8c48bee0d8cfff4": {"node_ids": ["d3a14fbe-49ed-429d-9914-8dc48b9e9612"], "metadata": {"filename": "koda_retriever.md", "author": "LlamaIndex"}}, "5f2819b90f26c11e3dfacecb6d9efb9e0889ab50": {"node_ids": ["82f82075-4388-41ba-8a12-032da77aa173"], "metadata": {"filename": "llama_dataset_metadata.md", "author": "LlamaIndex"}}, "c7e5dc0d40cb46cacc6d7cbb3e9f72f57721c33d": {"node_ids": ["de871bb0-e8da-4713-bbf5-fe63a11b7e1b"], "metadata": {"filename": "llama_guard_moderator.md", "author": "LlamaIndex"}}, "e7abb81699d52c5ae12ee43d557aa3f36c393d07": {"node_ids": ["d89ca5ae-bf3f-4a40-bd24-3c0cc5ea3808"], "metadata": {"filename": "llava_completion.md", "author": "LlamaIndex"}}, "a4d9d2d8cce00a7f749ee4913a6305f6caac60a9": {"node_ids": ["ad25ef65-2732-4e55-a908-2f9cd27274f0"], "metadata": {"filename": "mixture_of_agents.md", "author": "LlamaIndex"}}, "99cc50418ecc429fe30d364dd6e7ce6fbf295116": {"node_ids": ["11a99d8f-6a8e-4b68-bfcb-56888a6b711b"], "metadata": {"filename": "multi_document_agents.md", "author": "LlamaIndex"}}, "dd705e54bdb157be4215350464bd92cf5f64c710": {"node_ids": ["13b61079-ff1c-4634-aa4d-3a764ef5185e"], "metadata": {"filename": "multi_tenancy_rag.md", "author": "LlamaIndex"}}, "3ba372d831c3f7316ddd28bc5936d3778e61ac8b": {"node_ids": ["b1dcf25f-c16f-4212-b46e-e78c16f6eefc"], "metadata": {"filename": "multidoc_autoretrieval.md", "author": "LlamaIndex"}}, "483d1723e8905308f49f012eb62d0387f4df2546": {"node_ids": ["21096fd4-ab2a-4d12-9c48-af7c065abf0c"], "metadata": {"filename": "nebulagraph_query_engine.md", "author": "LlamaIndex"}}, "53923b68c11fc48b3fbce0a1e9de069ca90625a7": {"node_ids": ["3ba783bc-59d9-43d9-8164-730b31035e75"], "metadata": {"filename": "neo4j_query_engine.md", "author": "LlamaIndex"}}, "a146be0fad1c7a822210eb25eb8b0cd174bd0097": {"node_ids": ["55ac3ab4-57d8-478f-b8fc-cf0d8de3bbcc"], "metadata": {"filename": "node_parser_semantic_chunking.md", "author": "LlamaIndex"}}, "897252ff5f80ec87becbf86401829f3aa03e86b5": {"node_ids": ["e7c8cabe-649e-49ce-a5c8-771faf3954f5"], "metadata": {"filename": "ollama_query_engine.md", "author": "LlamaIndex"}}, "191d68103a91b70d758c4931b225a89aae60177f": {"node_ids": ["28f2fa26-91b5-45d0-be41-698c5f9bf8f0"], "metadata": {"filename": "panel_chatbot.md", "author": "LlamaIndex"}}, "ebeffed79f54516898b8a0a2ecdcac4053030f79": {"node_ids": ["09b8a0f4-1d4e-4c75-b239-36afeeb50a48"], "metadata": {"filename": "query_understanding_agent.md", "author": "LlamaIndex"}}, "5ece95c86fffa759a334a12d87c2c6e03bdff1ff": {"node_ids": ["78e15a8a-58eb-4b8d-a4b5-eb6da0ff70a7"], "metadata": {"filename": "raft_dataset.md", "author": "LlamaIndex"}}, "d84407d9f70b6ad3926aadeea5e775cb62337070": {"node_ids": ["d7f3246b-8a2d-45fb-8a2d-47731b25b226"], "metadata": {"filename": "rag_cli_local.md", "author": "LlamaIndex"}}, "bd4e6420a2433e453af8e20fbfe573ced6a2c7ca": {"node_ids": ["a02ac731-b8be-47bf-acb6-03efb06da6e2"], "metadata": {"filename": "rag_evaluator.md", "author": "LlamaIndex"}}, "35b966ee8ae82cf730ff1c8212d9b01adf75abb3": {"node_ids": ["961f8966-03d6-4add-8d92-42b11f21eb25"], "metadata": {"filename": "rag_fusion_query_pipeline.md", "author": "LlamaIndex"}}, "6366801e42c7d1f97045b3b65a43e33729708399": {"node_ids": ["1f96294b-d0ba-4ccd-8fec-8ecd7931b506"], "metadata": {"filename": "ragatouille_retriever.md", "author": "LlamaIndex"}}, "5cc4569ec83408df687be7cc36a6e6afa66f3539": {"node_ids": ["72b061f4-af16-4bd8-a0cc-98dc94a4fe64"], "metadata": {"filename": "raptor.md", "author": "LlamaIndex"}}, "43980882c896dd55a8628295e0b9bd5ba046e65a": {"node_ids": ["fcc6a650-5c23-4291-860c-2e6719c7a830"], "metadata": {"filename": "recursive_retriever.md", "author": "LlamaIndex"}}, "1fbef21a5d5e2e3be7c51ea575ef215cd73227c5": {"node_ids": ["97e7dc10-26ab-4ea6-b4ff-598ce541a17f"], "metadata": {"filename": "redis_ingestion_pipeline.md", "author": "LlamaIndex"}}, "caaa991886f6640c21314422751369aee412bbcc": {"node_ids": ["cae5264d-4b27-4982-afe4-97be32c2a1b1"], "metadata": {"filename": "resume_screener.md", "author": "LlamaIndex"}}, "48e1a7f49735b33c854551199a96842656f24bf0": {"node_ids": ["33879e2f-860b-4fc1-a611-8b045c079a32"], "metadata": {"filename": "retry_engine_weaviate.md", "author": "LlamaIndex"}}, "0ca0f9cc168dbc4800872c6d40639d52d5ac4dbe": {"node_ids": ["a1f40bbc-120c-4b8c-b171-c4953f948074"], "metadata": {"filename": "searchain.md", "author": "LlamaIndex"}}, "2da0a60c399c630ca77822b0896e2934efee23a3": {"node_ids": ["fe788021-75d0-492d-935a-658cb87c5801"], "metadata": {"filename": "secgpt.md", "author": "LlamaIndex"}}, "6a5ab91077ba8095195a3eb9e5d397d956e9bc30": {"node_ids": ["006b8f92-92e1-4c7d-9e48-9ad9eacd22ad"], "metadata": {"filename": "self_discover.md", "author": "LlamaIndex"}}, "2b5d9ffe10fbca2582c979bfba9ea241ab139e71": {"node_ids": ["451cb770-8928-412d-b5ee-d9d792847c9f"], "metadata": {"filename": "self_rag.md", "author": "LlamaIndex"}}, "505fe0393c3587376191355d48e5797d255ea0f0": {"node_ids": ["bf05d549-fc57-4378-a790-987897f5d55c"], "metadata": {"filename": "sentence_window_retriever.md", "author": "LlamaIndex"}}, "92db1008e836731e78a5ff2b84098429cc73373e": {"node_ids": ["692437fc-cd0f-4054-a2aa-5ec6be96af0e"], "metadata": {"filename": "snowflake_query_engine.md", "author": "LlamaIndex"}}, "0626c02421ca26ff8dafd9bca93b3c517ce7ebbc": {"node_ids": ["b473115d-ad39-40c4-b9fe-688ac3142da8"], "metadata": {"filename": "stock_market_data_query_engine.md", "author": "LlamaIndex"}}, "5d0968c1800e3b7e35971711ff8838bd5e5fca8d": {"node_ids": ["0b5ee20d-8f91-467b-a613-4f8145d6c9be"], "metadata": {"filename": "streamlit_chatbot.md", "author": "LlamaIndex"}}, "317767d77c9e7e5a8f99823e43382b8f34b8cb54": {"node_ids": ["a76fb8ba-842a-47ca-893d-b2683093500c"], "metadata": {"filename": "sub_question_weaviate.md", "author": "LlamaIndex"}}, "393009be341e9c0bbfb59c829898b97dcf94c556": {"node_ids": ["267db816-8b3e-4192-95ab-bb646179b69e"], "metadata": {"filename": "subdoc_summary.md", "author": "LlamaIndex"}}, "fe3f7f42272dcbf94f5d1e68a84a4e0e6c373c1c": {"node_ids": ["765614f0-6342-4e11-98dc-75302d0a23fe"], "metadata": {"filename": "tables.md", "author": "LlamaIndex"}}, "dfcf1ec300035f6570667eba8b3abc78fdee32cb": {"node_ids": ["a53a3d6e-bb7c-40de-ae0a-c636aee95fad"], "metadata": {"filename": "timescale_vector_autoretrieval.md", "author": "LlamaIndex"}}, "9c0fd9cbc734220f7844159e2fee6d6628697570": {"node_ids": ["c83a0ed9-4ce8-4cd9-9ff0-defb25ada350"], "metadata": {"filename": "trulens_eval_packs.md", "author": "LlamaIndex"}}, "639c67c3ab0789ab4fc4e2e0a98d9a4041924d64": {"node_ids": ["f907ce16-4df2-4d56-8d73-0588d713b382"], "metadata": {"filename": "vanna.md", "author": "LlamaIndex"}}, "0d3e44c8e90b949827671f8c6e4a2cbb7ab4b6e2": {"node_ids": ["bab2c2e3-b398-4405-a1c1-6137d61c9cf8"], "metadata": {"filename": "vectara_rag.md", "author": "LlamaIndex"}}, "c3f1227c67fab6106568d52eef84f3ae17c3213c": {"node_ids": ["f2548b2e-d510-41e8-a3b4-a7677d9c60ff"], "metadata": {"filename": "voyage_query_engine.md", "author": "LlamaIndex"}}, "95e5145a57017cc88d33a739d9f0b4f465ecde47": {"node_ids": ["ae34db84-f248-41db-b017-b6303d9bf975"], "metadata": {"filename": "zenguard.md", "author": "LlamaIndex"}}, "0ca6558db565f17fad49e07717d3fbd7e2139f93": {"node_ids": ["5ab6d12a-7b9b-456d-bf2b-8e042f07d346"], "metadata": {"filename": "zephyr_query_engine.md", "author": "LlamaIndex"}}, "30bc917e3f3f08b7c782248291f4a353cbecc0bf": {"node_ids": ["7a3d301f-d7ad-42a8-ad28-24f7f62f2876"], "metadata": {"filename": "NER_PII.md", "author": "LlamaIndex"}}, "38ff0adb180d9f5f605839f3bec5c4c1b22cecfc": {"node_ids": ["07fc7114-38cf-448e-9cfd-d6223d959c99"], "metadata": {"filename": "PII.md", "author": "LlamaIndex"}}, "a8c2cac0dccbdc3b584f7b1e29bc30107f1a3710": {"node_ids": ["304d9cb9-aa5a-4116-ae94-361bf6bcac37"], "metadata": {"filename": "auto_prev_next.md", "author": "LlamaIndex"}}, "6819b3bd28808e56ec8ee4a46896530247e22509": {"node_ids": ["0230e0ba-8103-4241-b8fb-61bb853b8d83"], "metadata": {"filename": "cohere_rerank.md", "author": "LlamaIndex"}}, "90548f00f95c74fce851a8cbdf848bf8a70e367f": {"node_ids": ["f009f7b5-d927-4bc3-86cc-c155b5403ea7"], "metadata": {"filename": "colbert_rerank.md", "author": "LlamaIndex"}}, "396de306fb36a381af379e5db478bafc88b9de75": {"node_ids": ["79a8f70a-c18c-4db6-a1b8-6de92127c12c"], "metadata": {"filename": "dashscope_rerank.md", "author": "LlamaIndex"}}, "41161917f7bf81be1efce7133c5a39261a992b02": {"node_ids": ["dac17d90-b4a2-46f3-8af8-8c57bda3b7ad"], "metadata": {"filename": "embedding_recency.md", "author": "LlamaIndex"}}, "ac0c6c932ceb663de686e2bfe6a085b09787c252": {"node_ids": ["62a10a11-7116-4394-b48f-bfda3780eb62"], "metadata": {"filename": "fixed_recency.md", "author": "LlamaIndex"}}, "096dff23fc3d7557391a9aa53d656f5571b30194": {"node_ids": ["e793cbf2-5df7-4b52-ae61-93525c823cf7"], "metadata": {"filename": "flag_embedding_reranker.md", "author": "LlamaIndex"}}, "2fafdec5d9459a0179b06d42b14398a8a74d4dc6": {"node_ids": ["275cae37-8ee4-418b-965f-bab2faa7e611"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "d4315961ec405da54ef65d95bb4458e691bf90ea": {"node_ids": ["6a1f1ec9-6237-4b92-af67-7832588e4268"], "metadata": {"filename": "jinaai_rerank.md", "author": "LlamaIndex"}}, "5b1669f1c6eea89badbdf01b0460cbef0c37946b": {"node_ids": ["0ae1eed6-b91f-4cef-a471-5ec0c775342b"], "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}}, "5c5c4603b86f39be53a727ec34543d6a40e2f54b": {"node_ids": ["2e0ce863-780f-47a7-842a-3d5a48a762b6"], "metadata": {"filename": "llm_rerank.md", "author": "LlamaIndex"}}, "1f28d351e804a5c6becab81f09aa50f5a39ac30d": {"node_ids": ["16941e8f-e5ea-4f4b-8e22-62abda57c0a3"], "metadata": {"filename": "long_context_reorder.md", "author": "LlamaIndex"}}, "e0987f232209a6c1b5d0496e877848efd3b84631": {"node_ids": ["d15ef93e-093d-4711-b9de-18e8b060bb39"], "metadata": {"filename": "longllmlingua.md", "author": "LlamaIndex"}}, "e2e7f1f832f93a12fedb36f3583a5072bdcb4166": {"node_ids": ["a1c92599-a62f-47cf-bab1-195e90e37112"], "metadata": {"filename": "metadata_replacement.md", "author": "LlamaIndex"}}, "a2809da0746fbee9aa15b379e0246f83ff324458": {"node_ids": ["664a460e-579d-45a8-8c02-f6934e2e088d"], "metadata": {"filename": "mixedbreadai_rerank.md", "author": "LlamaIndex"}}, "3758a15fdd4303efb67249f78ca11675f390e915": {"node_ids": ["bfbe5124-247a-44c0-8b53-994a90217100"], "metadata": {"filename": "nvidia_rerank.md", "author": "LlamaIndex"}}, "67d7504d32178700091ac0a81a081d038fb60146": {"node_ids": ["7da63f46-124c-4647-a330-3797d9d79508"], "metadata": {"filename": "openvino_rerank.md", "author": "LlamaIndex"}}, "3d11274cc985bb890af3e3fdf8bf7eb05b85346a": {"node_ids": ["31931501-8218-42b4-9c80-af75e6aaabfb"], "metadata": {"filename": "presidio.md", "author": "LlamaIndex"}}, "1aae1c50883e9114c85ea1dc91f90015e17d49a6": {"node_ids": ["f6af0580-621e-4b86-bfac-f391f76cff44"], "metadata": {"filename": "prev_next.md", "author": "LlamaIndex"}}, "c02591f3c6c772d32e396f9b377dd5fd9fc29af5": {"node_ids": ["c744f41a-ff4e-4cfa-b470-52e44f542b13"], "metadata": {"filename": "rankgpt_rerank.md", "author": "LlamaIndex"}}, "cff89b6afdc50d1beca0abdeaa8efd803c498557": {"node_ids": ["60387b50-0b6d-4c4f-9dab-5a3c41d90218"], "metadata": {"filename": "rankllm_rerank.md", "author": "LlamaIndex"}}, "233cf8942bf7115ba614f20861b2a976e408335a": {"node_ids": ["7df224f9-8a9f-411b-82ed-53014aaa8d2a"], "metadata": {"filename": "sbert_rerank.md", "author": "LlamaIndex"}}, "f511b9ddf67a6125bbedcfa829825980f73e834c": {"node_ids": ["f7915051-023b-461a-9dc8-46023991a991"], "metadata": {"filename": "sentence_optimizer.md", "author": "LlamaIndex"}}, "7afbbb4a6aba7af099b56895b45d02dc26c30a0d": {"node_ids": ["c75ee29e-efb6-4be3-99a3-409f6a7194f4"], "metadata": {"filename": "similarity.md", "author": "LlamaIndex"}}, "4b002c919739200212d11beca3c43a08d99d38cd": {"node_ids": ["63bb391c-035b-4810-9249-adb329d4997a"], "metadata": {"filename": "time_weighted.md", "author": "LlamaIndex"}}, "4b3470d23cb1925a902db320efc9ce2d1e2b1bf1": {"node_ids": ["d6f03d5f-490c-48d0-b654-dde3743cb3e5"], "metadata": {"filename": "voyageai_rerank.md", "author": "LlamaIndex"}}, "d8eb4d213ff1031a254b1e61037194b87ae8f230": {"node_ids": ["f0e722ac-f530-428d-8810-010481a2d120"], "metadata": {"filename": "evaporate.md", "author": "LlamaIndex"}}, "4ce5ebc5d202901163ba11456068515bc34513e8": {"node_ids": ["76b6ef14-0035-45c1-bc27-66be9771fd08"], "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}}, "1469cb0a57f85b992238492309457ba8516133cb": {"node_ids": ["7d20a048-3bb2-49b6-99db-a815d180c8d5"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "3506582a83f3ef3444a8aa9b01518d31efacdf40": {"node_ids": ["659cb26f-1727-4ce5-a6f7-5be747dd4a20"], "metadata": {"filename": "llm_text_completion.md", "author": "LlamaIndex"}}, "c9a7e92c697e3eef575351a643d0b1f0cca4496f": {"node_ids": ["016e0094-7c3f-4edd-8738-435a8b46c7d0"], "metadata": {"filename": "lmformatenforcer.md", "author": "LlamaIndex"}}, "28822ec547c0d60887cd4bb270dc2340bd248dd0": {"node_ids": ["2ba9d7d0-8580-46c5-ab9a-3bc794167e58"], "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}}, "41843a9d39987c7b7d5276f021d4ca3bf8e492a5": {"node_ids": ["85c43f39-f3d7-42c1-b867-eab9db40f156"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "e23da97edf8d96d827ce489d54c7d2620d630b08": {"node_ids": ["94d94985-50de-4244-9a00-97a7e4b6497a"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "d36ceaf20d1642bdd253c3dfb0d3640d5ea45735": {"node_ids": ["c33e30c4-09cf-4fc1-9ee8-ab0edff53569"], "metadata": {"filename": "FLARE.md", "author": "LlamaIndex"}}, "ebf3f775909cbd58cb433d3f3a0cd88bfd2068d9": {"node_ids": ["a41a1e09-d9e2-44fe-8614-00099bbeea96"], "metadata": {"filename": "JSONalayze.md", "author": "LlamaIndex"}}, "ece1e7b851ab4126a11ac2803f215101664111dd": {"node_ids": ["b9065d79-572d-4382-be99-d1d35cc2201b"], "metadata": {"filename": "NL_SQL_table.md", "author": "LlamaIndex"}}, "ee9fbeafb433e6bdeaec2ef98e03798f79d9c08a": {"node_ids": ["272ae961-e2ac-4469-8e15-1d697dad1cf9"], "metadata": {"filename": "PGVector_SQL.md", "author": "LlamaIndex"}}, "b99b574d83074753a9736ccf866d23821139d13a": {"node_ids": ["53ee1945-333d-4843-a3eb-a5a8d0b7c4cb"], "metadata": {"filename": "SQL_join.md", "author": "LlamaIndex"}}, "dde90532c7b6dbde48e037b3eef12ac2c224451b": {"node_ids": ["653382e2-67b2-42f7-8c16-f48c2f68a337"], "metadata": {"filename": "SQL_table_retriever.md", "author": "LlamaIndex"}}, "5f0371a3f9adf27ed86e5242f8cc934d4de4095d": {"node_ids": ["ddc7cc3e-1486-4b10-9db7-fde345e53d57"], "metadata": {"filename": "citation.md", "author": "LlamaIndex"}}, "4ba253eff24b7f8bca06da7da3e98d8e965296ef": {"node_ids": ["af6e710a-ce7c-48b9-bf1f-1dcdbc57d268"], "metadata": {"filename": "cogniswitch.md", "author": "LlamaIndex"}}, "a4f4ebf888e72a6133ebdf9e370a17ccceeac080": {"node_ids": ["e8ee322c-9e03-4eb5-9c78-10262d6fc473"], "metadata": {"filename": "custom.md", "author": "LlamaIndex"}}, "1f8385d02e134bb2edfa5f250c07e8124fad644b": {"node_ids": ["90d9c522-f255-4700-87de-334dbaa8708c"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "00bc83d779827dc0ccf7504d08b95007ec09e22f": {"node_ids": ["3c7eb89a-1ac9-4118-8f24-53bd7ed42d2c"], "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}}, "623bd8e5d53920a5d151cd4cd85cf2c1a2baf1e8": {"node_ids": ["3aa1d73c-517a-44c2-98d1-b5dc06752c66"], "metadata": {"filename": "multi_step.md", "author": "LlamaIndex"}}, "34c9740e3973eefd944dac92c32747a265ad3798": {"node_ids": ["84b63c72-e310-4271-98cd-ac36c19db6dd"], "metadata": {"filename": "pandas.md", "author": "LlamaIndex"}}, "8bee7cde3d96c38985640d695144c72f19ebd3fa": {"node_ids": ["9e63e9c7-697e-4e35-87f4-dd6d00f5c4b4"], "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}}, "3111f2357803513407a8e89d48670f4b7df56907": {"node_ids": ["d5d3d634-83fc-4ff7-85ce-a0157c97cf31"], "metadata": {"filename": "retriever_router.md", "author": "LlamaIndex"}}, "4e44d859d84869444c03dbd2cef1d63cbf9c43d4": {"node_ids": ["41ae5866-59ac-45c7-bb65-7ec85a82356b"], "metadata": {"filename": "retry.md", "author": "LlamaIndex"}}, "463c9401f53097c4b4b188c031506ea053dc6301": {"node_ids": ["c090e9dc-4aaa-4f08-a8b8-612db0c27cd8"], "metadata": {"filename": "router.md", "author": "LlamaIndex"}}, "c26f152ab67d2707afe697c4679a064ab66556e6": {"node_ids": ["3a73b319-361c-4e14-9a27-a7bee4dc84d0"], "metadata": {"filename": "simple_multi_modal.md", "author": "LlamaIndex"}}, "0809d75107799494fb8c16972d51f970de76ad30": {"node_ids": ["6723f58d-f951-4a2b-bb2a-60c513660031"], "metadata": {"filename": "sub_question.md", "author": "LlamaIndex"}}, "42199ba9aba0cca4a2a26542cb765283597105ed": {"node_ids": ["c439405e-e16e-4d61-ba97-e7feafb57390"], "metadata": {"filename": "tool_retriever_router.md", "author": "LlamaIndex"}}, "85e8b5587b6f31f9b448727975a1e3ff36833238": {"node_ids": ["002c8a0c-48d0-4fac-a87f-9cb0794f0dfa"], "metadata": {"filename": "transform.md", "author": "LlamaIndex"}}, "40dbe0cc13f191c976775c15adf916913fcaacf8": {"node_ids": ["9c61c742-2d76-4c8d-9c3d-299de43a5ef9"], "metadata": {"filename": "agent.md", "author": "LlamaIndex"}}, "00bfc9d550dc8385671f78f6db81e62b3870dafc": {"node_ids": ["c47dc2db-ecc6-4ba6-b218-10d8882f7108"], "metadata": {"filename": "arg_pack.md", "author": "LlamaIndex"}}, "6f218445a84916cda4ab84d2501af92ef2624fcd": {"node_ids": ["5acf6bc9-9f72-4937-843d-5f676e268df8"], "metadata": {"filename": "custom.md", "author": "LlamaIndex"}}, "d1a2a171beb24aac21c623d9fa3607ea8b292681": {"node_ids": ["f114bf48-435b-46ba-99f3-0172fa3a043b"], "metadata": {"filename": "function.md", "author": "LlamaIndex"}}, "8c4b9b3d5709cbd66b0d1e93f5a51c1aa281c41d": {"node_ids": ["72e55be4-ff14-46b1-86a2-40c762ac08c5"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "970cafdcfa65c29c86e8a91628282045b764f1d7": {"node_ids": ["e907f270-0da5-4deb-b910-c156368a2211"], "metadata": {"filename": "input.md", "author": "LlamaIndex"}}, "b93b7ef6f84fb060e458c7721cf967c6fe28a2cc": {"node_ids": ["29083378-e353-4980-bd49-b957fd91d837"], "metadata": {"filename": "llm.md", "author": "LlamaIndex"}}, "bb386af040fd28f68ab307e651d57a91ca9b18a0": {"node_ids": ["abb1cebb-532d-4000-b868-8563f6d9435f"], "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}}, "27d4f2754510f1ad3306d9c8469add6501eceeb6": {"node_ids": ["d07278b6-16e7-4214-a27d-44a2f889f0b2"], "metadata": {"filename": "object.md", "author": "LlamaIndex"}}, "a06423a8e321339a51d190da81eb3b294a449648": {"node_ids": ["3b94bf0f-0dd7-4086-afdd-aa4a618d0e84"], "metadata": {"filename": "output_parser.md", "author": "LlamaIndex"}}, "b967bc936db6ab9d3c0f3e6f2869bbb27b0ef57c": {"node_ids": ["44930d26-079d-4b07-a272-c132e4094f08"], "metadata": {"filename": "postprocessor.md", "author": "LlamaIndex"}}, "eceae1a86e8aa3288cc1a43800c351434f59b415": {"node_ids": ["7c48762a-3619-47e1-8226-e7c04ff637b4"], "metadata": {"filename": "prompt.md", "author": "LlamaIndex"}}, "6a99bbe6aca306dee217cee196ac8d2a27d0194c": {"node_ids": ["21f1f5a4-513c-4bf4-aeff-046d068e00bf"], "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}}, "a7aedc2af5dccf5e23d70755a09601a8f3b5d7a9": {"node_ids": ["a11074fa-eb19-437f-8b48-72709f9ff8f2"], "metadata": {"filename": "query_transform.md", "author": "LlamaIndex"}}, "1c026d42ccdda4de172bd6724ba1ab8a6e9126b5": {"node_ids": ["61446d34-5524-4d32-8dea-6623a4f8ef5f"], "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}}, "34339929e65ed6d22a4daf2f208094c329b5e136": {"node_ids": ["e2c4037a-4d11-4b22-9e58-21648b51f6dd"], "metadata": {"filename": "router.md", "author": "LlamaIndex"}}, "8df8dffc9f42e7841f6780d58b35c5aeb8399227": {"node_ids": ["be9861f1-e458-4a92-8aca-d131b796f45c"], "metadata": {"filename": "synthesizer.md", "author": "LlamaIndex"}}, "416a1c1201b91f1a634947ecf07be73d4b894bc9": {"node_ids": ["369f8b4b-1d27-41be-910f-80713dfe4837"], "metadata": {"filename": "tool_runner.md", "author": "LlamaIndex"}}, "8cc32ade63f68f6128d28564aae817b8c8211e4e": {"node_ids": ["d35002ca-f2fb-48d7-973a-55a83190fc6f"], "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}}, "3f982a315aced401f13bba71aad1b8ed7d0bd5b9": {"node_ids": ["2bbf90e8-b1a3-41b1-af00-a3a2f713c975"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "933712e1d0b9dbaf6149d80991fb7b5dd3b476a6": {"node_ids": ["a28cec98-aa6d-42ef-91e1-20052fc19e4c"], "metadata": {"filename": "llm_question_gen.md", "author": "LlamaIndex"}}, "2545f995aecf6fcf4928aee098f8afe13b54888d": {"node_ids": ["95d37e09-d156-4253-9ccd-a7db0eb3d477"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "538c2be6d3571518441d74d0ae2fad71ed7ab499": {"node_ids": ["174c791b-1dbc-49c4-89a5-0aa66bed6ca5"], "metadata": {"filename": "agent_search.md", "author": "LlamaIndex"}}, "8e7a9a3fa93ba4f04c3efc76b6d53de8330b159f": {"node_ids": ["e5d50b55-3a0a-4de0-ab90-bd9e6f56e2c2"], "metadata": {"filename": "airbyte_cdk.md", "author": "LlamaIndex"}}, "35babd83be1a7c82971424b2c56c07a686cb265f": {"node_ids": ["640ad0c3-708b-4ac0-a90c-41a76ad5ad60"], "metadata": {"filename": "airbyte_gong.md", "author": "LlamaIndex"}}, "9f8a67c04dec932bd7f9bf2e0865d7c9cace065a": {"node_ids": ["ab894481-83c6-47a8-bed0-2ccd98a5564e"], "metadata": {"filename": "airbyte_hubspot.md", "author": "LlamaIndex"}}, "dc6ad63a8fdb1acf9ca5633a804c96bb185871cd": {"node_ids": ["f7ce1ff6-02f0-4e89-9c02-3dd8d1127cfe"], "metadata": {"filename": "airbyte_salesforce.md", "author": "LlamaIndex"}}, "1f97f4831dd726aa459b6ad3e4918ad5b235bc74": {"node_ids": ["c0784f31-ad9f-408e-be2c-8b865d59ec71"], "metadata": {"filename": "airbyte_shopify.md", "author": "LlamaIndex"}}, "4219aacd5917410ada7578371c2438fc75044ae7": {"node_ids": ["b5894634-07ca-4b94-aa9f-4c1d2656409e"], "metadata": {"filename": "airbyte_stripe.md", "author": "LlamaIndex"}}, "9b0f086bcb57c5d72780c8577ad9cb4cca0aa91b": {"node_ids": ["d6522a31-8585-4fdc-bc26-10f905e99217"], "metadata": {"filename": "airbyte_typeform.md", "author": "LlamaIndex"}}, "73931cf84544f3d0ad1e93e5c2ded7b3fea84d35": {"node_ids": ["a53ba005-6004-4e98-bb39-09e445a041a0"], "metadata": {"filename": "airbyte_zendesk_support.md", "author": "LlamaIndex"}}, "d30b811d03d45da9374a82e644f8bf366128c8be": {"node_ids": ["183e17e5-4445-464b-9ee9-eb86ba7b7849"], "metadata": {"filename": "airtable.md", "author": "LlamaIndex"}}, "c87419376264fe68e6822fc06605da7dfd799a2f": {"node_ids": ["67876a23-b885-4ad6-bd8d-d7266d9aaf85"], "metadata": {"filename": "apify.md", "author": "LlamaIndex"}}, "08b0614909cd1811603a919ec2a8e032aa1d6e26": {"node_ids": ["487d1587-42e8-4de9-abc5-b1b63fe3b023"], "metadata": {"filename": "arango_db.md", "author": "LlamaIndex"}}, "48d0f51a27058ddbe8230a955175d2d6354077b2": {"node_ids": ["32597eac-33c1-40f0-8032-1d98be800345", "68d34a40-00e5-4cd3-b151-57c0b93858ed"], "metadata": {"filename": "papers.md", "author": "LlamaIndex"}}, "eb9d97ae9b9f049307b05ae99655a52f8bf5d21b": {"node_ids": ["cfdc0185-83f1-43b4-a9b3-33781bc68ae1"], "metadata": {"filename": "asana.md", "author": "LlamaIndex"}}, "a795673c5a71f4f4b4d089cf08871330b34cbeb5": {"node_ids": ["0ff5aae7-1b9c-43bf-bcfb-e5913152153f"], "metadata": {"filename": "assemblyai.md", "author": "LlamaIndex"}}, "8228cdd00f4751c0846e191773965c648940be90": {"node_ids": ["ecdd2a2d-801d-4588-8f56-59979a5ad5ef"], "metadata": {"filename": "astra_db.md", "author": "LlamaIndex"}}, "9e725e989a7664f0830c50a86b13829393b2065e": {"node_ids": ["13594da6-17d7-4bc7-9bfd-076888d5b71a"], "metadata": {"filename": "athena.md", "author": "LlamaIndex"}}, "9438e50dbe92161cf658710ab70d72ac14791148": {"node_ids": ["d40925ec-542b-4eac-bac6-5d0306603efc"], "metadata": {"filename": "awadb.md", "author": "LlamaIndex"}}, "65e229d914eaaeafee90b74aae91502f96f48eb5": {"node_ids": ["b8af8e66-d204-41ad-bb4e-a0e4076dd3c1"], "metadata": {"filename": "azcognitive_search.md", "author": "LlamaIndex"}}, "55d139eb9f2b47278624b8779915f2bda2dc6292": {"node_ids": ["b9aad2c6-2128-4b6c-9e6e-f5a15dade429"], "metadata": {"filename": "azstorage_blob.md", "author": "LlamaIndex"}}, "968e69eea53249b40a963414dafaf630bf731fb4": {"node_ids": ["ed4ff34e-306c-4cd7-bdef-efda21b31137"], "metadata": {"filename": "azure_devops.md", "author": "LlamaIndex"}}, "a3f581c0e175c5d94f3d3190860e2dc718d8b4e7": {"node_ids": ["d7b3f4b3-d591-40bc-bce4-c1ea9c352404"], "metadata": {"filename": "bagel.md", "author": "LlamaIndex"}}, "40ec2cdb9e53bc33ccf7d0b5713ce71c5f0a1ab2": {"node_ids": ["be2481e3-940e-41f0-a3d1-f509a8d6874b"], "metadata": {"filename": "bilibili.md", "author": "LlamaIndex"}}, "4ef2eba5f69e07e083e3bede7802ddd7dd82c3e7": {"node_ids": ["bef5a0fb-8b49-48d3-819e-a3974fd701e7"], "metadata": {"filename": "bitbucket.md", "author": "LlamaIndex"}}, "5c961591ca4316d537c54ae366b105188209f93a": {"node_ids": ["9b15527d-2f7f-4588-a98f-54f3d83fdbd1"], "metadata": {"filename": "boarddocs.md", "author": "LlamaIndex"}}, "5de35fdc046688c1ecc4dc6844bdbfff2f7dd0ed": {"node_ids": ["b46c8ca4-db84-4131-aa7e-262eb17a1072"], "metadata": {"filename": "box.md", "author": "LlamaIndex"}}, "7bfde8aaf3caadc721be5257d4353637b6a925e5": {"node_ids": ["86f9a39e-e126-4ae6-a7ca-f5f637b48ad1"], "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}}, "95f9ac48e5ab26a68d0419e7e512eb9e56f116bd": {"node_ids": ["884d5999-3bd9-41bf-8ee6-62a669ba60d7"], "metadata": {"filename": "chroma.md", "author": "LlamaIndex"}}, "d3b2ab411b4b812a9e117b7408bf55e2e5498abd": {"node_ids": ["61b9f278-0538-41da-8720-44e039e3c008"], "metadata": {"filename": "clickhouse.md", "author": "LlamaIndex"}}, "28cca30fc708b5aa6dd3ef2352afbd6a66f4ab94": {"node_ids": ["99424056-e37c-4744-8c5c-c31c994d1234"], "metadata": {"filename": "confluence.md", "author": "LlamaIndex"}}, "0edf0980ee6fbadb26c2da2d97d01712898ffe3b": {"node_ids": ["0a543bf0-d4d5-41d0-a8e9-2932339f948c"], "metadata": {"filename": "couchbase.md", "author": "LlamaIndex"}}, "708c39b8c2d11d81d2feaab86b27262bc9451cc7": {"node_ids": ["22d163a0-74a7-4438-b7be-9cf478136169"], "metadata": {"filename": "couchdb.md", "author": "LlamaIndex"}}, "9deb167eca77eed0f121233a821264fc4b5dc9ef": {"node_ids": ["6aaad9ed-b8fe-422d-8042-914ebd9057c6"], "metadata": {"filename": "dad_jokes.md", "author": "LlamaIndex"}}, "111d9451854da0e93fa67400322cd3b49660e216": {"node_ids": ["6235c7f8-ef2d-44c4-8ace-691cccd24c17"], "metadata": {"filename": "dashscope.md", "author": "LlamaIndex"}}, "5703a1f04551ed71420c36a24482d9056274a71b": {"node_ids": ["e8186450-cbbe-4aa7-80ea-301eda1f331d"], "metadata": {"filename": "dashvector.md", "author": "LlamaIndex"}}, "e65818ed0ceaf046428028d829514d5c444ea6e4": {"node_ids": ["52371d23-bf75-41c4-b8bc-323624cfe161"], "metadata": {"filename": "database.md", "author": "LlamaIndex"}}, "20d04acf6ee1912b7349417483d003f68c9669d6": {"node_ids": ["11690fe9-013a-48b6-99f5-808b88573a71"], "metadata": {"filename": "deeplake.md", "author": "LlamaIndex"}}, "af7601407b5960fe3c30a3f924fed10d5f9b0026": {"node_ids": ["df76d07d-ebe8-45e1-8a26-023ca1f300ab"], "metadata": {"filename": "discord.md", "author": "LlamaIndex"}}, "3fda42eaa673ff97b26a1e7a0b8f165665a24875": {"node_ids": ["7bda83c5-87e2-4603-a775-6abe3b931f7b"], "metadata": {"filename": "docstring_walker.md", "author": "LlamaIndex"}}, "56678f5674fcdb8e57d41ea4cde22589cca58404": {"node_ids": ["ebc91640-fb5c-4f3d-b624-8ac75c0f8a2d"], "metadata": {"filename": "docugami.md", "author": "LlamaIndex"}}, "a35b120c851acd7e9ddf9a21638a9d9420db70b5": {"node_ids": ["f7cbea75-8cae-4445-a055-d6f763b86a22"], "metadata": {"filename": "earnings_call_transcript.md", "author": "LlamaIndex"}}, "1b23649d2c29d5758bec9ec05d75035849e5d857": {"node_ids": ["3ee98647-bb51-4cae-ae27-efb6a9e9831c"], "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}}, "5f1cf93672363b8d2ca62c2c44249419dc4cd41b": {"node_ids": ["10b1f377-3be6-45aa-a356-356ea64f4460"], "metadata": {"filename": "faiss.md", "author": "LlamaIndex"}}, "073c0d1e0d48079d6a431f59105f9fe0692c129f": {"node_ids": ["0b0b843d-557c-4c9b-9b10-cca4f7960128"], "metadata": {"filename": "feedly_rss.md", "author": "LlamaIndex"}}, "d1f3b9613f634c5a7ca44acba9580820e4bf6384": {"node_ids": ["8916022c-2a20-4b7d-b036-ad836c93ebe6"], "metadata": {"filename": "feishu_docs.md", "author": "LlamaIndex"}}, "fe649b280ad28ebaa911b5f19160c46aab707892": {"node_ids": ["1f7b80ac-8520-4bd9-89e7-e3fb55036895"], "metadata": {"filename": "feishu_wiki.md", "author": "LlamaIndex"}}, "585f57f1747011659453ae04fc61bbeaba9a9e86": {"node_ids": ["b63572d9-8359-43b2-be38-6ab88b7191da"], "metadata": {"filename": "file.md", "author": "LlamaIndex"}}, "93a0fd585f76e379efa0f711177fec7ba0441a88": {"node_ids": ["1667c08e-07f6-453d-80e1-d5eea464cb36"], "metadata": {"filename": "firebase_realtimedb.md", "author": "LlamaIndex"}}, "af0889ad3ceef79b60a37d986f7ff5cd9812ac9a": {"node_ids": ["863c1935-d225-4bc5-a98b-50fe1e4f720c"], "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}}, "c3b28830cb8554237936a9c82b51a2450739512a": {"node_ids": ["52da75af-87f0-4706-8b92-afd4dfcb4982"], "metadata": {"filename": "gcs.md", "author": "LlamaIndex"}}, "b71e34d84d9579104d0cd97051ee7e2f76ba99e5": {"node_ids": ["933b7fbe-5397-4e88-a7a7-57be1d0ff893"], "metadata": {"filename": "genius.md", "author": "LlamaIndex"}}, "2942924bc485616364cf0689e9bef63f49a7fa02": {"node_ids": ["40ea3c87-7fec-4e8c-9465-5ef58c5c76ba"], "metadata": {"filename": "github.md", "author": "LlamaIndex"}}, "6494818b45eba0391bd0945864bf46d4bfb6463b": {"node_ids": ["2fbd5341-ce71-4292-a4af-84451e7941e8"], "metadata": {"filename": "google.md", "author": "LlamaIndex"}}, "bf705843796aa56369cc4e3113d93ad33b8d3baf": {"node_ids": ["4815a023-2ee0-4ab4-b80a-a354c6e34af9"], "metadata": {"filename": "gpt_repo.md", "author": "LlamaIndex"}}, "6864421bb639bb70c83e141fe8861f57da90663e": {"node_ids": ["51ea4b04-2c80-442f-ac5d-424dab6b0f2e"], "metadata": {"filename": "graphdb_cypher.md", "author": "LlamaIndex"}}, "cd5fc89e05a6085337a5db2aafc7418e763e0e2a": {"node_ids": ["680d0299-32bf-4976-ab70-3287c207a9e6"], "metadata": {"filename": "graphql.md", "author": "LlamaIndex"}}, "99591f15e438aec1cdb1969b8d299caf6af56f6d": {"node_ids": ["c0223e76-6475-494c-8f3e-620aeb17ff14"], "metadata": {"filename": "guru.md", "author": "LlamaIndex"}}, "470674882f0425673e271285fc98f788f314cb82": {"node_ids": ["76991441-812d-4103-8e53-032f96bd247d"], "metadata": {"filename": "hatena_blog.md", "author": "LlamaIndex"}}, "17dc6f68884f3c5fd83d2bf86d0886b008a2b120": {"node_ids": ["4fb223d6-9fd1-4d11-8815-9037c6853926"], "metadata": {"filename": "hive.md", "author": "LlamaIndex"}}, "a3c793394089374271bf4d22d350d63345c0c709": {"node_ids": ["395917b7-c3e6-47b7-8aef-48af468f417e"], "metadata": {"filename": "hubspot.md", "author": "LlamaIndex"}}, "85deed719c2bf7221170c5ca440ee7745ae62e0f": {"node_ids": ["247c10ff-3366-457e-b13a-d2aeba4b74c2"], "metadata": {"filename": "huggingface_fs.md", "author": "LlamaIndex"}}, "1466a864334e90a545eb87c8b9b89b20700bb634": {"node_ids": ["13d310f7-c6a1-4920-8357-31232f639912"], "metadata": {"filename": "hwp.md", "author": "LlamaIndex"}}, "eea962e0a1fc0db744a84a85d1f805688dea56c0": {"node_ids": ["294ca400-783e-4c95-bd2b-dbfb0ee8f8f3"], "metadata": {"filename": "iceberg.md", "author": "LlamaIndex"}}, "6e120cdb8ede6f4c518289399e9434407d6d08a5": {"node_ids": ["cb59511c-ad04-4909-8a08-7a45e3a4664a"], "metadata": {"filename": "imdb_review.md", "author": "LlamaIndex"}}, "93c5abc5d6f40ca768eff15129ab1dc28b1a8ddc": {"node_ids": ["9b578903-836e-4b6c-b0ab-64e858fa5b46"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "f61df34cc7535506442d9020dfce6f4a9c3769a8": {"node_ids": ["819b4f68-56de-41ea-8ec7-ade5f09997ee"], "metadata": {"filename": "intercom.md", "author": "LlamaIndex"}}, "812a79b09c3be2e63b7b03b2a5a2579e4fe872db": {"node_ids": ["2c98edb3-39e5-4882-9d98-c34b2e325dbf"], "metadata": {"filename": "jaguar.md", "author": "LlamaIndex"}}, "a355ac6f950a5c186a92d10be74f16773eb0aa45": {"node_ids": ["90a79192-b11e-4b07-9f5d-1a50bcc3038e"], "metadata": {"filename": "jira.md", "author": "LlamaIndex"}}, "35d731eb08fd9cfb35c2d282f7afc4c86484e26d": {"node_ids": ["9d74f94d-b1d7-4bfd-8f74-5770dc5e9ede"], "metadata": {"filename": "joplin.md", "author": "LlamaIndex"}}, "9670c55b69450b1cc1077e085201371e803f7c56": {"node_ids": ["36104a2e-48a8-4407-b123-ca749ae44aec"], "metadata": {"filename": "json.md", "author": "LlamaIndex"}}, "0bd37e6c3c91515ae46dc5f7dada9bb8bf073255": {"node_ids": ["4be2015e-690b-4ca8-95ac-a3d0a352f2ac"], "metadata": {"filename": "kaltura_esearch.md", "author": "LlamaIndex"}}, "146b90c79821009d243072444742da5904f7ddf8": {"node_ids": ["a0bc7711-05a1-4613-a43d-f4ec927a83fd"], "metadata": {"filename": "kibela.md", "author": "LlamaIndex"}}, "2f5e47a62b2956b5d7761868ca27bc304b607ba3": {"node_ids": ["749a830a-e84c-4a98-8dc2-f82e5571d2da"], "metadata": {"filename": "lilac.md", "author": "LlamaIndex"}}, "9a153a76d1a863a089b008119426634326c33670": {"node_ids": ["a3d54bd6-d359-4104-a8bd-fe3e42a786fc"], "metadata": {"filename": "linear.md", "author": "LlamaIndex"}}, "88c4aa106ca92abd7ddd89bda068f25b232dbfdd": {"node_ids": ["39a1b6e0-ad30-4160-951b-b09da91b5082"], "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}}, "1f4f18f9ff3683c22d2099c48117953600d2804c": {"node_ids": ["4a17a333-3d96-4861-87c2-a51fbf4ebb5f"], "metadata": {"filename": "macrometa_gdn.md", "author": "LlamaIndex"}}, "a266ef18e03fc2400a105c0ecc6f8b9e5a50748a": {"node_ids": ["715cf458-1d34-4915-8516-578cc98dc090"], "metadata": {"filename": "make_com.md", "author": "LlamaIndex"}}, "4011658728dca3169d4e10c488ee9964de3fdfeb": {"node_ids": ["9d5321cc-c330-4e4b-be5c-caac05445ed8"], "metadata": {"filename": "mangadex.md", "author": "LlamaIndex"}}, "8e9eb28eddac9513bf7441b93ff4331ace44d5bc": {"node_ids": ["3441db2a-45e2-436e-a2a1-030b9a1286c9"], "metadata": {"filename": "mangoapps_guides.md", "author": "LlamaIndex"}}, "ca7dfb98a4b2ec8a5a431eb03399c25aea19b3a4": {"node_ids": ["63d5548f-bf77-4a2b-9d6f-b8ba5d17568c"], "metadata": {"filename": "maps.md", "author": "LlamaIndex"}}, "7491957cafb40efc1f27ec60fd37677de4fb47f6": {"node_ids": ["7646ee70-3251-4179-a1e5-32b14dcd8bda"], "metadata": {"filename": "mbox.md", "author": "LlamaIndex"}}, "693177d604653663d10afccad72362b93a6dec8e": {"node_ids": ["c7868ea1-95ca-421b-a690-d375c5755ed4"], "metadata": {"filename": "memos.md", "author": "LlamaIndex"}}, "40a7fa6a6cacaedf05d04a7f5ea68a71527b044f": {"node_ids": ["51e5e9f6-04d7-4a64-94b5-3fc66466d4cd"], "metadata": {"filename": "metal.md", "author": "LlamaIndex"}}, "d2d46e9f195d857f3f9da7e417300e9b8ec72490": {"node_ids": ["e77fd714-c5d5-40fb-b45a-bde01b978701"], "metadata": {"filename": "microsoft_onedrive.md", "author": "LlamaIndex"}}, "4e2e4c253d0d9d75df540cf0a1748e6a6a1dbaa4": {"node_ids": ["f228ee35-ac64-45bd-8dc1-27d942fcde82"], "metadata": {"filename": "microsoft_outlook.md", "author": "LlamaIndex"}}, "f82a29aee5662e7f900228428dfb3fe176e67bb2": {"node_ids": ["9c6f0da1-15ea-4462-9f6d-7f2a0a4b2c71"], "metadata": {"filename": "microsoft_sharepoint.md", "author": "LlamaIndex"}}, "5affbb64582c3dc9d8a2d395a77786107939a1c9": {"node_ids": ["2122aa37-814f-4fca-a5a9-549a22d8d2f6"], "metadata": {"filename": "milvus.md", "author": "LlamaIndex"}}, "4bd28dc0d85d77931eec041337df2e7cf9e97113": {"node_ids": ["9977898f-d7e3-4781-a007-0c23e2db0af4"], "metadata": {"filename": "minio.md", "author": "LlamaIndex"}}, "2b36b4029bec0d03c0b330100ef29a6f1caa7847": {"node_ids": ["b70d1870-8033-413b-bcab-4f1c37d25906"], "metadata": {"filename": "mondaydotcom.md", "author": "LlamaIndex"}}, "0a6ca314971e60a1936935700957fda0bf89e806": {"node_ids": ["46a07493-1694-4638-8771-9f32206884ac"], "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}}, "8624f784c10d51511f4a718960f23c092e6052d3": {"node_ids": ["06ddef22-f3be-448a-bbd4-50cd633bba0a"], "metadata": {"filename": "myscale.md", "author": "LlamaIndex"}}, "3ebf340c0b9bc8b8306258e61c9c170e5462ca86": {"node_ids": ["ebea690f-e5d9-4ea0-897b-cbcfbc3de85a"], "metadata": {"filename": "notion.md", "author": "LlamaIndex"}}, "c6c43da91f698137800ce66f5830b2d0f9a9367c": {"node_ids": ["ecb51072-0d60-4723-a948-00a280931726"], "metadata": {"filename": "nougat_ocr.md", "author": "LlamaIndex"}}, "3eb70c5518ee54be5c2f78b8b5c860c3b0343d19": {"node_ids": ["cd9a6192-ec7f-4b35-9cc7-c42aa8a12a90"], "metadata": {"filename": "obsidian.md", "author": "LlamaIndex"}}, "2c7fd19c8a42353e861e3afbf09fc6bf2b2f8c42": {"node_ids": ["61edeba7-76fb-40c9-b104-650fcfa85abf"], "metadata": {"filename": "openalex.md", "author": "LlamaIndex"}}, "4e3d7754b23c84aaaf95ad6226cf6fd752005977": {"node_ids": ["9cd617e1-8362-4c93-a752-e5f58540ced7"], "metadata": {"filename": "openapi.md", "author": "LlamaIndex"}}, "fae51af416a3b67c5de7b7d7bf94a4cf728160b2": {"node_ids": ["9b865f83-856c-4fc1-b1ac-05d6d8736d62"], "metadata": {"filename": "opendal.md", "author": "LlamaIndex"}}, "833436604a77440b19eb68f0c374727630692910": {"node_ids": ["2078d050-a8b6-46fc-8a50-c93cd60aa67f"], "metadata": {"filename": "opensearch.md", "author": "LlamaIndex"}}, "910cd15c371d6413fd00b0c63396920a5c4ff81b": {"node_ids": ["2cb9aa63-03ab-468f-8ef2-4f668631d72a"], "metadata": {"filename": "pandas_ai.md", "author": "LlamaIndex"}}, "de64246483ae8c6df49eb973eef177f136d36c05": {"node_ids": ["f24e4521-2a60-482d-926e-5c7452925323"], "metadata": {"filename": "patentsview.md", "author": "LlamaIndex"}}, "93036c25eea447b615e6a9d0da5bd3ed72f5312b": {"node_ids": ["7a0df90f-47aa-4e83-9bf1-ab30d799fd1c"], "metadata": {"filename": "pathway.md", "author": "LlamaIndex"}}, "e37c240e3711d44c3deef8958510693c6ebc1d9b": {"node_ids": ["002b4dd4-2881-4a6d-adc5-cae1031197e9"], "metadata": {"filename": "pdb.md", "author": "LlamaIndex"}}, "022e849b38330614d62f493fd9d2674a1367b60d": {"node_ids": ["81ac9299-1c7d-4a94-9637-d38099e7ec59"], "metadata": {"filename": "pdf_marker.md", "author": "LlamaIndex"}}, "740507ae894011157ea7cbd6c215ce87e40e44b5": {"node_ids": ["e885a90c-656e-4917-aa42-2e3d919f1603"], "metadata": {"filename": "pdf_table.md", "author": "LlamaIndex"}}, "85c66da61db9e08f0b92e444d4ec4d7d036d3679": {"node_ids": ["38921181-cfed-4044-9a86-9edc979ae941"], "metadata": {"filename": "pebblo.md", "author": "LlamaIndex"}}, "408bc9de18eb9c1f945d66cb68772ecc2ad786b9": {"node_ids": ["39d6aea4-9204-417d-b9ee-53be2faba1bc"], "metadata": {"filename": "preprocess.md", "author": "LlamaIndex"}}, "914e511e523bf748d78568d41cbca3dfd69613e7": {"node_ids": ["25900ba9-4290-495a-af61-ed9887c017c1"], "metadata": {"filename": "psychic.md", "author": "LlamaIndex"}}, "31e933c95b0e910c41fd5fcabba64cc17bebfdd9": {"node_ids": ["daac4a1b-2828-4b15-95ba-dd3a4cd12e87"], "metadata": {"filename": "qdrant.md", "author": "LlamaIndex"}}, "1b0db256d641b3fe2b03ef491f9e4efbc7ca2cfb": {"node_ids": ["8afb54b7-8c71-490d-9a76-8ff8a67c3306"], "metadata": {"filename": "rayyan.md", "author": "LlamaIndex"}}, "0280cc1899d33902c0cd5c9d4b2faa1c58624ba0": {"node_ids": ["dd6f30f0-b0c2-48f9-898b-02046761f37f"], "metadata": {"filename": "readme.md", "author": "LlamaIndex"}}, "5b3029dd9921cc46763915be9e4e890297a07c51": {"node_ids": ["fbe401e7-09e5-46a7-a193-0db90f1cdaa8"], "metadata": {"filename": "readwise.md", "author": "LlamaIndex"}}, "734d6fce64418d0bfa0f062909198ee98aef37fd": {"node_ids": ["8dbbb499-a357-4f65-b825-d1301c2385af"], "metadata": {"filename": "reddit.md", "author": "LlamaIndex"}}, "586cc46d5c8f7b1f8b69c72c27f21f4e7db849b6": {"node_ids": ["bfde5c17-fda0-45fb-a940-dd55b072b24c"], "metadata": {"filename": "remote.md", "author": "LlamaIndex"}}, "9d5f8ec36d72cab49b56a8dc8df806a92af1a72e": {"node_ids": ["e1566b61-3f98-4ca9-936a-8d8da644649b"], "metadata": {"filename": "remote_depth.md", "author": "LlamaIndex"}}, "89fc49ef34fb7491d7c81318f97efe260beda8f1": {"node_ids": ["a312fd72-0774-4933-9e3b-486571f34d04"], "metadata": {"filename": "s3.md", "author": "LlamaIndex"}}, "357809ebfb6641a4be757a9f4a6e3d1f644a7839": {"node_ids": ["9a4dcee6-6760-497f-a98e-28287bb021cd"], "metadata": {"filename": "sec_filings.md", "author": "LlamaIndex"}}, "890be0da4b482be0ea867018dafd66c08ec2d0bd": {"node_ids": ["f4fe2c9c-979c-4e80-9655-abbd2fab1efc"], "metadata": {"filename": "semanticscholar.md", "author": "LlamaIndex"}}, "5d8ade8f37253419b9ba2c7b3ac738dbc098a3b1": {"node_ids": ["8fb5fd44-a5d1-4bb0-a702-a228aef2353f"], "metadata": {"filename": "simple_directory_reader.md", "author": "LlamaIndex"}}, "26752d4865eaef7fa41439e9aaf99317dd556a1a": {"node_ids": ["6e5d82df-9016-41c5-bea7-40ea92a16537"], "metadata": {"filename": "singlestore.md", "author": "LlamaIndex"}}, "db20c905e943a6d0170a5b66992865814602a6b6": {"node_ids": ["5d8adfd0-51d5-4d4d-9539-337e89876d9b"], "metadata": {"filename": "slack.md", "author": "LlamaIndex"}}, "b89eea42594b57e31d87954a2339bf21676a4291": {"node_ids": ["b5a31247-0f5a-4c81-af50-7295fd2cc50c"], "metadata": {"filename": "smart_pdf_loader.md", "author": "LlamaIndex"}}, "5b7f2acf072cea75f33acb439f9a25b7dfeed062": {"node_ids": ["78864713-c31d-4b5c-a75f-2e35224836fe"], "metadata": {"filename": "snowflake.md", "author": "LlamaIndex"}}, "8b7096a2c2ead5ae7d3a4f0478433226e54d2c16": {"node_ids": ["c1c3fc12-d7a8-440e-9720-178022885e6d"], "metadata": {"filename": "snscrape_twitter.md", "author": "LlamaIndex"}}, "30452628318def79ad377ea4cca17da1a3877b9c": {"node_ids": ["929deb35-19cc-4fa3-afd0-9623c4cccac5"], "metadata": {"filename": "spotify.md", "author": "LlamaIndex"}}, "031e51db9756578edb7f513446e090197f061c80": {"node_ids": ["16839a38-f5eb-4b32-bac8-15a918740a1c"], "metadata": {"filename": "stackoverflow.md", "author": "LlamaIndex"}}, "61d8bf6faeea49ad098334c547ede763dbdf3069": {"node_ids": ["6d755120-f481-407c-9a43-1c5948b941bc"], "metadata": {"filename": "steamship.md", "author": "LlamaIndex"}}, "fbd39b1fcd7d617c9be60de9ef4a1584e2a48dc4": {"node_ids": ["4a997189-0663-4709-8666-787982f28fe4"], "metadata": {"filename": "string_iterable.md", "author": "LlamaIndex"}}, "56318c43b0a532c109df5990766409115b864d41": {"node_ids": ["2cc0992a-76e7-4f44-b308-08b0ef886f0c"], "metadata": {"filename": "stripe_docs.md", "author": "LlamaIndex"}}, "46dd9dbac596eb7265dd0ff431dda3cefc2cef7c": {"node_ids": ["02043eb4-0375-4ce6-b095-a49f4649a197"], "metadata": {"filename": "structured_data.md", "author": "LlamaIndex"}}, "3f9ae643520832a6886ca0b483bf6d24f6205aae": {"node_ids": ["ea04c740-5b74-40cb-a2f8-284358e3907b"], "metadata": {"filename": "telegram.md", "author": "LlamaIndex"}}, "bc46978ee1045df6d5d89fc3f55617970ee7a3ff": {"node_ids": ["b24eef26-dee3-4cf0-bf36-cbfd8eb0f7fe"], "metadata": {"filename": "toggl.md", "author": "LlamaIndex"}}, "b8ae1db630dc30634d8404eb8c1365bafb99716c": {"node_ids": ["3126cd00-c833-40b8-b3e0-f6f66cb76234"], "metadata": {"filename": "trello.md", "author": "LlamaIndex"}}, "e038987c90f829c6fc21ec1f541abebd926d0686": {"node_ids": ["3c4d91f7-891e-4730-a246-bede2c6a2f4c"], "metadata": {"filename": "twitter.md", "author": "LlamaIndex"}}, "1fb219e1b1af75202a166d2e0ebac17a2f3d6739": {"node_ids": ["d880d95f-78f1-4ebe-8b8b-1635255841d0"], "metadata": {"filename": "txtai.md", "author": "LlamaIndex"}}, "cbda070e4bba3c7b07f6886c357b2c4d0e721b22": {"node_ids": ["22debcf5-b223-4b78-86d6-14259a6b1328"], "metadata": {"filename": "upstage.md", "author": "LlamaIndex"}}, "a7ee67e48f8c0c95618dc39221f5a27a34ec829f": {"node_ids": ["3a9aa7bb-69ed-48a3-a5cc-b949ce753bbd"], "metadata": {"filename": "weather.md", "author": "LlamaIndex"}}, "75481ec81e17040fe2ad0d03bd5d3dc928687a07": {"node_ids": ["c1ad5dc5-8922-4552-8f85-8eac211f1896"], "metadata": {"filename": "weaviate.md", "author": "LlamaIndex"}}, "c7c763148fe6a90ad34d1628b4197d9432dea81c": {"node_ids": ["ca9d25ae-725a-4949-a514-bcc16d700e0d"], "metadata": {"filename": "web.md", "author": "LlamaIndex"}}, "886d77cd229608205776525bb114a6444ead4aef": {"node_ids": ["f7446438-ebca-48cc-86e7-bc9b927518df"], "metadata": {"filename": "whatsapp.md", "author": "LlamaIndex"}}, "d86bdd5b6e515127e5cbf105b1aa8f4ee0dc659f": {"node_ids": ["f08ea7b2-4ad2-4065-8e7d-bb04336b1785"], "metadata": {"filename": "wikipedia.md", "author": "LlamaIndex"}}, "12af139e1de3404bef178badec24959cc81d6419": {"node_ids": ["74fbe73b-b9d7-4cd4-9615-3eba7820d8db"], "metadata": {"filename": "wordlift.md", "author": "LlamaIndex"}}, "99a1a37a467216a08a0cb98e469faa8e2cbfb01a": {"node_ids": ["378f5d30-e333-4928-be19-0d3c93abc2f4"], "metadata": {"filename": "wordpress.md", "author": "LlamaIndex"}}, "a8a7fd8d00ed3497217062af2fe4618463558839": {"node_ids": ["d6176453-1f33-4dad-b714-8bd3cd08379d"], "metadata": {"filename": "youtube_metadata.md", "author": "LlamaIndex"}}, "66ed75cc0f93c38031e833ac9b63b8736eb8c317": {"node_ids": ["a4b2530a-2bfb-4fa9-b962-bc1ca017ff01"], "metadata": {"filename": "youtube_transcript.md", "author": "LlamaIndex"}}, "e83205e9973e1635664456e9b37a316ff55571d9": {"node_ids": ["76b83612-9b9d-48c9-9bc4-3c0f4442dafe"], "metadata": {"filename": "zendesk.md", "author": "LlamaIndex"}}, "e9d2c6fcb1f11f8a89234ed887632f0a625821e3": {"node_ids": ["d58b485d-d61d-4301-a18a-8ea259cf2a70"], "metadata": {"filename": "zep.md", "author": "LlamaIndex"}}, "e80b6aed37a70227ddf23fcce6724c295f2703e6": {"node_ids": ["b41a84d8-69dd-4efb-b5cb-3cea6e71ef09"], "metadata": {"filename": "zulip.md", "author": "LlamaIndex"}}, "c07fb19215997328f6329fb50d4884f384a25ace": {"node_ids": ["fb7de2be-19d9-458a-a1af-bc6e056459b8"], "metadata": {"filename": "accumulate.md", "author": "LlamaIndex"}}, "ec4480f32b0bc57c291ec034219105a53f1134ad": {"node_ids": ["57a5e2e5-688e-4b9e-9a59-89abc195fefd"], "metadata": {"filename": "compact_accumulate.md", "author": "LlamaIndex"}}, "1da13086404288c96ea3cbd4f305f0c44e63d837": {"node_ids": ["c4290f30-c836-4472-8e09-cfca109c1624"], "metadata": {"filename": "compact_and_refine.md", "author": "LlamaIndex"}}, "dcbe227332a9b52d9dc36db4704cc284a4c3ca74": {"node_ids": ["8f3ea72e-34d6-428f-af3c-df8c3c46cae8"], "metadata": {"filename": "generation.md", "author": "LlamaIndex"}}, "f63a55d6923cfe8aeabe86ec140570a0ad82d2cb": {"node_ids": ["77ff3670-5771-4768-9894-01cdb058a559"], "metadata": {"filename": "google.md", "author": "LlamaIndex"}}, "1b8b541953a0f33ec63398f62b2a70f703a1d428": {"node_ids": ["c4f80904-63c6-4c9b-bf2f-4fd5f3f72e07"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "a69de484874bbab04cba05602a2a554cea3cc7a5": {"node_ids": ["ba0e5786-cf32-4cda-ad15-484f03a297c1"], "metadata": {"filename": "refine.md", "author": "LlamaIndex"}}, "2a4c3bc095b3bb31b754fa53904d845d0d0fd60f": {"node_ids": ["85822257-d65d-47bf-ad8b-78ae0827028f"], "metadata": {"filename": "simple_summarize.md", "author": "LlamaIndex"}}, "c0544a715d0d5b6af091dddc84b2df11ad1a8cf7": {"node_ids": ["27a6b5a2-d57c-4b86-a3bf-9cfe48d26b35"], "metadata": {"filename": "tree_summarize.md", "author": "LlamaIndex"}}, "0dea41f8e24efed91986d3032c78842346b2378d": {"node_ids": ["4042e84d-9eb5-46b8-9bc3-29152f924a23"], "metadata": {"filename": "auto_merging.md", "author": "LlamaIndex"}}, "d98fdf3dad795fff9e2e214644bb22ecc5b34364": {"node_ids": ["9dbc23f2-ab86-4442-9039-460b0b9a6324"], "metadata": {"filename": "bedrock.md", "author": "LlamaIndex"}}, "e8040efda72877bd805ef7d3a73a0780948350c6": {"node_ids": ["43f0da14-6f32-4312-8f63-6b9939717239"], "metadata": {"filename": "bm25.md", "author": "LlamaIndex"}}, "2eb6050b73d017c151d38f50f463322537859eed": {"node_ids": ["cf3a1721-66b1-4f82-82e8-bc8c825e9b74"], "metadata": {"filename": "duckdb_retriever.md", "author": "LlamaIndex"}}, "407afaa3eb4b69e39c60127c639b9b372b391e29": {"node_ids": ["aa39c52a-cb73-4da5-ad92-a49aabb6680d"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "663b4c0e043b116c3777b158246345ca89b77a05": {"node_ids": ["64805582-a164-4b41-97c4-5890e1d0c311"], "metadata": {"filename": "keyword.md", "author": "LlamaIndex"}}, "a97a5f79f3afc397269781615d15197ddca96952": {"node_ids": ["0d7fe055-404a-4bda-8464-53142d7d526d"], "metadata": {"filename": "knowledge_graph.md", "author": "LlamaIndex"}}, "10acecc248c980944d096d2ec407e5455d6d8497": {"node_ids": ["fd813165-dba1-4d50-97fd-1d3cba7f14ff"], "metadata": {"filename": "mongodb_atlas_bm25_retriever.md", "author": "LlamaIndex"}}, "5e2855ae06a12108ec939943dff84ad094a50ae0": {"node_ids": ["dddc7395-e6e7-4e62-83a7-0f453160505c"], "metadata": {"filename": "pathway.md", "author": "LlamaIndex"}}, "f5594011622b0bd3c18889073299e9376e67cc24": {"node_ids": ["484513f4-5da2-4769-b17b-986593a52707"], "metadata": {"filename": "query_fusion.md", "author": "LlamaIndex"}}, "7e6c3bf9596f03184005614b05008628bc7f7ff8": {"node_ids": ["1ad7a89b-994a-4afa-9a6b-0a1d4ac0d5c1"], "metadata": {"filename": "recursive.md", "author": "LlamaIndex"}}, "1f9732dc505c9a49a933d20dacf000be167648b4": {"node_ids": ["4e37b9d6-5600-4793-b58d-e0c2502dbab1"], "metadata": {"filename": "router.md", "author": "LlamaIndex"}}, "4a306ee8aeac99c9023623e5ed8bf7576e40c832": {"node_ids": ["127a515b-4631-41de-8396-e623f308e22a"], "metadata": {"filename": "sql.md", "author": "LlamaIndex"}}, "b300c65a0e069a78d9e6cce53f15e86651fc94f9": {"node_ids": ["b9c66e8c-4f8b-4736-9abe-61923a1bb331"], "metadata": {"filename": "summary.md", "author": "LlamaIndex"}}, "5aae897ec710accfb4ce1785058c8ce90e51f12c": {"node_ids": ["d209f22f-2f1e-4a25-81e0-08b993f97d73"], "metadata": {"filename": "transform.md", "author": "LlamaIndex"}}, "10279885bbd31a672544267253b384908ca09331": {"node_ids": ["c899d932-d5f2-4053-a7fc-29389606d912"], "metadata": {"filename": "tree.md", "author": "LlamaIndex"}}, "cc79b814af4864c553cfdf575c1bb6725bf7cfa8": {"node_ids": ["b1acd3ec-7877-4d2b-bf2e-09aabb025820"], "metadata": {"filename": "vector.md", "author": "LlamaIndex"}}, "e896c313b100dc5b23ffee9a80ff61430bf1008a": {"node_ids": ["ac3ddaf0-bad6-4be7-ade5-0493d244ed54"], "metadata": {"filename": "videodb.md", "author": "LlamaIndex"}}, "c5e26fd088717b776c077c9a26ae44752bd7a7b3": {"node_ids": ["8af98b70-df77-4035-959d-35aa28457212"], "metadata": {"filename": "you.md", "author": "LlamaIndex"}}, "c214862b81646beae9750041b319d2e7d840da07": {"node_ids": ["30c97429-409b-4a99-a5cb-d3f911d02969"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "371e0170c4fe3af69f4976ab7d79b5a8ec035128": {"node_ids": ["ed2dc0cd-c08b-4b4a-8772-0fed8e74664e"], "metadata": {"filename": "azure.md", "author": "LlamaIndex"}}, "76d26756aea7f5970b9df86db5cd9a2d3dd44df1": {"node_ids": ["35258239-163d-464c-8cd7-6ccd9afd3581"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "0c55ba01c3e62081cfa69a01a20958c447e33f12": {"node_ids": ["28ed0b4e-6ea5-496c-b20a-67392f6490ad"], "metadata": {"filename": "redis.md", "author": "LlamaIndex"}}, "0d49e91b9eb4f8dda620bd048f1bdb80aaabeb95": {"node_ids": ["a30e75bc-6821-4b97-b7a9-1fbead056471"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "670d8ac915cfdefef43c356afcba29adeb56d0a3": {"node_ids": ["09231805-90de-4b0b-87d5-0bb27982db95"], "metadata": {"filename": "azure.md", "author": "LlamaIndex"}}, "4996bd48dfe4e17bafd7fff506641920c39f0efc": {"node_ids": ["ff1b4e8f-78ed-4787-886e-ae7fbf3514cf"], "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}}, "02494a2d5e625e26c55cdb7e32b9a61d63cfddc7": {"node_ids": ["97c0c400-3127-4d0f-a40e-85fbfdf7e04a"], "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}}, "b7b8e3eee3a696326daeb646aa0c48368bfcdcff": {"node_ids": ["99af2be7-3cf4-4ac1-9e97-77265242b4a6"], "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}}, "607d0cfcf290d6cc1be543431d31abdf16c3b351": {"node_ids": ["59ce7ca1-00be-4ee3-8b95-833d7512d844"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "c1feb56dd0624fd7712e5c3d9dfdbb04488bf861": {"node_ids": ["4231905d-0e05-43a0-9653-93172c089305"], "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}}, "49ca31e540c7071afb3356e611410e178463ff92": {"node_ids": ["bb483ef3-3bda-47aa-8376-27878613187f"], "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}}, "b68e2c3d86b8f398e85e23caaafedb1dd9002315": {"node_ids": ["194bdd24-6d15-4379-9d5c-b64bc4a27fae"], "metadata": {"filename": "redis.md", "author": "LlamaIndex"}}, "cfe822056fcba34b0701800668bb2eb19a7488ce": {"node_ids": ["9c5ac026-e90c-42e0-9ea6-bad8f2771f33"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "12906a65f9a48692c714a887bb32d057e7e05259": {"node_ids": ["1b125ebb-320b-4ac0-8d40-d45fe3832c0a"], "metadata": {"filename": "falkordb.md", "author": "LlamaIndex"}}, "f6e0160d9595777106da177d7f67f7320713653a": {"node_ids": ["0caaa87c-90f0-4303-8755-e9e235464353"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "a19d228e43df1d13ae4600bc942cd8d65dc807f7": {"node_ids": ["a09193c4-d601-46c7-9629-98fb2f3da823"], "metadata": {"filename": "kuzu.md", "author": "LlamaIndex"}}, "8f01f3f46df5440243807f85939d147b0a030d6f": {"node_ids": ["b6059110-c9aa-4dee-a4f7-030268cc61eb"], "metadata": {"filename": "nebula.md", "author": "LlamaIndex"}}, "0957d156917f8cbba31d6ad54ca9442b165ad55b": {"node_ids": ["558faf48-0604-48cf-a0b9-4fb2058e12d7"], "metadata": {"filename": "neo4j.md", "author": "LlamaIndex"}}, "8be441642f4cb39603b91d41b37953e2d4d2f35f": {"node_ids": ["768d520a-6033-487e-b820-878c6fc0e2c8"], "metadata": {"filename": "neptune.md", "author": "LlamaIndex"}}, "3c725b57f5fd2d75a3ea881c5684802e8f01b095": {"node_ids": ["75856f2e-176a-4c1f-9b1c-adaa178b6a5b"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "6688e22be048a39850c496f2b862f5393ae9307b": {"node_ids": ["33a7c7f5-ef53-40ba-8a15-a266b16190b9"], "metadata": {"filename": "tidb.md", "author": "LlamaIndex"}}, "0627cf498dca4bf5ffa730dcf7794114a796079a": {"node_ids": ["6c2039c8-7e87-4f44-900a-3943e23cae22"], "metadata": {"filename": "azure.md", "author": "LlamaIndex"}}, "787d85fe604d459eb8d701822189317931a7351c": {"node_ids": ["806004c5-6aff-418e-8ffa-c5ab86443309"], "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}}, "d60e2e025dd8b3e2934fec4dc9454493a40de692": {"node_ids": ["21a064b2-5451-4da2-9b64-68fca1e19d86"], "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}}, "248d183b455a8698b2f33e7a761aa4d265c598d6": {"node_ids": ["36f05abc-4090-4610-80c9-78853b6fb116"], "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}}, "b5882e3c415a9bd436f9938faf7dbfa219a87165": {"node_ids": ["375d233a-3249-4a1c-a0d4-2ed8a455328d"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "63efb122574d65025688e1a907992915dca4b7a3": {"node_ids": ["831d6c7d-2cd5-4688-b99d-bcfeba52d8a7"], "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}}, "de08126f1174e7be0e25e1aa7f925b6c667a9450": {"node_ids": ["512aa655-a0c1-466a-a6ac-17a4bcfcd7e2"], "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}}, "c4b4e1574388385e3ac26e03a09e6cf7f83c2d8d": {"node_ids": ["9c88ce24-694a-4d42-a170-87dfa7066b67"], "metadata": {"filename": "redis.md", "author": "LlamaIndex"}}, "fb4785a2b2c175d5a08789e35f0f15bb271b385d": {"node_ids": ["b84ac2d9-3aaf-45d7-81f2-195683198b2a"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "b841797df11b7f8afeeaa248503cceb137258f43": {"node_ids": ["94bfb0ec-ead7-4a53-9a35-9d17eb294488"], "metadata": {"filename": "azure.md", "author": "LlamaIndex"}}, "7ddcca0436cd2835a84cbbdf8f60afa019d12d39": {"node_ids": ["3e273b43-8d3f-46f7-9aff-0a09a2b654b4"], "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}}, "5a90fc658257dd07a5d033e0b8aa6d67c59f534b": {"node_ids": ["8c72cbea-dbee-4208-b66a-5f075a7cfb8e"], "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}}, "be0c036c100e39b8be69830850c0650299627b2e": {"node_ids": ["793c8ed3-14e3-4585-b7b7-15003595deea"], "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}}, "9a6bd621bfd10e80c467c4f51742663d4db4f1b4": {"node_ids": ["010f397c-af09-474e-a8f8-04df085eaae6"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "40d22577b82504e8a7a79f2243f186ae3e80b8f3": {"node_ids": ["c9faec05-e494-4e0e-b16d-d8e1fc973848"], "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}}, "58c4d9506b715838d23c4080080e0287d0cbd4f2": {"node_ids": ["c277181a-1787-48de-b9fd-74b5850d48e4"], "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}}, "9d26b914dab720218475d97931dbbd716a46f6b4": {"node_ids": ["b69d7541-17a3-4d7f-82bf-aa048bc36222"], "metadata": {"filename": "redis.md", "author": "LlamaIndex"}}, "a2013198a78917e0ec6b303d6641d7cff6d026f5": {"node_ids": ["5aa74481-7ebd-457b-95e8-018977e5f191"], "metadata": {"filename": "s3.md", "author": "LlamaIndex"}}, "687787325c1484d6202c85754c9b28261432d8b5": {"node_ids": ["80d7d37b-d124-473a-a790-b215b9ba30fb"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "ca0f6e260c988c093738f5ca5c5e50d2211f5301": {"node_ids": ["4e637ebb-628a-49d9-8edf-5dbfc26215d2"], "metadata": {"filename": "storage_context.md", "author": "LlamaIndex"}}, "8333c9678e05f23879715e9ee3ef0b5eda62ce17": {"node_ids": ["226f32a7-2db5-4f0e-9ac4-f94ceb3435b4"], "metadata": {"filename": "alibabacloud_opensearch.md", "author": "LlamaIndex"}}, "b9652e5f424891c692db6c172ca4e6d7dda6d697": {"node_ids": ["14b092f1-14c6-4a2a-8518-b3014cb01c7d"], "metadata": {"filename": "analyticdb.md", "author": "LlamaIndex"}}, "cc2a34a3601b85df46b81895ae1dad043b8e116a": {"node_ids": ["11eea536-1df7-4f48-9767-b3f5b44c7c4f"], "metadata": {"filename": "astra_db.md", "author": "LlamaIndex"}}, "18c6d661141a680cb3f296e36c68176d31347dd2": {"node_ids": ["319b29c8-77b4-4dd5-9b32-78403002b385"], "metadata": {"filename": "awadb.md", "author": "LlamaIndex"}}, "c0eb48bb8ddbb48be83773877275d9c7412f52b4": {"node_ids": ["7096f605-b37d-4d46-ba65-8ff293460899"], "metadata": {"filename": "awsdocdb.md", "author": "LlamaIndex"}}, "a1acd7a0db42fdd0b1fb85e19766a17f8c884cf5": {"node_ids": ["86043378-1a60-4cdc-845c-97d47a44645a"], "metadata": {"filename": "azureaisearch.md", "author": "LlamaIndex"}}, "e3208fc943b3ea51088f4d3e785abec789f43b84": {"node_ids": ["bfa72da0-5c5e-4351-a7b1-e206464c5413"], "metadata": {"filename": "azurecosmosmongo.md", "author": "LlamaIndex"}}, "439d94ba03ab39cbb75c59ffa30edecf4cb193f8": {"node_ids": ["37b28f43-6dbc-470f-9406-c2f1345a3210"], "metadata": {"filename": "bagel.md", "author": "LlamaIndex"}}, "1db993e7176bbdf18529febf41278e148c0972a5": {"node_ids": ["75ca0cb7-2297-4fc4-ae29-25180bef664e"], "metadata": {"filename": "baiduvectordb.md", "author": "LlamaIndex"}}, "d340a58d8ba5237037a3ee74e87a2f8ca8f7a00f": {"node_ids": ["a9f4c44f-a239-416a-b664-c474b4011708"], "metadata": {"filename": "cassandra.md", "author": "LlamaIndex"}}, "76a756c480190a3eaef63bbf41e265fc24e35103": {"node_ids": ["67cbe419-72b2-4a98-99c0-64200f904c13"], "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}}, "808aeb5f89693308bec1e838703ae74cad8e261c": {"node_ids": ["8282ce5d-9667-4409-832a-46d88a9b6388"], "metadata": {"filename": "chroma.md", "author": "LlamaIndex"}}, "95060b04c5679701bcfd61ab2207cd94877949af": {"node_ids": ["24544fd9-7b81-4b7c-ad90-2c2fea80a0a7"], "metadata": {"filename": "clickhouse.md", "author": "LlamaIndex"}}, "86516421e1454770de5799541478fcf17790116d": {"node_ids": ["1120ddac-d410-4378-8d16-f6bf3906a0ca"], "metadata": {"filename": "couchbase.md", "author": "LlamaIndex"}}, "07170183cfb921c22440563582c084d7d5e6dd77": {"node_ids": ["7a595a12-802e-4576-8cdd-926152f2d439"], "metadata": {"filename": "dashvector.md", "author": "LlamaIndex"}}, "28806b8812ef1b644dc04237edf9d138b5d4ba20": {"node_ids": ["070f3a8a-072e-4dc9-aa0a-bc2a42292380"], "metadata": {"filename": "databricks.md", "author": "LlamaIndex"}}, "c2b05684b9db920793a0dc780850eb5d00e8e6f4": {"node_ids": ["55ff537d-0ee7-49ed-a9f2-dfae42ea4593"], "metadata": {"filename": "deeplake.md", "author": "LlamaIndex"}}, "0ad4a194f26680b8c109f5f9ff8c4375d3f561ba": {"node_ids": ["0bdf83bf-eb4c-4bf3-8b18-74701839ba23"], "metadata": {"filename": "docarray.md", "author": "LlamaIndex"}}, "4027451bcf03e4e37ec1cb57328d4268f575ff71": {"node_ids": ["77dabd47-c47f-4b37-a88c-4535d34a9953"], "metadata": {"filename": "duckdb.md", "author": "LlamaIndex"}}, "803443512a8d0db713ab54ce30daf529c1a5065e": {"node_ids": ["caf74100-f71f-40cd-98e6-5bcbac6525f1"], "metadata": {"filename": "dynamodb.md", "author": "LlamaIndex"}}, "8a6e63b8cf7dfeaeb6915069e417ff32562e9d00": {"node_ids": ["7a1a42ff-56dc-4a6d-b92b-dfb7791d824a"], "metadata": {"filename": "elasticsearch.md", "author": "LlamaIndex"}}, "1ef6af0468ba5c83ad8616fd047305b4213227c2": {"node_ids": ["fe93cfd2-fe63-4b92-b996-d1f4acbc5fde"], "metadata": {"filename": "epsilla.md", "author": "LlamaIndex"}}, "506b92e5e454cd2d09b924f620dc9b22f89a9e3e": {"node_ids": ["e14b8ee2-b91b-4858-8953-1c0041f4cdc8"], "metadata": {"filename": "faiss.md", "author": "LlamaIndex"}}, "f5f1f3cfd2e443207c801c5131ffafbf775309ba": {"node_ids": ["b9566af8-6bbf-446f-8267-546fa702b12d"], "metadata": {"filename": "firestore.md", "author": "LlamaIndex"}}, "7df120054cd53f159a9196a9132aa7b1c5c66eeb": {"node_ids": ["8654ce25-2581-4174-8f49-e998e0de68c0"], "metadata": {"filename": "google.md", "author": "LlamaIndex"}}, "2f87712504009f42009fdbc7a2363b6e5419dc7e": {"node_ids": ["631ad8ee-72f7-4dbe-b01c-4f3bf4b50cc6"], "metadata": {"filename": "hologres.md", "author": "LlamaIndex"}}, "3788d660894b98e477bb25c8f4e9ce36d4adab91": {"node_ids": ["f1bf0c83-d82d-48d7-acbc-ecc74fc23fc9"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "3172e0bc5c79c2938ae36e2dc7a3858f7c08d23c": {"node_ids": ["8841b879-49b3-41d5-92ac-cd7735ddee83"], "metadata": {"filename": "jaguar.md", "author": "LlamaIndex"}}, "a2bddfc7e89764b1d4ee5676c9283ba0fe28afc2": {"node_ids": ["232f6b0b-662e-4255-a6e3-c92253dc6097"], "metadata": {"filename": "kdbai.md", "author": "LlamaIndex"}}, "61e97f04829c81c62ff5872a3b457bac48185766": {"node_ids": ["c663bfaf-9c97-46b2-bde1-7ce5c16d04be"], "metadata": {"filename": "lancedb.md", "author": "LlamaIndex"}}, "4648d8f9b42fdc6d4e673a96f2315a0ccb274592": {"node_ids": ["9ed42e14-59b8-446b-ab8e-be1da4641135"], "metadata": {"filename": "lantern.md", "author": "LlamaIndex"}}, "7d68ffa95fc716fe5f74d71a3e9a8175f5dc7927": {"node_ids": ["2f268372-ead7-410d-8302-be5ff5e8977d"], "metadata": {"filename": "metal.md", "author": "LlamaIndex"}}, "7b0eca2344dbf2f17da0cab04601db9f32d8882b": {"node_ids": ["1996aa83-1ac4-405f-9342-07ad989996ba"], "metadata": {"filename": "milvus.md", "author": "LlamaIndex"}}, "cd90659e92e62339e45e814e4dfe6aa8cc1eb171": {"node_ids": ["4a9e948d-6200-4ea2-aec9-9230e46e45f2"], "metadata": {"filename": "mongodb.md", "author": "LlamaIndex"}}, "605595d2f0969942898003c5b8e44000a29540bb": {"node_ids": ["13ad0d37-77d5-49ca-9d4f-0aaf6ae3a615"], "metadata": {"filename": "myscale.md", "author": "LlamaIndex"}}, "554d048e98a3ca0268668636b9c59006464b6d81": {"node_ids": ["11e787e0-62e8-4511-8557-ff5e2b569398"], "metadata": {"filename": "neo4jvector.md", "author": "LlamaIndex"}}, "26b57e6987b8353385d4a5b17747660ee8a108f0": {"node_ids": ["473fe75f-af2e-414e-9dab-b645bc359f13"], "metadata": {"filename": "neptune.md", "author": "LlamaIndex"}}, "1e4b44ec6341dd51d58750aeafc652125870f289": {"node_ids": ["d0121c8c-949c-4e63-804d-62f8bdb99bd4"], "metadata": {"filename": "opensearch.md", "author": "LlamaIndex"}}, "b79974b6688f3972cf1cf3729a6442f735961484": {"node_ids": ["53d457a5-5104-4276-ada5-07ff7220f2e1"], "metadata": {"filename": "pgvecto_rs.md", "author": "LlamaIndex"}}, "eab5be5bb35894a3bbf43ea70fa2fb127b5b06fc": {"node_ids": ["1728e364-cbcd-42f6-bb07-da14d606cf00"], "metadata": {"filename": "pinecone.md", "author": "LlamaIndex"}}, "f29d4590fb52378bce5e54c338eec94fd7ab1d32": {"node_ids": ["af8f08bb-5153-4964-aed7-82cb33682a4e"], "metadata": {"filename": "postgres.md", "author": "LlamaIndex"}}, "46568006c34d31a56e87db66b658249b4c228b83": {"node_ids": ["153f1623-ea76-4890-a5a3-e108b9777ba2"], "metadata": {"filename": "qdrant.md", "author": "LlamaIndex"}}, "92e937765c2976bba8a932c68b3888f7c90757fa": {"node_ids": ["64c354d2-0bfe-4592-af71-1cc7ca1eb177"], "metadata": {"filename": "redis.md", "author": "LlamaIndex"}}, "1b097cf514f5e3880a331c1e2cc04a12965e68b0": {"node_ids": ["4354adc7-ba37-4b59-b14c-339e7c04d34f"], "metadata": {"filename": "relyt.md", "author": "LlamaIndex"}}, "365a4700e223b622add53e6f2027ff39675894e7": {"node_ids": ["26019d6e-4307-4954-9029-1a6e5c9f31ea"], "metadata": {"filename": "rocksetdb.md", "author": "LlamaIndex"}}, "aa4dbadcd55c85e01c59e5436fc6cdf8fe47f068": {"node_ids": ["f767aae3-538e-47d4-ab53-ad493925e9b5"], "metadata": {"filename": "simple.md", "author": "LlamaIndex"}}, "7e320c4588c63c842c5aa47a3102599daf7826f1": {"node_ids": ["deb48aef-b743-455f-b406-f1b108dc9cdb"], "metadata": {"filename": "singlestoredb.md", "author": "LlamaIndex"}}, "dacd27281e38a51183d1afc83ab5888030f354b0": {"node_ids": ["dce786da-1673-42f9-9588-d1b4b101e2b4"], "metadata": {"filename": "supabase.md", "author": "LlamaIndex"}}, "e4931c57e26726934cc7ae7e8824d8a1dd2b4c26": {"node_ids": ["6072ae69-8089-42d0-93e2-51ceb9919424"], "metadata": {"filename": "tair.md", "author": "LlamaIndex"}}, "71b6f40101c4f42bbf4109f7f67e247463d452d0": {"node_ids": ["96753138-f47b-4849-8860-ca567be9017c"], "metadata": {"filename": "tencentvectordb.md", "author": "LlamaIndex"}}, "8f091bb33b841ae45b35a0760e114bf1a89cd1e6": {"node_ids": ["be740b43-06f2-43b2-8cc7-bdc7faeb3ef5"], "metadata": {"filename": "tidbvector.md", "author": "LlamaIndex"}}, "d63e36d36fdec2f24b4e612b49e0acd3124bae93": {"node_ids": ["61896bf3-20a9-4827-8a55-eed0ac4804ec"], "metadata": {"filename": "timescalevector.md", "author": "LlamaIndex"}}, "bc03ae558d691e71ded8ce6be00440f0e0cc82ea": {"node_ids": ["9d562cd8-accc-4bd3-a49d-d4df85dff903"], "metadata": {"filename": "txtai.md", "author": "LlamaIndex"}}, "2fd22331dfbb5aedcb66650611a212328d9f30b8": {"node_ids": ["ebb4cc0e-a293-448d-9ca7-dd78f36c11ec"], "metadata": {"filename": "typesense.md", "author": "LlamaIndex"}}, "1f9be5197348b83a091314dfdbe2b0ea17783fd1": {"node_ids": ["5d73a4bb-6564-4640-9f72-e1b93cb697ce"], "metadata": {"filename": "upstash.md", "author": "LlamaIndex"}}, "fd572c3108df403ede670e66a9fe1dfe77105ccb": {"node_ids": ["2c6d08ce-85e3-4a32-a582-b77d98fee3e1"], "metadata": {"filename": "vearch.md", "author": "LlamaIndex"}}, "542bf85fb706637da54ddea9e8f5a25f7cf8d73a": {"node_ids": ["6cfbd38c-09b7-48ee-9998-3c748045b144"], "metadata": {"filename": "vertexaivectorsearch.md", "author": "LlamaIndex"}}, "1727c5dcf1859ade43c55464441430347ec33a10": {"node_ids": ["5345a1ef-26be-4e9e-9732-f5df28501c18"], "metadata": {"filename": "vespa.md", "author": "LlamaIndex"}}, "ba6220c4d1135e18526b108ac742322b1014217d": {"node_ids": ["23188879-d580-4d78-ac38-662f1fef9267"], "metadata": {"filename": "weaviate.md", "author": "LlamaIndex"}}, "be9a7e4fefe33933328aef0d5cf5dfcf2b4dffda": {"node_ids": ["2956c786-241c-48b9-9db1-3043e1b5943c"], "metadata": {"filename": "wordlift.md", "author": "LlamaIndex"}}, "ffcdba15338dfc47c9283667b90c8f66a5c9b8a9": {"node_ids": ["0f9265d3-13f6-4cad-90f4-ee5c76789d09"], "metadata": {"filename": "zep.md", "author": "LlamaIndex"}}, "2b4a3fc749f4d048c2ea6204c892209ebea3eb98": {"node_ids": ["76c91302-7080-434b-9d4d-d62a08560487"], "metadata": {"filename": "arxiv.md", "author": "LlamaIndex"}}, "8e2f71de0f7d7a9a07ea5e648d7452e048734181": {"node_ids": ["922b9b4d-8561-4fda-acb3-861af6969d67"], "metadata": {"filename": "azure_code_interpreter.md", "author": "LlamaIndex"}}, "3fb4214c7ed2b451df9586d4583e5165db32258c": {"node_ids": ["3390ca43-b087-48cf-a2b3-cbb2e37ae46b"], "metadata": {"filename": "azure_cv.md", "author": "LlamaIndex"}}, "c1a080368716b8002a0030cddc06c896fb148aae": {"node_ids": ["15c7affd-7942-47d9-9295-45a239eaaeb1"], "metadata": {"filename": "azure_speech.md", "author": "LlamaIndex"}}, "9293f065b3de98115a89c139d3cd943c59689e71": {"node_ids": ["cf142090-7adf-4ccf-8a13-c18990c4cc04"], "metadata": {"filename": "azure_translate.md", "author": "LlamaIndex"}}, "db9f58b998e05a0ae5b4bbd1f1e71943bd1302be": {"node_ids": ["ace5b223-3c3f-492d-b05a-f8f1dee1d297"], "metadata": {"filename": "bing_search.md", "author": "LlamaIndex"}}, "64466a160145bf1459ca20e9229319793f4a4b4d": {"node_ids": ["008aff95-6183-4df3-9e87-e2ce758d8627"], "metadata": {"filename": "brave_search.md", "author": "LlamaIndex"}}, "d3f3fea719d34c164af774dac1057541e9664843": {"node_ids": ["8bb63174-ff12-4157-85d2-3ca28772639f"], "metadata": {"filename": "cassandra.md", "author": "LlamaIndex"}}, "355baa269b5f919f13276d6e1e850850e8a6acb8": {"node_ids": ["1996a62f-99ac-406a-b16e-15efcae0611f"], "metadata": {"filename": "chatgpt_plugin.md", "author": "LlamaIndex"}}, "e3fa46ba542a11f7e57d6be7b26f163b5dca796c": {"node_ids": ["e8577d57-b362-4376-a01f-6c553296fb56"], "metadata": {"filename": "code_interpreter.md", "author": "LlamaIndex"}}, "709c7ef9e4dd22946d2fb1d95828bbb1bb963e64": {"node_ids": ["f7b597a8-a824-42f4-98e7-f2a7fc1e0658"], "metadata": {"filename": "cogniswitch.md", "author": "LlamaIndex"}}, "07504dcad16ce033453209dae761505727fcd499": {"node_ids": ["055e416e-1f0f-4559-afce-b30035ecd35b"], "metadata": {"filename": "database.md", "author": "LlamaIndex"}}, "5d48014c22505992c57c236c0a706f823adf94aa": {"node_ids": ["b28e076f-6e66-4d50-9882-5db330c1c922"], "metadata": {"filename": "duckduckgo.md", "author": "LlamaIndex"}}, "b4aa40d5e2d4d5142e21e83d58a1ca1d78255f8f": {"node_ids": ["c2cbba05-82ba-4def-86a5-341404c0981a"], "metadata": {"filename": "exa.md", "author": "LlamaIndex"}}, "185d816feff18846072b1907c15972a1882847cf": {"node_ids": ["0792ec0b-7959-441f-9dc4-736923e194ae"], "metadata": {"filename": "finance.md", "author": "LlamaIndex"}}, "116e4227ef84b8c40d1b29ae0741163409e2e7bb": {"node_ids": ["0b137793-b111-47e5-826c-c607a0bf5fad"], "metadata": {"filename": "function.md", "author": "LlamaIndex"}}, "6d854217c50954078a94f89a6e00e9314c689130": {"node_ids": ["ac89a504-3734-4ef4-8604-8a4a1448b954"], "metadata": {"filename": "google.md", "author": "LlamaIndex"}}, "b7d3b6f27c74f7f395a2673bcf18dc12795608db": {"node_ids": ["b2f5b22f-8a2a-43ba-bc5d-7ee6f9d6e1c9"], "metadata": {"filename": "graphql.md", "author": "LlamaIndex"}}, "2034149b2f552493715eee753be92de74c900e6b": {"node_ids": ["8589cb46-714d-4cd6-9d9c-f2724f2318a7"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "4317cb97eb1283f0b9a9f29b93bcf92af4e5036a": {"node_ids": ["aa58bebe-d887-4832-92a5-18e1ea195165"], "metadata": {"filename": "ionic_shopping.md", "author": "LlamaIndex"}}, "cc284055768ce117814504803eed3b6f274f792d": {"node_ids": ["4e372d18-974f-43f1-aa9f-a14938f381b5"], "metadata": {"filename": "jina.md", "author": "LlamaIndex"}}, "a8e9cfa2d224b5ff0c47542732693b077077076c": {"node_ids": ["2c8303b5-7b84-4353-9493-b6a1a4f05525"], "metadata": {"filename": "load_and_search.md", "author": "LlamaIndex"}}, "46f7195d9d60fd3c04da10c193bd535e3a765603": {"node_ids": ["6ad931c6-3a7c-4b35-ba4d-b0be8ae7d975"], "metadata": {"filename": "metaphor.md", "author": "LlamaIndex"}}, "6ef82f7852fe9bde8dc29e657aeba9183e0206c5": {"node_ids": ["d177ff11-7f57-47e4-90a8-8fa778bc6374"], "metadata": {"filename": "multion.md", "author": "LlamaIndex"}}, "9fd2b81cf0ad3f5097092f074b46c83b480c51ff": {"node_ids": ["c438d13d-beef-4565-bce9-0d99dfbfa37d"], "metadata": {"filename": "neo4j.md", "author": "LlamaIndex"}}, "abdccd6c70178d88056d5c3f95f5e682c01552d2": {"node_ids": ["938632da-d05f-4c8e-82e0-36918dbe9996"], "metadata": {"filename": "notion.md", "author": "LlamaIndex"}}, "71ab9b855e9fe249152b08e3a31bbfa2d94f2ef4": {"node_ids": ["0fc305b6-71f9-476a-8f45-aca460aed43f"], "metadata": {"filename": "ondemand_loader.md", "author": "LlamaIndex"}}, "333555e6837acfc985ee503aff3738b02e235a83": {"node_ids": ["debaa7e3-422d-4d07-9edd-755e731bfdea"], "metadata": {"filename": "openai.md", "author": "LlamaIndex"}}, "0e7bd1f42fb9f323c6aaf5b7b0cb850e085edd39": {"node_ids": ["c683e8f5-8c15-4e71-8511-410674b7f919"], "metadata": {"filename": "openapi.md", "author": "LlamaIndex"}}, "322464bc0ed19531e31cc13022d897b39ac35590": {"node_ids": ["fd30374a-e820-4eb9-9167-fd480e30924d"], "metadata": {"filename": "passio_nutrition_ai.md", "author": "LlamaIndex"}}, "9f4491da62a9777a1a33c7ce82d443ab3e6647ff": {"node_ids": ["55b573e0-3f17-4b46-834e-cd7edf3604cb"], "metadata": {"filename": "playgrounds.md", "author": "LlamaIndex"}}, "e0d6b2562242f80ba727b940ac329a95deca78b2": {"node_ids": ["09a5b943-1426-4988-89c5-7c2e4090c038"], "metadata": {"filename": "python_file.md", "author": "LlamaIndex"}}, "78435b150a662a66178e45f86288bda056ec9c0f": {"node_ids": ["592f800c-b972-4cdf-9133-4af9227a713b", "f5760e3b-f2f5-4bf4-a8c0-a5025df82f9d"], "metadata": {"filename": "query_plan.md", "author": "LlamaIndex"}}, "de6f92bf433429fd8b435bb1722fdb02989e28a9": {"node_ids": ["c59e0268-97f2-4726-b376-5e65cf68c40b"], "metadata": {"filename": "requests.md", "author": "LlamaIndex"}}, "3de3f185942a3ac31c1cfe13013090ad661047dc": {"node_ids": ["c9f6b55b-7a90-45b7-bf75-4583a2fa1124"], "metadata": {"filename": "retriever.md", "author": "LlamaIndex"}}, "cdabc731c10cbc98d779251698cb8708675bac6a": {"node_ids": ["a105becf-315d-4b1f-81d8-a38ff6255bc6"], "metadata": {"filename": "salesforce.md", "author": "LlamaIndex"}}, "db09301091bb16ac12c1b067d7d40b6bad9cdd58": {"node_ids": ["5758ba7e-24e0-4aea-bf1a-cbef907491b9"], "metadata": {"filename": "shopify.md", "author": "LlamaIndex"}}, "523be4b94a0d269e9a6cd3d6da72901750e0f104": {"node_ids": ["f7657528-a354-45c4-ac97-61baa19f3158"], "metadata": {"filename": "slack.md", "author": "LlamaIndex"}}, "f325adc1c1e0eda52891815d6ec3b9a952e76e97": {"node_ids": ["3940a82e-afd8-4901-ac8a-1e3945ac8c80"], "metadata": {"filename": "tavily_research.md", "author": "LlamaIndex"}}, "477869049c2a71e82df47e8a89bf134a5beac7ac": {"node_ids": ["34878d76-7c56-4264-8b52-7b4fc1a7598f"], "metadata": {"filename": "text_to_image.md", "author": "LlamaIndex"}}, "d6ea9f728754d2956e82560cbd9108bf82389c30": {"node_ids": ["9e65927b-ef8a-4f66-8867-49e0a51c20e7"], "metadata": {"filename": "tool_spec.md", "author": "LlamaIndex"}}, "79c5528e23c27c710fa527deb8f06b1f6523bb0c": {"node_ids": ["afaa2348-da83-4c89-86cb-bd8aad8781d5"], "metadata": {"filename": "vector_db.md", "author": "LlamaIndex"}}, "3cba2508ffe2fa6002ad3ce3cdddfb09d82d57dc": {"node_ids": ["58f02a33-44f3-48e2-b746-b051cc4bda79"], "metadata": {"filename": "waii.md", "author": "LlamaIndex"}}, "ab67ca65cd70b806625d9e2ca7b36affaae86440": {"node_ids": ["aa0302f7-f023-4246-8726-817e374aa6b3"], "metadata": {"filename": "weather.md", "author": "LlamaIndex"}}, "5236ea3806a6f74a0f39a8f8f5e6d81738863a1f": {"node_ids": ["445ce08e-85dc-4214-8d57-704488610acf"], "metadata": {"filename": "wikipedia.md", "author": "LlamaIndex"}}, "9289f3e605d97cf186122f19631d36af404a4716": {"node_ids": ["ce779173-1b03-4fe6-a456-655532dc0897"], "metadata": {"filename": "wolfram_alpha.md", "author": "LlamaIndex"}}, "53c3b8875410015ffd75bc751ef3aab7db685e16": {"node_ids": ["a8699425-f21a-4ad9-89f5-4fe4732afcf9"], "metadata": {"filename": "yahoo_finance.md", "author": "LlamaIndex"}}, "9a70b8dbca4493642a51af7ab7dfba62be404c6a": {"node_ids": ["d8977ccd-79c7-4522-b554-73bf1df21832"], "metadata": {"filename": "yelp.md", "author": "LlamaIndex"}}, "7415d36f570f9b2733da56309398b61852dff424": {"node_ids": ["1407522f-1913-4615-b6a5-a60c3339aef0"], "metadata": {"filename": "zapier.md", "author": "LlamaIndex"}}, "c0b157307b096ffca8d28c528806272969883a9c": {"node_ids": ["baffdb5b-10a8-433b-ab32-722caf695fc4"], "metadata": {"filename": "deprecated_terms.md", "author": "LlamaIndex"}}, "61b233f691ee83fd88e7f7199de862d110f01985": {"node_ids": ["b3d7e1e3-edbf-4fee-be3b-5e771d7a799d"], "metadata": {"filename": "chat_engines.md", "author": "LlamaIndex"}}, "8ebff447280ac4fe02791afad8900922d7d1c9a2": {"node_ids": ["23b06cd4-b1bc-4eaa-a8d1-f699060821c6"], "metadata": {"filename": "documents_and_nodes.md", "author": "LlamaIndex"}}, "a53735a48d5c8a415373181e7e97b83e2982ffe6": {"node_ids": ["d75ea97a-5bd0-4e3a-974c-d56342fc8162"], "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}}, "625b12aadf4db855f603ab991d12ed43dd7dff19": {"node_ids": ["2471f739-df7b-4d34-bf72-57bee1305c11"], "metadata": {"filename": "llms.md", "author": "LlamaIndex"}}, "5e7f93f9bb78878e74f9a5599bc9486ddd378819": {"node_ids": ["dd501d63-f773-47f8-b9c0-b7f2518fb8d6"], "metadata": {"filename": "query_engines.md", "author": "LlamaIndex"}}, "ff91e1a6eb875f799bb32c8fa58eaa54d04fe6e1": {"node_ids": ["020aea8b-1da4-43d3-bbc7-3c934e1b8b3e"], "metadata": {"filename": "vector_database.md", "author": "LlamaIndex"}}, "71b6ef509a2ab7c2175c22180fe1900b60f5a0f6": {"node_ids": ["945e1359-4010-4118-b2ab-850389660ab5"], "metadata": {"filename": "frequently_asked_questions.md", "author": "LlamaIndex"}}, "c76904866ae5040ec1441258d3543405e3b54537": {"node_ids": ["292179f6-10e8-444c-9f94-39e89c3c0b22"], "metadata": {"filename": "full_stack_projects.md", "author": "LlamaIndex"}}, "5a72cb856d4806600a20aebfe2269318aaf606d2": {"node_ids": ["51de17b2-66f0-46c5-80dd-e4d6320f87bc"], "metadata": {"filename": "integrations.md", "author": "LlamaIndex"}}, "816ae0aacc09beb27107f1a46864e8bf70945e7b": {"node_ids": ["d43b1aa6-4a86-44ca-9ef8-a4aa1acade23", "9e8bda47-2ea0-458f-9cd3-d0a05b9d569c"], "metadata": {"filename": "chatgpt_plugins.md", "author": "LlamaIndex"}}, "9ee13d2045ec53fe785be09e8395f3e5a7aa31bd": {"node_ids": ["ef08b63d-1ff5-4281-81dc-e05059f0da88"], "metadata": {"filename": "deepeval.md", "author": "LlamaIndex"}}, "59b15025d2314a7d91b792d39f42fec3ad8d9928": {"node_ids": ["85fa35a9-6d8c-4a8b-b6b5-2101e643907a", "ab2a1282-0a49-4fa2-ac77-4d914dfc5244"], "metadata": {"filename": "fleet_libraries_context.md", "author": "LlamaIndex"}}, "5404ce4620cc5b83879a69b31999055595dfcecb": {"node_ids": ["52efb7ed-147f-4041-b745-4298216dc14a"], "metadata": {"filename": "graph_stores.md", "author": "LlamaIndex"}}, "51fd3ad759e55dda2ab917c0de14aa3a85241481": {"node_ids": ["7c3f7c90-ae57-4a94-93e8-67ed3cde8a2f"], "metadata": {"filename": "graphsignal.md", "author": "LlamaIndex"}}, "10715a649a423561966a0e92d4b8b73dc8f759fb": {"node_ids": ["7eec17f9-6748-4cde-bb55-0ef1096cb92c"], "metadata": {"filename": "guidance.md", "author": "LlamaIndex"}}, "ff51ec2aaec9e9b17d6bd2eeccf865480c1ef8e6": {"node_ids": ["068fdf37-cffb-4ec8-aa88-804d885d395d"], "metadata": {"filename": "lmformatenforcer.md", "author": "LlamaIndex"}}, "7abc011a97485e5f645b91b8f5dc07b71b8fa8ef": {"node_ids": ["d1db8893-6947-4ad1-8efe-868d4db5a704", "69f14098-8699-43d2-b987-20acbb011b0a"], "metadata": {"filename": "managed_indices.md", "author": "LlamaIndex"}}, "453fd7e59647e91d21ed790a04b0abb9438daf1f": {"node_ids": ["8b3ac817-4e6a-4d82-9174-143c408338df", "374a059b-5a51-4cd4-b599-4f0be4d84033", "9d8e296b-b279-4ba6-8b45-6a6817bfbffa"], "metadata": {"filename": "tonicvalidate.md", "author": "LlamaIndex"}}, "6e0236302a716f81414e0841e20dbcf67e7a4510": {"node_ids": ["871f9e51-41b7-4c13-b5b0-58cd55c8bae0"], "metadata": {"filename": "trulens.md", "author": "LlamaIndex"}}, "8aa9273228ccd9b32167d9f4fc72091f96ad92d0": {"node_ids": ["9ae40079-a15c-4620-944f-afdcc08cc6d5", "04c517d5-22f8-40e9-9628-0df073b446d8", "90a3483c-3681-4dee-adb8-14fd194c0a9a", "a451b0e3-04d3-4f66-afc7-6680ea984f2d", "12ce2c8c-66b0-41d1-88c6-e33210581848", "1778b502-7e0a-4491-bbe0-28ea5af9b3cd", "6700c1d6-15f8-400e-8fd9-0fefa8f2ca28", "bddee492-c1fe-4af1-891e-67b228788bf4"], "metadata": {"filename": "uptrain.md", "author": "LlamaIndex"}}, "33e48c1eebf15a1de8645abdba0d9d3e14b52354": {"node_ids": ["762e4b18-2a15-4e06-bab1-26aaccd04d16", "f4abe42c-d54d-4d45-9c97-78c827dd66e8", "bbe143fa-1682-477b-be13-dc548e826e27", "aa459331-8fdc-4afe-8d6d-6f9ad154e68c", "821b4839-9952-4d71-9a18-f49a07ba3a8f", "b024718d-2f71-4a2e-86e5-3e9747c17c70", "9caa6f7d-2d68-4650-a0ed-9c6697363e25", "974be74c-dd52-4dcb-9a6d-31370839ae67", "749aab62-5a39-4801-8622-7ba1658e44ef"], "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}}, "916ff78cec40c2c67492af2e717ed2276683ec41": {"node_ids": ["2d35da44-b194-4e02-9d90-d84a6afde63e"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "d18e0fa6c6ef302a627263bbeaa9304cf00cf783": {"node_ids": ["cfcd6113-628a-412b-8e39-164aa737814f"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "bc10f69934e71f412af3a179f180659770cce0d0": {"node_ids": ["075c2f7b-2ecd-40fb-9026-c9e034f2d1db", "47463a30-136c-4f56-906d-2d95f521adfb"], "metadata": {"filename": "concepts.md", "author": "LlamaIndex"}}, "96e9abaceb5401652823a676eeea7ddd55e2f8dd": {"node_ids": ["c227ed5f-8fec-4610-835f-184a4c091c6a"], "metadata": {"filename": "create_llama.md", "author": "LlamaIndex"}}, "58672402edf2a60067ad81e382e144ca2a4384e8": {"node_ids": ["2ae9acf8-04d0-48e5-a543-b6ab5a16f6de", "6ff7170d-66ee-44b6-9589-2d0edbe0ef55"], "metadata": {"filename": "customization.md", "author": "LlamaIndex"}}, "cf80744237deefb64f56086b326ce0580b43033c": {"node_ids": ["1035c692-0298-480f-85b1-992d1d6e11f1"], "metadata": {"filename": "discover_llamaindex.md", "author": "LlamaIndex"}}, "6a47db45817a3ffd872c3924d6d0506c02130060": {"node_ids": ["85afe527-52eb-42ab-b52d-318713e0cb30"], "metadata": {"filename": "installation.md", "author": "LlamaIndex"}}, "c18c27023b15f1ba0a55f687ddb894da962ad0c6": {"node_ids": ["7c950f0c-b34b-41bc-bbce-054a3ff0c6da"], "metadata": {"filename": "reading.md", "author": "LlamaIndex"}}, "05be9f4d914e8dd038fb31eee61f1c41fd15afa8": {"node_ids": ["f89b8060-14c9-4920-b261-3564b01b807d"], "metadata": {"filename": "starter_example.md", "author": "LlamaIndex"}}, "bb99e8ea9919822e6be72f75bb353e047c2fc65d": {"node_ids": ["5832e624-06c2-450a-aa8d-b35a4ae589a8"], "metadata": {"filename": "starter_example_local.md", "author": "LlamaIndex"}}, "096a588190ce7458a2ee39bba893a36a105cddef": {"node_ids": ["7698bcfb-b51c-4088-acfe-f5f7094415c9"], "metadata": {"filename": "starter_projects.md", "author": "LlamaIndex"}}, "e806aa9811b226443b89d58d38d935b5870c416c": {"node_ids": ["85566be3-83f8-4b51-be47-7f0390262b33"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "9e8fec25473a3a8acbf31c22c0724e70656b79eb": {"node_ids": ["296105a5-d1ef-49a5-bc74-4257ce966caa", "f966ec6f-bad4-48fb-bdfb-1044c2ad8c91", "bdcec2eb-6d10-48ab-9710-cce8e567f72d"], "metadata": {"filename": "rag_cli.md", "author": "LlamaIndex"}}, "fc96944a4a6324fae65221bbb5ae98f94eab2d29": {"node_ids": ["d3ba5492-f10e-4161-9f52-419f51fed945"], "metadata": {"filename": "v0_10_0_migration.md", "author": "LlamaIndex"}}, "ea415d37e1d324bb37943c19ed2e911f8b6575a0": {"node_ids": ["c168c344-3d07-4216-879f-559c38f1dc68", "3e5d43b6-bd31-4fcd-9989-59c90b07994f"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "10936e3d4d5965b1d10371f938d4ec5412780f5f": {"node_ids": ["afa98542-ed95-4d19-899f-02ba8b72d152"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "3df7f73a5d0b0323962df3503372039615edce86": {"node_ids": ["e7ce5151-bfee-492e-93aa-1b706fe3d763"], "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}}, "8a5bcc880bf595840eb43742d69c694c7caa9ea6": {"node_ids": ["1b22d224-b093-4a7b-a833-c189f1c95769"], "metadata": {"filename": "agent_runner.md", "author": "LlamaIndex"}}, "97d01c63bebf308fa89525d6bc23a465ff5fe10b": {"node_ids": ["faa587c0-1bff-4ac2-90a7-5ffe096eb1a3"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "ffc2242219ed84c2611720b75a4dc898f8cf9137": {"node_ids": ["87f906b3-263d-4145-b0fd-7b06da6db63e"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "830b99379ad5a29329aa2d7c01205eb59cb03b07": {"node_ids": ["d68c4121-62b3-45af-b381-024dddb2a546", "6c2efcbb-f5b8-43fe-b397-0b9dbcb5431e"], "metadata": {"filename": "tools.md", "author": "LlamaIndex"}}, "7f783826a679fe51abb4c0d66fdc46c9c020e279": {"node_ids": ["f5043496-0a82-472c-8a40-d5078c3b8f93", "1d8ab80f-bb36-40b8-8ae0-8e8d774a27f7", "ca2dfc41-c648-406d-9a97-de4a51b4a9bb"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "90a29a791ea806e4f8165ea892ad3cab04964e80": {"node_ids": ["71c9be82-2fc3-4242-a4ad-a384caa76eca"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "5d514bf223b78bb6aec5f04d07eb8539aa351a2b": {"node_ids": ["adb51f20-8ed7-441e-9a2e-c11fe2c94ded"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "654c85513f7e2ebf22f882ab7584dbad7e414e4d": {"node_ids": ["7c6a06c3-5afb-46f0-99cf-abdf8c23675e"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "baad5c9fc1e138f10e9032dd52d6701b6afafc23": {"node_ids": ["8c14e1b4-3b02-43c8-8318-59b091d5891d"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "e1780c4afaa8a6255c1434ec43e9acc22bfc7074": {"node_ids": ["e40b6f0b-66c6-4b35-a706-97d76da18af9"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "b37a463d35c14412c1cc3c083db5849fe3546f17": {"node_ids": ["f78eba9b-4c52-4562-b58e-c3e5c3077c71"], "metadata": {"filename": "response_modes.md", "author": "LlamaIndex"}}, "f1ea31222f5293c15f9309b5e8fdc7b60fceb400": {"node_ids": ["d7f1df1b-6799-424c-ac1d-01fa8a9c4be4"], "metadata": {"filename": "streaming.md", "author": "LlamaIndex"}}, "e33362e052dbd20d533a14930fd8fb6fd8f0a1d6": {"node_ids": ["3ceb3a0f-5076-47bb-a58c-cb4ee72e3a6a"], "metadata": {"filename": "supporting_modules.md", "author": "LlamaIndex"}}, "26d2a4ea4bb24eebf07cad04779b578d822643cc": {"node_ids": ["d2603283-c8ba-4411-b89d-b1ad5ab873a7"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "69070b34fa86205567406e2c0eaadbb58f27ff4f": {"node_ids": ["275b0e72-b556-4a60-9f1f-04b37ad2d886"], "metadata": {"filename": "contributing_llamadatasets.md", "author": "LlamaIndex"}}, "48d75b05a3a9951a67b52b423e6527fb07921ebb": {"node_ids": ["923f5321-9efc-46a2-839c-a29566c04bfb"], "metadata": {"filename": "evaluating_evaluators_with_llamadatasets.md", "author": "LlamaIndex"}}, "cca36f8bb178dbcf9ade3e622a08968f6562ba4e": {"node_ids": ["d5541ebb-c5c0-479d-9c3b-06ee8b534a26", "7817930b-212b-49bd-908a-731e14618654"], "metadata": {"filename": "evaluating_with_llamadatasets.md", "author": "LlamaIndex"}}, "b9fda169d2e74ed48369492a5eda601273460ff7": {"node_ids": ["a86b4e56-4314-47da-98a8-d2631b6addc9"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "f881cab5d82a49f2ed10efd47d7828cfe9a8ab52": {"node_ids": ["502c2412-a41f-40d9-95ae-6b8b2cffd446"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "f8586922541acf4113bdcfe51f28d0ebad05f715": {"node_ids": ["51f15e10-99a6-462d-91dd-416c3ea15636", "a4b4cd52-be2a-4dbc-b30a-1aef6cd2bfb2"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "ffae42c9393a37ada2fbf9ed7d462e36ecd838e8": {"node_ids": ["83a5d336-db48-47fc-a5e6-bd51d457928e"], "metadata": {"filename": "usage_pattern_retrieval.md", "author": "LlamaIndex"}}, "be87b382526ef0e021562a7d760349640eb423ab": {"node_ids": ["ae7ff0f9-b680-4325-acf3-2a70ac99752a"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "97b753800ec87dcb4bdb5b0e49cd4c95d7e8b038": {"node_ids": ["7196dcb5-366b-4075-86cd-d4c9e3f528c1", "8ba3e436-d5db-4ebb-804c-1b87b9c8b4a1"], "metadata": {"filename": "document_management.md", "author": "LlamaIndex"}}, "1e48eb7420437477cadae15b0255a6661888fd83": {"node_ids": ["88b1db63-99cf-451a-bdd4-82497d2adb58"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "20c7bc0cd799f988a578e8ddf58de75afe4604ee": {"node_ids": ["a10a7420-9c69-4454-aa62-1eac19023ab2"], "metadata": {"filename": "index_guide.md", "author": "LlamaIndex"}}, "966b2643797e3d9a354326d911abf292c21f2eb5": {"node_ids": ["109027ef-a4c6-427b-985a-d6eb2873aff7"], "metadata": {"filename": "llama_cloud_index.md", "author": "LlamaIndex"}}, "505edb5b7bfd91a42b0ada3720ba24599dd9c885": {"node_ids": ["01a5c1a0-9148-4228-97b0-15885528aa9f", "99644ce3-417d-41a3-8512-fe058705dbfc", "d555cd51-f1f7-4fc0-b79e-b5f1b4fabc0e", "a1c0ae13-4b34-4b35-8aa6-cd9007b59c22", "256e9a35-9af5-4466-87b7-18b9f7a13201", "42cfa1ee-ce02-450e-bdd0-453468e0c07d"], "metadata": {"filename": "lpg_index_guide.md", "author": "LlamaIndex"}}, "003fdbf2cfd080fcda93238c19ad70a942de95db": {"node_ids": ["d1e2dcfa-a758-4dd2-97d8-3830c9bcdbcb"], "metadata": {"filename": "metadata_extraction.md", "author": "LlamaIndex"}}, "f9f07e858ff7f7ed3f676c006fc3295319ecd6d2": {"node_ids": ["bb9aa5f6-ebab-49c9-95b9-36f24a564c2f"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "f0d2076e49dc2ca51f0382a9ad9bbc12d1b196cc": {"node_ids": ["cac9ece9-1f14-4866-986b-31a55cf7bb5a", "12468260-8cef-435f-907a-04367701d5ea"], "metadata": {"filename": "vector_store_index.md", "author": "LlamaIndex"}}, "db1628c3f26ab1e24771a7bb9a9fded18ba0db60": {"node_ids": ["8d598c8a-2401-4a87-81fa-d1b6d168e09f"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "cb93eb0420a35bd32139dc025311c9f02731162a": {"node_ids": ["41bb98cb-73cd-4afe-9dd7-5cba47e857dd"], "metadata": {"filename": "llama_parse.md", "author": "LlamaIndex"}}, "a4e2b156a173ab2c193789762a34d7c4f2c1045b": {"node_ids": ["8bbc4f80-32ef-4f92-b8f3-b625a7bcfff9"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "febe1a6dc3ae70f1ccc84d9ab73b9b0d4d41f4d1": {"node_ids": ["ddefba23-0586-490c-912a-5ec6c11cb97e"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "b9a2cde29bc150fa880d1870c64523de9e3959eb": {"node_ids": ["55db4779-fe1f-4228-86ca-40db50c7cdc7"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "5aea5486d84d09f270c793a59536240261f8810a": {"node_ids": ["8d49d386-97a3-497b-b97d-6930b747b955", "3e213b74-6511-4a04-8d2b-ffbeda15e6bf"], "metadata": {"filename": "usage_documents.md", "author": "LlamaIndex"}}, "a6651b33b6853f1c4f0987cb8490f884551e448c": {"node_ids": ["9630c7e9-f254-418a-929d-e34957752420"], "metadata": {"filename": "usage_metadata_extractor.md", "author": "LlamaIndex"}}, "b9d2857886388f91254d810017f749b7a953526b": {"node_ids": ["0f571887-a1a9-418a-b82a-26bd8f9a6198"], "metadata": {"filename": "usage_nodes.md", "author": "LlamaIndex"}}, "3c59e954477b96d4c8ef57891fc813cf4a770611": {"node_ids": ["abc8f1f6-6097-4a59-9262-8c0251dd4b54"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "a7b4bc4956dc73a6179c2c38c289a7c578bd82f4": {"node_ids": ["de3fa269-76eb-4504-81aa-01c4d11e8623", "312097a2-60ff-4c06-a683-def9dde59b9b"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "945556c52ede7e60e504797cf3f6fd8ca72ea488": {"node_ids": ["b4b13daa-4c47-4309-bb1b-025cfda1a80d"], "metadata": {"filename": "transformations.md", "author": "LlamaIndex"}}, "fdacaa57234ac784192ea6e96219f5a450e733e6": {"node_ids": ["9e0e9601-4c6e-4b9b-943d-8792acada127"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "16a2e41dfd88d8ce6c6c34540bcba1a965173514": {"node_ids": ["b929670a-dd74-4230-a697-36b7945b3a77", "3cbb4201-a375-4ba0-98ba-57c95fcc1e85"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "dce108780a52d09a89f0720320a5dc820fa9887b": {"node_ids": ["d81c3e60-6c45-4fd7-839d-ba7b6e0e4076", "cfd62baa-6d4a-4f20-b1aa-58c2a3375864"], "metadata": {"filename": "simpledirectoryreader.md", "author": "LlamaIndex"}}, "3eaef138dde935320ea0ea0d9062eed782ee2d91": {"node_ids": ["c202918d-9b5f-4f43-80ea-e1abeb5c9609", "ef3eb03d-7622-4fc2-8248-b2dffea3a2cb", "e45c70e1-2d02-4c14-9d03-814dbd76623f"], "metadata": {"filename": "embeddings.md", "author": "LlamaIndex"}}, "2b1cd9fc723729fb62e156324143f99c75482674": {"node_ids": ["f9b3f4dd-d922-439c-a8fc-be1c6639103c"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "33b3f4b3aed48c0516c4e215faac5ca842d35987": {"node_ids": ["14cfdac9-4e3e-4918-8ff0-27c4a9fb256d", "9edfdc8b-14a2-4ef4-b70d-6d3e3ad8d220", "896cc3dc-06ad-4ca1-a99c-6311ec30e707", "9baea55c-b96f-4b84-89a3-82ec48aaa859"], "metadata": {"filename": "llms.md", "author": "LlamaIndex"}}, "59eb08939c32fd4818c84ce9d7e7590dc52305d2": {"node_ids": ["d2102222-0cd7-4c3b-9434-4cded109a41e"], "metadata": {"filename": "local.md", "author": "LlamaIndex"}}, "6a95eac841f49acc114f9ae658bbfcb61f581af5": {"node_ids": ["dc3d7afd-ca11-404d-a784-2dab32b61dcd", "3c33f1e3-3346-4155-8fda-b07b7f09346b"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "0f517a6b4d1de1f17ce42be578c9bb71d6f3645b": {"node_ids": ["dfcc0107-f2e4-45a7-bf3a-7e1c2bff1b30", "59af9214-fe9f-44c1-b909-4693911612b3", "511386b0-0f77-488b-af76-53e45eb182fd"], "metadata": {"filename": "usage_custom.md", "author": "LlamaIndex"}}, "7a2118da9168600fea22ccefe226ebe714f2e6c2": {"node_ids": ["5d68da8e-8a9c-425f-a5fd-8d111fe93921"], "metadata": {"filename": "usage_standalone.md", "author": "LlamaIndex"}}, "dc2edf5349b16d42d5547db8da4d674ea3f15254": {"node_ids": ["88f8da1f-8bdb-4d3e-bcbc-70d3c0ffccc8", "e2059480-fe0e-41b6-8653-0cb34fa5f4c5", "de8feeec-0063-4926-8d98-6720c89974b2"], "metadata": {"filename": "multi_modal.md", "author": "LlamaIndex"}}, "e0a9fc269f28231be4b85de9c8bb64f6dd0efd59": {"node_ids": ["40eed48a-468c-45e6-8754-b488fbe02c9d"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "0f9f0fda622658f6cda0735aa0c2948a60c5f9a5": {"node_ids": ["b47abb54-6d58-4d05-9566-73353805deff", "1dd800ac-82c6-4883-8ffb-1de7a0b4dbe5"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "46f4b004e019779b647ea31563438eea98a10cb0": {"node_ids": ["77ae39e2-9b9b-42c2-afce-463f8733d32e"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "32ed2ba753bd1b4012ea41e7ff2024775759e300": {"node_ids": ["7422c027-9d12-4fc9-98da-dc047a5d19c6"], "metadata": {"filename": "root.md", "author": "LlamaIndex"}}, "adeda652217e9ca8b70ffc662c540458dfb35011": {"node_ids": ["c94b2ee2-472b-4b75-b5b6-a867e5153535"], "metadata": {"filename": "token_counting_migration.md", "author": "LlamaIndex"}}, "e9aad35cf42e6bdc61b8812e47db2967cf06f4d7": {"node_ids": ["f69e6f63-7b75-4617-9f89-8046c04b91a7", "9881c9a2-7cd4-4693-adb4-f1893ffd6583", "271b31dd-0780-4d59-8afd-1564ec8174ab", "6c2d0c50-c887-4f6c-adb1-c3ee46b634e5", "9ea3e6dc-2ce2-402c-8900-85244acfa8dd"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "087f92b3f99e3b6f0bd593c8d761c192e801b497": {"node_ids": ["0b5966ce-1ee7-4440-810a-d8405e500f90", "5a43cce5-d1d7-4b5c-a87a-2a69a8753985", "440c4567-07d1-49dc-a3b0-b00ba048f8a9"], "metadata": {"filename": "instrumentation.md", "author": "LlamaIndex"}}, "90c457c0f61b534d3528b3768c98c033adf65c76": {"node_ids": ["ad32417d-ceea-4b77-acf5-32bc4d9510b4"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "7775a580b5808b1e16dcd3b82eca11a1cdaacaf1": {"node_ids": ["88fdeaf9-65be-4a04-943d-5ced50737a90"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "42918eb6e978a1c0bf4bf0726c56d7c8cba3e49c": {"node_ids": ["32b89b34-f75d-4e98-9f3b-b0d9ee92200a", "001aa0a5-bfb5-4ae2-8b97-26e646350e3e", "56e75dc0-8c34-4684-8dc8-3a794abe9f5b"], "metadata": {"filename": "node_postprocessors.md", "author": "LlamaIndex"}}, "6271897bdcd950cdf7bce0f307d1caf7b7bb03fd": {"node_ids": ["dd2c94fe-cb58-459d-9d5f-23d93c47e5bb"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "9b1ee63acc0a41105f5f20f3a969f3b7a7ba7f0a": {"node_ids": ["7344e602-b6a4-41d4-b350-9597cf183478"], "metadata": {"filename": "module_usage.md", "author": "LlamaIndex"}}, "4c0ab1e45f97c43b64548c664b4cf93fefd63c94": {"node_ids": ["e42fe406-f404-4183-b6d8-7298a465577c"], "metadata": {"filename": "modules.md", "author": "LlamaIndex"}}, "bd2aac0f6299e5f3d78d6adc03033b85752cea84": {"node_ids": ["e5ba981b-78f0-43c9-9d0d-a39343ee3110", "abb3f398-33ae-461f-bc6d-25976e8f0c1e"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "6bde276b52e12284b88a00264f708a23e045215e": {"node_ids": ["8ceb340a-2197-4aa2-9042-1de2c606f12a", "017ba432-8661-462e-ba0c-cc9858f88a91", "005dc515-1fd8-429c-9dc6-3c5ead7bd887"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "72bca5ffc14ac583cabd4f18a30c37965743159b": {"node_ids": ["95fd65dd-eb21-4164-bd0a-d9c6622f2041"], "metadata": {"filename": "response_synthesizers.md", "author": "LlamaIndex"}}, "6f4002b19f77e12dc75b5fcb4338ee6624d5f0e0": {"node_ids": ["30f35635-6c5a-42f1-91fb-1115acaa47c9"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "fd5a0040fe0758d794e9feaf60baa92ff4a4460e": {"node_ids": ["4d7add49-b129-435e-834d-7b315f696c35"], "metadata": {"filename": "retriever_modes.md", "author": "LlamaIndex"}}, "dace94e70011355e27cec93dee718c3e229afed4": {"node_ids": ["56d9a115-a1cc-4148-911d-c3be4426c228"], "metadata": {"filename": "retrievers.md", "author": "LlamaIndex"}}, "d9cff212012951393eb8ceb0352e73260c4eb3d8": {"node_ids": ["0a81b3d3-33df-4bf6-8fdc-1969fb80c46f", "ffdf68d7-4a10-409a-adbb-76892606a43e", "2ffd4a76-91d4-424a-b913-520a0720f543"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "f63d7cc27bc4c0726384f55108bfb73209ec8365": {"node_ids": ["3e10328c-b630-4d43-9868-a89922b86e7e"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "5048b1edf1ee1994496d53784f5453ba788c67c1": {"node_ids": ["74a0d9ad-2451-4f8e-8e01-99d386398b2a"], "metadata": {"filename": "output_parser.md", "author": "LlamaIndex"}}, "341efd4f1f6ce99a44b92bd8908e2e815a54ce3e": {"node_ids": ["03fc6590-bec3-4500-932b-31404888b9b2"], "metadata": {"filename": "pydantic_program.md", "author": "LlamaIndex"}}, "4c305db40c58f76897f40776d7794d0eafa9cd2e": {"node_ids": ["208d18a1-e92e-42b3-b3b2-8d093ecb6f66"], "metadata": {"filename": "query_engine.md", "author": "LlamaIndex"}}, "9e3c493bbf5db7cd81286fbc35667bf8ecb5988f": {"node_ids": ["a95e80af-b095-48e1-90c3-20a6f5f28076"], "metadata": {"filename": "chat_stores.md", "author": "LlamaIndex"}}, "6057fd181fd26a8d792742d392167abf078e3126": {"node_ids": ["cb4c93c5-d350-4ade-aa32-11424891af8c", "dba5f0fd-3e3b-48b6-8ace-799a80257088"], "metadata": {"filename": "customization.md", "author": "LlamaIndex"}}, "0b0825184a1ecb80c940fa0aa1e55f17f4cacd01": {"node_ids": ["2d328b13-658c-4a29-b294-2a5819cd9cc8"], "metadata": {"filename": "docstores.md", "author": "LlamaIndex"}}, "001f4ae9ca75b1982018cbfef5fe8d28a9dc1f71": {"node_ids": ["45910725-7ea8-44e7-9703-a2a9057dd277"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "f5951f73f265447ea5618ce68a03a45c2c764d99": {"node_ids": ["cdaf8922-814b-4224-907b-7a6515843a79"], "metadata": {"filename": "index_stores.md", "author": "LlamaIndex"}}, "2eb4ab9b0553400c4a180c36963d238e6f8d58b3": {"node_ids": ["5a7448de-a0da-46a1-8c19-c069c3e8dcb2"], "metadata": {"filename": "kv_stores.md", "author": "LlamaIndex"}}, "d2795506280f8e5a95edaa8833718e344f1432da": {"node_ids": ["bf5521ff-45b4-4de9-baa1-0d65c9446e0c"], "metadata": {"filename": "save_load.md", "author": "LlamaIndex"}}, "c7056de4a07291707fa62602421e326f4d6ae3a8": {"node_ids": ["ef18adb9-327b-46bc-b35e-c66fcfcff91a", "c4f7b06a-19df-4c55-8ab4-7ac2bd3c906d", "4efaff5c-1f5a-4192-9486-1bdbdcd5e215"], "metadata": {"filename": "vector_stores.md", "author": "LlamaIndex"}}, "f7e81ae86f06048d8cbebf43eb91533f569a167d": {"node_ids": ["227f4bbe-a402-49c4-b50c-19feef22cb65"], "metadata": {"filename": "service_context_migration.md", "author": "LlamaIndex"}}, "dab4ed48ae8ad35ad4ff9e8366b381a8214fbeb5": {"node_ids": ["286b3fc0-cf3c-48fa-a178-b9c49fe5a126"], "metadata": {"filename": "settings.md", "author": "LlamaIndex"}}, "53cb322c58610c30a3904b7f7bcd79366c6aa029": {"node_ids": ["567a245f-5513-4645-83d5-15ff1ccd23eb"], "metadata": {"filename": "supporting_modules.md", "author": "LlamaIndex"}}, "2bd3b4b59e759befd2a4bb027a80d643993a0059": {"node_ids": ["6e2ce25d-1928-4ce8-8aa7-e83df2d8a391"], "metadata": {"filename": "advanced_retrieval.md", "author": "LlamaIndex"}}, "6898fbb0aafcc638d46e1b15b1bb424ca2867931": {"node_ids": ["7442e7f0-c24f-4e3e-9898-951810821ab6"], "metadata": {"filename": "query_transformations.md", "author": "LlamaIndex"}}, "dea1bbdbdbc70740b749ce5983610afd9334c497": {"node_ids": ["707e0313-5c55-4df1-beed-54860b948737"], "metadata": {"filename": "agentic_strategies.md", "author": "LlamaIndex"}}, "3e7abbbb610c607fa279635a2db89b63e86b9186": {"node_ids": ["f5fa9e28-7781-4985-aaa5-c1eaa146de78", "aa0c69e5-815b-4534-8f91-d150133becf7"], "metadata": {"filename": "basic_strategies.md", "author": "LlamaIndex"}}, "f041c5d9d10959ba897d229a632c1cd8e0c6991c": {"node_ids": ["0499f9d9-a1cc-4303-903e-524c91532663"], "metadata": {"filename": "building_rag_from_scratch.md", "author": "LlamaIndex"}}, "972306828914dfa639c87d3c1a964daa88fca55e": {"node_ids": ["fc024ccc-c066-4912-8fbf-c2c9f0945243"], "metadata": {"filename": "custom_modules.md", "author": "LlamaIndex"}}, "c435a0cbf93fc8d2a21024417ba7efefbf636d40": {"node_ids": ["21c4b589-5fbb-479c-92c2-3715a68125b3"], "metadata": {"filename": "component_wise_evaluation.md", "author": "LlamaIndex"}}, "eb057e089363ffddbbda67e822923a7d173ca225": {"node_ids": ["ee03346f-d784-4531-b1a3-d6c994dbd7f1"], "metadata": {"filename": "e2e_evaluation.md", "author": "LlamaIndex"}}, "4dcc118424e9f2e4f070197ca13909f86a7471ae": {"node_ids": ["c9800936-57a0-4a2e-a038-fab77a504e3d"], "metadata": {"filename": "evaluation.md", "author": "LlamaIndex"}}, "96c6062b9c4e89910918ec78aa542f3a707d3996": {"node_ids": ["40cd32cf-499a-4950-8c89-e1c8249ee062", "1e697071-27d8-401e-ac2e-9e00dd24303f", "f9c95829-907b-4e36-8a72-7eea66638902"], "metadata": {"filename": "fine-tuning.md", "author": "LlamaIndex"}}, "ab61d80aa6d929518257d345d5c406ceb6fdb6c5": {"node_ids": ["76f70d8d-828a-4342-b948-4e982571e87c", "75ca9196-00d2-432e-84dc-4b7014baaa36"], "metadata": {"filename": "production_rag.md", "author": "LlamaIndex"}}, "976a4a7632a2f9e7ba16cd956f7dbc3b76f67b25": {"node_ids": ["45c94491-734f-47d5-8f41-6014f931b4b3"], "metadata": {"filename": "past_presentations.md", "author": "LlamaIndex"}}, "b4c73cf633f2552255eb077668754504eae8a1ac": {"node_ids": ["3e54c64d-d182-4632-b174-e929896d7d58", "9d594115-9d6f-4831-9134-b60bf087739a"], "metadata": {"filename": "basic_agent.md", "author": "LlamaIndex"}}, "9a87dd1768aa384a4a3009d61c7ebfb0e6435889": {"node_ids": ["a30ebdb5-6c98-4f36-ac2a-bb40d622f5ae"], "metadata": {"filename": "llamaparse.md", "author": "LlamaIndex"}}, "4b2d6332c6226c8a1ef662e0a627c426d8b2c0ee": {"node_ids": ["41ea0afc-4f7d-473d-b1a3-205993586bd7"], "metadata": {"filename": "local_models.md", "author": "LlamaIndex"}}, "bdfbeb720c9784aac369b677c942a5120965a285": {"node_ids": ["78c646e0-f29b-45e3-97ed-f0001c59ba9b"], "metadata": {"filename": "memory.md", "author": "LlamaIndex"}}, "06a667d42dac0856ec12be4bff99b2143826bd2f": {"node_ids": ["bdfb9cbf-ba3e-491e-a172-028b1ee06f2e"], "metadata": {"filename": "rag_agent.md", "author": "LlamaIndex"}}, "48eeef24eaa808ab07adea1985c470b3591c81f1": {"node_ids": ["e790141d-9464-4384-a282-0810c1e86e2f"], "metadata": {"filename": "tools.md", "author": "LlamaIndex"}}, "e0b064d3453212b574464941c25b04b017509200": {"node_ids": ["7bd7db2a-3554-4ecb-883f-6afd072cd25a"], "metadata": {"filename": "deployment.md", "author": "LlamaIndex"}}, "774bb04cb1c31b33aa85b57be0c5ae7a8ae18e77": {"node_ids": ["3b3d00d1-2498-4933-827f-faf5a9f48cdf"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "2b22daad3974506b6a282e44c164634cec28a52e": {"node_ids": ["7d8b94bf-6cb0-454f-a75e-f4dfb5a6d2d8"], "metadata": {"filename": "usage_pattern.md", "author": "LlamaIndex"}}, "2de1b4ec5dfa08e732d8f4473e58d35c93dbc3e7": {"node_ids": ["61188a5f-28ec-4b41-86f4-a46e41d33106"], "metadata": {"filename": "evaluating.md", "author": "LlamaIndex"}}, "2e02b57a874d53519207d53f185ae648374b1f85": {"node_ids": ["1c10ea44-83b4-44be-9bf9-79ff1cf414c2"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "2c4a022fa95ff87a103eb867d929db544435c103": {"node_ids": ["57f16fd3-792e-4f7c-ae44-c5933f25f59d"], "metadata": {"filename": "indexing.md", "author": "LlamaIndex"}}, "bd0b8ccc8d3b9c8780fcc91930ab64f9a69834e7": {"node_ids": ["3530afe5-a11c-47c6-b590-1cc33ab36cb0"], "metadata": {"filename": "llamahub.md", "author": "LlamaIndex"}}, "c0a21f7c3d8d525947d227d1d691e47fb9f4502e": {"node_ids": ["c3e48f27-b046-472a-a792-390a2efeecf8", "7cd9fb45-8eac-4efa-80df-93577844949c"], "metadata": {"filename": "loading.md", "author": "LlamaIndex"}}, "729325bb7f1845950d83b12f6e9fc8dfb284ab7b": {"node_ids": ["6eb34d70-5ecb-45ae-b991-0d3680d18164", "4a9f3d9a-cb31-4f4e-9701-8f574e686bdf"], "metadata": {"filename": "agents.md", "author": "LlamaIndex"}}, "39df3e5ba0f608dea58a9f0a2eeb641e34506ded": {"node_ids": ["347c8135-ffbe-4ca8-b8ff-f9bbff31eb6a", "f3a10881-f73a-478d-9993-9cc186892259", "bbc4eba6-cb60-416b-9d7e-e1afbe8512ef", "bd914d53-d2ce-4a99-b12e-cd80392ec556"], "metadata": {"filename": "fullstack_app_guide.md", "author": "LlamaIndex"}}, "a74576e96fd01aaea9aa727b3f77c8e0f9a9174d": {"node_ids": ["422a6791-d891-4b2f-8466-cd9d8136c974", "65aa08ac-e70b-4514-ab0e-a78f87004eba", "71886cb0-dbe9-41fd-8281-2dab901dd1c2", "2425e5b1-624d-4255-a2aa-cbee41ebd9f1", "33a4ab75-08f5-4b2b-958e-dec9292dcf95", "bf7f510d-543d-4b4a-8494-3ec40abf0c94", "f50c83d1-e9b4-4654-b695-286cd79d7597", "817c954f-e6bd-407e-a8b5-543d7e2bddd8"], "metadata": {"filename": "fullstack_with_delphic.md", "author": "LlamaIndex"}}, "e77465268f6d2163e2e83ad1735a1dfda606c08e": {"node_ids": ["0a74955e-4ee7-4173-85c9-ef2b803c265e"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "fdf22c259939ad9077de8acb6503b790e2775207": {"node_ids": ["10dbf4b5-7391-4676-82b3-558a725f7666", "fbb33c2b-debf-4108-869a-ebddf5e92b0a", "1dfb79e8-a648-4f51-ba8d-66776f20d2f4", "ebef20a2-ce27-487b-893f-605bc94bd753", "bab0d9af-e377-40e1-bebf-2bf0a0ff9766"], "metadata": {"filename": "building_a_chatbot.md", "author": "LlamaIndex"}}, "27b13e6441729713e8f50968e11ef423af665228": {"node_ids": ["91115a6a-6c16-4f21-9873-969b6468bb8b"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "643a6616d589550cad7e3db6883dcbea8df513de": {"node_ids": ["8fcbccc4-1ac1-4ed9-bb9a-25017ad1ab19", "ddcbf9d6-45c0-47ee-a6d5-728ae4a82244"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "2a92eb05a01438b1b5d90fe2baab978f918d124a": {"node_ids": ["409d0cba-7e87-4ae9-a286-83aa1cf69abc", "f96ed7e3-7f7f-4859-a2e0-82b36621c1f7", "cb56746a-c810-4001-97e2-a985b07c1fdd", "58eacbce-2989-4e1f-9581-bf0328b1d5b4", "ec5f532d-fbfd-4be4-a8c8-48dc9cf14170", "71bb5190-fd8f-4cdc-9530-39076b17a983"], "metadata": {"filename": "terms_definitions_tutorial.md", "author": "LlamaIndex"}}, "7befaf646f70f91314c80d8097a8d148fe1015a1": {"node_ids": ["fea0e53a-241e-4fc0-a2a6-ab1932e636a0", "c9b30e7b-2038-4bb2-966a-ec443e335c35"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "5fb0bbdb8aa45dde964666f84cfa8fea01418b6c": {"node_ids": ["f606be89-b4a0-42bb-bb34-d9c433cb21aa", "305cd500-0951-4741-9bf9-0928cb7d0e97"], "metadata": {"filename": "querying.md", "author": "LlamaIndex"}}, "06dc6f52bf6a6823f17a22e22673134108d60673": {"node_ids": ["e27520ab-6cc1-4379-a9e3-810e27aa43d4"], "metadata": {"filename": "retrieval.md", "author": "LlamaIndex"}}, "6c969568f721b247d9defebb6ae03a5c5d8ff92a": {"node_ids": ["66e4ab5e-7c95-4c3c-b965-86bc1092396f", "7c876828-10e0-45b5-b7de-28cb190564b2"], "metadata": {"filename": "storing.md", "author": "LlamaIndex"}}, "df7eace4860b2d8c98f8868c2f6841e161548b4e": {"node_ids": ["a85e69aa-3f37-46b8-8218-a3bfa369ffb2"], "metadata": {"filename": "synthesis.md", "author": "LlamaIndex"}}, "e766bd372640230aa294aea2c7fd035c4e0d195c": {"node_ids": ["0208844a-7048-4b01-ad10-7f2163d2f9a9"], "metadata": {"filename": "tracing_and_debugging.md", "author": "LlamaIndex"}}, "7a4d3488f00d7f794a7bffb539aa3d1148a98bd2": {"node_ids": ["8326b09a-f7d7-4f3f-a4c0-5297e98e487f"], "metadata": {"filename": "privacy.md", "author": "LlamaIndex"}}, "cb3e042eec51f285bd52d38b82135d4bd18d0e5a": {"node_ids": ["cdba2273-d567-4e1d-843e-03b10bbf2b38"], "metadata": {"filename": "using_llms.md", "author": "LlamaIndex"}}, "a4d7ec9b218757f8f4d3e28151a50802643dab81": {"node_ids": ["adf8d937-1604-4169-8ad3-ee24d77d21c0"], "metadata": {"filename": "agents.md", "author": "LlamaIndex"}}, "65964fb76fdbb78a66531039d7973ef96d047da0": {"node_ids": ["34944126-3440-49c7-9ca9-b5dba48a79e4"], "metadata": {"filename": "chatbots.md", "author": "LlamaIndex"}}, "b0653601b8b1944540c09fd36e4d3733b2c61375": {"node_ids": ["99887808-c943-4eb2-85ab-f170ecb3e83c"], "metadata": {"filename": "extraction.md", "author": "LlamaIndex"}}, "eda87abe70cf3fbe7f79f8366385815870f30363": {"node_ids": ["7fd7ad37-3c45-4cca-ab65-69bb95ec1d60", "ef0b6a9e-9ce0-4028-aec5-0d13a854eb69", "9346abed-ce4a-4c13-b768-fd32a4f0e171"], "metadata": {"filename": "fine_tuning.md", "author": "LlamaIndex"}}, "35c676db7a821c2b5816ef06d9205d88ac3df846": {"node_ids": ["b29b78e6-2217-42be-99c7-fa1ce32b36cf"], "metadata": {"filename": "graph_querying.md", "author": "LlamaIndex"}}, "4e2796b6f98168047d342c043567c094f98c95a3": {"node_ids": ["dc56db05-0ebd-4f44-abcd-c7edeabd0966"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "e8c82613334827ad40a42c0a42a6a985ac09ce2b": {"node_ids": ["0e47d823-d3f0-4e85-b927-0cd89c68ae02"], "metadata": {"filename": "multimodal.md", "author": "LlamaIndex"}}, "9390547c52f8e0469706addf6e38ed7870144bdd": {"node_ids": ["331ce9e5-8087-4cc4-9f94-5fbe5999b419"], "metadata": {"filename": "prompting.md", "author": "LlamaIndex"}}, "759650789e1100aad523f9b1b24b557e3b201696": {"node_ids": ["619b2dc5-eaa9-4ed0-8cbe-886ec0901d52"], "metadata": {"filename": "index.md", "author": "LlamaIndex"}}, "6fc068612c1f529e3e2b29f2be50094b30ab13ff": {"node_ids": ["2ca9a81f-1f42-45d2-9a15-f2f2536cbff4"], "metadata": {"filename": "querying_csvs.md", "author": "LlamaIndex"}}, "2b27999821cbc0546d25ec628a0a7a10f4d970b6": {"node_ids": ["c7ae54d0-a67c-439c-9f07-89ef97a7723e"], "metadata": {"filename": "tables_charts.md", "author": "LlamaIndex"}}, "7d021d3b23824ef25223b141405a6385e337a580": {"node_ids": ["1cab292f-157d-4f90-87a0-a4717b7d1eda"], "metadata": {"filename": "text_to_sql.md", "author": "LlamaIndex"}}}}